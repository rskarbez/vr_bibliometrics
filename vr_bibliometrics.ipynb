{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bibtexparser\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.cluster.hierarchy as shc\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import plotly.io as pio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global config - sorry about this\n",
    "# Epoch 0 = 1993 - 2003\n",
    "# Epoch 1 = 2004 - 2013\n",
    "# Epoch 2 = 2014 - 2023\n",
    "# Epoch 3 = 1993 - 2023\n",
    "EPOCH = 3\n",
    "WRITE_FILES = False\n",
    "SHOW_FIGURES = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_articles_by_year(year_range, article_list):\n",
    "    return_articles = []\n",
    "    for article in article_list:\n",
    "        if article.year in year_range:\n",
    "            return_articles.append(article)\n",
    "    return return_articles\n",
    "\n",
    "def count_keywords_in_articles(article_list):\n",
    "    kwidx_and_count = {}\n",
    "    for article in article_list:\n",
    "        for idx in article.keyword_indices:\n",
    "            if idx not in kwidx_and_count:\n",
    "                kwidx_and_count[idx] = 1\n",
    "            else:\n",
    "                kwidx_and_count[idx] += 1\n",
    "    return kwidx_and_count\n",
    "\n",
    "def parse_authors(authors):\n",
    "    return authors.split(' and ')\n",
    "\n",
    "def parse_keywords(keywords):\n",
    "    if ';' in keywords:\n",
    "        delimiter = ';'\n",
    "    else:\n",
    "        delimiter = ','\n",
    "    return [s.strip().lower().replace(' ', '__').replace('-', '_') for s in keywords.split(delimiter)]\n",
    "\n",
    "def association_strength(i, j, C):\n",
    "    return C[i][j] / (C[i][i] * C[j][j])\n",
    "\n",
    "def cosine_index(i, j, C):\n",
    "    return C[i][j] / ((C[i][i] * C[j][j]) ** 0.5)\n",
    "\n",
    "def inclusion_index(i, j, C):\n",
    "    return C[i][j] / min(C[i][i], C[j][j])\n",
    "\n",
    "def jaccard_index(i, j, C):\n",
    "    return C[i][j] / (C[i][i] + C[j][j] - C[i][j])\n",
    "\n",
    "years = range(1994, 2024)\n",
    "\n",
    "class Article():\n",
    "    def __init__(self, key, year, title, authors, venue, abstract, cc, ieee_terms, keywords):\n",
    "        self.key = key\n",
    "        self.year = year\n",
    "        self.title = title\n",
    "        self.authors = parse_authors(authors)\n",
    "        self.venue = venue\n",
    "        self.abstract = abstract\n",
    "        self.citation_count = cc\n",
    "        self.ieee_terms = parse_keywords(ieee_terms)\n",
    "        self.keywords = parse_keywords(keywords)\n",
    "        self.keyword_indices = set()\n",
    "        self.topic_indices = set()\n",
    "    def set_keyword_indices(self, in_dict):\n",
    "        for kw in self.keywords:\n",
    "            self.keyword_indices.add(in_dict.get(kw, 0))\n",
    "\n",
    "class BigramSimilarity():\n",
    "    def __init__(self, i, j, C, idx_to_kw):\n",
    "        self.i = idx_to_kw[i]\n",
    "        self.j = idx_to_kw[j]\n",
    "        self.association_strength = association_strength(i, j, C)\n",
    "        self.cosine_index = cosine_index(i, j, C)\n",
    "        self.inclusion_index = inclusion_index(i, j, C)\n",
    "        self.jaccard_index = inclusion_index(i, j, C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Load the article list from a tab-delimited txt file\n",
    "#\n",
    "\n",
    "articles_dict = {}\n",
    "\n",
    "with open('all_vr.txt') as infile:\n",
    "    for line in infile:\n",
    "        line_cells = line.split('\\t')\n",
    "        year = int(line_cells[0])\n",
    "        key = line_cells[1]\n",
    "        title = line_cells[2]\n",
    "        authors = line_cells[3]\n",
    "        venue = line_cells[4]\n",
    "        abstract = line_cells[5]\n",
    "        citation_count = int(line_cells[6])\n",
    "        ieee_terms = line_cells[7]\n",
    "        keywords = line_cells[8]\n",
    "        articles_dict[key] = Article(key, year, title, authors, venue, abstract, citation_count, ieee_terms, keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed keywords from disk\n",
    "# 'prettify' strings coming from Excel\n",
    "# Generate index values to go along with keyword lists\n",
    "\n",
    "indices_to_keywords_dict = {}\n",
    "keywords_to_indices_dict = {}\n",
    "index_count_dict = {}\n",
    "\n",
    "with open('vr_keywords_CLEAN_AND_PRUNED.txt') as infile:\n",
    "    for idx, line in enumerate(infile):\n",
    "        index_count_dict[idx] = 0\n",
    "        rough_words = line.strip().split('\\t')\n",
    "        words = []\n",
    "        for w in rough_words:\n",
    "            words.append(w.strip().replace(' ', '__').replace('-', '_'))\n",
    "        indices_to_keywords_dict[idx] = words\n",
    "        for word in words:\n",
    "            if len(word):\n",
    "                keywords_to_indices_dict[word] = idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Count occurrences of each keyword index across all articles\n",
    "#\n",
    "\n",
    "for _, article in articles_dict.items():\n",
    "    article.set_keyword_indices(keywords_to_indices_dict)\n",
    "    for idx in article.keyword_indices:\n",
    "        index_count_dict[idx] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Get a year range of articles, count the keywords that appear in those articles\n",
    "#\n",
    "\n",
    "if EPOCH == 0:\n",
    "    r = range(1993, 2004)\n",
    "elif EPOCH == 1:\n",
    "    r = range(2004, 2014)\n",
    "elif EPOCH == 2:\n",
    "    r = range(2014, 2024)\n",
    "elif EPOCH == 3:\n",
    "    r = range(1993, 2024)\n",
    "\n",
    "article_list = get_articles_by_year(r, articles_dict.values())\n",
    "\n",
    "num_articles = len(article_list)\n",
    "\n",
    "keyword_count_dict = count_keywords_in_articles(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort keywords in descending order of associated count\n",
    "\n",
    "sorted_kw_indices_by_count = sorted(keyword_count_dict.items(), key=lambda x:x[1], reverse=True)[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Get the top 100 (plus ties) keywords OR all keywords that appear more than once,\n",
    "#     whichever is smaller\n",
    "#\n",
    "\n",
    "top100_dict = {}\n",
    "top100_indices_dict = {}\n",
    "keyword_indices_from_top100_idx = {}\n",
    "checking_ties = False\n",
    "tie_count = -1\n",
    "repeated_keywords_dict = {}\n",
    "\n",
    "for _, v in sorted_kw_indices_by_count:\n",
    "    if v not in repeated_keywords_dict:\n",
    "        repeated_keywords_dict[v] = 1\n",
    "    else:\n",
    "        repeated_keywords_dict[v] += 1\n",
    "\n",
    "if WRITE_FILES:\n",
    "    with open('repeated_keywords.csv', 'w') as outfile:\n",
    "        for k, v in repeated_keywords_dict.items():\n",
    "            for _ in range(v):\n",
    "                outfile.write('{}\\n'.format(k))\n",
    "\n",
    "num_keywords = 0\n",
    "k, v = sorted_kw_indices_by_count[num_keywords]\n",
    "\n",
    "while v >= 2 and (num_keywords <= 100 or checking_ties):\n",
    "    if num_keywords < 100:\n",
    "        top100_dict[k] = v\n",
    "        top100_indices_dict[k] = num_keywords - 1\n",
    "        keyword_indices_from_top100_idx[num_keywords - 1] = k\n",
    "        num_keywords += 1\n",
    "    elif num_keywords == 100:\n",
    "        checking_ties = True\n",
    "        tie_count = v\n",
    "        top100_dict[k] = v\n",
    "        top100_indices_dict[k] = num_keywords - 1\n",
    "        keyword_indices_from_top100_idx[num_keywords - 1] = k\n",
    "        num_keywords += 1\n",
    "    elif checking_ties:\n",
    "        if v == tie_count:\n",
    "            top100_dict[k] = v\n",
    "            top100_indices_dict[k] = num_keywords - 1\n",
    "            keyword_indices_from_top100_idx[num_keywords - 1] = k\n",
    "            num_keywords += 1\n",
    "        else:\n",
    "            checking_ties = False\n",
    "    k, v = sorted_kw_indices_by_count[num_keywords]\n",
    "\n",
    "num_columns = num_keywords - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate the occurrence matrix O for the top100 keywords\n",
    "# (Each row is an article, each column is a keyword, 1 indicates keyword on article)\n",
    "#\n",
    "\n",
    "occurrence_matrix = np.zeros((num_articles, num_columns))\n",
    "for a_idx, article in enumerate(article_list):\n",
    "    for kw_idx in article.keyword_indices:\n",
    "        if kw_idx in top100_dict:\n",
    "            occurrence_matrix[a_idx][top100_indices_dict[kw_idx]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate co-occurrence matrix C\n",
    "# C = OTO (O-transpose-O), see (van Eck and Waltman, 2009)\n",
    "#\n",
    "\n",
    "co_occurrence_matrix = np.matmul(occurrence_matrix.T, occurrence_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Generate direct similarity scores for each pair of top100 keywords\n",
    "# Again, for definitions, see (van Eck and Waltman, 2009)\n",
    "#\n",
    "\n",
    "kw_bigrams = itertools.combinations(range(num_columns), 2)\n",
    "bigram_scores = [BigramSimilarity(i, j, co_occurrence_matrix, keyword_indices_from_top100_idx)\n",
    "                    for (i, j) in kw_bigrams]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Prepare for hierarchical clustering by creating a pandas dataframe of the occurrence matrix\n",
    "#\n",
    "\n",
    "kw_labels = []\n",
    "for i in range(num_columns):\n",
    "    kw_idx = keyword_indices_from_top100_idx[i]\n",
    "    kw_labels.append('{}'.format(indices_to_keywords_dict[kw_idx][0]))\n",
    "\n",
    "occurrence_df = pd.DataFrame(occurrence_matrix.T, index=kw_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Compute the cosine distances between keywords\n",
    "#\n",
    "\n",
    "cosine_distance_matrix = scipy.spatial.distance.pdist(occurrence_matrix.T, 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Perform hierarchical clustering using Ward's method\n",
    "#\n",
    "\n",
    "clusters = shc.linkage(cosine_distance_matrix, method='ward')\n",
    "\n",
    "if EPOCH == 0:\n",
    "    cluster_threshold = 1.39\n",
    "elif EPOCH == 1:\n",
    "    cluster_threshold = 1.42\n",
    "elif EPOCH == 2:\n",
    "    cluster_threshold = 1.36\n",
    "elif EPOCH == 3:\n",
    "    cluster_threshold = 1.25\n",
    "\n",
    "T = shc.fcluster(clusters,\n",
    "                 t=cluster_threshold,\n",
    "                 criterion='distance')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Prepare for the strategic diagrams\n",
    "#\n",
    "\n",
    "topic_clusters = [[] for _ in range(len(T))]\n",
    "cluster_counts = [0 for _ in range(len(T))]\n",
    "\n",
    "for idx, cluster_id in enumerate(T):\n",
    "    kw_idx = keyword_indices_from_top100_idx[idx]\n",
    "    temp = '{}'.format(indices_to_keywords_dict[kw_idx][0])\n",
    "    topic_clusters[cluster_id - 1].append(temp)\n",
    "    cluster_counts[cluster_id - 1] += top100_dict[kw_idx]\n",
    "\n",
    "threshholded_topic_clusters = {i:topic_clusters[i] for i in range(len(T)) if cluster_counts[i] >= 1}\n",
    "TTC = threshholded_topic_clusters   # Renamed for convenience\n",
    "topic_kws_dict = {idx:topic_clusters[idx] for idx in TTC}\n",
    "topic_sizes_dict = {idx:len(topic_clusters[idx]) for idx in TTC}\n",
    "cluster_counts_dict = {idx:cluster_counts[idx] for idx in TTC}\n",
    "\n",
    "total_papers_count = len(article_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Unique-ify the keyword list\n",
    "#\n",
    "\n",
    "unique_keywords = set()\n",
    "for article in article_list:\n",
    "    unique_keywords.update(article.keywords)\n",
    "total_unique_kws = len(unique_keywords)\n",
    "\n",
    "if WRITE_FILES:\n",
    "    with open('keywords.csv', 'w') as outfile:\n",
    "        outfile.write('{},{},{},{},{}\\n'.format('index',\n",
    "                                             'keyword',\n",
    "                                             'cluster',\n",
    "                                             'topic',\n",
    "                                             'observations'))\n",
    "        for kw_idx, (k, v) in enumerate(top100_dict.items()):\n",
    "            for topic_idx in TTC:\n",
    "                keyword = indices_to_keywords_dict[k][0]\n",
    "                if keyword in topic_kws_dict[topic_idx]:\n",
    "                    outfile.write('{},{},{},{},{}\\n'.format(k,\n",
    "                                                            keyword,\n",
    "                                                            topic_idx,\n",
    "                                                            topic_kws_dict[topic_idx][0],\n",
    "                                                            v))\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Compute frequency, co-word frequency, and cohesion\n",
    "#\n",
    "\n",
    "frequency_dict = {}\n",
    "coword_frequency_dict = {}\n",
    "\n",
    "\n",
    "for topic_idx in TTC:\n",
    "    frequency_dict[topic_idx] = 0\n",
    "    coword_frequency_dict[topic_idx] = 0\n",
    "\n",
    "    for article in article_list:\n",
    "        topic_match = False\n",
    "        co_words = False\n",
    "        for kw in article.keywords:\n",
    "            if topic_match and not co_words:\n",
    "                co_words = True\n",
    "            if kw in topic_kws_dict[topic_idx]:\n",
    "                frequency_dict[topic_idx] += 1\n",
    "                topic_match = True\n",
    "        if topic_match:\n",
    "            article.topic_indices.add(topic_idx)\n",
    "            if co_words:\n",
    "                coword_frequency_dict[topic_idx] += 1\n",
    "\n",
    "for topic_idx in TTC:\n",
    "    frequency_dict[topic_idx] /= num_articles\n",
    "    coword_frequency_dict[topic_idx] /= num_articles\n",
    "    # cohesion is probability of multiple kws on a paper given probability of single kw\n",
    "    # P(multiple | single) = p(multiple AND single) / p(single) = p(multiple) / p(single) = coword_freq / freq\n",
    "    #   probability of a single keyword on a paper is frequency\n",
    "    #   probability of multiple keywords on a paper is coword_frequency\n",
    "\n",
    "cohesion_dict = {idx:(coword_frequency_dict[idx] / frequency_dict[idx]) for idx in TTC}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute connectedness and centrality\n",
    "\n",
    "list_of_kws_in_topics = []\n",
    "\n",
    "for topic_idx in TTC:\n",
    "    for kw in topic_kws_dict[topic_idx]:\n",
    "        list_of_kws_in_topics.append(kw)\n",
    "\n",
    "connected_kws_1step_dict = {}\n",
    "\n",
    "for kw in list_of_kws_in_topics:\n",
    "    connected_kws_1step_dict[kw] = set()\n",
    "    for article in article_list:\n",
    "        if kw in article.keywords:\n",
    "            connected_kws_1step_dict[kw].update(article.keywords)\n",
    "\n",
    "connected_kws_2step_dict = {}\n",
    "\n",
    "for kw in list_of_kws_in_topics:\n",
    "    connected_kws_2step_dict[kw] = set()\n",
    "    for kw2 in connected_kws_1step_dict[kw]:\n",
    "        if kw2 in connected_kws_1step_dict:\n",
    "            connected_kws_2step_dict[kw].update(connected_kws_1step_dict[kw2])\n",
    "\n",
    "kw_connectedness_dict = {}\n",
    "\n",
    "for kw in list_of_kws_in_topics:\n",
    "    kw_connectedness_dict[kw] = len(connected_kws_2step_dict[kw]) / total_unique_kws\n",
    "\n",
    "centrality_dict = {}\n",
    "for topic_idx in TTC:\n",
    "    centrality_dict[topic_idx] = 0\n",
    "    for kw in topic_kws_dict[topic_idx]:\n",
    "        centrality_dict[topic_idx] += kw_connectedness_dict[kw]\n",
    "    centrality_dict[topic_idx] /= len(topic_kws_dict[topic_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute density\n",
    "\n",
    "density_dict = {}\n",
    "\n",
    "for topic_idx in TTC:\n",
    "    n = topic_sizes_dict[topic_idx]\n",
    "    kw_cooccurrence_matrix = np.zeros((n, n))\n",
    "    for article in article_list:\n",
    "        if topic_idx in article.topic_indices:\n",
    "            kws_present = []\n",
    "            for kw_idx, kw in enumerate(topic_kws_dict[topic_idx]):\n",
    "                if kw in article.keywords:\n",
    "                    kws_present.append(kw_idx)\n",
    "            if len(kws_present) >= 2:\n",
    "                kw_permutations = itertools.permutations(kws_present, 2)\n",
    "                for perm in kw_permutations:\n",
    "                    kw_cooccurrence_matrix[perm] += 1\n",
    "    non_diag_matrix_cells = (n * n) - n\n",
    "    cooccurrence = np.count_nonzero(kw_cooccurrence_matrix)\n",
    "    density_dict[topic_idx] = cooccurrence / non_diag_matrix_cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Produce output\n",
    "\n",
    "print('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}'.format('idx',\n",
    "                                                          'size',\n",
    "                                                          'Freq',\n",
    "                                                          'CW-F',\n",
    "                                                          'Cohes',\n",
    "                                                          'Cent',\n",
    "                                                          'Dens'))\n",
    "print('=' * 80)\n",
    "for topic_idx in TTC:\n",
    "    print('{}\\t{}\\t{}\\t{}\\t{}\\t{}\\t{}'.format(topic_idx,\n",
    "                                                              topic_sizes_dict[topic_idx],\n",
    "                                                              frequency_dict[topic_idx],\n",
    "                                                              coword_frequency_dict[topic_idx],\n",
    "                                                              cohesion_dict[topic_idx],\n",
    "                                                              centrality_dict[topic_idx],\n",
    "                                                              density_dict[topic_idx]))\n",
    "print('=' * 80)\n",
    "\n",
    "if EPOCH == 0:\n",
    "    filename = 'topicclusters_1993-2003.csv'\n",
    "elif EPOCH == 1:\n",
    "    filename = 'topicclusters_2004-2013.csv'\n",
    "elif EPOCH == 2:\n",
    "    filename = 'topicclusters_2014-2023.csv'\n",
    "elif EPOCH == 3:\n",
    "    filename = 'topicclusters_1993-2023.csv'\n",
    "\n",
    "if WRITE_FILES:\n",
    "    with open(filename, 'w') as outfile:\n",
    "        outfile.write('{},{},{},{},{},{},{},{}\\n'.format('idx',\n",
    "                                                              'size',\n",
    "                                                              'Observations',\n",
    "                                                              'Frequency',\n",
    "                                                              'Co-word Frequency',\n",
    "                                                              'Cohesion',\n",
    "                                                              'Centrality',\n",
    "                                                              'Density'))\n",
    "\n",
    "        for topic_idx in TTC:\n",
    "            outfile.write('{},{},{},{},{},{},{},{}\\n'.format(topic_idx,\n",
    "                                                                  topic_sizes_dict[topic_idx],\n",
    "                                                                  cluster_counts_dict[topic_idx],\n",
    "                                                                  frequency_dict[topic_idx],\n",
    "                                                                  coword_frequency_dict[topic_idx],\n",
    "                                                                  cohesion_dict[topic_idx],\n",
    "                                                                  centrality_dict[topic_idx],\n",
    "                                                                  density_dict[topic_idx]))\n",
    "\n",
    "    if EPOCH == 0:\n",
    "        filename = 'latex_table_1993-2003.csv'\n",
    "    elif EPOCH == 1:\n",
    "        filename = 'latex_table_2004-2013.csv'\n",
    "    elif EPOCH == 2:\n",
    "        filename = 'latex_table_2014-2023.csv'\n",
    "    elif EPOCH == 3:\n",
    "        filename = 'latex_table_1993-2023.csv'\n",
    "\n",
    "    with open(filename, 'w') as outfile:\n",
    "        for topic_idx in TTC:\n",
    "            outfile.write('D{} & XXX & {} & {} & {:.3f} & {:.3f} & {:.3f} & {:.3f} & {:.3f}\\\\\\\\'.format(topic_idx,\n",
    "                                                                                    topic_kws_dict[topic_idx],\n",
    "                                                                                    cluster_counts_dict[topic_idx],\n",
    "                                                                                    frequency_dict[topic_idx],\n",
    "                                                                                    coword_frequency_dict[topic_idx],\n",
    "                                                                                    cohesion_dict[topic_idx],\n",
    "                                                                                    centrality_dict[topic_idx],\n",
    "                                                                                    density_dict[topic_idx]\n",
    "                                                                                    ))\n",
    "\n",
    "#\n",
    "# Plot Strategic Diagram of topic_clusters (Density vs Centrality)\n",
    "#\n",
    "\n",
    "# Plotly\n",
    "strategic_diagram_nparray = np.zeros((len(TTC), 4))\n",
    "\n",
    "for idx, topic_idx in enumerate(TTC):\n",
    "    strategic_diagram_nparray[idx][0] = topic_idx\n",
    "    strategic_diagram_nparray[idx][1] = frequency_dict[topic_idx]\n",
    "    strategic_diagram_nparray[idx][2] = centrality_dict[topic_idx]\n",
    "    strategic_diagram_nparray[idx][3] = density_dict[topic_idx]\n",
    "\n",
    "strategic_diagram_df = pd.DataFrame(data=strategic_diagram_nparray,\n",
    "                                    columns=['id', 'frequency', 'centrality', 'density'])\n",
    "\n",
    "topic_leaders = []\n",
    "for topic_idx in TTC:\n",
    "    topic_leaders.append(topic_kws_dict[topic_idx][0])\n",
    "strategic_diagram_df['leader'] = topic_leaders\n",
    "strategic_diagram_df['id'] = strategic_diagram_df['id'].astype(int).astype(str)\n",
    "\n",
    "fig = go.Figure()\n",
    "\n",
    "fig = px.scatter(strategic_diagram_df,\n",
    "                    x='centrality',\n",
    "                    y='density',\n",
    "                    size='frequency',\n",
    "                    color='leader',\n",
    "                    size_max=60)\n",
    "\n",
    "if EPOCH == 0:\n",
    "    fig_title = 'Strategic diagram for VRAIS/VR 1993-2003'\n",
    "elif EPOCH == 1:\n",
    "    fig_title = 'Strategic diagram for VR 2004-2013'\n",
    "elif EPOCH == 2:\n",
    "    fig_title = 'Strategic diagram for VR 2014-2023'\n",
    "elif EPOCH == 3:\n",
    "    fig_title = 'Strategic diagram for VRAIS/VR 1993-2023'\n",
    "\n",
    "fig.update_layout(title=dict(\n",
    "                        text=fig_title,\n",
    "                        x=0.5,\n",
    "                        y=0.975,\n",
    "                        xanchor='center',\n",
    "                        yanchor='top'\n",
    "                    ),\n",
    "                    xaxis=dict(\n",
    "                        title=dict(text='Density of topic',\n",
    "                                    standoff=460),\n",
    "                        gridcolor='white',\n",
    "                        gridwidth=2,\n",
    "                        linecolor='black',\n",
    "                        linewidth=4,\n",
    "                        anchor='free',\n",
    "                        position=0.5\n",
    "                    ),\n",
    "                    yaxis=dict(\n",
    "                        title=dict(text='Centrality of topic',\n",
    "                                    standoff=475),\n",
    "                        gridcolor='white',\n",
    "                        gridwidth=2,\n",
    "                        linecolor='black',\n",
    "                        linewidth=4,\n",
    "                        anchor='free',\n",
    "                        position=0.5\n",
    "                    ),\n",
    "                    paper_bgcolor='rgb(243, 243, 243)',\n",
    "                    plot_bgcolor='rgb(243, 243, 243)')\n",
    "\n",
    "if WRITE_FILES:\n",
    "    if EPOCH == 0:\n",
    "        filename = 'strategicdiagram_1993-2003.png'\n",
    "    elif EPOCH == 1:\n",
    "        filename = 'strategicdiagram_2004-2013.png'\n",
    "    elif EPOCH == 2:\n",
    "        filename = 'strategicdiagram_2004-2023.png'\n",
    "    elif EPOCH == 3:\n",
    "        filename = 'strategicdiagram_1993-2023.png'\n",
    "\n",
    "    pio.write_image(fig,\n",
    "                    filename,\n",
    "                    scale=12,\n",
    "                    width=1308,\n",
    "                    height=1080)\n",
    "\n",
    "if SHOW_FIGURES:\n",
    "    fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
