%% This BibTeX bibliography file was created using BibDesk.
%% http://bibdesk.sourceforge.net/


%% Created for Richard Skarbez at 2024-03-18 02:47:13 -0400 


%% Saved with string encoding Unicode (UTF-8) 



@article{8267487,
	abstract = {As virtual reality expands in popularity, an increasingly diverse audience is gaining exposure to immersive virtual environments (IVEs). A significant body of research has demonstrated how perception and action work in such environments, but most of this work has been done studying adults. Less is known about how physical and cognitive development affect perception and action in IVEs, particularly as applied to preteen and teenage children. Accordingly, in the current study we assess how preteens (children aged 8--12 years) and teenagers (children aged 15-18 years) respond to mismatches between their motor behavior and the visual information presented by an IVE. Over two experiments, we evaluate how these individuals recalibrate their actions across functionally distinct systems of movement. The first experiment analyzed forward walking recalibration after exposure to an IVE with either increased or decreased visual flow. Visual flow during normal bipedal locomotion was manipulated to be either twice or half as fast as the physical gait. The second experiment leveraged a prism throwing adaptation paradigm to test the effect of recalibration on throwing movement. In the first experiment, our results show no differences across age groups, although subjects generally experienced a post-exposure effect of shortened distance estimation after experiencing visually faster flow and longer distance estimation after experiencing visually slower flow. In the second experiment, subjects generally showed the typical prism adaptation behavior of a throwing after-effect error. The error lasted longer for preteens than older children. Our results have implications for the design of virtual systems with children as a target audience.},
	address = {Los Alamitos, CA, USA},
	author = {H. Adams and G. Narasimham and J. Rieser and S. Creem-Regehr and J. Stefanucci and B. Bodenheimer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794072},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;visualization;virtual environments;biomechanics;calibration;electronic mail},
	month = {apr},
	number = {04},
	pages = {1408-1417},
	publisher = {IEEE Computer Society},
	title = {Locomotive Recalibration and Prism Adaptation of Children and Teens in Immersive Virtual Environments},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794072}}

@article{9382836,
	abstract = {Entering text in virtual environments can be challenging, especially without auxiliary input devices. We investigate text input in virtual reality using hand-tracking and speech. Our system visualizes users&#x27; hands in the virtual environment, allowing typing on an auto-correcting midair keyboard. It also supports speaking a sentence and then correcting errors by selecting alternative words proposed by a speech recognizer. We conducted a user study in which participants wrote sentences with and without speech. Using only the keyboard, users wrote at 11 words-per-minute at a 1.2% error rate. Speaking and correcting sentences was faster and more accurate at 28 words-per-minute and a 0.5% error rate. Participants achieved this performance despite half of sentences containing an uncommon out-of-vocabulary word (e.g. proper name). For sentences with only in-vocabulary words, performance using speech and midair keyboard corrections was faster at 36 words-per-minute with a low 0.3% error rate.},
	address = {Los Alamitos, CA, USA},
	author = {J. Adhikary and K. Vertanen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067776},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {keyboards;speech recognition;virtual environments;resists;visualization;error analysis;layout},
	month = {may},
	number = {05},
	pages = {2648-2658},
	publisher = {IEEE Computer Society},
	title = {Text Entry in Virtual Environments using Speech and a Midair Keyboard},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067776}}

@article{8642529,
	abstract = {Traditional optical manufacturing poses a great challenge to near-eye display designers due to large lead times in the order of multiple weeks, limiting the abilities of optical designers to iterate fast and explore beyond conventional designs. We present a complete near-eye display manufacturing pipeline with a day lead time using commodity hardware. Our novel manufacturing pipeline consists of several innovations including a rapid production technique to improve surface of a 3D printed component to optical quality suitable for near-eye display application, a computational design methodology using machine learning and ray tracing to create freeform static projection screen surfaces for near-eye displays that can represent arbitrary focal surfaces, and a custom projection lens design that distributes pixels non-uniformly for a foveated near-eye display hardware design candidate. We have demonstrated untethered augmented reality near-eye display prototypes to assess success of our technique, and show that a ski-goggles form factor, a large monocular field of view $(30^{o}\times 55^{o})$, and a resolution of 12 cycles per degree can be achieved.},
	address = {Los Alamitos, CA, USA},
	author = {K. Aksit and P. Chakravarthula and K. Rathinavel and Y. Jeong and R. Albert and H. Fuchs and D. Luebke},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898781},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;optical refraction;optical waveguides;optical design;adaptive optics;optical surface waves},
	month = {may},
	number = {05},
	pages = {1928-1939},
	publisher = {IEEE Computer Society},
	title = {Manufacturing Application-Driven Foveated Near-Eye Displays},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898781}}

@article{8643846,
	abstract = {Being able to hear objects in an environment, for example using echolocation, is a challenging task. The main goal of the current work is to use virtual environments (VEs) to train novice users to navigate using echolocation. Previous studies have shown that musicians are able to differentiate sound pulses from reflections. This paper presents design patterns for VE simulators for both training and testing procedures, while classifying users&#x27; navigation strategies in the VE. Moreover, the paper presents features that increase users&#x27; performance in VEs. We report the findings of two user studies: a pilot test that helped improve the sonic interaction design, and a primary study exposing participants to a spatial orientation task during four conditions which were early reflections (RF), late reverberation (RV), early reflections-reverberation (RR) and visual stimuli (V). The latter study allowed us to identify navigation strategies among the users. Some users (10/26) reported an ability to create spatial cognitive maps during the test with auditory echoes, which may explain why this group performed better than the remaining participants in the RR condition.},
	address = {Los Alamitos, CA, USA},
	author = {A. Andreasen and M. Geronazzo and N. Nilsson and J. Zovnercuka and K. Konovalov and S. Serafin},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898787},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {navigation;visualization;training;task analysis;reverberation;auditory system},
	month = {may},
	number = {05},
	pages = {1876-1886},
	publisher = {IEEE Computer Society},
	title = {Auditory Feedback for Navigation with Echoes in Virtual Environments: Training Procedure and Orientation Strategies},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898787}}

@article{9389490,
	abstract = {The cameras in modern gaze-tracking systems suffer from fundamental bandwidth and power limitations, constraining data acquisition speed to 300 Hz realistically. This obstructs the use of mobile eye trackers to perform, e.g., low latency predictive rendering, or to study quick and subtle eye motions like microsaccades using head-mounted devices in the wild. Here, we propose a hybrid frame-event-based near-eye gaze tracking system offering update rates beyond 10,000 Hz with an accuracy that matches that of high-end desktop-mounted commercial trackers when evaluated in the same conditions. Our system, previewed in Figure 1, builds on emerging event cameras that simultaneously acquire regularly sampled frames and adaptively sampled events. We develop an online 2D pupil fitting method that updates a parametric model every one or few events. Moreover, we propose a polynomial regressor for estimating the point of gaze from the parametric pupil model in real time. Using the first event-based gaze dataset, we demonstrate that our system achieves accuracies of 0.45$\,^{\circ}$-1.75$\,^{\circ}$ for fields of view from 45$\,^{\circ}$ to 98$\,^{\circ}$. With this technology, we hope to enable a new generation of ultra-low-latency gaze-contingent rendering and display techniques for virtual and augmented reality.},
	address = {Los Alamitos, CA, USA},
	author = {A. N. Angelopoulos and J. P. Martel and A. P. Kohli and J. Conradt and G. Wetzstein},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067784},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {tracking;pupils;gaze tracking;cameras;rendering (computer graphics);real-time systems;bandwidth},
	month = {may},
	number = {05},
	pages = {2577-2586},
	publisher = {IEEE Computer Society},
	title = {Event-Based Near-Eye Gaze Tracking Beyond 10,000 Hz},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067784}}

@article{6479183,
	abstract = {We present an efficient algorithm to compute spatially-varying, direction-dependent artificial reverberation and reflection filters in large dynamic scenes for interactive sound propagation in virtual environments and video games. Our approach performs Monte Carlo integration of local visibility and depth functions to compute directionally-varying reverberation effects. The algorithm also uses a dynamically-generated rectangular aural proxy to efficiently model 2-4 orders of early reflections. These two techniques are combined to generate reflection and reverberation filters which vary with the direction of incidence at the listener. This combination leads to better sound source localization and immersion. The overall algorithm is efficient, easy to implement, and can handle moving sound sources, listeners, and dynamic scenes, with minimal storage overhead. We have integrated our approach with the audio rendering pipeline in Valve&#x27;s Source game engine, and use it to generate realistic directional sound propagation effects in indoor and outdoor scenes in real-time. We demonstrate, through quantitative comparisons as well as evaluations, that our approach leads to enhanced, immersive multi-modal interaction.},
	address = {Los Alamitos, CA, USA},
	author = {L. Antani and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.27},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {reverberation;computational modeling;geometry;face;games;mathematical model;absorption},
	month = {apr},
	number = {04},
	pages = {567-575},
	publisher = {IEEE Computer Society},
	title = {Aural Proxies and Directionally-Varying Reverberation for Interactive Sound Propagation in Virtual Environments},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.27}}

@article{9714039,
	abstract = {In optical see-through augmented reality (AR), information is often distributed between real and virtual contexts, and often appears at different distances from the user. To integrate information, users must repeatedly switch context and change focal distance. If the user&#x27;s task is conducted under time pressure, they may attempt to integrate information while their eye is still changing focal distance, a phenomenon we term transient focal blur. Previously, Gabbard, Mehra, and Swan (2018) examined these issues, using a text-based visual search task on a one-eye optical see-through AR display. This paper reports an experiment that partially replicates and extends this task on a custom-built AR Haploscope. The experiment examined the effects of context switching, focal switching distance, binocular and monocular viewing, and transient focal blur on task performance and eye fatigue. Context switching increased eye fatigue but did not decrease performance. Increasing focal switching distance increased eye fatigue and decreased performance. Monocular viewing also increased eye fatigue and decreased performance. The transient focal blur effect resulted in additional performance decrements, and is an addition to knowledge about AR user interface design issues.},
	address = {Los Alamitos, CA, USA},
	author = {M. Arefin and N. Phillips and A. Plopski and J. L. Gabbard and J. Swan},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150503},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {optical switches;task analysis;monitoring;transient analysis;fatigue;augmented reality;meters},
	month = {may},
	number = {05},
	pages = {2014-2025},
	publisher = {IEEE Computer Society},
	title = {The Effect of Context Switching, Focal Switching Distance, Binocular and Monocular Viewing, and Transient Focal Blur on Human Performance in Optical See-Through Augmented Reality},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150503}}

@article{9714051,
	abstract = {We propose augmenting immersive telepresence by adding a virtual body, representing the user&#x27;s own arm motions, as realized through a head-mounted display and a 360-degree camera. Previous research has shown the effectiveness of having a virtual body in simulated environments; however, research on whether seeing one&#x27;s own virtual arms increases presence or preference for the user in an immersive telepresence setup is limited. We conducted a study where a host introduced a research lab while participants wore a head-mounted display which allowed them to be telepresent at the host&#x27;s physical location via a 360-degree camera, either with or without a virtual body. We first conducted a pilot study of 20 participants, followed by a pre-registered 62 participant confirmatory study. Whereas the pilot study showed greater presence and preference when the virtual body was present, the confirmatory study failed to replicate these results, with only behavioral measures suggesting an increase in presence. After analyzing the qualitative data and modeling interactions, we suspect that the quality and style of the virtual arms, and the contrast between animation and video, led to individual differences in reactions to the virtual body which subsequently moderated feelings of presence.},
	address = {Los Alamitos, CA, USA},
	author = {N. Arora and M. Suomalainen and M. Pouke and E. G. Center and K. J. Mimnaugh and A. P. Chambers and S. Pouke and S. M. LaValle},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150473},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {telepresence;robots;cameras;resists;robot vision systems;avatars;task analysis},
	month = {may},
	number = {05},
	pages = {2135-2145},
	publisher = {IEEE Computer Society},
	title = {Augmenting Immersive Telepresence Experience with a Virtual Body},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150473}}

@article{9382845,
	abstract = {Current avatar representations used in immersive VR applications lack features that may be important for supporting natural behaviors and effective communication among individuals. This study investigates the impact of the visual and nonverbal cues afforded by three different types of avatar representations in the context of several cooperative tasks. The avatar types we compared are No_Avatar (HMD and controllers only), Scanned_Avatar (wearing an HMD), and Heal_Avatar (video-see-through). The subjective and objective measures we used to assess the quality of interpersonal communication include surveys of social presence, interpersonal trust, communication satisfaction, and attention to behavioral cues, plus two behavioral measures: duration of mutual gaze and number of unique words spoken. We found that participants reported higher levels of trustworthiness in the Real_Avatar condition compared to the Scanned_Avatar and No_Avatar conditions. They also reported a greater level of attentional focus on facial expressions compared to the No_Avatar condition and spent more extended time, for some tasks, attempting to engage in mutual gaze behavior compared to the Scanned_Avatar and No_Avatar conditions. In both the Heal_Avatar and Scanned_Avatar conditions, participants reported higher levels of co-presence compared with the No_Avatar condition. In the Scanned_Avatar condition, compared with the Heal_Avatar and No_Avatar conditions, participants reported higher levels of attention to body posture. Overall, our exit survey revealed that a majority of participants (66.67%) reported a preference for the Real_Avatar, compared with 25.00% for the Scanned_Avatar and 8.33% for the No_Avatar, These findings provide novel insight into how a user&#x27;s experience in a social VR scenario is affected by the type of avatar representation provided.},
	address = {Los Alamitos, CA, USA},
	author = {S. Aseeri and V. Interrante},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067783},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;task analysis;resists;particle measurements;atmospheric measurements;virtual environments;three-dimensional displays},
	month = {may},
	number = {05},
	pages = {2608-2617},
	publisher = {IEEE Computer Society},
	title = {The Influence of Avatar Representation on Interpersonal Communication in Virtual Social Environments},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067783}}

@article{9715721,
	abstract = {Developing effective strategies for redirected walking requires extensive evaluations across a variety of factors that influence performance. Because these large-scale experiments are often not practical with user studies, researchers have instead utilized simulations to systematically test different algorithm parameters, physical space configurations, and virtual walking paths. Although simulation offers an efficient way to evaluate redirected walking algorithms, it remains an open question whether this evaluation methodology is ecologically valid. In this paper, we investigate the interaction between locomotion behavior and redirection gains at a micro-level (across small path segments) and macro-level (across an entire experience). This examination involves analyzing data from real users and comparing algorithm performance metrics with a simulated user model. The results identify specific properties of user locomotion behavior that influence the application of redirected walking gains and resets. Overall, we found that the simulation provided a conservative estimate of the average performance with real users and observed that performance trends when comparing two redirected walking algorithms were preserved. In general, these results indicate that simulation is an empirically valid evaluation methodology for redirected walking algorithms.},
	address = {Los Alamitos, CA, USA},
	author = {M. Azmandian and R. Yahata and T. Grechkin and J. Thomas and E. Rosenberg},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150466},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;solid modeling;prediction algorithms;biological system modeling;task analysis;virtual environments;heuristic algorithms},
	month = {may},
	number = {05},
	pages = {2288-2298},
	publisher = {IEEE Computer Society},
	title = {Validating Simulation-Based Evaluation of Redirected Walking Systems},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150466}}

@article{9715723,
	abstract = {Previous research has established redirected walking as a potential answer to exploring large virtual environments via natural locomotion within a limited physical space. However, much of the previous work has either focused on investigating human perception of redirected walking illusions or developing novel redirection techniques. In this paper, we take a broader look at the problem and formalize the concept of a complete redirected walking system. This work establishes the theoretical foundations for combining multiple redirection strategies into a unified framework known as adaptive redirection. This meta-strategy adapts based on the context, switching between a suite of strategies with a priori knowledge of their performance under the various circumstances. This paper also introduces a novel static planning strategy that optimizes gain parameters for a predetermined virtual path, known as the Combinatorially Optimized Pre-Planned Exploration Redirector (COPPER). We conducted a simulation-based experiment that demonstrates how adaptation rules can be determined empirically using machine learning, which involves partitioning the spectrum of contexts into regions according to the redirection strategy that performs best. Adaptive redirection provides a foundation for making redirected walking work in practice and can be extended to improve performance in the future as new techniques are integrated into the framework.},
	address = {Los Alamitos, CA, USA},
	author = {M. Azmandian and R. Yahata and T. Grechkin and E. Rosenberg},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150500},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;planning;trajectory;virtual environments;task analysis;copper;adaptive systems},
	month = {may},
	number = {05},
	pages = {2277-2287},
	publisher = {IEEE Computer Society},
	title = {Adaptive Redirection: A Context-Aware Redirected Walking Meta-Strategy},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150500}}

@article{8645818,
	abstract = {Head-mounted displays (HMDs) and large area position tracking systems can enable users to navigate virtual worlds through natural walking. Redirected walking (RDW) imperceptibly steers immersed users away from physical world obstacles allowing them to explore unbounded virtual worlds while walking in limited physical space. In cases of imminent collisions, resetting techniques can reorient them into open space. This work introduces categorically new RDW and resetting algorithms based on the use of artificial potential fields that ``push'' users away from obstacles and other users. Data from human subject experiments indicate that these methods reduce potential single-user resets by 66% and increase the average distance between resets by 86% compared to previous techniques. A live multi-user study demonstrates the viability of the algorithm with up to 3 concurrent users, and simulation results indicate that the algorithm scales efficiently up to at least 8 users and is effective with larger groups.},
	address = {Los Alamitos, CA, USA},
	author = {E. R. Bachmann and E. Hodgson and C. Hoffbauer and J. Messinger},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898764},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;navigation;space vehicles;tracking;force;orbits},
	month = {may},
	number = {05},
	pages = {2022-2031},
	publisher = {IEEE Computer Society},
	title = {Multi-User Redirected Walking and Resetting Using Artificial Potential Fields},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898764}}

@article{10049695,
	abstract = {Text entry remains challenging in virtual environments, where users may quickly experience physical fatigue in some body parts using existing methods. In this paper, we propose ``CrowbarLimbs,'' a novel virtual reality (VR) text entry metaphor with two deformable extended virtual limbs. By using a crowbar-like metaphor and placing the virtual keyboard at a user-preferred location based on the user&#x27;s physical stature, our method can assist the user in placing their hands and arms in a comfortable posture, thus effectively reducing the physical fatigue in various body parts, such as hands, wrists, and elbows. In an initial user study, we found that CrowbarLimbs achieved text entry speed, accuracy, and system usability comparable to those of previous VR typing methods. To investigate the proposed metaphor in more depth, we further conducted two additional user studies to explore the ergonomically user-friendly shapes of CrowbarLimbs and virtual keyboard locations. The experimental results indicate that the shapes of CrowbarLimbs have significant effects on the fatigue ratings in various body parts and text entry speed. Furthermore, placing the virtual keyboard near the user and at half their height can lead to a satisfactory text entry rate of 28.37 words per minute.},
	address = {Los Alamitos, CA, USA},
	author = {M. Bakar and Y. Tsai and H. Hsueh and E. Li},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247060},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {keyboards;fatigue;shape;virtual environments;usability;electronic mail;visualization},
	month = {may},
	number = {05},
	pages = {2806-2815},
	publisher = {IEEE Computer Society},
	title = {CrowbarLimbs: A Fatigue-Reducing Virtual Reality Text Entry Metaphor},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247060}}

@article{9384477,
	abstract = {We propose a new thin and flat virtual reality (VR) display design using a Fresnel lenslet array, a Fresnel lens, and a polarization-based optical folding technique. The proposed optical system has a wide field of view (FOV) of 102$\,^{\circ}$x102$\,^{\circ}$, a wide eye-box of 8.8 mm, and an ergonomic eye-relief of 20 mm. Simultaneously, only 3.3 mm of physical distance is required between the display panel and the lens, so that the integrated VR display can have a compact form factor like sunglasses. Moreover, since all lenslet of the lenslet array is designed to operate under on-axis condition with low aberration, the discontinuous pupil swim distortion between the lenslets is hardly observed. In addition, all on-axis lenslets can be designed identically, reducing production cost, and even off-the-shelf Fresnel optics can be used. In this paper, we introduce how we design system parameters and analyze system performance. Finally, we demonstrate two prototypes and experimentally verify that the proposed VR display system has the expected performance while having a glasses-like form factor.},
	address = {Los Alamitos, CA, USA},
	author = {K. Bang and Y. Jo and M. Chae and B. Lee},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067758},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {lenses;prototypes;optical imaging;optical distortion;optics;optical polarization;optical design},
	month = {may},
	number = {05},
	pages = {2545-2554},
	publisher = {IEEE Computer Society},
	title = {Lenslet VR: Thin, Flat and Wide-FOV Virtual Reality Display Using Fresnel Lens and Lenslet Array},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067758}}

@article{9714038,
	abstract = {In this paper, we present a novel monocular visual-inertial odometry system with pre-built maps deployed on the remote server, which can robustly run in real-time on a mobile device even in high latency situations. By tightly coupling VIO with geometric priors from pre-built maps, our system can tolerate the high latency and low frequency of global localization service, which is especially suitable for practical applications when the localization service is deployed on the remote server. Firstly, sparse point clouds are obtained from the dense mesh by the ray casting method according to the localization results. The dense mesh can be reconstructed from the point clouds generated by Structure-from-Motion. We directly use the sparse point clouds in feature tracking and state update to suppress drift. In the process of feature tracking, the high local accuracy of VIO is fully utilized to effectively remove outliers and make our system robust. The experiments on EurocMav datasets and simulation datasets show that compared with state-of-the-art methods, our method can achieve better results in terms of both precision and robustness. The effectiveness of the proposed method is further demonstrated through a real-time AR demo on a mobile phone with the aid of visual localization on the remote server.},
	address = {Los Alamitos, CA, USA},
	author = {H. Bao and W. Xie and Q. Qian and D. Chen and S. Zhai and N. Wang and G. Zhang},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150495},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {location awareness;real-time systems;visualization;point cloud compression;global positioning system;trajectory;servers},
	month = {may},
	number = {05},
	pages = {2212-2222},
	publisher = {IEEE Computer Society},
	title = {Robust Tightly-Coupled Visual-Inertial Odometry with Pre-built Maps in High Latency Situations},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150495}}

@article{7383309,
	abstract = {Sleep deprivation is known to have serious deleterious effects on executive functioning and job performance. Augmented reality has an ability to place pertinent information at the fore, guiding visual focus and reducing instructional complexity. This paper presents a study to explore how spatial augmented reality instructions impact procedural task performance on sleep deprived users. The user study was conducted to examine performance on a procedural task at six time points over the course of a night of total sleep deprivation. Tasks were provided either by spatial augmented reality-based projections or on an adjacent monitor. The results indicate that participant errors significantly increased with the monitor condition when sleep deprived. The augmented reality condition exhibited a positive influence with participant errors and completion time having no significant increase when sleep deprived. The results of our study show that spatial augmented reality is an effective sleep deprivation countermeasure under laboratory conditions.},
	address = {Los Alamitos, CA, USA},
	author = {J. Baumeister and J. Dorrlan and S. Banks and A. Chatburn and R. T. Smith and M. A. Carskadon and K. Lushington and B. H. Thomas},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518133},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {sleep;augmented reality;monitoring;australia;safety;biomedical monitoring;visualization},
	month = {apr},
	number = {04},
	pages = {1396-1405},
	publisher = {IEEE Computer Society},
	title = {Augmented Reality as a Countermeasure for Sleep Deprivation},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518133}}

@article{6479190,
	abstract = {We present a novel immersive telepresence system that allows distributed groups of users to meet in a shared virtual 3D world. Our approach is based on two coupled projection-based multi-user setups, each providing multiple users with perspectively correct stereoscopic images. At each site the users and their local interaction space are continuously captured using a cluster of registered depth and color cameras. The captured 3D information is transferred to the respective other location, where the remote participants are virtually reconstructed. We explore the use of these virtual user representations in various interaction scenarios in which local and remote users are face-to-face, side-by-side or decoupled. Initial experiments with distributed user groups indicate the mutual understanding of pointing and tracing gestures independent of whether they were performed by local or remote participants. Our users were excited about the new possibilities of jointly exploring a virtual city, where they relied on a world-in-miniature metaphor for mutual awareness of their respective locations.},
	address = {Los Alamitos, CA, USA},
	author = {S. Beck and A. Kunert and A. Kulik and B. Froehlich},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.33},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {calibration;cameras;servers;streaming media;image reconstruction;image color analysis;virtual reality},
	month = {apr},
	number = {04},
	pages = {616-625},
	publisher = {IEEE Computer Society},
	title = {Immersive Group-to-Group Telepresence},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.33}}

@article{6479212,
	abstract = {Autism Spectrum Disorders (ASD) are characterized by atypical patterns of behaviors and impairments in social communication. Among the fundamental social impairments in the ASD population are challenges in appropriately recognizing and responding to facial expressions. Traditional intervention approaches often require intensive support and well-trained therapists to address core deficits, with many with ASD having tremendous difficulty accessing such care due to lack of available trained therapists as well as intervention costs. As a result, emerging technology such as virtual reality (VR) has the potential to offer useful technology-enabled intervention systems. In this paper, an innovative VR-based facial emotional expression presentation system was developed that allows monitoring of eye gaze and physiological signals related to emotion identification to explore new efficient therapeutic paradigms. A usability study of this new system involving ten adolescents with ASD and ten typically developing adolescents as a control group was performed. The eye tracking and physiological data were analyzed to determine intragroup and intergroup variations of gaze and physiological patterns. Performance data, eye tracking indices and physiological features indicated that there were differences in the way adolescents with ASD process and recognize emotional faces compared to their typically developing peers. These results will be used in the future for an online adaptive VR-based multimodal social interaction system to improve emotion recognition abilities of individuals with ASD.},
	address = {Los Alamitos, CA, USA},
	author = {E. Bekele and Zhi Zheng and A. Swanson and J. Crittendon and Z. Warren and N. Sarkar},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.42},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {variable speed drives;biomedical monitoring;monitoring;animation;emotion recognition;physiology;autism},
	month = {apr},
	number = {04},
	pages = {711-720},
	publisher = {IEEE Computer Society},
	title = {Understanding How Adolescents with Autism Respond to Facial Expressions in Virtual Reality Environments},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.42}}

@article{10049674,
	abstract = {Augmented reality (AR) has shown potential in computer-aided surgery. It allows for the visualization of hidden anatomical structures as well as assists in navigating and locating surgical instruments at the surgical site. Various modalities (devices and/or visualizations) have been used in the literature, but few studies investigated the adequacy/superiority of one modality over the other. For instance, the use of optical see-through (OST) HMDs has not always been scientifically justified. Our goal is to compare various visualization modalities for catheter insertion in external ventricular drain and ventricular shunt procedures. We investigate two AR approaches: (1) 2D approaches consisting of a smartphone and a 2D window visualized through an OST (Microsoft HoloLens 2), and (2) 3D approaches consisting of a fully aligned patient model and a model that is adjacent to the patient and is rotationally aligned using an OST. 32 participants joined this study. For each visualization approach, participants were asked to perform five insertions after which they filled NASA-TLX and SUS forms. Moreover, the position and orientation of the needle with respect to the planning during the insertion task were collected. The results show that participants achieved a better insertion performance significantly under 3D visualizations, and the NASA-TLX and SUS forms reflected the preference of participants for these approaches compared to 2D approaches.},
	address = {Los Alamitos, CA, USA},
	author = {M. Benmahdjoub and A. Thabit and M. C. van Veelen and W. J. Niessen and E. B. Wolvius and T. Walsum},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247042},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {surgery;visualization;three-dimensional displays;needles;task analysis;catheters;solid modeling},
	month = {may},
	number = {05},
	pages = {2434-2445},
	publisher = {IEEE Computer Society},
	title = {Evaluation of AR visualization approaches for catheter insertion into the ventricle cavity},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247042}}

@article{7835714,
	abstract = {We describe an experiment that explores the contribution of auditory and other features to the illusion of plausibility in a virtual environment that depicts the performance of a string quartet. &#x60;Plausibility&#x27; refers to the component of presence that is the illusion that the perceived events in the virtual environment are really happening. The features studied were: Gaze (the musicians ignored the participant, the musicians sometimes looked towards and followed the participant&#x27;s movements), Sound Spatialization (Mono, Stereo, Spatial), Auralization (no sound reflections, reflections corresponding to a room larger than the one perceived, reflections that exactly matched the virtual room), and Environment (no sound from outside of the room, birdsong and wind corresponding to the outside scene). We adopted the methodology based on color matching theory, where 20 participants were first able to assess their feeling of plausibility in the environment with each of the four features at their highest setting. Then five times participants started from a low setting on all features and were able to make transitions from one system configuration to another until they matched their original feeling of plausibility. From these transitions a Markov transition matrix was constructed, and also probabilities of a match conditional on feature configuration. The results show that Environment and Gaze were individually the most important factors influencing the level of plausibility. The highest probability transitions were to improve Environment and Gaze, and then Auralization and Spatialization. We present this work as both a contribution to the methodology of assessing presence without questionnaires, and showing how various aspects of a musical performance can influence plausibility.},
	address = {Los Alamitos, CA, USA},
	author = {I. Bergstrom and S. Azevedo and P. Papiotis and N. Saldanha and M. Slater},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657138},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {reflection;virtual environments;electronic mail;instruments;auditory system;music},
	month = {apr},
	number = {04},
	pages = {1352-1359},
	publisher = {IEEE Computer Society},
	title = {The Plausibility of a String Quartet Performance in Virtual Reality},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657138}}

@article{8651483,
	abstract = {The ubiquity of smart mobile devices, such as phones and tablets, enables users to casually capture 360$\,^{\circ}$ panoramas with a single camera sweep to share and relive experiences. However, panoramas lack motion parallax as they do not provide different views for different viewpoints. The motion parallax induced by translational head motion is a crucial depth cue in daily life. Alternatives, such as omnidirectional stereo panoramas, provide different views for each eye (binocular disparity), but they also lack motion parallax as the left and right eye panoramas are stitched statically. Methods based on explicit scene geometry reconstruct textured 3D geometry, which provides motion parallax, but suffers from visible reconstruction artefacts. The core of our method is a novel multi-perspective panorama representation, which can be casually captured and rendered with motion parallax for each eye on the fly. This provides a more realistic perception of panoramic environments which is particularly useful for virtual reality applications. Our approach uses a single consumer video camera to acquire 200-400 views of a real 360$\,^{\circ}$ environment with a single sweep. By using novel-view synthesis with flow-based blending, we show how to turn these input views into an enriched 360$\,^{\circ}$ panoramic experience that can be explored in real time, without relying on potentially unreliable reconstruction of scene geometry. We compare our results with existing omnidirectional stereo and image-based rendering methods to demonstrate the benefit of our approach, which is the first to enable casual consumers to capture and view high-quality 360$\,^{\circ}$ panoramas with motion parallax.},
	address = {Los Alamitos, CA, USA},
	author = {T. Bertel and N. F. Campbell and C. Richardt},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898799},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cameras;rendering (computer graphics);geometry;image reconstruction;three-dimensional displays;real-time systems;streaming media},
	month = {may},
	number = {05},
	pages = {1828-1835},
	publisher = {IEEE Computer Society},
	title = {MegaParallax: Casual 360$\,^{\circ}$ Panoramas with Motion Parallax},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898799}}

@article{8260967,
	abstract = {With costs of head-mounted displays (HMDs) and tracking technology decreasing rapidly, various virtual reality applications are being widely adopted for education and training. Hardware advancements have enabled replication of real-world interactions in virtual environments to a large extent, paving the way for commercial grade applications that provide a safe and risk-free training environment at a fraction of the cost. But this also mandates the need to develop more intrinsic interaction techniques and to empirically evaluate them in a more comprehensive manner. Although there exists a body of previous research that examines the benefits of selected levels of interaction fidelity on performance, few studies have investigated the constituent components of fidelity in a Interaction Fidelity Continuum (IFC) with several system instances and their respective effects on performance and learning in the context of a real-world skills training application. Our work describes a large between-subjects investigation conducted over several years that utilizes bimanual interaction metaphors at six discrete levels of interaction fidelity to teach basic precision metrology concepts in a near-field spatial interaction task in VR. A combined analysis performed on the data compares and contrasts the six different conditions and their overall effects on performance and learning outcomes, eliciting patterns in the results between the discrete application points on the IFC. With respect to some performance variables, results indicate that simpler restrictive interaction metaphors and highest fidelity metaphors perform better than medium fidelity interaction metaphors. In light of these results, a set of general guidelines are created for developers of spatial interaction metaphors in immersive virtual environments for precise fine-motor skills training simulations.},
	address = {Los Alamitos, CA, USA},
	author = {A. Bhargava and J. W. Bertrand and A. K. Gramopadhye and K. C. Madathil and S. V. Babu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794639},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {training;aerospace electronics;solid modeling;metrology;mice;virtual environments},
	month = {apr},
	number = {04},
	pages = {1418-1427},
	publisher = {IEEE Computer Society},
	title = {Evaluating Multiple Levels of an Interaction Fidelity Continuum on Performance and Learning in Near-Field Training Simulations},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794639}}

@article{10049626,
	abstract = {With the popularity of Virtual Reality (VR) on the rise, creators from a variety of fields are building increasingly complex experiences that allow users to express themselves more naturally. Self-avatars and object interaction in virtual worlds are at the heart of these experiences. However, these give rise to several perception based challenges that have been the focus of research in recent years. One area that garners most interest is understanding the effects of self-avatars and object interaction on action capabilities or affordances in VR. Affordances have been shown to be influenced by the anthropometric and anthropomorphic properties of the self-avatar embodied. However, self-avatars cannot fully represent real world interaction and fail to provide information about the dynamic properties of surfaces in the environment. For example, pressing against a board to feel its rigidity. This lack of accurate dynamic information can be further amplified when interacting with virtual handheld objects as the weight and inertial feedback associated with them is often mismatched. To investigate this phenomenon, we looked at how the absence of dynamic surface properties affect lateral passability judgments when carrying virtual handheld objects in the presence or absence of gender matched body-scaled self-avatars. Results suggest that participants can calibrate to the missing dynamic information in the presence of self-avatars to make lateral passability judgments, but rely on their internal body schema of a compressed physical body depth in the absence of self-avatars.},
	address = {Los Alamitos, CA, USA},
	author = {A. Bhargava and R. Venkatakrishnan and R. Venkatakrishnan and K. Lucaites and H. Solini and A. C. Robb and C. C. Pagano and S. V. Babu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247067},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {affordances;apertures;calibration;visualization;training;task analysis;propioception},
	month = {may},
	number = {05},
	pages = {2348-2357},
	publisher = {IEEE Computer Society},
	title = {Can I Squeeze Through? Effects of Self-Avatars and Calibration in a Person-Plus-Virtual-Object System on Perceived Lateral Passability in VR},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247067}}

@article{9714052,
	abstract = {Virtual Reality (VR) has the potential to support mobile knowledge workers by complementing traditional input devices with a large three-dimensional output space and spatial input. Previous research on supporting VR knowledge work explored domains such as text entry using physical keyboards and spreadsheet interaction using combined pen and touch input. Inspired by such work, this paper probes the VR design space for authoring presentations in mobile settings. We propose PoVRPoint-a set of tools coupling pen- and touch-based editing of presentations on mobile devices, such as tablets, with the interaction capabilities afforded by VR. We study the utility of extended display space to, for example, assist users in identifying target slides, supporting spatial manipulation of objects on a slide, creating animations, and facilitating arrangements of multiple, possibly occluded shapes or objects. Among other things, our results indicate that 1) the wide field of view afforded by VR results in significantly faster target slide identification times compared to a tablet-only interface for visually salient targets; and 2) the three-dimensional view in VR enables significantly faster object reordering in the presence of occlusion compared to two baseline interfaces. A user study further confirmed that the interaction techniques were found to be usable and enjoyable.},
	address = {Los Alamitos, CA, USA},
	author = {V. Biener and T. Gesslein and D. Schneider and F. Kawala and A. Otte and P. Kristensson and M. Pahud and E. Ofek and C. Campos and M. Kljun and K. Pucihar and J. Grubert},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150474},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;task analysis;mobile handsets;visualization;animation;usability;shape},
	month = {may},
	number = {05},
	pages = {2069-2079},
	publisher = {IEEE Computer Society},
	title = {PoVRPoint: Authoring Presentations in Mobile Virtual Reality},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150474}}

@article{8658185,
	abstract = {This paper presents the implementation and evaluation of a 50,000-pose-sample-per-second, 6-degree-of-freedom optical head tracking instrument with motion-to-pose latency of 28s and dynamic precision of 1--2 arcminutes. The instrument uses high-intensity infrared emitters and two duo-lateral photodiode-based optical sensors to triangulate pose. This instrument serves two purposes: it is the first step towards the requisite head tracking component in 100s motion-to-photon latency optical see-through augmented reality (OST AR) head-mounted display (HMD) systems; and it enables new avenues of research into human visual perception -- including measuring the thresholds for perceptible real-virtual displacement during head rotation and other human research requiring high-sample-rate motion tracking. The instrument&#x27;s tracking volume is limited to about 120120250mm  but allows for the full range of natural head rotation and is sufficient for research involving seated users. We discuss how the instrument&#x27;s tracking volume is scalable in multiple ways and some of the trade-offs involved therein. Finally, we introduce a novel laser-pointer-based measurement technique for assessing the instrument&#x27;s tracking latency and repeatability. We show that the instrument&#x27;s motion-to-pose latency is 28s and that it is repeatable within 1--2 arcminutes at mean rotational velocities (yaw) in excess of 500$\,^{\circ}$/sec.},
	address = {Los Alamitos, CA, USA},
	author = {A. Blate and M. Whitton and M. Singh and G. Welch and A. State and T. Whitted and H. Fuchs},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899233},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {instruments;target tracking;optical sensors;photodiodes;adaptive optics},
	month = {may},
	number = {05},
	pages = {1970-1980},
	publisher = {IEEE Computer Society},
	title = {Implementation and Evaluation of a 50 kHz, 28s Motion-to-Pose Latency Head Tracking Instrument},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899233}}

@article{8645699,
	abstract = {Real walking is the most natural way to locomote in virtual reality (VR), but a confined physical walking space limits its applicability. Redirected walking (RDW) is a collection of techniques to solve this problem. One of these techniques aims to imperceptibly rotate the user&#x27;s view of the virtual scene in order to steer her along a confined path whilst giving the impression of walking in a straight line in a large virtual space. Measurement of perceptual thresholds for the detection of such a modified curvature gain have indicated a radius that is still larger than most room sizes. Since the brain is an adaptive system and thresholds usually depend on previous stimulations, we tested if prolonged exposure to an immersive virtual environment (IVE) with increased curvature gain produces adaptation to that gain and modifies thresholds such that, over time, larger curvature gains can be applied for RDW. Therefore, participants first completed a measurement of their perceptual threshold for curvature gain. In a second session, the same participants were exposed to an IVE with a constant curvature gain in which they walked between two targets for about 20 minutes. Afterwards, their perceptual thresholds were measured again. The results show that the psychometric curves shifted after the exposure session and perceptual thresholds for increased curvature gain further increased. The increase of the detection threshold suggests that participants adapt to the manipulation and stronger curvature gains can be applied in RDW, and therefore improves its applicability in such situations.},
	address = {Los Alamitos, CA, USA},
	author = {L. Bolling and N. Stein and F. Steinicke and M. Lappe},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899228},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;resists;tracking;gain measurement;visualization;glass;atmospheric measurements},
	month = {may},
	number = {05},
	pages = {2032-2039},
	publisher = {IEEE Computer Society},
	title = {Shrinking Circles: Adaptation to Increased Curvature Gain in Redirected Walking},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899228}}

@article{7010955,
	abstract = {Virtual reality strives to provide a user with an experience of a simulated world that feels as natural as the real world. Yet, to induce this feeling, sometimes it becomes necessary for technical reasons to deviate from a one-to-one correspondence between the real and the virtual world, and to reorient or reposition the user&#x27;s viewpoint. Ideally, users should not notice the change of the viewpoint to avoid breaks in perceptual continuity. Saccades, the fast eye movements that we make in order to switch gaze from one object to another, produce a visual discontinuity on the retina, but this is not perceived because the visual system suppresses perception during saccades. As a consequence, our perception fails to detect rotations of the visual scene during saccades. We investigated whether saccadic suppression of image displacement (SSID) can be used in an immersive virtual environment (VE) to unconsciously rotate and translate the observer&#x27;s viewpoint. To do this, the scene changes have to be precisely time-locked to the saccade onset. We used electrooculography (EOG) for eye movement tracking and assessed the performance of two modified eye movement classification algorithms for the challenging task of online saccade detection that is fast enough for SSID. We investigated the sensitivity of participants to translations (forward/backward) and rotations (in the transverse plane) during trans-saccadic scene changes. We found that participants were unable to detect approximately $\pm$0.5m translations along the line of gaze and $\pm$5$\,^{\circ}$ rotations in the transverse plane during saccades with an amplitude of 15$\,^{\circ}$. If the user stands still, our approach exploiting SSID thus provides the means to unconsciously change the user&#x27;s virtual position and/or orientation. For future research and applications, exploiting SSID has the potential to improve existing redirected walking and change blindness techniques for unlimited navigation through arbitrarily-sized VEs by real walking.},
	address = {Los Alamitos, CA, USA},
	author = {B. Bolte and M. Lappe},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391851},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {electrooculography;heuristic algorithms;acceleration;tracking;visualization;sensitivity;legged locomotion},
	month = {apr},
	number = {04},
	pages = {545-552},
	publisher = {IEEE Computer Society},
	title = {Subliminal Reorientation and Repositioning in Immersive Virtual Environments using Saccadic Suppression},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391851}}

@article{6479186,
	abstract = {We present a novel technique for animating self-avatar eye movements in an immersive virtual environment without the use of eye-tracking hardware, and evaluate our technique via a two-alternative, forced-choice-with-confidence experiment that compares this simulated-eye-tracking condition to a no-eye-tracking condition and a real-eye-tracking condition in which the avatar&#x27;s eyes were rotated with an eye tracker. Viewing the reflection of a tracked self-avatar is often used in virtual-embodiment scenarios to induce in the participant the illusion that the virtual body of the self-avatar belongs to them, however current tracking methods do not account for the movements of the participants eyes, potentially lessening this body-ownership illusion. The results of our experiment indicate that, although blind to the experimental conditions, participants noticed differences between eye behaviors, and found that the real and simulated conditions represented their behavior better than the no-eye-tracking condition. Additionally, no statistical difference was found when choosing between the real and simulated conditions. These results suggest that adding eye movements to selfavatars produces a subjective increase in self-identification with the avatar due to a more complete representation of the participant&#x27;s behavior, which may be beneficial for inducing virtual embodiment, and that effective results can be obtained without the need for any specialized eye-tracking hardware.},
	address = {Los Alamitos, CA, USA},
	author = {D. Borland and T. Peck and M. Slater},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.24},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;tracking;mirrors;calibration;visualization;hardware;standards},
	month = {apr},
	number = {04},
	pages = {591-596},
	publisher = {IEEE Computer Society},
	title = {An Evaluation of Self-Avatar Eye Movement for Virtual Embodiment},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.24}}

@article{10064045,
	abstract = {Pseudo-Haptic techniques, or visuo-haptic illusions, leverage user&#x27;s visual dominance over haptics to alter the users&#x27; perception. As they create a discrepancy between virtual and physical interactions, these illusions are limited to a perceptual threshold. Many haptic properties have been studied using pseudo-haptic techniques, such as weight, shape or size. In this paper, we focus on estimating the perceptual thresholds for pseudo-stiffness in a virtual reality grasping task. We conducted a user study (n &#x3D; 15) where we estimated if compliance can be induced on a non-compressible tangible object and to what extent. Our results show that (1) compliance can be induced in a rigid tangible object and that (2) pseudo-haptics can simulate beyond 24 N/cm stiffness (k &gt; 24N/cm, between a gummy bear and a raisin, up to rigid objects). Pseudo-stiffness efficiency is (3) enhanced by the objects&#x27; scales, but mostly (4) correlated to the user input force. Taken altogether, our results offer novel opportunities to simplify the design of future haptic interfaces, and extend the haptic properties of passive props in VR.},
	address = {Los Alamitos, CA, USA},
	author = {E. Bouzbib and C. Pacchierotti and A. Lecuyer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247083},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;force;deformation;visualization;grasping;shape;rendering (computer graphics)},
	month = {may},
	number = {05},
	pages = {2743-2752},
	publisher = {IEEE Computer Society},
	title = {When Tangibles Become Deformable: Studying Pseudo-Stiffness Perceptual Thresholds in a VR Grasping Task},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247083}}

@article{8283639,
	abstract = {With the broad range of motion capture devices available on the market, it is now commonplace to directly control the limb movement of an avatar during immersion in a virtual environment. Here, we study how the subjective experience of embodying a full-body controlled avatar is influenced by motor alteration and self-contact mismatches. Self-contact is in particular a strong source of passive haptic feedback and we assume it to bring a clear benefit in terms of embodiment. For evaluating this hypothesis, we experimentally manipulate self-contacts and the virtual hand displacement relatively to the body. We introduce these body posture transformations to experimentally reproduce the imperfect or incorrect mapping between real and virtual bodies, with the goal of quantifying the limits of acceptance for distorted mapping on the reported body ownership and agency. We first describe how we exploit egocentric coordinate representations to perform a motion capture ensuring that real and virtual hands coincide whenever the real hand is in contact with the body. Then, we present a pilot study that focuses on quantifying our sensitivity to visuo-tactile mismatches. The results are then used to design our main study with two factors, offset (for self-contact) and amplitude (for movement amplification). Our main result shows that subjects&#x27; embodiment remains important, even when an artificially amplified movement of the hand was performed, but provided that correct self-contacts are ensured.},
	address = {Los Alamitos, CA, USA},
	author = {S. Bovet and H. Debarba and B. Herbelin and E. Molla and R. Boulic},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794658},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;sensitivity;haptic interfaces;visualization;virtual environments;task analysis},
	month = {apr},
	number = {04},
	pages = {1428-1436},
	publisher = {IEEE Computer Society},
	title = {The Critical Role of Self-Contact for Embodiment in Virtual Reality},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794658}}

@article{Brady2012:Evaluating-Display-Fidelity,
	abstract = {In recent years, consumers have witnessed a technological revolution that has delivered more-realistic experiences in their own homes through high-definition, stereoscopic televisions and natural, gesture-based video game consoles. Although these experiences are more realistic, offering higher levels of fidelity, it is not clear how the increased display and interaction aspects of fidelity impact the user experience. Since immersive virtual reality (VR) allows us to achieve very high levels of fidelity, we designed and conducted a study that used a six-sided CAVE to evaluate display fidelity and interaction fidelity independently, at extremely high and low levels, for a VR first-person shooter (FPS) game. Our goal was to gain a better understanding of the effects of fidelity on the user in a complex, performance-intensive context. The results of our study indicate that both display and interaction fidelity significantly affect strategy and performance, as well as subjective judgments of presence, engagement, and usability. In particular, performance results were strongly in favor of two conditions: low-display, low-interaction fidelity (representative of traditional FPS games) and high-display, high-interaction fidelity (similar to the real world).},
	address = {Los Alamitos, CA, USA},
	author = {R. B. Brady and D. J. Zielinski and D. A. Bowman and R. P. McMahan},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.43},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;computer games;display instrumentation;human computer interaction;high-interaction fidelity condition;display fidelity evaluation;interaction fidelity evaluation;virtual reality game;user experience;immersive virtual reality;six-sided cave;vr first-person shooter game;performance-intensive context;subjective presence judgement;subjective engagement judgement;subjective usability judgement;low-display condition;low-interaction fidelity condition;high-display condition;games;mice;turning;humans;keyboards;usability;accuracy;engagement.;virtual reality;display fidelity;interaction fidelity;presence},
	month = {apr},
	number = {04},
	pages = {626-633},
	publisher = {IEEE Computer Society},
	title = {Evaluating Display Fidelity and Interaction Fidelity in a Virtual Reality Game},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.43}}

@article{9714117,
	abstract = {A novel theoretical model recently introduced coherence and plausibility as the essential conditions of XR experiences, challenging contemporary presence-oriented concepts. This article reports on two experiments validating this model, which assumes coherence activation on three layers (cognition, perception, and sensation) as the potential sources leading to a condition of plausibility and from there to other XR qualia such as presence or body ownership. The experiments introduce and utilize breaks in plausibility (in analogy to breaks in presence): We induce incoherence on the perceptual and the cognitive layer simultaneously by a simulation of object behaviors that do not conform to the laws of physics, i.e., gravity. We show that this manipulation breaks plausibility and hence confirm that it results in the desired effects in the theorized condition space but that the breaks in plausibility did not affect presence. In addition, we show that a cognitive manipulation by a storyline framing is too weak to successfully counteract the strong bottom-up inconsistencies. Both results are in line with the predictions of the recently introduced three-layer model of coherence and plausibility, which incorporates well-known top-down and bottom-up rivalries and its theorized increased independence between plausibility and presence.},
	address = {Los Alamitos, CA, USA},
	author = {L. Brubach and F. Westermeier and C. Wienrich and M. Latoschik},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150496},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {coherence;x reality;cognitive science;high-temperature superconductors;physics;marine vehicles;licenses},
	month = {may},
	number = {05},
	pages = {2267-2276},
	publisher = {IEEE Computer Society},
	title = {Breaking Plausibility Without Breaking Presence - Evidence For The Multi-Layer Nature Of Plausibility},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150496}}

@article{7036075,
	abstract = {Redirected walking allows users to walk through a large-scale immersive virtual environment (IVE) while physically remaining in a reasonably small workspace. Therefore, manipulations are applied to virtual camera motions so that the user&#x27;s self-motion in the virtual world differs from movements in the real world. Previous work found that the human perceptual system tolerates a certain amount of inconsistency between proprioceptive, vestibular and visual sensation in IVEs, and even compensates for slight discrepancies with recalibrated motor commands. Experiments showed that users are not able to detect an inconsistency if their physical path is bent with a radius of at least 22 meters during virtual straightforward movements. If redirected walking is applied in a smaller workspace, manipulations become noticeable, but users are still able to move through a potentially infinitely large virtual world by walking. For this semi-natural form of locomotion, the question arises if such manipulations impose cognitive demands on the user, which may compete with other tasks in IVEs for finite cognitive resources. In this article we present an experiment in which we analyze the mutual influence between redirected walking and verbal as well as spatial working memory tasks using a dual-tasking method. The results show an influence of redirected walking on verbal as well as spatial working memory tasks, and we also found an effect of cognitive tasks on walking behavior. We discuss the implications and provide guidelines for using redirected walking in virtual reality laboratories.},
	address = {Los Alamitos, CA, USA},
	author = {G. Bruder and P. Lubos and F. Steinicke},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391864},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;laboratories;standards;visualization;virtual environments;cameras;wireless sensor networks},
	month = {apr},
	number = {04},
	pages = {539-544},
	publisher = {IEEE Computer Society},
	title = {Cognitive Resource Demands of Redirected Walking},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391864}}

@article{7014249,
	abstract = {When avoiding a group, a walker has two possibilities: either he goes through it or around it. Going through very dense groups or around huge ones would not seem natural and could break any sense of presence in a virtual environment. This paper aims to enable crowd simulators to handle such situations correctly. To this end, we need to understand how real humans decide to go through or around groups. As a first hypothesis, we apply the Principle of Minimum Energy (PME) on different group sizes and density. According to this principle, a walker should go around small and dense groups whereas he should go through large and sparse groups. Such principle has already been used for crowd simulation; the novelty here is to apply it to decide on a global avoidance strategy instead of local adaptations only. Our study quantifies decision thresholds. However, PME leaves some inconclusive situations for which the two solutions paths have similar energetic costs. In a second part, we propose an experiment to corroborate PME decisions thresholds with real observations. As controlling the factors of an experiment with many people is extremely hard, we propose to use Virtual Reality as a new method to observe human behavior. This work represents the first crowd simulation algorithm component directly designed from a VR-based study. We also consider the role of secondary factors in inconclusive situations. We show the influence of the group appearance and direction of relative motion in the decision process. Finally, we draw some guidelines to integrate our conclusions to existing crowd simulators and show an example of such integration. We evaluate the achieved improvements.},
	address = {Los Alamitos, CA, USA},
	author = {J. Bruneau and A. Olivier and J. Pettre},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391862},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {trajectory;virtual environments;solid modeling;collision avoidance;algorithm design and analysis;biological system modeling},
	month = {apr},
	number = {04},
	pages = {520-528},
	publisher = {IEEE Computer Society},
	title = {Going Through, Going Around: A Study on Individual Avoidance of Groups},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391862}}

@article{8643340,
	abstract = {An essential question in understanding how to develop and build collaborative immersive virtual environments (IVEs) is recognizing how people perform actions together. Many actions in the real world require that people act without prior planning, and these actions are executed quite successfully. In this paper, we study the common action of two people passing through an aperture together in both the real world (Experiment 1) and in a distributed, collaborative IVE (Experiment 2). The aperture&#x27;s width is varied from too narrow to be passable to so wide as to be easily passable by both participants together simultaneously. We do this in the real world for all possible gender-based pairings. In virtual reality, however, there is potential for the gender of the participant and the gender of the self-avatar to be different. We also investigate the joint action for all possible gender-based pairings in the distributed IVE. Results indicated that, in the real world, social dynamics between gendered pairings emerged; male-male pairings refused to concede to one another until absolutely necessary while other pairings did not. Male-female pairings were most likely to provide ample space to one another during passage. These behaviors seemed not to appear in the IVE, and avatar gender across all pairings generated no significant behavioral differences. In addition, participants tended to require wider gaps to allow for passage in the IVE. These findings establish base knowledge of social dynamics and affordance behaviors within multi-user IVEs.},
	address = {Los Alamitos, CA, USA},
	author = {L. E. Buck and J. J. Rieser and G. Narasimham and B. Bodenheimer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899232},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;avatars;apertures;collaboration;virtual environments;legged locomotion},
	month = {may},
	number = {05},
	pages = {2123-2133},
	publisher = {IEEE Computer Society},
	title = {Interpersonal Affordances and Social Dynamics in Collaborative Immersive Virtual Environments: Passing Together Through Apertures},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899232}}

@article{9714123,
	abstract = {In this paper, we examine how embodiment and manipulation of a self-avatar&#x27;s dimensions --- specifically the arm length --- affect users&#x27; judgments of the personal space around them in an immersive virtual environment. In the real world, personal space is the immediate space around the body in which physical interactions are possible. Personal space is increasingly studied in virtual environments because of its importance to social interactions. Here, we specifically look at two components of personal space, interpersonal and peripersonal space, and how they are affected by embodiment and the sizing of a self-avatar. We manipulated embodiment, hypothesizing that higher levels of embodiment will result in larger measures of interpersonal space and smaller measures of peripersonal space. Likewise, we manipulated the arm length of a self-avatar, hypothesizing that while interpersonal space would change with changing arm length, peripersonal space would not. We found that the representation of both interpersonal and peripersonal space change when the user experiences differing levels of embodiment in accordance with our hypotheses, and that only interpersonal space was sensitive to changes in the dimensions of a self-avatar&#x27;s arms. These findings provide increased understanding of the role of embodiment and self-avatars in the regulation of personal space, and provide foundations for improved design of social interaction in virtual environments.},
	address = {Los Alamitos, CA, USA},
	author = {L. E. Buck and S. Chakraborty and B. Bodenheimer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150483},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual environments;avatars;aerospace electronics;extraterrestrial measurements;contracts;visualization;particle measurements},
	month = {may},
	number = {05},
	pages = {2102-2113},
	publisher = {IEEE Computer Society},
	title = {The Impact of Embodiment and Avatar Sizing on Personal Space in Immersive Virtual Environments},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150483}}

@article{Burton2012:The-Right-View-from,
	abstract = {Stereoscopic depth cues improve depth perception and increase immersion within virtual environments (VEs). However, improper display of these cues can distort perceived distances and directions. Consider a multi-user VE, where all users view identical stereoscopic images regardless of physical location. In this scenario, cues are typically customized for one &quot;leader&quot; equipped with a head-tracking device. This user stands at the center of projection (CoP) and all other users (&quot;followers&quot;) view the scene from other locations and receive improper depth cues. This paper examines perceived depth distortion when viewing stereoscopic VEs from follower perspectives and the impact of these distortions on collaborative spatial judgments. Pairs of participants made collaborative depth judgments of virtual shapes viewed from the CoP or after displacement forward or backward. Forward and backward displacement caused perceived depth compression and expansion, respectively, with greater compression than expansion. Furthermore, distortion was less than predicted by a ray-intersection model of stereo geometry. Collaboration times were significantly longer when participants stood at different locations compared to the same location, and increased with greater perceived depth discrepancy between the two viewing locations. These findings advance our understanding of spatial distortions in multi-user VEs, and suggest a strategy for reducing distortion.},
	address = {Los Alamitos, CA, USA},
	author = {M. Burton and B. Pollock and J. W. Kelly and S. Gilbert and E. Winer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.58},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;computer displays;stereo image processing;user interfaces;stereoscopic displays;depth perception;stereoscopic multiuser virtual environment;stereoscopic depth cue;immersion;stereoscopic image;head-tracking device;center-of-projection;follower perspective;leader perspective;collaborative spatial judgment;virtual shape;forward displacement;backward displacement;perceived depth compression;perceived depth expansion;ray-intersection model;stereo geometry;distortion reduction strategy;virtual environments;predictive models;stereo image processing;shape;collaboration;educational institutions;and collaborative interaction.;perception;stereoscopy},
	month = {apr},
	number = {04},
	pages = {581-588},
	publisher = {IEEE Computer Society},
	title = {The Right View from the Wrong Location: Depth Perception in Stereoscopic Multi-User Virtual Environments},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.58}}

@article{8642450,
	abstract = {Precomputed sound propagation samples acoustics at discrete scene probe positions to support dynamic listener locations. An offline 3D numerical simulation is performed at each probe and the resulting field is encoded for runtime rendering with dynamic sources. Prior work place probes on a uniform grid, requiring high density to resolve narrow spaces. Our adaptive sampling approach varies probe density based on a novel ``local diameter'' measure of the space surrounding a given point, evaluated by stochastically tracing paths in the scene. We apply this measure to layout probes so as to smoothly adapt resolution and eliminate undersampling in corners, narrow corridors and stairways, while coarsening appropriately in more open areas. Coupled with a new runtime interpolator based on radial weights over geodesic paths, we achieve smooth acoustic effects that respect scene boundaries as both the source or listener move, unlike existing visibility-based solutions. We consistently demonstrate quality improvement over prior work at fixed cost.},
	address = {Los Alamitos, CA, USA},
	author = {C. Chaitanya and J. M. Snyder and K. Godin and D. Nowrouzezahrai and N. Raghuvanshi},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898765},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {probes;runtime;interpolation;acoustics;games;geometry;three-dimensional displays},
	month = {may},
	number = {05},
	pages = {1846-1854},
	publisher = {IEEE Computer Society},
	title = {Adaptive Sampling for Sound Propagation},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898765}}

@article{9712215,
	abstract = {Media streaming, with an edge-cloud setting, has been adopted for a variety of applications such as entertainment, visualization, and design. Unlike video/audio streaming where the content is usually consumed passively, virtual reality applications require 3D assets stored on the edge to facilitate frequent edge-side interactions such as object manipulation and viewpoint movement. Compared to audio and video streaming, 3D asset streaming often requires larger data sizes and yet lower latency to ensure sufficient rendering quality, resolution, and latency for perceptual comfort. Thus, streaming 3D assets faces remarkably additional than streaming audios/videos, and existing solutions often suffer from long loading time or limited quality. To address this challenge, we propose a perceptually-optimized progressive 3D streaming method for spatial quality and temporal consistency in immersive interactions. On the cloud-side, our main idea is to estimate perceptual importance in 2D image space based on user gaze behaviors, including where they are looking and how their eyes move. The estimated importance is then mapped to 3D object space for scheduling the streaming priorities for edge-side rendering. Since this computational pipeline could be heavy, we also develop a simple neural network to accelerate the cloud-side scheduling process. We evaluate our method via subjective studies and objective analysis under varying network conditions (from 3G to 5G) and edge devices (HMD and traditional displays), and demonstrate better visual quality and temporal consistency than alternative solutions.},
	address = {Los Alamitos, CA, USA},
	author = {S. Chen and B. Duinkharjav and X. Sun and L. Wei and S. Petrangeli and J. Echevarria and C. Silva and Q. Sun},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150522},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;visualization;rendering (computer graphics);cloud computing;streaming media;mathematical models;sensitivity},
	month = {may},
	number = {05},
	pages = {2157-2167},
	publisher = {IEEE Computer Society},
	title = {Instant Reality: Gaze-Contingent Perceptual Optimization for 3D Virtual Reality Streaming},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150522}}

@article{9382928,
	abstract = {Low-cost virtual-reality (VR) head-mounted displays (HMDs) with the integration of smartphones have brought the immersive VR to the masses, and increased the ubiquity of VR. However, these systems are often limited by their poor interactivity. In this paper, we present GestOnHMD, a gesture-based interaction technique and a gesture-classification pipeline that leverages the stereo microphones in a commodity smartphone to detect the tapping and the scratching gestures on the front, the left, and the right surfaces on a mobile VR headset. Taking the Google Cardboard as our focused headset, we first conducted a gesture-elicitation study to generate 150 user-defined gestures with 50 on each surface. We then selected 15, 9, and 9 gestures for the front, the left, and the right surfaces respectively based on user preferences and signal detectability. We constructed a data set containing the acoustic signals of 18 users performing these on-surface gestures, and trained the deep-learning classification pipeline for gesture detection and recognition. Lastly, with the real-time demonstration of GestOnHMD, we conducted a series of online participatory-design sessions to collect a set of user-defined gesture-referent mappings that could potentially benefit from GestOnHMD.},
	address = {Los Alamitos, CA, USA},
	author = {T. Chen and L. Xu and X. Xu and K. Zhu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067689},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {internet;headphones;acoustics;sensors;pipelines;smart phones;microphones},
	month = {may},
	number = {05},
	pages = {2597-2607},
	publisher = {IEEE Computer Society},
	title = {GestOnHMD: Enabling Gesture-based Interaction on Low-cost VR Head-Mounted Display},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067689}}

@article{7014255,
	abstract = {Thanks to the increasing availability of consumer head-mounted displays, educational applications of immersive VR could now reach to the general public, especially if they include gaming elements (immersive serious games). Safety education of citizens could be a particularly promising domain for immersive serious games, because people tend not to pay attention to and benefit from current safety materials. In this paper, we propose an HMD-based immersive game for educating passengers about aviation safety that allows players to experience a serious aircraft emergency with the goal of surviving it. We compare the proposed approach to a traditional aviation safety education method (the safety card) used by airlines. Unlike most studies of VR for safety knowledge acquisition, we do not focus only on assessing learning immediately after the experience but we extend our attention to knowledge retention over a longer time span. This is a fundamental requirement, because people need to retain safety procedures in order to apply them when faced with danger. A knowledge test administered before, immediately after and one week after the experimental condition showed that the immersive serious game was superior to the safety card. Moreover, subjective as well as physiological measurements employed in the study showed that the immersive serious game was more engaging and fear-arousing than the safety card, a factor that can contribute to explain the obtained superior retention, as we discuss in the paper.},
	address = {Los Alamitos, CA, USA},
	author = {L. Chittaro and F. Buttussi},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391853},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {games;safety;aircraft;education;materials;avatars;engines},
	month = {apr},
	number = {04},
	pages = {529-538},
	publisher = {IEEE Computer Society},
	title = {Assessing Knowledge Retention of an Immersive Serious Game vs. a Traditional Education Method in Aviation Safety},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391853}}

@article{6479180,
	abstract = {Health sciences students often practice and are evaluated on interview and exam skills by working with standardized patients (people that role play having a disease or condition). However, standardized patients do not exist for certain vulnerable populations such as children and the intellectually disabled. As a result, students receive little to no exposure to vulnerable populations before becoming working professionals. To address this problem and thereby increase exposure to vulnerable populations, we propose using virtual humans to simulate members of vulnerable populations. We created a mixed reality pediatric patient that allowed students to practice pediatric developmental exams. Practicing several exams is necessary for students to understand how to properly interact with and correctly assess a variety of children. Practice also increases a student&#x27;s confidence in performing the exam. Effective practice requires students to treat the virtual child realistically. Treating the child realistically might be affected by how the student and virtual child physically interact, so we created two object interaction interfaces - a natural interface and a mouse-based interface. We tested the complete mixed reality exam and also compared the two object interaction interfaces in a within-subjects user study with 22 participants. Our results showed that the participants accepted the virtual child as a child and treated it realistically. Participants also preferred the natural interface, but the interface did not affect how realistically participants treated the virtual child.},
	address = {Los Alamitos, CA, USA},
	author = {Joon Hao Chuah and B. Lok and E. Black},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.25},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {pediatrics;shape;virtual reality;sociology;statistics;training;tutorials},
	month = {apr},
	number = {04},
	pages = {539-546},
	publisher = {IEEE Computer Society},
	title = {Applying Mixed Reality to Simulate Vulnerable Populations for Practicing Clinical Communication Skills},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.25}}

@article{6479208,
	abstract = {Virtual walking, a fundamental task in Virtual Reality (VR), is greatly influenced by the locomotion interface being used, by the specificities of input and output devices, and by the way the virtual environment is represented. No matter how virtual walking is controlled, the generation of realistic virtual trajectories is absolutely required for some applications, especially those dedicated to the study of walking behaviors in VR, navigation through virtual places for architecture, rehabilitation and training. Previous studies focused on evaluating the realism of locomotion trajectories have mostly considered the result of the locomotion task (efficiency, accuracy) and its subjective perception (presence, cybersickness). Few focused on the locomotion trajectory itself, but in situation of geometrically constrained task. In this paper, we study the realism of unconstrained trajectories produced during virtual walking by addressing the following question: did the user reach his destination by virtually walking along a trajectory he would have followed in similar real conditions? To this end, we propose a comprehensive evaluation framework consisting on a set of trajectographical criteria and a locomotion model to generate reference trajectories. We consider a simple locomotion task where users walk between two oriented points in space. The travel path is analyzed both geometrically and temporally in comparison to simulated reference trajectories. In addition, we demonstrate the framework over a user study which considered an initial set of common and frequent virtual walking conditions, namely different input devices, output display devices, control laws, and visualization modalities. The study provides insight into the relative contributions of each condition to the overall realism of the resulting virtual trajectories.},
	address = {Los Alamitos, CA, USA},
	author = {G. Cirio and A. Olivier and M. Marchal and J. Pettre},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.34},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {trajectory;legged locomotion;logic gates;visualization;cameras;virtual environments;angular velocity},
	month = {apr},
	number = {04},
	pages = {671-680},
	publisher = {IEEE Computer Society},
	title = {Kinematic Evaluation of Virtual Walking Trajectories},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.34}}

@article{Cohen2007:Comparing-Interpersonal-Interactions,
	abstract = {Abstract---This paper provides key insights into the construction and evaluation of interpersonal simulators---systems that enable interpersonal interaction with virtual humans. Using an interpersonal simulator, two studies were conducted that compare interactions with a virtual human to interactions with a similar real human. The specific interpersonal scenario employed was that of a medical interview. Medical students interacted with either a virtual human simulating appendicitis or a real human pretending to have the same symptoms. In Study I (n &#x3D; 24), medical students elicited the same information from the virtual and real human, indicating that the content of the virtual and real interactions were similar. However, participants appeared less engaged and insincere with the virtual human. These behavioral differences likely stemmed from the virtual human&#x27;s limited expressive behavior. Study II (n &#x3D; 58) explored participant behavior using new measures. Nonverbal behavior appeared to communicate lower interest and a poorer attitude toward the virtual human. Some subjective measures of participant behavior yielded contradictory results, highlighting the need for objective, physically-based measures in future studies.},
	address = {Los Alamitos, CA, USA},
	author = {M. S. Cohen and R. Pauly and K. Johnsen and A. B. Raij and B. C. Lok and A. O. Stevens and P. Wagner and R. F. Dickerson and D. Lind and M. Duerson},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2007.1030},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {medical simulation;particle measurements;educational institutions;virtual reality;computational modeling;aerospace simulation;computer science;anthropometry;human computer interaction;user interfaces},
	month = {may},
	number = {03},
	pages = {443-457},
	publisher = {IEEE Computer Society},
	title = {Comparing Interpersonal Interactions with a Virtual Human to Those with a Real Human},
	volume = {13},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2007.1030}}

@article{10049730,
	abstract = {We present Monte-Carlo Redirected Walking (MCRDW), a gain selection algorithm for redirected walking. MCRDW applies the Monte-Carlo method to redirected walking by simulating a large number of simple virtual walks, then inversely applying redirection to the virtual paths. Different gain levels and directions are applied, producing differing physical paths. Each physical path is scored and the results used to select the best gain level and direction. We provide a simple example implementation and a simulation-based study for validation. In our study, when compared with the next best technique, MCRDW reduced incidence of boundary collisions by over 50% while reducing total rotation and position gain.},
	address = {Los Alamitos, CA, USA},
	author = {B. J. Congdon and A. Steed},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247093},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual environments;legged locomotion;monte carlo methods;target tracking;optimization;solid modeling;resists},
	month = {may},
	number = {05},
	pages = {2637-2646},
	publisher = {IEEE Computer Society},
	title = {Monte-Carlo Redirected Walking: Gain Selection Through Simulated Walks},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247093}}

@article{9382914,
	abstract = {Eye-tracking technology is being increasingly integrated into mixed reality devices. Although critical applications are being enabled, there are significant possibilities for violating user privacy expectations. We show that there is an appreciable risk of unique user identification even under natural viewing conditions in virtual reality. This identification would allow an app to connect a user&#x27;s personal ID with their work ID without needing their consent, for example. To mitigate such risks we propose a framework that incorporates gatekeeping via the design of the application programming interface and via software-implemented privacy mechanisms. Our results indicate that these mechanisms can reduce the rate of identification from as much as 85% to as low as 30%. The impact of introducing these mechanisms is less than 1.5$\,^{\circ}$ error in gaze position for gaze prediction. Gaze data streams can thus be made private while still allowing for gaze prediction, for example, during foveated rendering. Our approach is the first to support privacy-by-design in the flow of eye-tracking data within mixed reality use cases.},
	address = {Los Alamitos, CA, USA},
	author = {B. David-John and D. Hosfelt and K. Butler and E. Jain},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067787},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {mixed reality;privacy;training;tracking;gaze tracking;streaming media;rendering (computer graphics)},
	month = {may},
	number = {05},
	pages = {2555-2565},
	publisher = {IEEE Computer Society},
	title = {A privacy-preserving approach to streaming eye-tracking data},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067787}}

@article{10049660,
	abstract = {Virtual and mixed-reality (XR) technology has advanced significantly in the last few years and will enable the future of work, education, socialization, and entertainment. Eye-tracking data is required for supporting novel modes of interaction, animating virtual avatars, and implementing rendering or streaming optimizations. While eye tracking enables many beneficial applications in XR, it also introduces a risk to privacy by enabling re-identification of users. We applied privacy definitions of k-anonymity and plausible deniability (PD) to datasets of eye-tracking samples and evaluated them against the state-of-the-art differential privacy (DP) approach. Two VR datasets were processed to reduce identification rates while minimizing the impact on the performance of trained machine-learning models. Our results suggest that both PD and DP mechanisms produced practical privacy-utility trade-offs with respect to re-identification and activity classification accuracy, while k-anonymity performed best at retaining utility for gaze prediction.},
	address = {Los Alamitos, CA, USA},
	author = {B. David-John and K. Butler and E. Jain},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247048},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {decoding;computer architecture},
	month = {may},
	number = {05},
	pages = {2774-2784},
	publisher = {IEEE Computer Society},
	title = {Privacy-preserving datasets of eye-tracking samples with applications in XR},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247048}}

@article{DeFanti2008:Advances-in-the-Dynallax-Solid-State,
	abstract = {A solid-state dynamic parallax barrier autostereoscopic display mitigates some of the restrictions present in static barrier systems, such as fixed view-distance range, slow response to head movements, and fixed stereo operating mode. By dynamically varying barrier parameters in real time, viewers may move closer to the display and move faster laterally than with a static barrier system, and the display can switch between 3D and 2D modes by disabling the barrier on a per-pixel basis. Moreover, Dynallax can output four independent eye channels when two viewers are present, and both head-tracked viewers receive an independent pair of left-eye and right-eye perspective views based on their position in 3D space. The display device is constructed by using a dual-stacked LCD monitor where a dynamic barrier is rendered on the front display and a modulated virtual environment composed of two or four channels is rendered on the rear display. Dynallax was recently demonstrated in a small-scale head-tracked prototype system. This paper summarizes the concepts presented earlier, extends the discussion of various topics, and presents recent improvements to the system.},
	address = {Los Alamitos, CA, USA},
	author = {T. A. DeFanti and J. Leigh and A. Johnson and R. L. Kooima and D. J. Sandin and T. Peterka},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2007.70627},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional graphics and realism;three-dimensional displays},
	month = {may},
	number = {03},
	pages = {487-499},
	publisher = {IEEE Computer Society},
	title = {Advances in the Dynallax Solid-State Dynamic Parallax Barrier Autostereoscopic Visualization Display System},
	volume = {14},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2007.70627}}

@article{8643566,
	abstract = {Rendering in virtual reality (VR) requires substantial computational power to generate 90 frames per second at high resolution with good-quality antialiasing. The video data sent to a VR headset requires high bandwidth, achievable only on dedicated links. In this paper we explain how rendering requirements and transmission bandwidth can be reduced using a conceptually simple technique that integrates well with existing rendering pipelines. Every even-numbered frame is rendered at a lower resolution, and every odd-numbered frame is kept at high resolution but is modified in order to compensate for the previous loss of high spatial frequencies. When the frames are seen at a high frame rate, they are fused and perceived as high-resolution and high-frame-rate animation. The technique relies on the limited ability of the visual system to perceive high spatio-temporal frequencies. Despite its conceptual simplicity, correct execution of the technique requires a number of non-trivial steps: display photometric temporal response must be modeled, flicker and motion artifacts must be avoided, and the generated signal must not exceed the dynamic range of the display. Our experiments, performed on a high-frame-rate LCD monitor and OLED-based VR headsets, explore the parameter space of the proposed technique and demonstrate that its perceived quality is indistinguishable from full-resolution rendering. The technique is an attractive alternative to reprojection and resolution reduction of all frames.},
	address = {Los Alamitos, CA, USA},
	author = {G. Denes and K. Maruszczyk and G. Ash and R. K. Mantiuk},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898741},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);bandwidth;liquid crystal displays;switches;multiplexing;visualization;encoding},
	month = {may},
	number = {05},
	pages = {2072-2082},
	publisher = {IEEE Computer Society},
	title = {Temporal Resolution Multiplexing: Exploiting the limitations of spatio-temporal vision for more efficient VR rendering},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898741}}

@article{Deng2012:On-Delay-Adjustment-for-Dynamic,
	abstract = {Distributed virtual environments (DVEs) are becoming very popular in recent years, due to the rapid growing of applications, such as massive multiplayer online games (MMOGs). As the number of concurrent users increases, scalability becomes one of the major challenges in designing an interactive DVE system. One solution to address this scalability problem is to adopt a multi-server architecture. While some methods focus on the quality of partitioning the load among the servers, others focus on the efficiency of the partitioning process itself. However, all these methods neglect the effect of network delay among the servers on the accuracy of the load balancing solutions. As we show in this paper, the change in the load of the servers due to network delay would affect the performance of the load balancing algorithm. In this work, we conduct a formal analysis of this problem and discuss two efficient delay adjustment schemes to address the problem. Our experimental results show that our proposed schemes can significantly improve the performance of the load balancing algorithm with neglectable computation overhead.},
	address = {Los Alamitos, CA, USA},
	author = {Yunhua Deng and R. H. Lau},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.52},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;formal specification;formal verification;resource allocation;dynamic load balancing algorithm;distributed virtual environment;massive multiplayer online game;interactive dve system;multiserver architecture;load partitioning;network delay effect;server load;formal analysis;delay adjustment schemes;servers;silicon;load management;delay;heating;heuristic algorithms;load modeling;distributed virtual environments.;multi-server architecture;dynamic load balancing;delay adjustment},
	month = {apr},
	number = {04},
	pages = {529-537},
	publisher = {IEEE Computer Society},
	title = {On Delay Adjustment for Dynamic Load Balancing in Distributed Virtual Environments},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.52}}

@article{9714045,
	abstract = {In virtual reality, several manipulation techniques distort users&#x27; motions, for example to reach remote objects or increase precision. These techniques can become problematic when used with avatars, as they create a mismatch between the real performed action and the corresponding displayed action, which can negatively impact the sense of embodiment. In this paper, we propose to use a dual representation during anisomorphic interaction. A co-located representation serves as a spatial reference and reproduces the exact users&#x27; motion, while an interactive representation is used for distorted interaction. We conducted two experiments, investigating the use of dual representations with amplified motion (with the Go-Go technique) and decreased motion (with the PRISM technique). Two visual appearances for the interactive representation and the co-located one were explored. This exploratory study investigating dual representations in this context showed that people globally preferred having a single representation, but opinions diverged for the Go-Go technique. Also, we could not find significant differences in terms of performance. While interacting seemed more important than showing exact movements for agency during out-of-reach manipulation, people felt more in control of the realistic arm during close manipulation.},
	address = {Los Alamitos, CA, USA},
	author = {D. Dewez and L. Hoyet and A. Lecuyer and F. Argelaguet},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150501},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;task analysis;distortion;arms;visualization;three-dimensional displays;rubber},
	month = {may},
	number = {05},
	pages = {2047-2057},
	publisher = {IEEE Computer Society},
	title = {Do You Need Another Hand? Investigating Dual Body Representations During Anisomorphic 3D Manipulation},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150501}}

@article{9714116,
	abstract = {We propose a marker-based geometric framework for the high-frequency absolute 3D pose estimation of a binocular camera system by using the data captured during the exposure of a single rolling shutter scanline. In contrast to existing approaches enforcing temporal or motion models among scanlines (e.g. linear motion, constant velocity or small motion assumptions), we strive to determine the pose from instantaneous binocular capture (i.e. without using data from previous scanlines) and achieve drift-free pose estimation. We leverage the projective invariants of a novel rigid planar pattern, to both define a geometric reference as well as to determine 2D-3D correspondences from raw edge detection measurements from individual scanlines. Moreover, to tackle the ensuing multi-view estimation problem, achieve real-time operation, and minimize latency, we develop a pair of custom solvers leveraging our geometric setup. To mitigate sensitivity to noise, we propose a geometrically consistent measurement refinement mechanism. We verify the quality of our solvers by comparing with state of the art general solvers for absolute pose estimation of generalized cameras. Finally, we demonstrate the effectiveness of our proposed approach with an FPGA-based implementation which achieves a localization throughput of 129.6 KHz with a $1.5\ \mu \mathsf{s}$ latency.},
	address = {Los Alamitos, CA, USA},
	author = {J. Dibene and Y. Maldonado and L. Trujillo and E. Dunn},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150485},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cameras;pose estimation;three-dimensional displays;tracking;real-time systems;location awareness;calibration},
	month = {may},
	number = {05},
	pages = {2201-2211},
	publisher = {IEEE Computer Society},
	title = {Prepare for Ludicrous Speed: Marker-based Instantaneous Binocular Rolling Shutter Localization},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150485}}

@article{10049692,
	abstract = {Multi-user redirected walking (RDW) is widely used in large-scale virtual scenes because it allows more users to move synchronously in both virtual and physical environments. To ensure the freedom of virtual roaming, which can be used in various situations, some redirected algorithms have been dedicated to non-forward movements, such as vertical movement and jumping. However, the existing RDW methods still mainly focus on forward steps, ignoring sideward and backward steps, which are also common and necessary in virtual reality. RDW algorithms for non-forward steps can enrich the movement direction of users&#x27; virtual roaming and improve the realism of VR roaming. In addition, the non-forward motions have a larger curvature gain, which can be used to better reduce resets in RDW. Therefore, this paper presents a new method of multi-user redirected walking for supporting non-forward steps (FREE-RDW), which adds the options of sideward and backward steps to extend the VR locomotion. Our method adopts a user collision avoidance strategy based on optimal reciprocal collision avoidance (ORCA) and optimizes it into a linear programming problem to obtain the optimal velocity for users. Furthermore, our method uses APF to expose the user to repulsive forces from other users and walls, thus further reducing potential collisions and improving the utilization of physical space. The experiments show that our method performs well in virtual scenes with forward and non-forward steps. In addition, our method can significantly reduce the number of resets compared with reactive RDW algorithms such as DDB-RDW and APF-RDW in multi-user forward-step virtual scenes.},
	address = {Los Alamitos, CA, USA},
	author = {T. Dong and T. Gao and Y. Dong and L. Wang and K. Hu and J. Fan},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247107},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;prediction algorithms;collision avoidance;distortion;optimized production technology;gain measurement;force},
	month = {may},
	number = {05},
	pages = {2315-2325},
	publisher = {IEEE Computer Society},
	title = {FREE-RDW: A Multi-user Redirected Walking Method for Supporting Non-forward Steps},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247107}}

@article{8642346,
	abstract = {Virtual Environments (VEs) provide the opportunity to simulate a wide range of applications, from training to entertainment, in a safe and controlled manner. For applications which require realistic representations of real world environments, the VEs need to provide multiple, physically accurate sensory stimuli. However, simulating all the senses that comprise the human sensory system (HSS) is a task that requires significant computational resources. Since it is intractable to deliver all senses at the highest quality, we propose a resource distribution scheme in order to achieve an optimal perceptual experience within the given computational budgets. This paper investigates resource balancing for multi-modal scenarios composed of aural, visual and olfactory stimuli. Three experimental studies were conducted. The first experiment identified perceptual boundaries for olfactory computation. In the second experiment, participants ($N&#x3D;25$) were asked, across a fixed number of budgets ($M&#x3D;5$), to identify what they perceived to be the best visual, acoustic and olfactory stimulus quality for a given computational budget. Results demonstrate that participants tend to prioritize visual quality compared to other sensory stimuli. However, as the budget size is increased, users prefer a balanced distribution of resources with an increased preference for having smell impulses in the VE. Based on the collected data, a quality prediction model is proposed and its accuracy is validated against previously unused budgets and an untested scenario in a third and final experiment.},
	address = {Los Alamitos, CA, USA},
	author = {E. Doukakis and K. Debattista and T. Bashford-Rogers and A. Dhokia and A. Asadipour and A. Chalmers and C. Harvey},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898823},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {olfactory;visualization;computational modeling;resource management;mathematical model;auditory system;virtual environments},
	month = {may},
	number = {05},
	pages = {1865-1875},
	publisher = {IEEE Computer Society},
	title = {Audio-Visual-Olfactory Resource Allocation for Tri-modal Virtual Environments},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898823}}

@article{7829412,
	abstract = {Accommodative depth cues, a wide field of view, and ever-higher resolutions all present major hardware design challenges for near-eye displays. Optimizing a design to overcome one of these challenges typically leads to a trade-off in the others. We tackle this problem by introducing an all-in-one solution - a new wide field of view, gaze-tracked near-eye display for augmented reality applications. The key component of our solution is the use of a single see-through, varifocal deformable membrane mirror for each eye reflecting a display. They are controlled by airtight cavities and change the effective focal power to present a virtual image at a target depth plane which is determined by the gaze tracker. The benefits of using the membranes include wide field of view (100$\,^{\circ}$ diagonal) and fast depth switching (from 20 cm to infinity within 300 ms). Our subjective experiment verifies the prototype and demonstrates its potential benefits for near-eye see-through displays.},
	address = {Los Alamitos, CA, USA},
	author = {D. Dunn and C. Tippets and K. Torell and P. Kellnhofer and K. Aksit and P. Didyk and K. Myszkowski and D. Luebke and H. Fuchs},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657058},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {mirrors;image resolution;prototypes;optical imaging;holography;holographic optical components},
	month = {apr},
	number = {04},
	pages = {1322-1331},
	publisher = {IEEE Computer Society},
	title = {Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657058}}

@article{9714124,
	abstract = {This work introduces the first approach to video see-through mixed reality with full support for focus cues. By combining the flexibility to adjust the focus distance found in varifocal designs with the robustness to eye-tracking error found in multifocal designs, our novel display architecture reliably delivers focus cues over a large workspace. In particular, we introduce gaze-contingent layered displays and mixed reality focal stacks, an efficient representation of mixed reality content that lends itself to fast processing for driving layered displays in real time. We thoroughly evaluate this approach by building a complete end-to-end pipeline for capture, render, and display of focus cues in video see-through displays that uses only off-the-shelf hardware and compute components.},
	address = {Los Alamitos, CA, USA},
	author = {C. Ebner and S. Mori and P. Mohr and Y. Peng and D. Schmalstieg and G. Wetzstein and D. Kalkofen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150504},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;rendering (computer graphics);mixed reality;real-time systems;lenses;additives;pipelines},
	month = {may},
	number = {05},
	pages = {2256-2266},
	publisher = {IEEE Computer Society},
	title = {Video See-Through Mixed Reality with Focus Cues},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150504}}

@article{10049728,
	abstract = {This work introduces off-axis layered displays, the first approach to stereoscopic direct-view displays with support for focus cues. Off-axis layered displays combine a head-mounted display with a traditional direct-view display for encoding a focal stack and thus, for providing focus cues. To explore the novel display architecture, we present a complete processing pipeline for the real-time computation and post-render warping of off-axis display patterns. In addition, we build two prototypes using a head-mounted display in combination with a stereoscopic direct-view display, and a more widely available monoscopic direct-view display. In addition we show how extending off-axis layered displays with an attenuation layer and with eye-tracking can improve image quality. We thoroughly analyze each component in a technical evaluation and present examples captured through our prototypes.},
	address = {Los Alamitos, CA, USA},
	author = {C. Ebner and P. Mohr and T. Langlotz and Y. Peng and D. Schmalstieg and G. Wetzstein and D. Kalkofen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247077},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;two dimensional displays;resists;stereo image processing;prototypes;image color analysis;lenses},
	month = {may},
	number = {05},
	pages = {2816-2825},
	publisher = {IEEE Computer Society},
	title = {Off-Axis Layered Displays: Hybrid Direct-View/Near-Eye Mixed Reality with Focus Cues},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247077}}

@article{10049647,
	abstract = {Using a map in an unfamiliar environment requires identifying correspondences between elements of the map&#x27;s allocentric representation and elements in egocentric views. Aligning the map with the environment can be challenging. Virtual reality (VR) allows learning about unfamiliar environments in a sequence of egocentric views that correspond closely to the perspectives and views that are experienced in the actual environment. We compared three methods to prepare for localization and navigation tasks performed by teleoperating a robot in an office building: studying a floor plan of the building and two forms of VR exploration. One group of participants studied a building plan, a second group explored a faithful VR reconstruction of the building from a normal-sized avatar&#x27;s perspective, and a third group explored the VR from a giant-sized avatar&#x27;s perspective. All methods contained marked checkpoints. The subsequent tasks were identical for all groups. The self-localization task required indication of the approximate location of the robot in the environment. The navigation task required navigation between checkpoints. Participants took less time to learn with the giant VR perspective and with the floorplan than with the normal VR perspective. Both VR learning methods significantly outperformed the floorplan in the orientation task. Navigation was performed quicker after learning in the giant perspective compared to the normal perspective and the building plan. We conclude that the normal perspective and especially the giant perspective in VR are viable options for preparing for teleoperation in unfamiliar environments when a virtual model of the environment is available.},
	address = {Los Alamitos, CA, USA},
	author = {K. Eisentrager and J. Haubner and J. Brade and W. Einhauser and A. Bendixen and S. Winkler and P. Klimant and G. Jahn},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247052},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {navigation;buildings;task analysis;robots;avatars;floors;virtual environments},
	month = {may},
	number = {05},
	pages = {2220-2229},
	publisher = {IEEE Computer Society},
	title = {Evaluating the Effects of Virtual Reality Environment Learning on Subsequent Robot Teleoperation in an Unfamiliar Building},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247052}}

@article{Ellis2007:Demand-Characteristics-in-Assessing,
	abstract = {Abstract---The experience of motion sickness in a virtual environment may be measured through pre and postexperiment self-reported questionnaires such as the Simulator Sickness Questionnaire (SSQ). Although research provides converging evidence that users of virtual environments can experience motion sickness, there have been no controlled studies to determine to what extent the user&#x27;s subjective response is a demand characteristic resulting from pre and posttest measures. In this study, subjects were given either SSQ&#x27;s both pre and postvirtual environment immersion, or only postimmersion. This technique tested for contrast effects due to demand characteristics in which administration of the questionnaire itself suggested to the participant that the virtual environment may produce motion sickness. Results indicate that reports of motion sickness after immersion in a virtual environment are much greater when both pre and postquestionnaires are given than when only a posttest questionnaire is used. The implications for assessments of motion sickness in virtual environments are discussed.},
	address = {Los Alamitos, CA, USA},
	author = {S. R. Ellis and S. D. Young and B. D. Adelstein},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2007.1029},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual environment;testing;motion measurement;horses;motion control;human factors;educational institutions;mathematics;surgery;pain},
	month = {may},
	number = {03},
	pages = {422-428},
	publisher = {IEEE Computer Society},
	title = {Demand Characteristics in Assessing Motion Sickness in a Virtual Environment: Or Does Taking a Motion Sickness Questionnaire Make You Sick?},
	volume = {13},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2007.1029}}

@article{8260942,
	abstract = {Drones allow exploring dangerous or impassable areas safely from a distant point of view. However, flight control from an egocentric view in narrow or constrained environments can be challenging. Arguably, an exocentric view would afford a better overview and, thus, more intuitive flight control of the drone. Unfortunately, such an exocentric view is unavailable when exploring indoor environments. This paper investigates the potential of drone-augmented human vision, i.e., of exploring the environment and controlling the drone indirectly from an exocentric viewpoint. If used with a see-through display, this approach can simulate X-ray vision to provide a natural view into an otherwise occluded environment. The user&#x27;s view is synthesized from a three-dimensional reconstruction of the indoor environment using image-based rendering. This user interface is designed to reduce the cognitive load of the drone&#x27;s flight control. The user can concentrate on the exploration of the inaccessible space, while flight control is largely delegated to the drone&#x27;s autopilot system. We assess our system with a first experiment showing how drone-augmented human vision supports spatial understanding and improves natural interaction with the drone.},
	address = {Los Alamitos, CA, USA},
	author = {O. Erat and W. Isop and D. Kalkofen and D. Schmalstieg},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794058},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {drones;cameras;streaming media;three-dimensional displays;visualization;robot vision systems},
	month = {apr},
	number = {04},
	pages = {1437-1446},
	publisher = {IEEE Computer Society},
	title = {Drone-Augmented Human Vision: Exocentric Control for Drones Exploring Hidden Areas},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794058}}

@article{8998348,
	abstract = {Human gaze awareness is important for social and collaborative interactions. Recent technological advances in augmented reality (AR) displays and sensors provide us with the means to extend collaborative spaces with real-time dynamic AR indicators of one&#x27;s gaze, for example via three-dimensional cursors or rays emanating from a partner&#x27;s head. However, such gaze cues are only as useful as the quality of the underlying gaze estimation and the accuracy of the display mechanism. Depending on the type of the visualization, and the characteristics of the errors, AR gaze cues could either enhance or interfere with collaborations. In this paper, we present two human-subject studies in which we investigate the influence of angular and depth errors, target distance, and the type of gaze visualization on participants&#x27; performance and subjective evaluation during a collaborative task with a virtual human partner, where participants identified targets within a dynamically walking crowd. First, our results show that there is a significant difference in performance for the two gaze visualizations ray and cursor in conditions with simulated angular and depth errors: the ray visualization provided significantly faster response times and fewer errors compared to the cursor visualization. Second, our results show that under optimal conditions, among four different gaze visualization methods, a ray without depth information provides the worst performance and is rated lowest, while a combination of a ray and cursor with depth information is rated highest. We discuss the subjective and objective performance thresholds and provide guidelines for practitioners in this field.},
	address = {Los Alamitos, CA, USA},
	author = {A. Erickson and N. Norouzi and K. Kim and J. J. LaViola and G. Bruder and G. F. Welch},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973054},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;task analysis;collaboration;three-dimensional displays;real-time systems;gaze tracking;augmented reality},
	month = {may},
	number = {05},
	pages = {1934-1944},
	publisher = {IEEE Computer Society},
	title = {Effects of Depth Information on Visual Target Identification Task Performance in Shared Gaze Environments},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973054}}

@article{9393620,
	abstract = {Virtual reality (VR) video streaming (a.k.a., 360-degree video streaming) has been gaining popularity recently as a new form of multimedia providing the users with immersive viewing experience. However, the high volume of data for the 360-degree video frames creates significant bandwidth challenges. Research efforts have been made to reduce the bandwidth consumption by predicting and selectively streaming the user&#x27;s viewports. However, the existing approaches require historical user or video data and cannot be applied to live streaming, the most attractive VR streaming scenario. We develop a live viewport prediction mechanism, namely LiveObj, by detecting the objects in the video based on their semantics. The detected objects are then tracked to infer the user&#x27;s viewport in real time by employing a reinforcement learning algorithm. Our evaluations based on 48 users watching 10 VR videos demonstrate high prediction accuracy and significant bandwidth savings obtained by LiveObj. Also, LiveObj achieves real-time performance with low processing delays, meeting the requirement of live VR streaming.},
	address = {Los Alamitos, CA, USA},
	author = {X. Feng and Z. Bao and S. Wei},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067686},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {streaming media;bandwidth;real-time systems;semantics;trajectory;sports;resists},
	month = {may},
	number = {05},
	pages = {2736-2745},
	publisher = {IEEE Computer Society},
	title = {LiveObj: Object Semantics-based Viewport Prediction for Live Mobile Virtual Reality Streaming},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067686}}

@article{10049755,
	abstract = {In this study, we establish a much-needed baseline for evaluating eye tracking interactions using an eye tracking enabled Meta Quest 2 VR headset with 30 participants. Each participant went through 1098 targets using multiple conditions representative of AR/VR targeting and selecting tasks, including both traditional standards and those more aligned with AR/VR interactions today. We use circular white world-locked targets, and an eye tracking system with sub-1-degree mean accuracy errors running at approximately 90Hz. In a targeting and button press selection task, we, by design, compare completely unadjusted, cursor-less, eye tracking with controller and head tracking, which both had cursors. Across all inputs, we presented targets in a configuration similar to the ISO 9241--9 reciprocal selection task and another format with targets more evenly distributed near the center. Targets were laid out either flat on a plane or tangent to a sphere and rotated toward the user. Even though we intended this to be a baseline study, we see unmodified eye tracking, without any form of a cursor, or feedback, outperformed the head by 27.9% and performed comparably to the controller (5.63% decrease) in throughput. Eye tracking had improved subjective ratings relative to head in Ease of Use, Adoption, and Fatigue (66.4%, 89.8%, and 116.1 % improvements, respectively) and had similar ratings relative to the controller (reduction by 4.2%, 8.9%, and 5.2% respectively). Eye tracking had a higher miss percentage than controller and head (17.3% vs 4.7% vs 7.2% respectively). Collectively, the results of this baseline study serve as a strong indicator that eye tracking, with even minor sensible interaction design modifications, has tremendous potential in reshaping interactions in next-generation AR/VR head mounted displays.},
	address = {Los Alamitos, CA, USA},
	author = {A. S. Fernandes and T. Murdison and M. J. Proulx},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247058},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {gaze tracking;target tracking;task analysis;visualization;throughput;performance evaluation;three-dimensional displays},
	month = {may},
	number = {05},
	pages = {2269-2279},
	publisher = {IEEE Computer Society},
	title = {Leveling the Playing Field: A Comparative Reevaluation of Unmodified Eye Tracking as an Input and Interaction Modality for VR},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247058}}

@article{10049704,
	abstract = {The recent pandemic, war, and oil crises have caused many to reconsider their need to travel for education, training, and meetings. Providing assistance and training remotely has thus gained importance for many applications, from industrial maintenance to surgical telemonitoring. Current solutions such as video conferencing platforms lack essential communication cues such as spatial referencing, which negatively impacts both time completion and task performance. Mixed Reality (MR) offers opportunities to improve remote assistance and training, as it opens the way to increased spatial clarity and large interaction space. We contribute a survey of remote assistance and training in MR environments through a systematic literature review to provide a deeper understanding of current approaches, benefits and challenges. We analyze 62 articles and contextualize our findings along a taxonomy based on degree of collaboration, perspective sharing, MR space symmetry, time, input and output modality, visual display, and application domain. We identify the main gaps and opportunities in this research area, such as exploring collaboration scenarios beyond one-expert-to-one-trainee, enabling users to move across the reality-virtuality spectrum during a task, or exploring advanced interaction techniques that resort to hand or eye tracking. Our survey informs and helps researchers in different domains, including maintenance, medicine, engineering, or education, build and evaluate novel MR approaches to remote training and assistance. All supplemental materials are available at https://augmented-perception.org/publications/2023-training-survey.html.},
	address = {Los Alamitos, CA, USA},
	author = {C. G. Fidalgo and Y. Yan and H. Cho and M. Sousa and D. Lindlbauer and J. Jorge},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247081},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {training;virtual reality;collaboration;mixed reality;task analysis;maintenance engineering;visualization},
	month = {may},
	number = {05},
	pages = {2291-2303},
	publisher = {IEEE Computer Society},
	title = {A Survey on Remote Assistance and Training in Mixed Reality Environments},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247081}}

@article{Finkelstein2012:Impossible-Spaces:-Maximizing,
	abstract = {Walking is only possible within immersive virtual environments that fit inside the boundaries of the user&#x27;s physical workspace. To reduce the severity of the restrictions imposed by limited physical area, we introduce &quot;impossible spaces,&quot; a new design mechanic for virtual environments that wish to maximize the size of the virtual environment that can be explored with natural locomotion. Such environments make use of self-overlapping architectural layouts, effectively compressing comparatively large interior environments into smaller physical areas. We conducted two formal user studies to explore the perception and experience of impossible spaces. In the first experiment, we showed that reasonably small virtual rooms may overlap by as much as 56% before users begin to detect that they are in an impossible space, and that the larger virtual rooms that expanded to maximally fill our available 9.14m  9.14m workspace may overlap by up to 31%. Our results also demonstrate that users perceive distances to objects in adjacent overlapping rooms as if the overall space was uncompressed, even at overlap levels that were overtly noticeable. In our second experiment, we combined several well-known redirection techniques to string together a chain of impossible spaces in an expansive outdoor scene. We then conducted an exploratory analysis of users&#x27; verbal feedback during exploration, which indicated that impossible spaces provide an even more powerful illusion when users are naive to the manipulation.},
	address = {Los Alamitos, CA, USA},
	author = {S. Finkelstein and Z. Lipps and E. A. Suma and D. M. Krum and M. Bolas},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.47},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;feedback;gait analysis;human computer interaction;illusion;natural walking;self-overlapping architectural layout;immersive virtual environment;user physical workspace;impossible spaces;design mechanic;natural locomotion;virtual room;adjacent overlapping room;redirection technique;expansive outdoor scene;users verbal feedback;virtual environments;legged locomotion;buildings;educational institutions;estimation;layout;space exploration;redirection.;virtual environments;perception;spatial illusions},
	month = {apr},
	number = {04},
	pages = {555-564},
	publisher = {IEEE Computer Society},
	title = {Impossible Spaces: Maximizing Natural Walking in Virtual Environments with Self-Overlapping Architecture},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.47}}

@article{7829420,
	abstract = {Modularity, modifiability, reusability, and API usability are important software qualities that determine the maintainability of software architectures. Virtual, Augmented, and Mixed Reality (VR, AR, MR) systems, modern computer games, as well as interactive human-robot systems often include various dedicated input-, output-, and processing subsystems. These subsystems collectively maintain a real-time simulation of a coherent application state. The resulting interdependencies between individual state representations, mutual state access, overall synchronization, and flow of control implies a conceptual close coupling whereas software quality asks for a decoupling to develop maintainable solutions. This article presents five semantics-based software techniques that address this contradiction: Semantic grounding, code from semantics, grounded actions, semantic queries, and decoupling by semantics. These techniques are applied to extend the well-established entity-component-system (ECS) pattern to overcome some of this pattern&#x27;s deficits with respect to the implied state access. A walk-through of central implementation aspects of a multimodal (speech and gesture) VR-interface is used to highlight the techniques&#x27; benefits. This use-case is chosen as a prototypical example of complex architectures with multiple interacting subsystems found in many VR, AR and MR architectures. Finally, implementation hints are given, lessons learned regarding maintainability pointed-out, and performance implications discussed.},
	address = {Los Alamitos, CA, USA},
	author = {M. Fischbach and D. Wiebusch and M. Latoschik},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657098},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {semantics;couplings;software quality;computer architecture;data models;virtual environments},
	month = {apr},
	number = {04},
	pages = {1342-1351},
	publisher = {IEEE Computer Society},
	title = {Semantic Entity-Component State Management Techniques to Enhance Software Quality for Multimodal VR-Systems},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657098}}

@article{7384536,
	abstract = {When moving through a tracked immersive virtual environment, it is sometimes useful to deviate from the normal one-to-one mapping of real to virtual motion. One option is the application of rotation gain, where the virtual rotation of a user around the vertical axis is amplified or reduced by a factor. Previous research in head-mounted display environments has shown that rotation gain can go unnoticed to a certain extent, which is exploited in redirected walking techniques. Furthermore, it can be used to increase the effective field of regard in projection systems. However, rotation gain has never been studied in CAVE systems, yet. In this work, we present an experiment with 87 participants examining the effects of rotation gain in a CAVE-like virtual environment. The results show no significant effects of rotation gain on simulator sickness, presence, or user performance in a cognitive task, but indicate that there is a negative influence on spatial knowledge especially for inexperienced users. In secondary results, we could confirm results of previous work and demonstrate that they also hold for CAVE environments, showing a negative correlation between simulator sickness and presence, cognitive performance and spatial knowledge, a positive correlation between presence and spatial knowledge, a mitigating influence of experience with 3D applications and previous CAVE exposure on simulator sickness, and a higher incidence of simulator sickness in women.},
	address = {Los Alamitos, CA, USA},
	author = {S. Freitag and B. Weyers and T. W. Kuhlen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518298},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;head;virtual environments;visualization;tracking;space exploration;delays},
	month = {apr},
	number = {04},
	pages = {1462-1471},
	publisher = {IEEE Computer Society},
	title = {Examining Rotation Gain in CAVE-like Virtual Environments},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518298}}

@article{8998305,
	abstract = {In Virtual Reality, a number of studies have been conducted to assess the influence of avatar appearance, avatar control and user point of view on the Sense of Embodiment (SoE) towards a virtual avatar. However, such studies tend to explore each factor in isolation. This paper aims to better understand the inter-relations among these three factors by conducting a subjective matching experiment. In the presented experiment (n&#x3D;40), participants had to match a given ``optimal'' SoE avatar configuration (realistic avatar, full-body motion capture, first-person point of view), starting by a ``minimal'' SoE configuration (minimal avatar, no control, third-person point of view), by iteratively increasing the level of each factor. The choices of the participants provide insights about their preferences and perception over the three factors considered. Moreover, the subjective matching procedure was conducted in the context of four different interaction tasks with the goal of covering a wide range of actions an avatar can do in a VE. The paper also describes a baseline experiment (n&#x3D;20) which was used to define the number and order of the different levels for each factor, prior to the subjective matching experiment (e.g. different degrees of realism ranging from abstract to personalised avatars for the visual appearance). The results of the subjective matching experiment show that point of view and control levels were consistently increased by users before appearance levels when it comes to enhancing the SoE. Second, several configurations were identified with equivalent SoE as the one felt in the optimal configuration, but vary between the tasks. Taken together, our results provide valuable insights about which factors to prioritize in order to enhance the SoE towards an avatar in different tasks, and about configurations which lead to fulfilling SoE in VE.},
	address = {Los Alamitos, CA, USA},
	author = {R. Fribourg and F. Argelaguet and A. Lecuyer and L. Hoyet},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973077},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;task analysis;animation;visualization;three-dimensional displays;legged locomotion},
	month = {may},
	number = {05},
	pages = {2062-2072},
	publisher = {IEEE Computer Society},
	title = {Avatar and Sense of Embodiment: Studying the Relative Preference Between Appearance, Control and Point of View},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973077}}

@article{6777458,
	abstract = {Latency of interactive computer systems is a product of the processing, transport and synchronisation delays inherent to the components that create them. In a virtual environment (VE) system, latency is known to be detrimental to a user&#x27;s sense of immersion, physical performance and comfort level. Accurately measuring the latency of a VE system for study or optimisation, is not straightforward. A number of authors have developed techniques for characterising latency, which have become progressively more accessible and easier to use. In this paper, we characterise these techniques. We describe a simple mechanical simulator designed to simulate a VE with various amounts of latency that can be finely controlled (to within 3ms). We develop a new latency measurement technique called Automated Frame Counting to assist in assessing latency using high speed video (to within 1ms). We use the mechanical simulator to measure the accuracy of Steed&#x27;s and Di Luca&#x27;s measurement techniques, proposing improvements where they may be made. We use the methods to measure latency of a number of interactive systems that may be of interest to the VE engineer, with a significant level of confidence. All techniques were found to be highly capable however Steed&#x27;s Method is both accurate and easy to use without requiring specialised hardware.},
	address = {Los Alamitos, CA, USA},
	author = {S. Friston and A. Steed},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.30},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {feature extraction;cameras;estimation;measurement techniques;target tracking;delays},
	month = {apr},
	number = {04},
	pages = {616-625},
	publisher = {IEEE Computer Society},
	title = {Measuring Latency in Virtual Environments},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.30}}

@article{7383313,
	abstract = {Latency - the delay between a user&#x27;s action and the response to this action - is known to be detrimental to virtual reality. Latency is typically considered to be a discrete value characterising a delay, constant in time and space - but this characterisation is incomplete. Latency changes across the display during scan-out, and how it does so is dependent on the rendering approach used. In this study, we present an ultra-low latency real-time ray-casting renderer for virtual reality, implemented on an FPGA. Our renderer has a latency of ~1 ms from `tracker to pixel&#x27;. Its frameless nature means that the region of the display with the lowest latency immediately follows the scan-beam. This is in contrast to frame-based systems such as those using typical GPUs, for which the latency increases as scan-out proceeds. Using a series of high and low speed videos of our system in use, we confirm its latency of ~1 ms. We examine how the renderer performs when driving a traditional sequential scan-out display on a readily available HMO, the Oculus Rift OK2. We contrast this with an equivalent apparatus built using a GPU. Using captured human head motion and a set of image quality measures, we assess the ability of these systems to faithfully recreate the stimuli of an ideal virtual reality system - one with a zero latency tracker, renderer and display running at 1 kHz. Finally, we examine the results of these quality measures, and how each rendering approach is affected by velocity of movement and display persistence. We find that our system, with a lower average latency, can more faithfully draw what the ideal virtual reality system would. Further, we find that with low display persistence, the sensitivity to velocity of both systems is lowered, but that it is much lower for ours.},
	address = {Los Alamitos, CA, USA},
	author = {S. Friston and A. Steed and S. Tilbury and G. Gaydadjiev},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518079},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);hardware;virtual reality;image quality;delays;ray tracing;graphics processing units},
	month = {apr},
	number = {04},
	pages = {1377-1386},
	publisher = {IEEE Computer Society},
	title = {Construction and Evaluation of an Ultra Low Latency Frameless Renderer for VR},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518079}}

@article{9382921,
	abstract = {Mobile HMDs must sacrifice compute performance to achieve ergonomic and power requirements for extended use. Consequently, applications must either reduce rendering and simulation complexity - along with the richness of the experience - or offload complexity to a server. Within the context of edge-computing, a popular way to do this is through render streaming. Render streaming has been demonstrated for desktops and consoles. It has also been explored for HMDs. However, the latency requirements of head tracking make this application much more challenging. While mobile GPUs are not yet as capable as their desktop counterparts, we note that they are becoming more powerful and efficient. With the hard requirements of VR, it is worth continuing to investigate what schemes could optimally balance load, latency and quality. We propose an alternative we call edge-physics: streaming at the scene-graph level from a simulation running on edge-resources, analogous to cluster rendering. Scene streaming is not only straightforward, but compute and bandwidth efficient. The most demanding loops run locally. Jobs that hit the power-wall of mobile CPUs are off-loaded, while improving GPUs are leveraged, maximising compute utilisation. In this paper we create a prototypical implementation and evaluate its potential in terms of fidelity, bandwidth and performance. We show that an effective system which maintains high consistencies on typical edge-links can be easily built, but that some traditional concepts are not applicable, and a better understanding of the perception of motion is required to evaluate such a system comprehensively.},
	address = {Los Alamitos, CA, USA},
	author = {S. Friston and E. Griffith and D. Swapp and C. Lrondi and F. Jjunju and R. Ward and A. Marshall and A. Steed},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067757},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {streaming media;physics;bandwidth;quality of service;servers;visualization;rendering (computer graphics)},
	month = {may},
	number = {05},
	pages = {2691-2701},
	publisher = {IEEE Computer Society},
	title = {Quality of Service Impact on Edge Physics Simulations for VR},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067757}}

@article{6777427,
	abstract = {Projection-based Augmented Reality commonly employs a rigid substrate as the projection surface and does not support scenarios where the substrate can be reshaped. This investigation presents a projection-based AR system that supports deformable substrates that can be bent, twisted or folded. We demonstrate a new invisible marker embedded into a deformable substrate and an algorithm that identifies deformations to project geometrically correct textures onto the deformable object. The geometrically correct projection-based texture mapping onto a deformable marker is conducted using the measurement of the 3D shape through the detection of the retro-reflective marker on the surface. In order to achieve accurate texture mapping, we propose a marker pattern that can be partially recognized and can be registered to an object&#x27;s surface. The outcome of this work addresses a fundamental vision recognition challenge that allows the underlying material to change shape and be recognized by the system. Our evaluation demonstrated the system achieved geometrically correct projection under extreme deformation conditions. We envisage the techniques presented are useful for domains including prototype development, design, entertainment and information based AR systems.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Fujimoto and R. T. Smith and T. Taketomi and G. Yamamoto and J. Miyazaki and H. Kato and B. H. Thomas},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.25},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cameras;shape;substrates;surface treatment;three-dimensional displays;pattern recognition},
	month = {apr},
	number = {04},
	pages = {540-549},
	publisher = {IEEE Computer Society},
	title = {Geometrically-Correct Projection-Based Texture Mapping onto a Deformable Object},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.25}}

@article{8643571,
	abstract = {A recently developed light projection technique can add dynamic impressions to static real objects without changing their original visual attributes such as surface colors and textures. It produces illusory motion impressions in the projection target by projecting gray-scale motion-inducer patterns that selectively drive the motion detectors in the human visual system. Since a compelling illusory motion can be produced by an inducer pattern weaker than necessary to perfectly reproduce the shift of the original pattern on an object&#x27;s surface, the technique works well under bright environmental light conditions. However, determining the best deformation sizes is often difficult: When users try to add a large deformation, the deviation in the projected patterns from the original surface pattern on the target object becomes apparent. Therefore, to obtain satisfactory results, they have to spend much time and effort to manually adjust the shift sizes. Here, to overcome this limitation, we propose an optimization framework that adaptively retargets the displacement vectors based on a perceptual model. The perceptual model predicts the subjective inconsistency between a projected pattern and an original one by simulating responses in the human visual system. The displacement vectors are adaptively optimized so that the projection effect is maximized within the tolerable range predicted by the model. We extensively evaluated the perceptual model and optimization method through a psychophysical experiment as well as user studies.},
	address = {Los Alamitos, CA, USA},
	author = {T. Fukiage and T. Kawabe and S. Nishida},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898738},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {computational modeling;adaptation models;visualization;predictive models;surface texture;image quality;optimization},
	month = {may},
	number = {05},
	pages = {2061-2071},
	publisher = {IEEE Computer Society},
	title = {Perceptually Based Adaptive Motion Retargeting to Animate Real Objects by Light Projection},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898738}}

@article{9382916,
	abstract = {Theories of cognition inform our decisions when designing human-computer interfaces, and immersive systems enable us to examine these theories. This work explores the sensemaking process in an immersive environment through studying both internal and external user behaviors with a classical visualization problem: a visual comparison and clustering task. We developed an immersive system to perform a user study, collecting user behavior data from different channels: AR HMD for capturing external user interactions, functional near-infrared spectroscopy (fNIRS) for capturing internal neural sequences, and video for references. To examine sensemaking, we assessed how the layout of the interface (planar 2D vs. cylindrical 3D layout) and the challenge level of the task (low vs. high cognitive load) influenced the users&#x27; interactions, how these interactions changed over time, and how they influenced task performance. We also developed a visualization system to explore joint patterns among all the data channels. We found that increased interactions and cerebral hemodynamic responses were associated with more accurate performance, especially on cognitively demanding trials. The layout types did not reliably influence interactions or task performance. We discuss how these findings inform the design and evaluation of immersive systems, predict user performance and interaction, and offer theoretical insights about sensemaking from the perspective of embodied and distributed cognition.},
	address = {Los Alamitos, CA, USA},
	author = {A. Galati and R. Schoppa and A. Lu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067693},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;layout;cognition;data visualization;three-dimensional displays;tools;two dimensional displays},
	month = {may},
	number = {05},
	pages = {2714-2724},
	publisher = {IEEE Computer Society},
	title = {Exploring the SenseMaking Process through Interactions and fNIRS in Immersive Visualization},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067693}}

@article{10049658,
	abstract = {Virtual grasping is one of the most common and important interactions performed in a Virtual Environment (VE). Even though there has been substantial research using hand tracking methods exploring different ways of visualizing grasping, there are only a few studies that focus on handheld controllers. This gap in research is particularly crucial, since controllers remain the most used input modality in commercial Virtual Reality (VR). Extending existing research, we designed an experiment comparing three different grasping visualizations when users are interacting with virtual objects in immersive VR using controllers. We examine the following visualizations: the Auto-Pose (AP), where the hand is automatically adjusted to the object upon grasping; the Simple-Pose (SP), where the hand closes fully when selecting the object; and the Disappearing-Hand (DH), where the hand becomes invisible after selecting an object, and turns visible again after positioning it on the target. We recruited 38 participants in order to measure if and how their performance, sense of embodiment, and preference are affected. Our results show that while in terms of performance there is almost no significant difference in any of the visualizations, the perceived sense of embodiment is stronger with the AP, and is generally preferred by the users. Thus, this study incentivizes the inclusion of similar visualizations in relevant future research and VR experiences.},
	address = {Los Alamitos, CA, USA},
	author = {G. Ganias and C. Lougiakis and A. Katifori and M. Roussou and Y. Ioannidis and l. Ioannidis},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247039},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	month = {may},
	number = {05},
	pages = {2369-2378},
	publisher = {IEEE Computer Society},
	title = {Comparing Different Grasping Visualizations for Object Manipulation in VR using Controllers},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247039}}

@article{6479193,
	abstract = {Pie menus are a well-known technique for interacting with 2D environments and so far a large body of research documents their usage and optimizations. Yet, comparatively little research has been done on the usability of pie menus in immersive virtual environments (IVEs). In this paper we reduce this gap by presenting an implementation and evaluation of an extended hierarchical pie menu system for IVEs that can be operated with a six-degrees-of-freedom input device. Following an iterative development process, we first developed and evaluated a basic hierarchical pie menu system. To better understand how pie menus should be operated in IVEs, we tested this system in a pilot user study with 24 participants and focus on item selection. Regarding the results of the study, the system was tweaked and elements like check boxes, sliders, and color map editors were added to provide extended functionality. An expert review with five experts was performed with the extended pie menus being integrated into an existing VR application to identify potential design issues. Overall results indicated high performance and efficient design.},
	address = {Los Alamitos, CA, USA},
	author = {S. Gebhardt and S. Pick and F. Leithold and B. Hentschel and T. Kuhlen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.31},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {layout;usability;error analysis;context;performance evaluation;atmospheric measurements;particle measurements},
	month = {apr},
	number = {04},
	pages = {644-651},
	publisher = {IEEE Computer Society},
	title = {Extended Pie Menus for Immersive Virtual Environments},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.31}}

@article{8998401,
	abstract = {Directivity and gain in microphone array systems for hearing aids or hearable devices allow users to acoustically enhance the information of a source of interest. This source is usually positioned directly in front. This feature is called acoustic beamforming. The current study aimed to improve users&#x27; interactions with beamforming via a virtual prototyping approach in immersive virtual environments (VEs). Eighteen participants took part in experimental sessions composed of a calibration procedure and a selective auditory attention voice-pairing task. Eight concurrent speakers were placed in an anechoic environment in two virtual reality (VR) scenarios. The scenarios were a purely virtual scenario and a realistic 360$\,^{\circ}$ audio-visual recording. Participants were asked to find an individual optimal parameterization for three different virtual beamformers: (i) head-guided, (ii) eye gaze-guided, and (iii) a novel interaction technique called dual beamformer, where head-guided is combined with an additional hand-guided beamformer. None of the participants were able to complete the task without a virtual beamformer (i.e., in normal hearing condition) due to the high complexity introduced by the experimental design. However, participants were able to correctly pair all speakers using all three proposed interaction metaphors. Providing superhuman hearing abilities in the form of a dual acoustic beamformer guided by head and hand movements resulted in statistically significant improvements in terms of pairing time, suggesting the task-relevance of interacting with multiple points of interests.},
	address = {Los Alamitos, CA, USA},
	author = {M. Geronazzo and L. S. Vieira and N. Nilsson and J. Udesen and S. Serafin},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973059},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {auditory system;array signal processing;hearing aids;acoustics;task analysis;microphones;ear},
	month = {may},
	number = {05},
	pages = {1912-1922},
	publisher = {IEEE Computer Society},
	title = {Superhuman Hearing - Virtual Prototyping of Artificial Hearing: a Case Study on Interactions and Acoustic Beamforming},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973059}}

@article{7829415,
	abstract = {While nothing can be more vivid, immediate and real than our own sensorial experiences, emerging virtual reality technologies are playing with the possibility of being able to share someone else&#x27;s sensory reality. The Painter Project is a virtual environment where users see a video from a painter&#x27;s point of view in tandem with a tracked rendering of their own hand while they paint on a physical canvas. The end result is an experiment in superimposition of one experiential reality on top of another, hopefully opening a new window into an artist&#x27;s creative process. This explorative study tested this virtual environment on stimulating empathy and creativity. The findings indicate potential for this technology as a new expert-novice mentorship simulation.},
	address = {Los Alamitos, CA, USA},
	author = {L. Gerry},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657239},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;virtual environments;solid modeling;rubber;paints;creativity},
	month = {apr},
	number = {04},
	pages = {1418-1426},
	publisher = {IEEE Computer Society},
	title = {Paint with Me: Stimulating Creativity and Empathy While Painting with a Painter in Virtual Reality},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657239}}

@article{8260856,
	abstract = {The proliferation of high resolution and affordable virtual reality (VR) headsets is quickly making room-scale VR experiences available in our homes. Most VR experiences strive to achieve complete immersion by creating a disconnect from the real world. However, due to the lack of a standardized notification management system and minimal context awareness in VR, an immersed user may face certain situations such as missing an important phone call (digital scenario), tripping over wandering pets (physical scenario), or losing track of time (temporal scenario). In this paper, we present the results of 1) a survey across 61 VR users to understand common interruptions and scenarios that would benefit from some form of notifications; 2) a design exercise with VR professionals to explore possible notification methods; and 3) an empirical study on the noticeability and perception of 5 different VR interruption scenarios across 6 modality combinations (e.g., audio, visual, haptic, audio + haptic, visual + haptic, and audio + visual) implemented in Unity and presented using the HTC Vive headset. Finally, we combine key learnings from each of these steps along with participant feedback to present a set of observations and recommendations for notification design in VR.},
	address = {Los Alamitos, CA, USA},
	author = {S. Ghosh and L. Winston and N. Panchal and P. Kimura-Thollander and J. Hotnog and D. Cheong and G. Reyes and G. D. Abowd},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793698},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;haptic interfaces;vibrations;headphones;virtual environments;legged locomotion},
	month = {apr},
	number = {04},
	pages = {1447-1456},
	publisher = {IEEE Computer Society},
	title = {NotifiVR: Exploring Interruptions and Notifications in Virtual Reality},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793698}}

@article{8998352,
	abstract = {Through avatar embodiment in Virtual Reality (VR) we can achieve the illusion that an avatar is substituting our body: the avatar moves as we move and we see it from a first person perspective. However, self-identification, the process of identifying a representation as being oneself, poses new challenges because a key determinant is that we see and have agency in our own face. Providing control over the face is hard with current HMD technologies because face tracking is either cumbersome or error prone. However, limited animation is easily achieved based on speaking. We investigate the level of avatar enfacement, that is believing that a picture of a face is one&#x27;s own face, with three levels of facial animation: (i) one in which the facial expressions of the avatars are static, (ii) one in which we implement lip-sync motion and (iii) one in which the avatar presents lip-sync plus additional facial animations, with blinks, designed by a professional animator. We measure self-identification using a face morphing tool that morphs from the face of the participant to the face of a gender matched avatar. We find that self-identification on avatars can be increased through pre-baked animations even when these are not photorealistic nor look like the participant.},
	address = {Los Alamitos, CA, USA},
	author = {M. Gonzalez-Franco and A. Steed and S. Hoogendyk and E. Ofek},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973075},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;face;facial animation;resists;mirrors},
	month = {may},
	number = {05},
	pages = {2023-2029},
	publisher = {IEEE Computer Society},
	title = {Using Facial Animation to Increase the Enfacement Illusion and Avatar Self-Identification},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973075}}

@article{6777453,
	abstract = {We investigated how the properties of interactive virtual reality systems affect user behavior in full-body embodied interactions. Our experiment compared four interactive virtual reality systems using different display types (CAVE vs. HMD) and modes of locomotion (walking vs. joystick). Participants performed a perceptual-motor coordination task, in which they had to choose among a series of opportunities to pass through a gate that cycled open and closed and then board a moving train. Mode of locomotion, but not type of display, affected how participants chose opportunities for action. Both mode of locomotion and display affected performance when participants acted on their choices. We conclude that technological properties of virtual reality system (both display and mode of locomotion) significantly affected opportunities for action available in the environment (affordances) and discuss implications for design and practical applications of immersive interactive systems.},
	address = {Los Alamitos, CA, USA},
	author = {T. Y. Grechkin and J. M. Plumert and J. K. Kearney},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.18},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {logic gates;legged locomotion;interactive systems;virtual environments;tracking;psychology},
	month = {apr},
	number = {04},
	pages = {596-605},
	publisher = {IEEE Computer Society},
	title = {Dynamic Affordances in Embodied Interactive Systems: The Role of Display and Mode of Locomotion},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.18}}

@article{7383306,
	abstract = {A major training device used to train all Landing Signal Officers (LSOs) for several decades has been the Landing Signal Officer Trainer, Device 2H111. This simulator, located in Oceana, VA, is contained within a two story tall room; it consists of several large screens and a physical rendition of the actual instruments used by LSOs in their operational environment. The young officers who serve in this specialty will typically encounter this system for only a short period of formal instruction (six one-hour long sessions), leaving multiple gaps in training. While experience with 2H111 is extremely valuable for all LSO officers, the amount of time they can spend using this training device is undeniably too short. The need to provide LSOs with an unlimited number of training opportunities unrestricted by location and time, married with recent advancements in commercial off the shelf (COTS) immersive technologies, provided an ideal platform to create a lightweight training solution that would fill those gaps and extend beyond the capabilities currently offered in the 2H111 simulator. This paper details our efforts on task analysis, surveying of user domain, mapping of 2H111 training capabilities to new prototype system to ensure its support of major training objectives of 2H111, design and development of prototype training system, and a feasibility study that included tests of technical system performance and informal testing with trainees at the LSO Schoolhouse. The results achieved in this effort indicate that the time for LSO training to make the leap to immersive VR has decidedly come.},
	address = {Los Alamitos, CA, USA},
	author = {L. Greunke and A. Sadagic},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518098},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {training;prototypes;instruments;aircraft;object recognition;visualization;navigation},
	month = {apr},
	number = {04},
	pages = {1482-1491},
	publisher = {IEEE Computer Society},
	title = {Taking Immersive VR Leap in Training of Landing Signal Officers},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518098}}

@article{9714040,
	abstract = {In this paper we propose omnidirectional galvanic vestibular stimulation (GVS) to mitigate cybersickness in virtual reality applications. One of the most accepted theories indicates that Cybersickness is caused by the visually induced impression of ego motion while physically remaining at rest. As a result of this sensory mismatch, people associate negative symptoms with VR and sometimes avoid the technology altogether. To reconcile the two contradicting sensory perceptions, we investigate GVS to stimulate the vestibular canals behind our ears with low-current electrical signals that are specifically attuned to the visually displayed camera motion. We describe how to calibrate and generate the appropriate GVS signals in real-time for pre-recorded omnidirectional videos exhibiting ego-motion in all three spatial directions. For validation, we conduct an experiment presenting real-world 360$\,^{\circ}$ videos shot from a moving first-person perspective in a VR head-mounted display. Our findings indicate that GVS is able to significantly reduce discomfort for cybersickness-susceptible VR users, creating a deeper and more enjoyable immersive experience for many people.},
	address = {Los Alamitos, CA, USA},
	author = {C. Groth and J. Tauscher and N. Heesen and M. Hattenbach and S. Castillo and M. Magnor},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150506},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;games;cybersickness;streaming media;cameras;motion pictures;three-dimensional displays},
	month = {may},
	number = {05},
	pages = {2234-2244},
	publisher = {IEEE Computer Society},
	title = {Omnidirectional Galvanic Vestibular Stimulation in Virtual Reality},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150506}}

@article{10049696,
	abstract = {In this paper, we propose a wavelet-based video codec specifically designed for VR displays that enables real-time playback of high-resolution 360$\,^{\circ}$ videos. Our codec exploits the fact that only a fraction of the full 360$\,^{\circ}$ video frame is visible on the display at any time. To load and decode the video viewport-dependently in real time, we make use of the wavelet transform for intra- as well as inter-frame coding. Thereby, the relevant content is directly streamed from the drive, without the need to hold the entire frames in memory. With an average of 193 frames per second at 8192  8192 -pixel full-frame resolution, the conducted evaluation demonstrates that our codec&#x27;s decoding performance is up to 272% higher than that of the state-of-the-art video codecs H.265 and AV1 for typical VR displays. By means of a perceptual study, we further illustrate the necessity of high frame rates for a better VR experience. Finally, we demonstrate how our wavelet-based codec can also directly be used in conjunction with foveation for further performance increase.},
	address = {Los Alamitos, CA, USA},
	author = {C. Groth and S. Fricke and S. Castillo and M. Magnor},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247080},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {videos;wavelet transforms;decoding;image coding;transform coding;video codecs;encoding},
	month = {may},
	number = {05},
	pages = {2508-2516},
	publisher = {IEEE Computer Society},
	title = {Wavelet-Based Fast Decoding of 360$\,^{\circ}$ Videos},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247080}}

@article{10049631,
	abstract = {As virtual reality (VR) is typically designed in terms of visual experience, it poses major challenges for blind people to understand and interact with the environment. To address this, we propose a design space to explore how to augment objects and their behaviours in VR with a nonvisual audio representation. It intends to support designers in creating accessible experiences by explicitly considering alternative representations to visual feedback. To demonstrate its potential, we recruited 16 blind users and explored the design space under two scenarios in the context of boxing: understanding the location of objects (the opponent&#x27;s defensive stance) and their movement (opponent&#x27;s punches). We found that the design space enables the exploration of multiple engaging approaches for the auditory representation of virtual objects. Our findings depicted shared preferences but no one-size-fits-all solution, suggesting the need to understand the consequences of each design choice and their impact on the individual user experience.},
	address = {Los Alamitos, CA, USA},
	author = {J. Guerreiro and Y. Kim and R. Nogueira and S. Chung and A. Rodrigues and U. Oh},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247094},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {space exploration;blindness;visualization;virtual reality;virtual environments;haptic interfaces;games},
	month = {may},
	number = {05},
	pages = {2763-2773},
	publisher = {IEEE Computer Society},
	title = {The Design Space of the Auditory Representation of Objects and Their Behaviours in Virtual Reality for Blind People},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247094}}

@article{10049707,
	abstract = {lmmersive virtual reality (VR) technologies can produce powerful illusions of being in another place or inhabiting another body, and theories of presence and embodiment provide valuable guidance to designers of VR applications that use these illusions to ``take us elsewhere.'' However, an increasingly common design goal for VR experiences is to develop a deeper awareness of the internal landscape of one&#x27;s own body (i.e., interoceptive awareness); here, design guidelines and evaluative techniques are less clear. To address this, we present a methodology, including a reusable codebook, for adapting the five dimensions of the Multidimensional Assessment of Interoceptive Awareness (MAIA) conceptual framework to explore interoceptive awareness in VR experiences via qualitative interviews. We report results from a first exploratory study (n&#x3D;21) applying this method to understand the interoceptive experiences of users in a VR environment. The environment includes a guided body scan exercise with a motion-tracked avatar visible in a virtual mirror and an interactive visualization of a biometric signal detected via a heartbeat sensor. The results provide new insights on how this example VR experience might be refined to better support interoceptive awareness and how the methodology might continue to be refined for understanding other ``inward-facing'' VR experiences.},
	address = {Los Alamitos, CA, USA},
	author = {A. C. Haley and D. Thorpe and A. Pelletier and S. Yarosh and D. F. Keefe},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247074},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {biological control systems;pain;visualization;mirrors;behavioral sciences;heart beat;avatars},
	month = {may},
	number = {05},
	pages = {2557-2566},
	publisher = {IEEE Computer Society},
	title = {Inward VR: Toward a Qualitative Method for Investigating Interoceptive Awareness in VR},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247074}}

@article{8260968,
	abstract = {Spatial augmented reality (SAR) pursues realism in rendering materials and objects. To advance this goal, we propose a hybrid SAR (HySAR) that combines a projector with optical see-through head-mounted displays (OST-HMD). In an ordinary SAR scenario with co-located viewers, the viewers perceive the same virtual material on physical surfaces. In general, the material consists of two components: a view-independent (VI) component such as diffuse reflection, and a view-dependent (VD) component such as specular reflection. The VI component is static over viewpoints, whereas the VD should change for each viewpoint even if a projector can simulate only one viewpoint at one time. In HySAR, a projector only renders the static VI components. In addition, the OST-HMD renders the dynamic VD components according to the viewer&#x27;s current viewpoint. Unlike conventional SAR, the HySAR concept theoretically allows an unlimited number of co-located viewers to see the correct material over different viewpoints. Furthermore, the combination enhances the total dynamic range, the maximum intensity, and the resolution of perceived materials. With proof-of-concept systems, we demonstrate HySAR both qualitatively and quantitatively with real objects. First, we demonstrate HySAR by rendering synthetic material properties on a real object from different viewpoints. Our quantitative evaluation shows that our system increases the dynamic range by 2.24 times and the maximum intensity by 2.12 times compared to an ordinary SAR system. Second, we replicate the material properties of a real object by SAR and HySAR, and show that HySAR outperforms SAR in rendering VD specular components.},
	address = {Los Alamitos, CA, USA},
	author = {T. Hamasaki and Y. Itoh and Y. Hiroi and D. Iwai and M. Sugimoto},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793659},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	month = {apr},
	number = {04},
	pages = {1457-1466},
	publisher = {IEEE Computer Society},
	title = {HySAR: Hybrid Material Rendering by an Optical See-Through Head-Mounted Display with Spatial Augmented Reality Projection},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793659}}

@article{8676155,
	abstract = {We propose a varifocal occlusion technique for optical see-through head-mounted displays (OST-HMDs). Occlusion in OST-HMDs is a powerful visual cue that enables depth perception in augmented reality (AR). Without occlusion, virtual objects rendered by an OST-HMD appear semi-transparent and less realistic. A common occlusion technique is to use spatial light modulators (SLMs) to block incoming light rays at each pixel on the SLM selectively. However, most of the existing methods create an occlusion mask only at a single, fixed depth-typically at infinity. With recent advances in varifocal OST-HMDs, such traditional fixed-focus occlusion causes a mismatch in depth between the occlusion mask plane and the virtual object to be occluded, leading to an uncomfortable user experience with blurred occlusion masks. In this paper, we thus propose an OST-HMD system with varifocal occlusion capability: we physically slide a transmissive liquid crystal display (LCD) to optically shift the occlusion plane along the optical path so that the mask appears sharp and aligns to a virtual image at a given depth. Our solution has several benefits over existing varifocal occlusion methods: it is computationally less demanding and, more importantly, it is optically consistent, i.e., when a user loses focus on the corresponding virtual image, the mask again gets blurred consistently as the virtual image does. In the experiment, we build a proof-of-concept varifocal occlusion system implemented with a custom retinal projection display and demonstrate that the system can shift the occlusion plane to depths ranging from 25 cm to infinity.},
	address = {Los Alamitos, CA, USA},
	author = {T. Hamasaki and Y. Itoh},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899249},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {optical imaging;liquid crystal displays;cameras;adaptive optics;lenses;optical distortion;glass},
	month = {may},
	number = {05},
	pages = {1961-1969},
	publisher = {IEEE Computer Society},
	title = {Varifocal Occlusion for Optical See-Through Head-Mounted Displays using a Slide Occlusion Mask},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899249}}

@article{8260974,
	abstract = {Virtual reality often uses motion tracking to incorporate physical hand movements into interaction techniques for selection and manipulation of virtual objects. To increase realism and allow direct hand interaction, real-world physical objects can be aligned with virtual objects to provide tactile feedback and physical grasping. However, unless a physical space is custom configured to match a specific virtual reality experience, the ability to perfectly match the physical and virtual objects is limited. Our research addresses this challenge by studying methods that allow one physical object to be mapped to multiple virtual objects that can exist at different virtual locations in an egocentric reference frame. We study two such techniques: one that introduces a static translational offset between the virtual and physical hand before a reaching action, and one that dynamically interpolates the position of the virtual hand during a reaching motion. We conducted two experiments to assess how the two methods affect reaching effectiveness, comfort, and ability to adapt to the remapping techniques when reaching for objects with different types of mismatches between physical and virtual locations. We also present a case study to demonstrate how the hand remapping techniques could be used in an immersive game application to support realistic hand interaction while optimizing usability. Overall, the translational technique performed better than the interpolated reach technique and was more robust for situations with larger mismatches between virtual and physical objects.},
	address = {Los Alamitos, CA, USA},
	author = {D. T. Han and M. Suhail and E. D. Ragan},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794659},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;visualization;tactile sensors;virtual environments},
	month = {apr},
	number = {04},
	pages = {1467-1476},
	publisher = {IEEE Computer Society},
	title = {Evaluating Remapped Physical Reach for Hand Interactions with Passive Haptics in Virtual Reality},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794659}}

@article{8998140,
	abstract = {Semantic understanding of 3D environments is critical for both the unmanned system and the human involved virtual/augmented reality (VR/AR) immersive experience. Spatially-sparse convolution, taking advantage of the intrinsic sparsity of 3D point cloud data, makes high resolution 3D convolutional neural networks tractable with state-of-the-art results on 3D semantic segmentation problems. However, the exhaustive computations limits the practical usage of semantic 3D perception for VR/AR applications in portable devices. In this paper, we identify that the efficiency bottleneck lies in the unorganized memory access of the sparse convolution steps, i.e., the points are stored independently based on a predefined dictionary, which is inefficient due to the limited memory bandwidth of parallel computing devices (GPU). With the insight that points are continuous as 2D surfaces in 3D space, a chunk-based sparse convolution scheme is proposed to reuse the neighboring points within each spatially organized chunk. An efficient multi-layer adaptive fusion module is further proposed for employing the spatial consistency cue of 3D data to further reduce the computational burden. Quantitative experiments on public datasets demonstrate that our approach works 11 faster than previous approaches with competitive accuracy. By implementing both semantic and geometric 3D reconstruction simultaneously on a portable tablet device, we demo a foundation platform for immersive AR applications.},
	address = {Los Alamitos, CA, USA},
	author = {L. Han and T. Zheng and Y. Zhu and L. Xu and L. Fang},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973477},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;convolution;semantics;two dimensional displays;image segmentation;graphics processing units;solid modeling},
	month = {may},
	number = {05},
	pages = {2012-2022},
	publisher = {IEEE Computer Society},
	title = {Live Semantic 3D Perception for Immersive Augmented Reality},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973477}}

@article{10054238,
	abstract = {Utilizing haptic devices to enhance the immersive experience is a direct approach in virtual reality (VR) applications. Various studies develop haptic feedback using force, wind, and thermal mechanisms. However, most haptic devices simulate feedback in dry environments such as the living room, prairie, or city. Water-related environments are thus less explored, for example, rivers, beaches, and swimming pools. In this paper we present GroundFlow, a liquid-based haptic floor system for simulating fluid on the ground in VR. We discuss design considerations and propose a system architecture and interaction design. We conduct two user studies to assist in the design of a multiple-flow feedback mechanism, develop three applications to explore the potential uses of the mechanism, and consider the limitations and challenges thereof to inform VR developers and haptic practitioners.},
	address = {Los Alamitos, CA, USA},
	author = {P. Han and T. Wang and C. Chou},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247073},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;floors;foot;liquids;rivers;valves;water conservation},
	month = {may},
	number = {05},
	pages = {2670-2679},
	publisher = {IEEE Computer Society},
	title = {GroundFlow: Liquid-based Haptics for Simulating Fluid on the Ground in Virtual Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247073}}

@article{7384528,
	abstract = {To avoid simulator sickness and improve presence in immersive virtual environments (IVEs), high frame rates and low latency are required. In contrast, volume rendering applications typically strive for high visual quality that induces high computational load and, thus, leads to low frame rates. To evaluate this trade-off in IVEs, we conducted a controlled user study with 53 participants. Search and count tasks were performed in a CAVE with varying volume rendering conditions which are applied according to viewer position updates corresponding to head tracking. The results of our study indicate that participants preferred the rendering condition with continuous adjustment of the visual quality over an instantaneous adjustment which guaranteed for low latency and over no adjustment providing constant high visual quality but rather low frame rates. Within the continuous condition, the participants showed best task performance and felt less disturbed by effects of the visualization during movements. Our findings provide a good basis for further evaluations of how to accelerate volume rendering in IVEs according to user&#x27;s preferences.},
	address = {Los Alamitos, CA, USA},
	author = {C. Hanel and B. Weyers and B. Hentschel and T. W. Kuhlen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518338},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;rendering (computer graphics);virtual environments;data visualization;tracking;head},
	month = {apr},
	number = {04},
	pages = {1472-1481},
	publisher = {IEEE Computer Society},
	title = {Visual Quality Adjustment for Volume Rendering in a Head-Tracked Virtual Environment},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518338}}

@article{8998361,
	abstract = {Immersive environments have been successfully applied to a broad range of safety training in high-risk domains. However, very little research has used these systems to evaluate the risk-taking behavior of construction workers. In this study, we investigated the feasibility and usefulness of providing passive haptics in a mixed-reality environment to capture the risk-taking behavior of workers, identify at-risk workers, and propose injury-prevention interventions to counteract excessive risk-taking and risk-compensatory behavior. Within a mixed-reality environment in a CAVE-like display system, our subjects installed shingles on a (physical) sloped roof of a (virtual) two-story residential building on a morning in a suburban area. Through this controlled, within-subject experimental design, we exposed each subject to three experimental conditions by manipulating the level of safety intervention. Workers&#x27; subjective reports, physiological signals, psychophysical responses, and reactionary behaviors were then considered as promising measures of Presence. The results showed that our mixed-reality environment was a suitable platform for triggering behavioral changes under different experimental conditions and for evaluating the risk perception and risk-taking behavior of workers in a risk-free setting. These results demonstrated the value of immersive technology to investigate natural human factors.},
	address = {Los Alamitos, CA, USA},
	author = {S. Hasanzadeh and N. F. Polys and J. M. de la Garza},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973055},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {safety;haptic interfaces;training;virtual environments;physiology;human factors},
	month = {may},
	number = {05},
	pages = {2115-2125},
	publisher = {IEEE Computer Society},
	title = {Presence, Mixed Reality, and Risk-Taking Behavior: A Study in Safety Interventions},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973055}}

@article{8260917,
	abstract = {We propose a system that controls the spatial distribution of odors in an environment by generating electronically steerable ultrasound-driven narrow air flows. The proposed system is designed not only to remotely present a preset fragrance to a user, but also to provide applications that would be conventionally inconceivable, such as: 1) fetching the odor of a generic object placed at a location remote from the user and guiding it to his or her nostrils, or 2) nullifying the odor of an object near a user by carrying it away before it reaches his or her nostrils (Fig. 1). These are all accomplished with an ultrasound-driven air stream serving as an airborne carrier of fragrant substances. The flow originates from a point in midair located away from the ultrasound source and travels while accelerating and maintaining its narrow cross-sectional area. These properties differentiate the flow from conventional jet- or fan-driven flows and contribute to achieving a midair flow. In our system, we employed a phased array of ultrasound transducers so that the traveling direction of the flow could be electronically and instantaneously controlled. In this paper, we describe the physical principle of odor control, the system construction, and experiments conducted to evaluate remote fragrance presentation and fragrance tracking.},
	address = {Los Alamitos, CA, USA},
	author = {K. Hasegawa and L. Qiu and H. Shinoda},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794118},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {ultrasonic imaging;phased arrays;transducers;olfactory;acceleration;acoustics;acoustic beams},
	month = {apr},
	number = {04},
	pages = {1477-1485},
	publisher = {IEEE Computer Society},
	title = {Midair Ultrasound Fragrance Rendering},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794118}}

@article{10049693,
	abstract = {This paper presents a shadowless projection mapping system for interactive applications in which a target surface is frequently occluded from a projector with a user&#x27;s body. We propose a delay-free optical solution for this critical problem. Specifically, as the primary technical contribution, we apply a large format retrotransmissive plate to project images onto the target surface from wide viewing angles. We also tackle technical issues unique to the proposed shadowless principle. First, the retrotransmissive optics inevitably suffer from stray light, which leads to significant contrast degradation of the projected result. We propose to block the stray light by covering the retrotransmissive plate with a spatial mask. Because the mask reduces not only the stray light but the achievable luminance of the projected result, we develop a computational algorithm that determines the shape of the mask to balance the image quality. Second, we propose a touch sensing technique by leveraging the optically bidirectional property of the retrotransmissive plate to support interaction between the user and the projected contents on the target object. We implement a proof-of-concept prototype and validate the above-mentioned techniques through experiments.},
	address = {Los Alamitos, CA, USA},
	author = {K. Hiratani and D. Iwai and Y. Kageyama and P. Punpongsanon and T. Hiraki and K. Sato},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247104},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {optical imaging;optics;apertures;shape;optical sensors;adaptive optics;image quality},
	month = {may},
	number = {05},
	pages = {2280-2290},
	publisher = {IEEE Computer Society},
	title = {Shadowless Projection Mapping using Retrotransmissive Optics},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247104}}

@article{6479192,
	abstract = {Redirected walking algorithms imperceptibly rotate a virtual scene and scale movements to guide users of immersive virtual environment systems away from tracking area boundaries. These distortions ideally permit users to explore large and potentially unbounded virtual worlds while walking naturally through a physically limited space. Estimates of the physical space required to perform effective redirected walking have been based largely on the ability of humans to perceive the distortions introduced by redirected walking and have not examined the impact the overall steering strategy used. This work compares four generalized redirected walking algorithms, including Steer-to-Center, Steer-to-Orbit, Steer-to-Multiple-Targets and Steer-to-Multiple+Center. Two experiments are presented based on simulated navigation as well as live-user navigation carried out in a large immersive virtual environment facility. Simulations were conducted with both synthetic paths and previously-logged user data. Primary comparison metrics include mean and maximum distances from the tracking area center for each algorithm, number of wall contacts, and mean rates of redirection. Results indicated that Steer-to-Center out-performed all other algorithms relative to these metrics. Steer-to-Orbit also performed well in some circumstances.},
	address = {Los Alamitos, CA, USA},
	author = {E. Hodgson and E. Bachmann},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.28},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;orbits;navigation;algorithm design and analysis;space vehicles;visualization;tracking},
	month = {apr},
	number = {04},
	pages = {634-643},
	publisher = {IEEE Computer Society},
	title = {Comparing Four Approaches to Generalized Redirected Walking: Simulation and Live User Data},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.28}}

@article{6777456,
	abstract = {Redirected walking algorithms imperceptibly rotate a virtual scene about users of immersive virtual environment systems in order to guide them away from tracking area boundaries. Ideally, these distortions permit users to explore large unbounded virtual worlds while walking naturally within a physically limited space. Many potential virtual worlds are composed of corridors, passageways, or aisles. Assuming users are not expected to walk through walls or other objects within the virtual world, these constrained worlds limit the directions of travel and as well as the number of opportunities to change direction. The resulting differences in user movement characteristics within the physical world have an impact on redirected walking algorithm performance. This work presents a comparison of generalized RDW algorithm performance within a constrained virtual world. In contrast to previous studies involving unconstrained virtual worlds, experimental results indicate that the steer-to-orbit keeps users in a smaller area than the steer-to-center algorithm. Moreover, in comparison to steer-to-center, steer-to-orbit is shown to reduce potential wall contacts by over 29%.},
	address = {Los Alamitos, CA, USA},
	author = {E. Hodgson and E. Bachmann and T. Thrash},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.34},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;navigation;orbits;rendering (computer graphics);tracking;extraterrestrial measurements;virtual environments},
	month = {apr},
	number = {04},
	pages = {579-587},
	publisher = {IEEE Computer Society},
	title = {Performance of Redirected Walking Algorithms in a Constrained Virtual World},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.34}}

@article{Hollerer2008:Heads-Up-and-Camera-Down:,
	abstract = {Anywhere Augmentation pursues the goal of lowering the initial investment of time and money necessary to participate in mixed reality work, bridging the gap between researchers in the field and regular computer users. Our paper contributes to this goal by introducing the GroundCam, a cheap tracking modality with no significant setup necessary. By itself, the GroundCam provides high frequency, high resolution relative position information similar to an inertial navigation system, but with significantly less drift. We present the design and implementation of the GroundCam, analyze the impact of several design and run-time factors on tracking accuracy, and consider the implications of extending our GroundCam to different hardware configurations. Motivated by the performance analysis, we developed a hybrid tracker that couples the GroundCam with a wide area tracking modality via a complementary Kalman filter, resulting in a powerful base for indoor and outdoor mobile mixed reality work. To conclude, the performance of the hybrid tracker and its utility within mixed reality applications is discussed.},
	address = {Los Alamitos, CA, USA},
	author = {T. H{\"o}llerer and S. DiVerdi},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2008.26},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;motion;tracking;motion},
	month = {may},
	number = {03},
	pages = {500-512},
	publisher = {IEEE Computer Society},
	title = {Heads Up and Camera Down: A Vision-Based Tracking Modality for Mobile Mixed Reality},
	volume = {14},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2008.26}}

@article{8643434,
	abstract = {We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. We also find that there exists a latency between eye movements and head movements. SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.},
	address = {Los Alamitos, CA, USA},
	author = {Z. Hu and C. Zhang and S. Li and G. Wang and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899187},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {head;solid modeling;predictive models;visualization;computational modeling;rendering (computer graphics);navigation},
	month = {may},
	number = {05},
	pages = {2002-2010},
	publisher = {IEEE Computer Society},
	title = {SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899187}}

@article{8998375,
	abstract = {We conduct novel analyses of users&#x27; gaze behaviors in dynamic virtual scenes and, based on our analyses, we present a novel CNN-based model called DGaze for gaze prediction in HMD-based applications. We first collect 43 users&#x27; eye tracking data in 5 dynamic scenes under free-viewing conditions. Next, we perform statistical analysis of our data and observe that dynamic object positions, head rotation velocities, and salient regions are correlated with users&#x27; gaze positions. Based on our analysis, we present a CNN-based model (DGaze) that combines object position sequence, head velocity sequence, and saliency features to predict users&#x27; gaze positions. Our model can be applied to predict not only realtime gaze positions but also gaze positions in the near future and can achieve better performance than prior method. In terms of realtime prediction, DGaze achieves a 22.0% improvement over prior method in dynamic scenes and obtains an improvement of 9.5% in static scenes, based on using the angular distance as the evaluation metric. We also propose a variant of our model called DGaze_ET that can be used to predict future gaze positions with higher precision by combining accurate past gaze data gathered using an eye tracker. We further analyze our CNN architecture and verify the effectiveness of each component in our model. We apply DGaze to gaze-contingent rendering and a game, and also present the evaluation results from a user study.},
	address = {Los Alamitos, CA, USA},
	author = {Z. Hu and S. Li and C. Zhang and K. Yi and G. Wang and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973473},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {predictive models;gaze tracking;solid modeling;head;analytical models;data models;rendering (computer graphics)},
	month = {may},
	number = {05},
	pages = {1902-1911},
	publisher = {IEEE Computer Society},
	title = {DGaze: CNN-Based Gaze Prediction in Dynamic Scenes},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973473}}

@article{9382883,
	abstract = {Human visual attention in immersive virtual reality (VR) is key for many important applications, such as content design, gaze-contingent rendering, or gaze-based interaction. However, prior works typically focused on free-viewing conditions that have limited relevance for practical applications. We first collect eye tracking data of 27 participants performing a visual search task in four immersive VR environments. Based on this dataset, we provide a comprehensive analysis of the collected data and reveal correlations between users&#x27; eye fixations and other factors, i.e. users&#x27; historical gaze positions, task-related objects, saliency information of the VR content, and users&#x27; head rotation velocities. Based on this analysis, we propose FixationNet - a novel learning-based model to forecast users&#x27; eye fixations in the near future in VR. We evaluate the performance of our model for free-viewing and task-oriented settings and show that it outperforms the state of the art by a large margin of 19.8% (from a mean error of 2.93$\,^{\circ}$ to 2.35$\,^{\circ}$) in free-viewing and of 15.1% (from 2.05$\,^{\circ}$ to 1.74$\,^{\circ}$) in task-oriented situations. As such, our work provides new insights into task-oriented attention in virtual environments and guides future work on this important topic in VR research.},
	address = {Los Alamitos, CA, USA},
	author = {Z. Hu and A. Bulling and S. Li and G. Wang},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067779},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;task analysis;solid modeling;predictive models;virtual environments;computational modeling;two dimensional displays},
	month = {may},
	number = {05},
	pages = {2681-2690},
	publisher = {IEEE Computer Society},
	title = {FixationNet: Forecasting Eye Fixations in Task-Oriented Virtual Environments},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067779}}

@article{9382896,
	abstract = {Image-based relighting, projector compensation and depth/normal reconstruction are three important tasks of projector-camera systems (ProCams) and spatial augmented reality (SAR). Although they share a similar pipeline of finding projector-camera image mappings, in tradition, they are addressed independently, sometimes with different prerequisites, devices and sampling images. In practice, this may be cumbersome for SAR applications to address them one-by-one. In this paper, we propose a novel end-to-end trainable model named DeProCams to explicitly learn the photometric and geometric mappings of ProCams, and once trained, DeProCams can be applied simultaneously to the three tasks. DeProCams explicitly decomposes the projector-camera image mappings into three subprocesses: shading attributes estimation, rough direct light estimation and photorealistic neural rendering. A particular challenge addressed by DeProCams is occlusion, for which we exploit epipolar constraint and propose a novel differentiable projector direct light mask. Thus, it can be learned end-to-end along with the other modules. Afterwards, to improve convergence, we apply photometric and geometric constraints such that the intermediate results are plausible. In our experiments, DeProCams shows clear advantages over previous arts with promising quality and meanwhile being fully differentiable. Moreover, by solving the three tasks in a unified model, DeProCams waives the need for additional optical devices, radiometric calibrations and structured light.},
	address = {Los Alamitos, CA, USA},
	author = {B. Huang and H. Ling},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067771},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cameras;calibration;task analysis;radiometry;image reconstruction;estimation;shape},
	month = {may},
	number = {05},
	pages = {2725-2735},
	publisher = {IEEE Computer Society},
	title = {DeProCams: Simultaneous Relighting, Compensation and Shape Reconstruction for Projector-Camera Systems},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067771}}

@article{Swan:2007:Egocentric-Depth-Judgments,
	abstract = {Abstract---A fundamental problem in optical, see-through augmented reality (AR) is characterizing how it affects the perception of spatial layout and depth. This problem is important because AR system developers need to both place graphics in arbitrary spatial relationships with real-world objects, and to know that users will perceive them in the same relationships. Furthermore, AR makes possible enhanced perceptual techniques that have no real-world equivalent, such as x-ray vision, where AR users are supposed to perceive graphics as being located behind opaque surfaces. This paper reviews and discusses protocols for measuring egocentric depth judgments in both virtual and augmented environments, and discusses the well-known problem of depth underestimation in virtual environments. It then describes two experiments that measured egocentric depth judgments in AR. Experiment I used a perceptual matching protocol to measure AR depth judgments at medium and far-field distances of 5 to 45 meters. The experiment studied the effects of upper versus lower visual field location, the x-ray vision condition, and practice on the task. The experimental findings include evidence for a switch in bias, from underestimating to overestimating the distance of AR-presented graphics, at \sim23 meters, as well as a quantification of how much more difficult the x-ray vision condition makes the task. Experiment II used blind walking and verbal report protocols to measure AR depth judgments at distances of 3 to 7 meters. The experiment examined real-world objects, real-world objects seen through the AR display, virtual objects, and combined real and virtual objects. The results give evidence that the egocentric depth of AR objects is underestimated at these distances, but to a lesser degree than has previously been found for most virtual reality environments. The results are consistent with previous studies that have implicated a restricted field-of-view, combined with an inability for observers to scan the ground plane in a near-to-far direction, as explanations for the observed depth underestimation.},
	address = {Los Alamitos, CA, USA},
	author = {J. Swan II and E. Kolstad and H. S. Smallman and A. Jones and M. A. Livingston},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2007.1035},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	month = {may},
	number = {03},
	pages = {429-442},
	publisher = {IEEE Computer Society},
	title = {Egocentric Depth Judgments in Optical, See-Through Augmented Reality},
	volume = {13},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2007.1035}}

@article{II2008:Usability-Engineering-for-Augmented,
	abstract = {A major challenge, and thus opportunity, in the field of human-computer interaction and specifically usability engineering is designing effective user interfaces for emerging technologies that have no established design guidelines or interaction metaphors or introduce completely new ways for users to perceive and interact with technology and the world around them. Clearly, augmented reality is one such emerging technology. We propose a usability engineering approach that employs user-based studies to inform design, by iteratively inserting a series of user-based studies into a traditional usability engineering lifecycle to better inform initial user interface designs. We present an exemplar user-based study conducted to gain insight into how users perceive text in outdoor augmented reality settings and to derive implications for design in outdoor augmented reality. We also describe &quot;lessons learned&quot; from our experiences conducting user-based studies as part of the design process.},
	address = {Los Alamitos, CA, USA},
	author = {J. Swan II and J. L. Gabbard},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2008.24},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {user interfaces;evaluation/methodology;graphical user interfaces;artificial;augmented;and virtual realities;user-centered design},
	month = {may},
	number = {03},
	pages = {513-525},
	publisher = {IEEE Computer Society},
	title = {Usability Engineering for Augmented Reality: Employing User-Based Studies to Inform Design},
	volume = {14},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2008.24}}

@article{Interrante2012:Redirecting-Walking-and-Driving,
	abstract = {Walking is the most natural form of locomotion for humans, and real walking interfaces have demonstrated their benefits for several navigation tasks. With recently proposed redirection techniques it becomes possible to overcome space limitations as imposed by tracking sensors or laboratory setups, and, theoretically, it is now possible to walk through arbitrarily large virtual environments. However, walking as sole locomotion technique has drawbacks, in particular, for long distances, such that even in the real world we tend to support walking with passive or active transportation for longer-distance travel. In this article we show that concepts from the field of redirected walking can be applied to movements with transportation devices. We conducted psychophysical experiments to determine perceptual detection thresholds for redirected driving, and set these in relation to results from redirected walking. We show that redirected walking-and-driving approaches can easily be realized in immersive virtual reality laboratories, e. g., with electric wheelchairs, and show that such systems can combine advantages of real walking in confined spaces with benefits of using vehiclebased self-motion for longer-distance travel.},
	address = {Los Alamitos, CA, USA},
	author = {V. Interrante and G. Bruder and L. Phillips and F. Steinicke},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.55},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;interactive devices;user interfaces;navigation task;natural navigation;immersive virtual environment;walking locomotion;driving locomotion;walking interface;redirection technique;passive transportation;active transportation;redirected walking;transportation device;perceptual detection threshold;redirected driving;electric wheelchair;vehicle-based self-motion;longer-distance travel;legged locomotion;wheelchairs;laboratories;visualization;navigation;vehicles;space exploration;motion perception.;redirected walking;redirected driving;natural locomotion;self&amp;#8211},
	month = {apr},
	number = {04},
	pages = {538-545},
	publisher = {IEEE Computer Society},
	title = {Redirecting Walking and Driving for Natural Navigation in Immersive Virtual Environments},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.55}}

@article{10054094,
	abstract = {This work introduces a perspective-corrected video see-through mixed-reality head-mounted display with edge-preserving occlusion and low-latency capabilities. To realize the consistent spatial and temporal composition of a captured real world containing virtual objects, we perform three essential tasks: 1) to reconstruct captured images so as to match the user&#x27;s view; 2) to occlude virtual objects with nearer real objects, to provide users with correct depth cues; and 3) to reproject the virtual and captured scenes to be matched and to keep up with users&#x27; head motions. Captured image reconstruction and occlusion-mask generation require dense and accurate depth maps. However, estimating these maps is computationally difficult, which results in longer latencies. To obtain an acceptable balance between spatial consistency and low latency, we rapidly generated depth maps by focusing on edge smoothness and disocclusion (instead of fully accurate maps), to shorten the processing time. Our algorithm refines edges via a hybrid method involving infrared masks and color-guided filters, and it fills disocclusions using temporally cached depth maps. Our system combines these algorithms in a two-phase temporal warping architecture based upon synchronized camera pairs and displays. The first phase of warping is to reduce registration errors between the virtual and captured scenes. The second is to present virtual and captured scenes that correspond with the user&#x27;s head motion. We implemented these methods on our wearable prototype and performed end-to-end measurements of its accuracy and latency. We achieved an acceptable latency due to head motion (less than 4 ms) and spatial accuracy (less than 0.1$\,^{\circ}$ in size and less than 0.3$\,^{\circ}$ in position) in our test environment. We anticipate that this work will help improve the realism of mixed reality systems.},
	address = {Los Alamitos, CA, USA},
	author = {A. Ishihara and H. Aga and Y. Ishihara and H. Ichikawa and H. Kaji and K. Kawasaki and D. Kobayashi and T. Kobayashi and K. Nishida and T. Hamasaki and H. Mori and Y. Morikubo},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247460},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {image reconstruction;head;cameras;image edge detection;rendering (computer graphics);prototypes;magnetic heads},
	month = {may},
	number = {05},
	pages = {2826-2836},
	publisher = {IEEE Computer Society},
	title = {Integrating Both Parallax and Latency Compensation into Video See-through Head-mounted Display},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247460}}

@article{7064856,
	abstract = {A critical requirement for AR applications with Optical See-Through Head-Mounted Displays (OST-HMD) is to project 3D information correctly into the current viewpoint of the user - more particularly, according to the user&#x27;s eye position. Recently-proposed interaction-free calibration methods [16], [17] automatically estimate this projection by tracking the user&#x27;s eye position, thereby freeing users from tedious manual calibrations. However, the method is still prone to contain systematic calibration errors. Such errors stem from eye-/HMD-related factors and are not represented in the conventional eye-HMD model used for HMD calibration. This paper investigates one of these factors - the fact that optical elements of OST-HMDs distort incoming world-light rays before they reach the eye, just as corrective glasses do. Any OST-HMD requires an optical element to display a virtual screen. Each such optical element has different distortions. Since users see a distorted world through the element, ignoring this distortion degenerates the projection quality. We propose a light-field correction method, based on a machine learning technique, which compensates the world-scene distortion caused by OST-HMD optics. We demonstrate that our method reduces the systematic error and significantly increases the calibration accuracy of the interaction-free calibration.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Itoh and G. Klinker},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391859},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {optical distortion;cameras;calibration;three-dimensional displays;optical imaging;adaptive optics;lenses},
	month = {apr},
	number = {04},
	pages = {471-480},
	publisher = {IEEE Computer Society},
	title = {Light-Field Correction for Spatial Calibration of Optical See-Through Head-Mounted Displays},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391859}}

@article{8676153,
	abstract = {We present a display for optical see-through near-eye displays based on light attenuation, a new paradigm that forms images by spatially subtracting colors of light. Existing optical see-through head-mounted displays (OST-HMDs) form virtual images in an additive manner-they optically combine the light from an embedded light source such as a microdisplay into the users&#x27; field of view (FoV). Instead, our light attenuation display filters the color of the real background light pixel-wise in the users&#x27; see-through view, resulting in an image as a spatial color filter. Our image formation is complementary to existing light-additive OST-HMDs. The core optical component in our system is a phase-only spatial light modulator (PSLM), a liquid crystal module that can control the phase of the light in each pixel. By combining PSLMs with polarization optics, our system realizes a spatially programmable color filter. In this paper, we introduce our optics design, evaluate the spatial color filter, consider applications including image rendering and FoV color control, and discuss the limitations of the current prototype.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Itoh and T. Langlotz and D. Iwai and K. Kiyokawa and T. Amano},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899229},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {image color analysis;lighting;attenuation;liquid crystal displays;optical imaging;optical attenuators;optical polarization},
	month = {may},
	number = {05},
	pages = {1951-1960},
	publisher = {IEEE Computer Society},
	title = {Light Attenuation Display: Subtractive See-Through Near-Eye Display via Spatial Color Filtering},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899229}}

@article{9383112,
	abstract = {Existing near-eye display designs struggle to balance between multiple trade-offs such as form factor, weight, computational requirements, and battery life. These design trade-offs are major obstacles on the path towards an all-day usable near-eye display. In this work, we address these trade-offs by, paradoxically, removing the display from near-eye displays. We present the beaming displays, a new type of near-eye display system that uses a projector and an all passive wearable headset. We modify an off-the-shelf projector with additional lenses. We install such a projector to the environment to beam images from a distance to a passive wearable headset. The beaming projection system tracks the current position of a wearable headset to project distortion-free images with correct perspectives. In our system, a wearable headset guides the beamed images to a user&#x27;s retina, which are then perceived as an augmented scene within a user&#x27;s field of view. In addition to providing the system design of the beaming display, we provide a physical prototype and show that the beaming display can provide resolutions as high as consumer-level near-eye displays. We also discuss the different aspects of the design space for our proposal.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Itoh and T. Kaminokado and K. Aksit},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067764},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {lenses;headphones;retina;proposals;optical imaging;optical distortion;mirrors},
	month = {may},
	number = {05},
	pages = {2659-2668},
	publisher = {IEEE Computer Society},
	title = {Beaming Displays},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067764}}

@article{7014259,
	abstract = {A simple and cost-efficient method for extending a projector&#x27;s depth-of-field (DOF) is proposed. By leveraging liquid lens technology, we can periodically modulate the focal length of a projector at a frequency that is higher than the critical flicker fusion (CFF) frequency. Fast periodic focal length modulation results in forward and backward sweeping of focusing distance. Fast focal sweep projection makes the point spread function (PSF) of each projected pixel integrated over a sweep period (IPSF; integrated PSF) nearly invariant to the distance from the projector to the projection surface as long as it is positioned within sweep range. This modulation is not perceivable by human observers. Once we compensate projection images for the IPSF, the projected results can be focused at any point within the range. Consequently, the proposed method requires only a single offline PSF measurement; thus, it is an open-loop process. We have proved the approximate invariance of the projector&#x27;s IPSF both numerically and experimentally. Through experiments using a prototype system, we have confirmed that the image quality of the proposed method is superior to that of normal projection with fixed focal length. In addition, we demonstrate that a structured light pattern projection technique using the proposed method can measure the shape of an object with large depth variances more accurately than normal projection techniques.},
	address = {Los Alamitos, CA, USA},
	author = {D. Iwai and S. Mihara and K. Sato},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391861},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {focusing;frequency modulation;lenses;cameras;computational modeling;semiconductor device measurement},
	month = {apr},
	number = {04},
	pages = {462-470},
	publisher = {IEEE Computer Society},
	title = {Extended Depth-of-Field Projector by Fast Focal Sweep Projection},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391861}}

@article{7383322,
	abstract = {Three-dimensional modeling has long been regarded as an ideal application for virtual reality (VR), but current VR-based 3D modeling tools suffer from two problems that limit creativity and applicability: (1) the lack of control for freehand modeling, and (2) the difficulty of starting from scratch. To address these challenges, we present Lift-Off, an immersive 3D interface for creating complex models with a controlled, handcrafted style. Artists start outside of VR with 2D sketches, which are then imported and positioned in VR. Then, using a VR interface built on top of image processing algorithms, 2D curves within the sketches are selected interactively and ``lifted'' into space to create a 3D scaffolding for the model. Finally, artists sweep surfaces along these curves to create 3D models. Evaluations are presented for both long-term users and for novices who each created a 3D sailboat model from the same starting sketch. Qualitative results are positive, with the visual style of the resulting models of animals and other organic subjects as well as architectural models matching what is possible with traditional fine art media. In addition, quantitative data from logging features built into the software are used to characterize typical tool use and suggest areas for further refinement of the interface.},
	address = {Los Alamitos, CA, USA},
	author = {B. Jackson and D. F. Keefe},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518099},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;solid modeling;computational modeling;user interfaces;surface treatment;art;shape},
	month = {apr},
	number = {04},
	pages = {1442-1451},
	publisher = {IEEE Computer Society},
	title = {Lift-Off: Using Reference Imagery and Freehand Sketching to Create 3D Models in VR},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518099}}

@article{7014300,
	abstract = {In this paper we present a novel framework for simultaneous detection of click action and estimation of occluded fingertip positions from egocentric viewed single-depth image sequences. For the detection and estimation, a novel probabilistic inference based on knowledge priors of clicking motion and clicked position is presented. Based on the detection and estimation results, we were able to achieve a fine resolution level of a bare hand-based interaction with virtual objects in egocentric viewpoint. Our contributions include: (i) a rotation and translation invariant finger clicking action and position estimation using the combination of 2D image-based fingertip detection with 3D hand posture estimation in egocentric viewpoint. (ii) a novel spatio-temporal random forest, which performs the detection and estimation efficiently in a single framework. We also present (iii) a selection process utilizing the proposed clicking action detection and position estimation in an arm reachable AR/VR space, which does not require any additional device. Experimental results show that the proposed method delivers promising performance under frequent self-occlusions in the process of selecting objects in AR/VR space whilst wearing an egocentric-depth camera-attached HMD.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Jang and S. Noh and H. Chang and T. Kim and W. Woo},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391860},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;estimation;joints;vectors;feature extraction;thumb},
	month = {apr},
	number = {04},
	pages = {501-510},
	publisher = {IEEE Computer Society},
	title = {3D Finger CAPE: Clicking Action and Position Estimation under Self-Occlusions in Egocentric Viewpoint},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391860}}

@article{6777424,
	abstract = {In this paper we study how the visual animation of a self-avatar can be artificially modified in real-time in order to generate different haptic perceptions. In our experimental setup, participants could watch their self-avatar in a virtual environment in mirror mode while performing a weight lifting task. Users could map their gestures on the self-animated avatar in real-time using a Kinect. We introduce three kinds of modification of the visual animation of the self-avatar according to the effort delivered by the virtual avatar: 1) changes on the spatial mapping between the user&#x27;s gestures and the avatar, 2) different motion profiles of the animation, and 3) changes in the posture of the avatar (upper-body inclination). The experimental task consisted of a weight lifting task in which participants had to order four virtual dumbbells according to their virtual weight. The user had to lift each virtual dumbbells by means of a tangible stick, the animation of the avatar was modulated according to the virtual weight of the dumbbell. The results showed that the altering the spatial mapping delivered the best performance. Nevertheless, participants globally appreciated all the different visual effects. Our results pave the way to the exploitation of such novel techniques in various VR applications such as sport training, exercise games, or industrial training scenarios in single or collaborative mode.},
	address = {Los Alamitos, CA, USA},
	author = {D. Gomez Jauregui and F. Argelaguet and A. Olivier and M. Marchal and F. Multon and A. Lecuyer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.45},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;animation;visualization;wrist;virtual environments;joints;visual effects},
	month = {apr},
	number = {04},
	pages = {654-661},
	publisher = {IEEE Computer Society},
	title = {Toward &quot;Pseudo-Haptic Avatars&quot;: Modifying the Visual Animation of Self-Avatar Can Simulate the Perception of Weight Lifting},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.45}}

@article{7829406,
	abstract = {Due to the perceptual characteristics of the head, vibrotactile Head-mounted Displays are built with low actuator density. Therefore, vibrotactile guidance is mostly assessed by pointing towards objects in the azimuthal plane. When it comes to multisensory interaction in 3D environments, it is also important to convey information about objects in the elevation plane. In this paper, we design and assess a haptic guidance technique for 3D environments. First, we explore the modulation of vibration frequency to indicate the position of objects in the elevation plane. Then, we assessed a vibrotactile HMD made to render the position of objects in a 3D space around the subject by varying both stimulus loci and vibration frequency. Results have shown that frequencies modulated with a quadratic growth function allowed a more accurate, precise, and faster target localization in an active head pointing task. The technique presented high usability and a strong learning effect for a haptic search across different scenarios in an immersive VR setup.},
	address = {Los Alamitos, CA, USA},
	author = {V. de Jesus Oliveira and L. Brayda and L. Nedel and A. Maciel},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657238},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;frequency modulation;vibrations;azimuthal plane;haptic interfaces;visualization;skin},
	month = {apr},
	number = {04},
	pages = {1409-1417},
	publisher = {IEEE Computer Society},
	title = {Designing a Vibrotactile Head-Mounted Display for Spatial Awareness in 3D Spaces},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657238}}

@article{8260944,
	abstract = {While the Sense of Agency (SoA) has so far been predominantly characterised in VR as a component of the Sense of Embodiment, other communities (e.g., in psychology or neurosciences) have investigated the SoA from a different perspective proposing complementary theories. Yet, despite the acknowledged potential benefits of catching up with these theories a gap remains. This paper first aims to contribute to fill this gap by introducing a theory according to which the SoA can be divided into two components, the feeling and the judgment of agency, and relies on three principles, namely the principles of priority, exclusivity and consistency. We argue that this theory could provide insights on the factors influencing the SoA in VR systems. Second, we propose novel approaches to manipulate the SoA in controlled VR experiments (based on these three principles) as well as to measure the SoA, and more specifically its two components based on neurophysiological markers, using ElectroEncephaloGraphy (EEG). We claim that these approaches would enable us to deepen our understanding of the SoA in VR contexts. Finally, we validate these approaches in an experiment. Our results (N&#x3D;24) suggest that our approach was successful in manipulating the SoA as the modulation of each of the three principles induced significant decreases of the SoA (measured using questionnaires). In addition, we recorded participants&#x27; EEG signals during the VR experiment, and neurophysiological markers of the SoA, potentially reflecting the feeling and judgment of agency specifically, were revealed. Our results also suggest that users&#x27; profile, more precisely their Locus of Control (LoC), influences their level of immersion and SoA.},
	address = {Los Alamitos, CA, USA},
	author = {C. Jeunet and L. Albert and F. Argelaguet and A. Lecuyer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794598},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {electroencephalography;virtual environments;modulation;psychology;biology},
	month = {apr},
	number = {04},
	pages = {1486-1495},
	publisher = {IEEE Computer Society},
	title = {``Do You Feel in Control?'': Towards Novel Approaches to Characterise, Manipulate and Measure the Sense of Agency in Virtual Environments},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794598}}

@article{8998133,
	abstract = {The gaze behavior of virtual avatars is critical to social presence and perceived eye contact during social interactions in Virtual Reality. Virtual Reality headsets are being designed with integrated eye tracking to enable compelling virtual social interactions. This paper shows that the near infra-red cameras used in eye tracking capture eye images that contain iris patterns of the user. Because iris patterns are a gold standard biometric, the current technology places the user&#x27;s biometric identity at risk. Our first contribution is an optical defocus based hardware solution to remove the iris biometric from the stream of eye tracking images. We characterize the performance of this solution with different internal parameters. Our second contribution is a psychophysical experiment with a same-different task that investigates the sensitivity of users to a virtual avatar&#x27;s eye movements when this solution is applied. By deriving detection threshold values, our findings provide a range of defocus parameters where the change in eye movements would go unnoticed in a conversational setting. Our third contribution is a perceptual study to determine the impact of defocus parameters on the perceived eye contact, attentiveness, naturalness, and truthfulness of the avatar. Thus, if a user wishes to protect their iris biometric, our approach provides a solution that balances biometric protection while preventing their conversation partner from perceiving a difference in the user&#x27;s virtual avatar. This work is the first to develop secure eye tracking configurations for VR/AR/XR applications and motivates future work in the area.},
	address = {Los Alamitos, CA, USA},
	author = {B. John and S. Jorg and S. Koppal and E. Jain},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973052},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {iris recognition;avatars;gaze tracking;cameras;privacy;animation;security},
	month = {may},
	number = {05},
	pages = {1880-1890},
	publisher = {IEEE Computer Society},
	title = {The Security-Utility Trade-off for Iris Authentication and Eye Animation for Social Virtual Avatars},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973052}}

@article{6777460,
	abstract = {Novel approaches are needed to reduce the high rates of childhood obesity in the developed world. While multifactorial in cause, a major factor is an increasingly sedentary lifestyle of children. Our research shows that a mixed reality system that is of interest to children can be a powerful motivator of healthy activity. We designed and constructed a mixed reality system that allowed children to exercise, play with, and train a virtual pet using their own physical activity as input. The health, happiness, and intelligence of each virtual pet grew as its associated child owner exercised more, reached goals, and interacted with their pet. We report results of a research study involving 61 children from a local summer camp that shows a large increase in recorded and observed activity, alongside observational evidence that the virtual pet was responsible for that change. These results, and the ease at which the system integrated into the camp environment, demonstrate the practical potential to impact the exercise behaviors of children with mixed reality.},
	address = {Los Alamitos, CA, USA},
	author = {K. Johnsen and Sun Joo Ahn and J. Moore and S. Brown and T. P. Robertson and A. Marable and A. Basu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.33},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {positron emission tomography;games;monitoring;pediatrics;obesity;avatars},
	month = {apr},
	number = {04},
	pages = {523-530},
	publisher = {IEEE Computer Society},
	title = {Mixed Reality Virtual Pets to Reduce Childhood Obesity},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.33}}

@article{8642384,
	abstract = {Spatial perception in virtual environments has been a topic of intense research. Arguably, the majority of this work has focused on distance perception. However, orientation perception is also an important factor. In this paper, we systematically investigate allocentric orientation judgments in both real and virtual contexts over the course of four experiments. A pattern of sinusoidal judgment errors known to exist in 2D perspective displays is found to persist in immersive virtual environments. This pattern also manifests itself in a real world setting using two differing judgment methods. The findings suggest the presence of a radial anisotropy that persists across viewing contexts. Additionally, there is some evidence to suggest that observers have multiple strategies for processing orientations but further investigation is needed to fully describe this phenomenon. We also offer design suggestions for 3D user interfaces where users may perform orientation judgments.},
	address = {Los Alamitos, CA, USA},
	author = {J. Jones and J. E. Hopper and M. T. Bolas and D. M. Krum},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898798},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual environments;task analysis;observers;anisotropic magnetoresistance;visualization;gravity;legged locomotion},
	month = {may},
	number = {05},
	pages = {2050-2060},
	publisher = {IEEE Computer Society},
	title = {Orientation Perception in Real and Virtual Environments},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898798}}

@article{6479211,
	abstract = {The following series of experiments explore the effect of static peripheral stimulation on the perception of distance and spatial scale in a typical head-mounted virtual environment. It was found that applying constant white light in an observers far periphery enabled the observer to more accurately judge distances using blind walking. An effect of similar magnitude was also found when observers estimated the size of a virtual space using a visual scale task. The presence of the effect across multiple psychophysical tasks provided confidence that a perceptual change was, in fact, being invoked by the addition of the peripheral stimulation. These results were also compared to observer performance in a very large field of view virtual environment and in the real world. The subsequent findings raise the possibility that distance judgments in virtual environments might be considerably more similar to those in the real world than previous work has suggested.},
	address = {Los Alamitos, CA, USA},
	author = {J. A. Jones and J. Swan and M. Bolas},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.37},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual environments;legged locomotion;visualization;observers;adaptive optics;stimulated emission;optical imaging},
	month = {apr},
	number = {04},
	pages = {701-710},
	publisher = {IEEE Computer Society},
	title = {Peripheral Stimulation and its Effect on Perceived Spatial Scale in Virtual Environments},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.37}}

@article{9386008,
	abstract = {This work addresses cybersickness, a major barrier to successful long-exposure immersive virtual reality (VR) experiences since user discomfort frequently leads to prematurely ending such experiences. Starting from sensory conflict theory, we posit that if a vibrating floor delivers vestibular stimuli that minimally match the vibration characteristics of a scenario, the size of the conflict between the visual and vestibular senses will be reduced and, thus, the incidence and/or severity of cybersickness will also be reduced. We integrated a custom-built, computer-controlled vibrating floor in our VR system. To evaluate the system, we implemented a realistic off-road vehicle driving simulator in which participants rode multiple laps as passengers on an off-road course. We programmed the floor to generate vertical vibrations similar to those experienced in real off-road vehicle travel. The scenario and driving conditions were designed to be cybersickness-inducing for users in both the Vibration and No-vibration conditions. We collected subjective and objective data for variables previously shown to be related to levels of cybersickness or presence. These included presence and simulator sickness questionnaires (SSQ), self-rated discomfort levels, and the physiological signals of heart rate, galvanic skin response (GSR), and pupil size. Comparing data between participants in the Vibration group (N&#x3D;11) to the No-Vibration group (N&#x3D;11), we found that Delta-SSQ Oculomotor response and the GSR physiological signal, both known to be positively correlated with cybersickness, were significantly lower (with large effect sizes) for the Vibration group. Other variables differed between groups in the same direction, but with trivial or small effect sizes. The results indicate that the floor vibration significantly reduced some measures of cybersickness.},
	address = {Los Alamitos, CA, USA},
	author = {S. Jung and R. Li and R. McKee and M. C. Whitton and R. W. Lindeman},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067773},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cybersickness;vibrations;visualization;physiology;particle measurements;atmospheric measurements;tactile sensors},
	month = {may},
	number = {05},
	pages = {2669-2680},
	publisher = {IEEE Computer Society},
	title = {Floor-vibration VR: Mitigating Cybersickness Using Whole-body Tactile Stimuli in Highly Realistic Vehicle Driving Experiences},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067773}}

@article{9714047,
	abstract = {Projector deblurring is an important technology for dynamic projection mapping (PM), where the distance between a projector and a projection surface changes in time. However, conventional projector deblurring techniques do not support dynamic PM because they need to project calibration patterns to estimate the amount of defocus blur each time the surface moves. We present a deep neural network that can compensate for defocus blur in dynamic PM. The primary contribution of this paper is a unique network structure that consists of an extractor and a generator. The extractor explicitly estimates a defocus blur map and a luminance attenuation map. These maps are then injected into the middle layers of the generator network that computes the compensation image. We also propose a pseudo-projection technique for synthesizing physically plausible training data, considering the geometric misregistration that potentially happens in actual PM systems. We conducted simulation and actual PM experiments and confirmed that: (1) the proposed network structure is more suitable than a simple, more general structure for projector deblurring; (2) the network trained with the proposed pseudo-projection technique can compensate projection images for defocus blur artifacts in dynamic PM; and (3) the network supports the translation speed of the surface movement within a certain range that covers normal human motions.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Kageyama and D. Iwai and K. Sato},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150465},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {generators;attenuation;cameras;deep learning;calibration;neural networks;estimation},
	month = {may},
	number = {05},
	pages = {2223-2233},
	publisher = {IEEE Computer Society},
	title = {Online Projector Deblurring Using a Convolutional Neural Network},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150465}}

@article{8260970,
	abstract = {Today&#x27;s virtual reality (VR) applications such as gaming, multisensory entertainment, remote dining, and online shopping are mainly based on audio, visual, and touch interactions between humans and virtual worlds. Integrating the sense of taste into VR is difficult since humans are dependent on chemical-based taste delivery systems. This paper presents the `Thermal Taste Machine', a new digital taste actuation technology that can effectively produce and modify thermal taste sensations on the tongue. It modifies the temperature of the surface of the tongue within a short period of time (from 25$\,^{\circ}$C to 40 $\,^{\circ}$C while heating, and from 25$\,^{\circ}$C to 10 $\,^{\circ}$C while cooling). We tested this device on human subjects and described the experience of thermal taste using 20 known (taste and non-taste) sensations. Our results suggested that rapidly heating the tongue produces sweetness, fatty/oiliness, electric taste, warmness, and reduces the sensibility for metallic taste. Similarly, cooling the tongue produced mint taste, pleasantness, and coldness. By conducting another user study on the perceived sweetness of sucrose solutions after the thermal stimulation, we found that heating the tongue significantly enhances the intensity of sweetness for both thermal tasters and non-thermal tasters. Also, we found that faster temperature rises on the tongue produce more intense sweet sensations for thermal tasters. This technology will be useful in two ways: First, it can produce taste sensations without using chemicals for the individuals who are sensitive to thermal taste. Second, the temperature rise of the device can be used as a way to enhance the intensity of sweetness. We believe that this technology can be used to digitally produce and enhance taste sensations in future virtual reality applications. The key novelties of this paper are as follows: 1. Development of a thermal taste actuation technology for stimulating the human taste receptors, 2. Characterization of the thermal taste produced by the device using taste-related sensations and non-taste related sensations, 3. Research on enhancing the intensity for sucrose solutions using thermal stimulation, 4. Research on how different speeds of heating affect the intensity of sweetness produced by thermal stimulation.},
	address = {Los Alamitos, CA, USA},
	author = {K. Karunanayaka and N. Johari and S. Hariri and H. Camelia and K. Bielawski and A. Cheok},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794073},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {tongue;temperature sensors;silver;heating systems;chemicals;cooling;virtual reality},
	month = {apr},
	number = {04},
	pages = {1496-1505},
	publisher = {IEEE Computer Society},
	title = {New Thermal Taste Actuation Technology for Future Multisensory Virtual Reality and Internet},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794073}}

@article{8267106,
	abstract = {Most existing locomotion devices that represent the sensation of walking target a user who is actually performing a walking motion. Here, we attempted to represent the walking sensation, especially a kinesthetic sensation and advancing feeling (the sense of moving forward) while the user remains seated. To represent the walking sensation using a relatively simple device, we focused on the force rendering and its evaluation of the longitudinal friction force applied on the sole during walking. Based on the measurement of the friction force applied on the sole during actual walking, we developed a novel friction force display that can present the friction force without the influence of body weight. Using performance evaluation testing, we found that the proposed method can stably and rapidly display friction force. Also, we developed a virtual reality (VR) walk-through system that is able to present the friction force through the proposed device according to the avatar&#x27;s walking motion in a virtual world. By evaluating the realism, we found that the proposed device can represent a more realistic advancing feeling than vibration feedback.},
	address = {Los Alamitos, CA, USA},
	author = {G. Kato and Y. Kuroda and K. Kiyokawa and H. Takemura},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793641},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;force;friction;force measurement;foot;actuators;performance evaluation},
	month = {apr},
	number = {04},
	pages = {1506-1514},
	publisher = {IEEE Computer Society},
	title = {Force Rendering and its Evaluation of a Friction-Based Walking Sensation Display for a Seated User},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793641}}

@article{6777445,
	abstract = {Distance in immersive virtual reality is commonly underperceived relative to intended distance, causing virtual environments to appear smaller than they actually are. However, a brief period of interaction by walking through the virtual environment with visual feedback can cause dramatic improvement in perceived distance. The goal of the current project was to determine how quickly improvement occurs as a result of walking interaction (Experiment 1) and whether improvement is specific to the distances experienced during interaction, or whether improvement transfers across scales of space (Experiment 2). The results show that five interaction trials resulted in a large improvement in perceived distance, and that subsequent walking interactions showed continued but diminished improvement. Furthermore, interaction with near objects (1-2 m) improved distance perception for near but not far (4-5 m) objects, whereas interaction with far objects broadly improved distance perception for both near and far objects. These results have practical implications for ameliorating distance underperception in immersive virtual reality, as well as theoretical implications for distinguishing between theories of how walking interaction influences perceived distance.},
	address = {Los Alamitos, CA, USA},
	author = {J. W. Kelly and W. W. Hammel and Z. D. Siegel and L. A. Sjolund},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.36},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;virtual environments;visualization;educational institutions;atmospheric measurements;particle measurements},
	month = {apr},
	number = {04},
	pages = {588-595},
	publisher = {IEEE Computer Society},
	title = {Recalibration of Perceived Distance in Virtual Environments Occurs Rapidly and Transfers Asymmetrically Across Scale},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.36}}

@article{8998297,
	abstract = {Virtual reality systems typically allow users to physically walk and turn, but virtual environments (VEs) often exceed the available walking space. Teleporting has become a common user interface, whereby the user aims a laser pointer to indicate the desired location, and sometimes orientation, in the VE before being transported without self-motion cues. This study evaluated the influence of rotational self-motion cues on spatial updating performance when teleporting, and whether the importance of rotational cues varies across movement scale and environment scale. Participants performed a triangle completion task by teleporting along two outbound path legs before pointing to the unmarked path origin. Rotational self-motion reduced overall errors across all levels of movement scale and environment scale, though it also introduced a slight bias toward under-rotation. The importance of rotational self-motion was exaggerated when navigating large triangles and when the surrounding environment was large. Navigating a large triangle within a small VE brought participants closer to surrounding landmarks and boundaries, which led to greater reliance on piloting (landmark-based navigation) and therefore reduced-but did not eliminate-the impact of rotational self-motion cues. These results indicate that rotational self-motion cues are important when teleporting, and that navigation can be improved by enabling piloting.},
	address = {Los Alamitos, CA, USA},
	author = {J. W. Kelly and A. G. Ostrander and A. F. Lim and L. A. Cherep and S. B. Gilbert},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973051},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;visualization;task analysis;navigation;space exploration;virtual environments;cognition},
	month = {may},
	number = {05},
	pages = {1841-1850},
	publisher = {IEEE Computer Society},
	title = {Teleporting through virtual environments: Effects of path scale and environment scale on spatial updating},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973051}}

@article{9714054,
	abstract = {The wide availability of consumer-oriented virtual reality (VR) equipment has enabled researchers to recruit existing VR owners to participate remotely using their own equipment. Yet, there are many differences between lab environments and home environments, as well as differences between participant samples recruited for lab studies and remote studies. This paper replicates a lab-based experiment on VR locomotion interfaces using a remote sample. Participants completed a triangle-completion task (travel two path legs, then point to the path origin) using their own VR equipment in a remote, unsupervised setting. Locomotion was accomplished using two versions of the teleporting interface varying in availability of rotational self-motion cues. The size of the traveled path and the size of the surrounding virtual environment were also manipulated. Results from remote participants largely mirrored lab results, with overall better performance when rotational self-motion cues were available. Some differences also occurred, including a tendency for remote participants to rely less on nearby landmarks, perhaps due to increased competence with using the teleporting interface to update self-location. This replication study provides insight for VR researchers on aspects of lab studies that may or may not replicate remotely.},
	address = {Los Alamitos, CA, USA},
	author = {J. W. Kelly and M. Hoover and T. A. Doty and A. Renner and M. Zimmerman and K. Knuth and L. A. Cherep and S. B. Gilbert},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150475},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;navigation;games;virtual environments;legged locomotion;headphones;green products},
	month = {may},
	number = {05},
	pages = {2037-2046},
	publisher = {IEEE Computer Society},
	title = {Remote research on locomotion interfaces for virtual reality: Replication of a lab-based study on teleporting interfaces},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150475}}

@article{10049665,
	abstract = {This article compares two state-of-the-art text input techniques between non-stationary virtual reality (VR) and video see-through augmented reality (VST AR) use-cases as XR display condition. The developed contact-based mid-air virtual tap and word-gesture (swipe) keyboard provide established support functions for text correction, word suggestions, capitalization, and punctuation. A user evaluation with 64 participants revealed that XR displays and input techniques strongly affect text entry performance, while subjective measures are only influenced by the input techniques. We found significantly higher usability and user experience ratings for tap keyboards compared to swipe keyboards in both VR and VST AR. Task load was also lower for tap keyboards. In terms of performance, both input techniques were significantly faster in VR than in VST AR. Further, the tap keyboard was significantly faster than the swipe keyboard in VR. Participants showed a significant learning effect with only ten sentences typed per condition. Our results are consistent with previous work in VR and optical see-through (OST) AR, but additionally provide novel insights into usability and performance of the selected text input techniques for VST AR. The significant differences in subjective and objective measures emphasize the importance of specific evaluations for each possible combination of input techniques and XR displays to provide reusable, reliable, and high-quality text input solutions. With our work, we form a foundation for future research and XR workspaces. Our reference implementation is publicly available to encourage replicability and reuse in future XR workspaces.},
	address = {Los Alamitos, CA, USA},
	author = {F. Kern and F. Niebling and M. Latoschik},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247098},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {keyboards;x reality;usability;performance evaluation;user experience;task analysis;tracking},
	month = {may},
	number = {05},
	pages = {2658-2669},
	publisher = {IEEE Computer Society},
	title = {Text Input for Non-Stationary XR Workspaces: Investigating Tap and Word-Gesture Keyboards in Virtual and Augmented Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247098}}

@article{6479188,
	abstract = {It has been shown that it is possible to generate perceptual illusions of ownership in immersive virtual reality (IVR) over a virtual body seen from first person perspective, in other words over a body that visually substitutes the person&#x27;s real body. This can occur even when the virtual body is quite different in appearance from the person&#x27;s real body. However, investigation of the psychological, behavioral and attitudinal consequences of such body transformations remains an interesting problem with much to be discovered. Thirty six Caucasian people participated in a between-groups experiment where they played a West-African Djembe hand drum while immersed in IVR and with a virtual body that substituted their own. The virtual hand drum was registered with a physical drum. They were alongside a virtual character that played a drum in a supporting, accompanying role. In a baseline condition participants were represented only by plainly shaded white hands, so that they were able merely to play. In the experimental condition they were represented either by a casually dressed dark-skinned virtual body (Casual Dark-Skinned - CD) or by a formal suited light-skinned body (Formal Light-Skinned - FL). Although participants of both groups experienced a strong body ownership illusion towards the virtual body, only those with the CD representation showed significant increases in their movement patterns for drumming compared to the baseline condition and compared with those embodied in the FL body. Moreover, the stronger the illusion of body ownership in the CD condition, the greater this behavioral change. A path analysis showed that the observed behavioral changes were a function of the strength of the illusion of body ownership towards the virtual body and its perceived appropriateness for the drumming task. These results demonstrate that full body ownership illusions can lead to substantial behavioral and possibly cognitive changes depending on the appearance of the virtual body. This could be important for many applications such as learning, education, training, psychotherapy and rehabilitation using IVR.},
	address = {Los Alamitos, CA, USA},
	author = {K. Kilteni and I. Bergstrom and M. Slater},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.29},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;rubber;mirrors;correlation;visualization;instruments},
	month = {apr},
	number = {04},
	pages = {597-605},
	publisher = {IEEE Computer Society},
	title = {Drumming in Immersive Virtual Reality: The Body Shapes the Way We Play},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.29}}

@article{8302393,
	abstract = {This article investigates the effects of visual warning presentation methods on human performance in augmented reality (AR) driving. An experimental user study was conducted in a parking lot where participants drove a test vehicle while braking for any cross traffic with assistance from AR visual warnings presented on a monoscopic and volumetric head-up display (HUD). Results showed that monoscopic displays can be as effective as volumetric displays for human performance in AR braking tasks. The experiment also demonstrated the benefits of conformal graphics, which are tightly integrated into the real world, such as their ability to guide drivers&#x27; attention and their positive consequences on driver behavior and performance. These findings suggest that conformal graphics presented via monoscopic HUDs can enhance driver performance by leveraging the effectiveness of monocular depth cues. The proposed approaches and methods can be used and further developed by future researchers and practitioners to better understand driver performance in AR as well as inform usability evaluation of future automotive AR applications.},
	address = {Los Alamitos, CA, USA},
	author = {H. Kim and J. L. Gabbard and A. Anon and T. Misu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793680},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;visualization;vehicles;task analysis;stereo image processing;observers},
	month = {apr},
	number = {04},
	pages = {1515-1524},
	publisher = {IEEE Computer Society},
	title = {Driver Behavior and Performance with Augmented Reality Pedestrian Collision Warning: An Outdoor User Study},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793680}}

@article{9714043,
	abstract = {Studies in virtual reality (VR) have introduced numerous multisensory simulation techniques for more immersive VR experiences. However, although they primarily focus on expanding sensory types or increasing individual sensory quality, they lack consensus in designing appropriate interactions between different sensory stimuli. This paper explores how the congruence between auditory and visual (AV) stimuli, which are the sensory stimuli typically provided by VR devices, affects the cognition and experience of VR users as a critical interaction factor in promoting multisensory integration. We defined the types of (in)congruence between AV stimuli, and then designed 12 virtual spaces with different types or degrees of congruence between AV stimuli. We then evaluated the presence, immersion, motion sickness, and cognition changes in each space. We observed the following key findings: 1) there is a limit to the degree of temporal or spatial incongruence that can be tolerated, with few negative effects on user experience until that point is exceeded; 2) users are tolerant of semantic incongruence; 3) a simulation that considers synesthetic congruence contributes to the user&#x27;s sense of immersion and presence. Based on these insights, we identified the essential considerations for designing sensory simulations in VR and proposed future research directions.},
	address = {Los Alamitos, CA, USA},
	author = {H. Kim and I. Lee},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150514},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;user experience;solid modeling;psychology;multisensory integration;semantics;legged locomotion},
	month = {may},
	number = {05},
	pages = {2080-2090},
	publisher = {IEEE Computer Society},
	title = {Studying the Effects of Congruence of Auditory and Visual Stimuli on Virtual Reality Experiences},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150514}}

@article{7829423,
	abstract = {The reduced gravity experienced in lunar or Martian surfaces can be simulated on the earth using a cable-driven system, where the cable lifts a person to reduce his or her weight. This paper presents a novel cable-driven system designed for the purpose. It is integrated with a head-mounted display and a motion capture system. Focusing on jump motion within the system, this paper proposes to scale the jump and reports the experiments made for quantifying the extent to which a jump can be scaled without the discrepancy between physical and virtual jumps being noticed by the user. With the tolerable range of scaling computed from these experiments, an application named retargeted jump is developed, where a user can jump up onto virtual objects while physically jumping in the real-world flat floor. The core techniques presented in this paper can be extended to develop extreme-sport simulators such as parasailing and skydiving.},
	address = {Los Alamitos, CA, USA},
	author = {M. Kim and S. Cho and T. Tran and S. Kim and O. Kwon and J. Han},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657139},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {moon;gravity;resists;wires;earth;winches;virtual environments},
	month = {apr},
	number = {04},
	pages = {1360-1368},
	publisher = {IEEE Computer Society},
	title = {Scaled Jump in Gravity-Reduced Virtual Environments},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657139}}

@article{6479184,
	abstract = {In this paper, we present a novel rendering method which integrates reflective or refractive objects into a differential instant radiosity (DIR) framework usable for mixed-reality (MR) applications. This kind of objects are very special from the light interaction point of view, as they reflect and refract incident rays. Therefore they may cause high-frequency lighting effects known as caustics. Using instant-radiosity (IR) methods to approximate these high-frequency lighting effects would require a large amount of virtual point lights (VPLs) and is therefore not desirable due to real-time constraints. Instead, our approach combines differential instant radiosity with three other methods. One method handles more accurate reflections compared to simple cubemaps by using impostors. Another method is able to calculate two refractions in real-time, and the third method uses small quads to create caustic effects. Our proposed method replaces parts in light paths that belong to reflective or refractive objects using these three methods and thus tightly integrates into DIR. In contrast to previous methods which introduce reflective or refractive objects into MR scenarios, our method produces caustics that also emit additional indirect light. The method runs at real-time frame rates, and the results show that reflective and refractive objects with caustics improve the overall impression for MR scenarios.},
	address = {Los Alamitos, CA, USA},
	author = {M. Knecht and C. Traxler and C. Winklhofer and M. Wimmer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.39},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;image color analysis;rendering (computer graphics);lighting;equations;cameras;streaming media},
	month = {apr},
	number = {04},
	pages = {576-582},
	publisher = {IEEE Computer Society},
	title = {Reflective and Refractive Objects for Mixed Reality},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.39}}

@article{Koch2012:Geometric-Calibration-of-Head-Mounted,
	abstract = {Head-mounted displays (HMDs) allow users to observe virtual environments (VEs) from an egocentric perspective. However, several experiments have provided evidence that egocentric distances are perceived as compressed in VEs relative to the real world. Recent experiments suggest that the virtual view frustum set for rendering the VE has an essential impact on the user&#x27;s estimation of distances. In this article we analyze if distance estimation can be improved by calibrating the view frustum for a given HMD and user. Unfortunately, in an immersive virtual reality (VR) environment, a full per user calibration is not trivial and manual per user adjustment often leads to mini- or magnification of the scene. Therefore, we propose a novel per user calibration approach with optical see-through displays commonly used in augmented reality (AR). This calibration takes advantage of a geometric scheme based on 2D point - 3D line correspondences, which can be used intuitively by inexperienced users and requires less than a minute to complete. The required user interaction is based on taking aim at a distant target marker with a close marker, which ensures non-planar measurements covering a large area of the interaction space while also reducing the number of required measurements to five. We found the tendency that a calibrated view frustum reduced the average distance underestimation of users in an immersive VR environment, but even the correctly calibrated view frustum could not entirely compensate for the distance underestimation effects.},
	address = {Los Alamitos, CA, USA},
	author = {R. Koch and M. Lappe and F. Steinicke and U. Rautenberg and G. Bruder and B. Bolte and F. Kellner},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.45},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);augmented reality;calibration;helmet mounted displays;human computer interaction;distance estimation;geometric calibration;head-mounted display;hmd;egocentric perspective;egocentric distance;virtual view frustum set;rendering;immersive virtual reality environment;full per user calibration;manual per user adjustment;optical see-through display;augmented reality;geometric scheme;2d point-3d line correspondences;user interaction;calibrated view frustum;average distance underestimation reduction;immersive vr environment;distance underestimation effects;cameras;calibration;three dimensional displays;estimation;noise;vectors;target tracking;distance perception.;optical see-through;hmd calibration},
	month = {apr},
	number = {04},
	pages = {589-596},
	publisher = {IEEE Computer Society},
	title = {Geometric Calibration of Head-Mounted Displays and its Effects on Distance Estimation},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.45}}

@article{10049764,
	abstract = {Virtual reality (VR) is a promising tool for motor skill learning. Previous studies have indicated that observing and following a teacher&#x27;s movements from a first-person perspective using VR facilitates motor skill learning. Conversely, it has also been pointed out that this learning method makes the learner so strongly aware of the need to follow that it weakens their sense of agency (SoA) for motor skills and prevents them from updating the body schema, thereby preventing long-term retention of motor skills. To address this problem, we propose applying ``virtual co-embodiment'' to motor skill learning. Virtual co-embodiment is a system in which a virtual avatar is controlled based on the weighted average of the movements of multiple entities. Because users in virtual co-embodiment overestimate their SoA, we hypothesized that learning using virtual co-embodiment with a teacher would improve motor skill retention. In this study, we focused on learning a dual task to evaluate the automation of movement, which is considered an essential element of motor skills. As a result, learning in virtual co-embodiment with the teacher improves motor skill learning efficiency compared with sharing the teacher&#x27;s first-person perspective or learning alone.},
	address = {Los Alamitos, CA, USA},
	author = {D. Kodama and T. Mizuho and Y. Hatada and T. Narumi and M. Hirose},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247112},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;task analysis;training;quaternions;virtual environments;trajectory;stars},
	month = {may},
	number = {05},
	pages = {2304-2314},
	publisher = {IEEE Computer Society},
	title = {Effects of Collaborative Training Using Virtual Co-embodiment on Motor Skill Learning},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247112}}

@article{Kohlmann2012:Effective-Replays-and-Summarization,
	abstract = {Direct replay of the experience of a user in a virtual environment is difficult for others to watch due to unnatural camera motions. We present methods for replaying and summarizing these egocentric experiences that effectively communicate the user&#x27;s observations while reducing unwanted camera movements. Our approach summarizes the viewpoint path as a concise sequence of viewpoints that cover the same parts of the scene. The core of our approach is a novel content-dependent metric that can be used to identify similarities between viewpoints. This enables viewpoints to be grouped by similar contextual view information and provides a means to generate novel viewpoints that can encapsulate a series of views. These resulting encapsulated viewpoints are used to synthesize new camera paths that convey the content of the original viewer&#x27;s experience. Projecting the initial movement of the user back on the scene can be used to convey the details of their observations, and the extracted viewpoints can serve as bookmarks for control or analysis. Finally we present performance analysis along with two forms of validation to test whether the extracted viewpoints are representative of the viewer&#x27;s original observations and to test for the overall effectiveness of the presented replay methods.},
	address = {Los Alamitos, CA, USA},
	author = {J. Kohlmann and K. Ponto and M. Gleicher},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.41},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;cameras;performance analysis;virtual experience replay;virtual experience summarization;virtual environment;unnatural camera motions;egocentric experiences;user observations;viewpoint path summarization;content-dependent metric;contextual view information;viewpoint extraction;cameras;measurement;graphics processing unit;equations;three dimensional displays;geometry;virtual environments;bookmarking.;virtual reality;viewpoint similarity;summarization;gpu},
	month = {apr},
	number = {04},
	pages = {607-616},
	publisher = {IEEE Computer Society},
	title = {Effective Replays and Summarization of Virtual Experiences},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.41}}

@article{9389650,
	abstract = {Ellipse fitting, an essential component in pupil or iris tracking based video oculography, is performed on previously segmented eye parts generated using various computer vision techniques. Several factors, such as occlusions due to eyelid shape, camera position or eyelashes, frequently break ellipse fitting algorithms that rely on well-defined pupil or iris edge segments. In this work, we propose training a convolutional neural network to directly segment entire elliptical structures and demonstrate that such a framework is robust to occlusions and offers superior pupil and iris tracking performance (at least 10% and 24% increase in pupil and iris center detection rate respectively within a two-pixel error margin) compared to using standard eye parts segmentation for multiple publicly available synthetic segmentation datasets.},
	address = {Los Alamitos, CA, USA},
	author = {R. S. Kothari and A. K. Chaudhary and R. J. Bailey and J. B. Pelz and G. J. Diaz},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067765},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {pupils;iris;image segmentation;computer architecture;feature extraction;cameras;decoding},
	month = {may},
	number = {05},
	pages = {2757-2767},
	publisher = {IEEE Computer Society},
	title = {EllSeg: An Ellipse Segmentation Framework for Robust Gaze Tracking},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067765}}

@article{Kotranza2009:Mixed-Reality-Humans:,
	abstract = {This paper presents Mixed Reality Humans (MRHs), a new type of embodied agent enabling touch-driven communication. Affording touch between human and agent allows MRHs to simulate interpersonal scenarios in which touch is crucial. Two studies provide initial evaluation of user behavior with a MRH patient and the usability and acceptability of a MRH patient for practice and evaluation of medical students&#x27; clinical skills. In Study I (n&#x3D;8) it was observed that students treated MRHs as social actors more than students in prior interactions with virtual human patients (n&#x3D;27), and used interpersonal touch to comfort and reassure the MRH patient similarly to prior interactions with human patients (n&#x3D;76). In the within-subjects Study II (n&#x3D;11), medical students performed a clinical breast exam on each of a MRH and human patient. Participants performed equivalent exams with the MRH and human patients, demonstrating the usability of MRHs to evaluate students&#x27; exam skills. The acceptability of the MRH patient for practicing exam skills was high as students rated the experience as believable and educationally beneficial. Acceptability was improved from Study I to Study II due to an increase in the MRH&#x27;s visual realism, demonstrating that visual realism is critical for simulation of specific interpersonal scenarios.},
	address = {Los Alamitos, CA, USA},
	author = {A. Kotranza and C. M. Pugh and B. Lok and D. Lind and A. Deladisma},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2008.195},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {intelligent agents;virtual reality;life and medical sciences;artificial;augmented;and virtual realities},
	month = {may},
	number = {03},
	pages = {369-382},
	publisher = {IEEE Computer Society},
	title = {Mixed Reality Humans: Evaluating Behavior, Usability, and Acceptability},
	volume = {15},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2008.195}}

@article{10049731,
	abstract = {Recent research has attempted to identify methods to mitigate cybersickness and examine its aftereffects. In this direction, this paper examines the effects of cybersickness on cognitive, motor, and reading performance in VR. Also, this paper evaluates the mitigating effects of music on cybersickness, as well as the role of gender, and the computing, VR, and gaming experience of the user. This paper reports two studies. In the 1st study, 92 participants selected the music tracks considered most calming (low valence) or joyful (high valence) to be used in the 2nd study. In the 2nd study, 39 participants performed an assessment four times, once before the rides (baseline), and then once after each ride (3 rides). In each ride either Calming, or Joyful, or No Music was played. During each ride, linear and angular accelerations took place to induce cybersickness in the participants. In each assessment, while immersed in VR, the participants evaluated their cybersickness symptomatology and performed a verbal working memory task, a visuospatial working memory task, and a psychomotor task. While responding to the cybersickness questionnaire (3D UI), eye-tracking was conducted to measure reading time and pupillometry. The results showed that Joyful and Calming music substantially decreased the intensity of nausea-related symptoms. However, only Joyful music significantly decreased the overall cybersickness intensity. Importantly, cybersickness was found to decrease verbal working memory performance and pupil size. Also, it significantly decelerated psychomotor (reaction time) and reading abilities. Higher gaming experience was associated with lower cybersickness. When controlling for gaming experience, there were no significant differences between female and male participants in terms of cybersickness. The outcomes indicated the efficiency of music in mitigating cybersickness, the important role of gaming experience in cybersickness, and the significant effects of cybersickness on pupil size, cognition, psychomotor skills, and reading ability.},
	address = {Los Alamitos, CA, USA},
	author = {P. Kourtesis and R. Amir and J. Linnell and F. Argelaguet and S. E. MacPherson},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247062},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cybersickness;motion sickness;cognition;task analysis;pupils;visualization;tracking},
	month = {may},
	number = {05},
	pages = {2326-2336},
	publisher = {IEEE Computer Society},
	title = {Cybersickness, Cognition, &amp; Motor Skills: The Effects of Music, Gender, and Gaming Experience},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247062}}

@article{8998139,
	abstract = {Occlusion is a powerful visual cue that is crucial for depth perception and realism in optical see-through augmented reality (OST-AR). However, existing OST-AR systems additively overlay physical and digital content with beam combiners - an approach that does not easily support mutual occlusion, resulting in virtual objects that appear semi-transparent and unrealistic. In this work, we propose a new type of occlusion-capable OST-AR system. Rather than additively combining the real and virtual worlds, we employ a single digital micromirror device (DMD) to merge the respective light paths in a multiplicative manner. This unique approach allows us to simultaneously block light incident from the physical scene on a pixel-by-pixel basis while also modulating the light emitted by a light-emitting diode (LED) to display digital content. Our technique builds on mixed binary/continuous factorization algorithms to optimize time-multiplexed binary DMD patterns and their corresponding LED colors to approximate a target augmented reality (AR) scene. In simulations and with a prototype benchtop display, we demonstrate hard-edge occlusions, plausible shadows, and also gaze-contingent optimization of this novel display mode, which only requires a single spatial light modulator.},
	address = {Los Alamitos, CA, USA},
	author = {B. Krajancich and N. Padmanaban and G. Wetzstein},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973443},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {image color analysis;optical diffraction;mirrors;light emitting diodes;optical imaging;augmented reality;modulation},
	month = {may},
	number = {05},
	pages = {1871-1879},
	publisher = {IEEE Computer Society},
	title = {Factored Occlusion: Single Spatial Light Modulator Occlusion-capable Optical See-through Augmented Reality Display},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973443}}

@article{8998368,
	abstract = {Fitts&#x27;s law facilitates approximate comparisons of target acquisition performance across a variety of settings. Conceptually, also the index of difficulty of 3D object manipulation with six degrees of freedom can be computed, which allows the comparison of results from different studies. Prior experiments, however, often revealed much worse performance than one would reasonably expect on this basis. We argue that this discrepancy stems from confounding variables and show how Fitts&#x27;s law and related research methods can be applied to isolate and identify relevant factors of motor performance in 3D manipulation tasks. The results of a formal user study (n&#x3D;21) demonstrate competitive performance in compliance with Fitts&#x27;s model and provide empirical evidence that simultaneous 3D rotation and translation can be beneficial.},
	address = {Los Alamitos, CA, USA},
	author = {A. Kulik and A. Kunert and B. Froehlich},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973034},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;task analysis;solid modeling;mathematical model;throughput;two dimensional displays;computational modeling},
	month = {may},
	number = {05},
	pages = {2041-2050},
	publisher = {IEEE Computer Society},
	title = {On Motor Performance in Virtual 3D Object Manipulation},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973034}}

@article{6479179,
	abstract = {In our research agenda to study the effects of immersion (level of fidelity) on various tasks in virtual reality (VR) systems, we have found that the most generalizable findings come not from direct comparisons of different technologies, but from controlled simulations of those technologies. We call this the mixed reality (MR) simulation approach. However, the validity of MR simulation, especially when different simulator platforms are used, can be questioned. In this paper, we report the results of an experiment examining the effects of field of regard (FOR) and head tracking on the analysis of volume visualized micro-CT datasets, and compare them with those from a previous study. The original study used a CAVE-like display as the MR simulator platform, while the present study used a high-end head-mounted display (HMD). Out of the 24 combinations of system characteristics and tasks tested on the two platforms, we found that the results produced by the two different MR simulators were similar in 20 cases. However, only one of the significant effects found in the original experiment for quantitative tasks was reproduced in the present study. Our observations provide evidence both for and against the validity of MR simulation, and give insight into the differences caused by different MR simulator platforms. The present experiment also examined new conditions not present in the original study, and produced new significant results, which confirm and extend previous existing knowledge on the effects of FOR and head tracking. We provide design guidelines for choosing display systems that can improve the effectiveness of volume visualization applications.},
	address = {Los Alamitos, CA, USA},
	author = {B. Laha and D. A. Bowman and J. D. Schiffbauer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.43},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;visualization;mice;solid modeling;head;training;computational modeling},
	month = {apr},
	number = {04},
	pages = {529-538},
	publisher = {IEEE Computer Society},
	title = {Validation of the MR Simulation Approach for Evaluating the Effects of Immersion on Visual Analysis of Volume Data},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.43}}

@article{6777465,
	abstract = {Volume visualization is an important technique for analyzing datasets from a variety of different scientific domains. Volume data analysis is inherently difficult because volumes are three-dimensional, dense, and unfamiliar, requiring scientists to precisely control the viewpoint and to make precise spatial judgments. Researchers have proposed that more immersive (higher fidelity) VR systems might improve task performance with volume datasets, and significant results tied to different components of display fidelity have been reported. However, more information is needed to generalize these results to different task types, domains, and rendering styles. We visualized isosurfaces extracted from synchrotron microscopic computed tomography (SR-CT) scans of beetles, in a CAVE-like display. We ran a controlled experiment evaluating the effects of three components of system fidelity (field of regard, stereoscopy, and head tracking) on a variety of abstract task categories that are applicable to various scientific domains, and also compared our results with those from our prior experiment using 3D texture-based rendering. We report many significant findings. For example, for search and spatial judgment tasks with isosurface visualization, a stereoscopic display provides better performance, but for tasks with 3D texture-based rendering, displays with higher field of regard were more effective, independent of the levels of the other display components. We also found that systems with high field of regard and head tracking improve performance in spatial judgment tasks. Our results extend existing knowledge and produce new guidelines for designing VR systems to improve the effectiveness of volume data analysis.},
	address = {Los Alamitos, CA, USA},
	author = {B. Laha and D. A. Bowman and J. J. Socha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.20},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);isosurfaces;three-dimensional displays;visualization;abstracts;measurement;computed tomography},
	month = {apr},
	number = {04},
	pages = {513-522},
	publisher = {IEEE Computer Society},
	title = {Effects of VR System Fidelity on Analyzing Isosurface Visualization of Volume Datasets},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.20}}

@article{7833190,
	abstract = {Redirected walking (RDW) promises to allow near-natural walking in an infinitely large virtual environment (VE) by subtle manipulations of the virtual camera. Previous experiments analyzed the human sensitivity to RDW manipulations by focusing on the worst-case scenario, in which users walk perfectly straight ahead in the VE, whereas they are redirected on a circular path in the real world. The results showed that a physical radius of at least 22 meters is required for undetectable RDW. However, users do not always walk exactly straight in a VE. So far, it has not been investigated how much a physical path can be bent in situations in which users walk a virtual curved path instead of a straight one. Such curved walking paths can be often observed, for example, when users walk on virtual trails, through bent corridors, or when circling around obstacles. In such situations the question is not, whether or not the physical path can be bent, but how much the bending of the physical path may vary from the bending of the virtual path. In this article, we analyze this question and present redirection by means of bending gains that describe the discrepancy between the bending of curved paths in the real and virtual environment. Furthermore, we report the psychophysical experiments in which we analyzed the human sensitivity to these gains. The results reveal encouragingly wider detection thresholds than for straightforward walking. Based on our findings, we discuss the potential of curved walking and present a first approach to leverage bent paths in a way that can provide undetectable RDW manipulations even in room-scale VR.},
	address = {Los Alamitos, CA, USA},
	author = {E. Langbehn and P. Lubos and G. Bruder and F. Steinicke},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657220},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;sensitivity;virtual environments;cameras;visualization;space vehicles;human computer interaction},
	month = {apr},
	number = {04},
	pages = {1389-1398},
	publisher = {IEEE Computer Society},
	title = {Bending the Curve: Sensitivity to Bending of Curved Paths and Application in Room-Scale VR},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657220}}

@article{8643417,
	abstract = {This article investigates performance and user experience in Social Virtual Reality (SVR) targeting distributed, embodied, and immersive, face-to-face encounters. We demonstrate the close relationship between scalability, reproduction accuracy, and the resulting performance characteristics, as well as the impact of these characteristics on users co-located with larger groups of embodied virtual others. System scalability provides a variable number of co-located avatars and Al-controlled agents with a variety of different appearances, including realistic-looking virtual humans generated from photogrammetry scans. The article reports on how to meet the requirements of embodied SVR with today&#x27;s technical off-the-shelf solutions and what to expect regarding features, performance, and potential limitations. Special care has been taken to achieve low latencies and sufficient frame rates necessary for reliable communication of embodied social signals. We propose a hybrid evaluation approach which coherently relates results from technical benchmarks to subjective ratings and which confirms required performance characteristics for the target scenario of larger distributed groups. A user-study reveals positive effects of an increasing number of co-located social companions on the quality of experience of virtual worlds, i.e., on presence, possibility of interaction, and co-presence. It also shows that variety in avatar/agent appearance might increase eeriness but might also stimulate an increased interest of participants about the environment.},
	address = {Los Alamitos, CA, USA},
	author = {M. Latoschik and F. Kern and J. Stauffert and A. Bartl and M. Botsch and J. Lugrin},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899250},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;open area test sites;scalability;animation;bandwidth;tracking},
	month = {may},
	number = {05},
	pages = {2134-2144},
	publisher = {IEEE Computer Society},
	title = {Not Alone Here?! Scalability and User Experience of Embodied Ambient Crowds in Distributed Social Virtual Reality},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899250}}

@article{LaViola2012:Interactive-3D-Model-Acquisition,
	abstract = {We present a prototype system for interactive construction and modification of 3D physical models using building blocks.Our system uses a depth sensing camera and a novel algorithm for acquiring and tracking the physical models. The algorithm,Lattice-First, is based on the fact that building block structures can be arranged in a 3D point lattice where the smallest block unit is a basis in which to derive all the pieces of the model. The algorithm also makes it possible for users to interact naturally with the physical model as it is acquired, using their bare hands to add and remove pieces. We present the details of our algorithm, along with examples of the models we can acquire using the interactive system. We also show the results of an experiment where participants modify a block structure in the absence of visual feedback. Finally, we discuss two proof-of-concept applications: a collaborative guided assembly system where one user is interactively guided to build a structure based on another user&#x27;s design, and a game where the player must build a structure that matches an on-screen silhouette.},
	address = {Los Alamitos, CA, USA},
	author = {J. J. LaViola and Z. Kanzler and E. Charbonneau and B. White and A. Miller},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.48},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {user interfaces;cameras;data acquisition;solid modelling;on-screen silhouette;interactive 3d model;3d model acquisition;3d model tracking;building block structure;interactive construction;interactive modification;3d physical model;depth sensing camera;lattice-first algorithm;3d point lattice;visual feedback;user interaction;collaborative guided assembly system;user design;solid modeling;lattices;three dimensional displays;computational modeling;cameras;image color analysis;visualization;building block structures.;interactive physical model building;3d model acquisition;object tracking;depth cameras},
	month = {apr},
	number = {04},
	pages = {651-659},
	publisher = {IEEE Computer Society},
	title = {Interactive 3D Model Acquisition and Tracking of Building Block Structures},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.48}}

@article{LaViola2012:Dense-and-Dynamic-3D-Selection,
	abstract = {3D object selection is more demanding when, 1) objects densly surround the target object, 2) the target object is significantly occluded, and 3) when the target object is dynamically changing location. Most 3D selection techniques and guidelines were developed and tested on static or mostly sparse environments. In contrast, games tend to incorporate densly packed and dynamic objects as part of their typical interaction. With the increasing popularity of 3D selection in games using hand gestures or motion controllers, our current understanding of 3D selection needs revision. We present a study that compared four different selection techniques under five different scenarios based on varying object density and motion dynamics. We utilized two existing techniques, Raycasting and SQUAD, and developed two variations of them, Zoom and Expand, using iterative design. Our results indicate that while Raycasting and SQUAD both have weaknesses in terms of speed and accuracy in dense and dynamic environments, by making small modifications to them (i.e., flavoring), we can achieve significant performance increases.},
	address = {Los Alamitos, CA, USA},
	author = {J. J. LaViola and J. Cashion and C. Wingrave},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.40},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;computer games;iterative methods;user interfaces;iterative design;dense 3d object selection;dynamic 3d object selection;game-based virtual environment;3d selection techniques;3d selection guidelines;sparse environment;hand gesture;motion controller;object density;motion dynamics;raycasting technique;squad technique;zoom variation;expand variation;three dimensional displays;games;guidelines;accuracy;usability;context;color;dense and dynamic objects.;interaction techniques;game-based virtual environments;3d object selection},
	month = {apr},
	number = {04},
	pages = {634-642},
	publisher = {IEEE Computer Society},
	title = {Dense and Dynamic 3D Selection for Game-Based Virtual Environments},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.40}}

@article{6479181,
	abstract = {In this paper, we investigate the validity of Mixed Reality (MR) Simulation by conducting an experiment studying the effects of the visual realism of the simulated environment on various search tasks in Augmented Reality (AR). MR Simulation is a practical approach to conducting controlled and repeatable user experiments in MR, including AR. This approach uses a high-fidelity Virtual Reality (VR) display system to simulate a wide range of equal or lower fidelity displays from the MR continuum, for the express purpose of conducting user experiments. For the experiment, we created three virtual models of a real-world location, each with a different perceived level of visual realism. We designed and executed an AR experiment using the real-world location and repeated the experiment within VR using the three virtual models we created. The experiment looked into how fast users could search for both physical and virtual information that was present in the scene. Our experiment demonstrates the usefulness of MR Simulation and provides early evidence for the validity of MR Simulation with respect to AR search tasks performed in immersive VR.},
	address = {Los Alamitos, CA, USA},
	author = {Cha Lee and G. A. Rincon and G. Meyer and T. Hollerer and D. A. Bowman},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.41},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;solid modeling;lighting;cameras;geometry;virtual environments},
	month = {apr},
	number = {04},
	pages = {547-556},
	publisher = {IEEE Computer Society},
	title = {The Effects of Visual Realism on Search Tasks in Mixed Reality Simulation},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.41}}

@article{8302409,
	abstract = {In this paper, we investigate factors and issues related to human locomotion behavior and proxemics in the presence of a real or virtual human in augmented reality (AR). First, we discuss a unique issue with current-state optical see-through head-mounted displays, namely the mismatch between a small augmented visual field and a large unaugmented periphery, and its potential impact on locomotion behavior in close proximity of virtual content. We discuss a potential simple solution based on restricting the field of view to the central region, and we present the results of a controlled human-subject study. The study results show objective benefits for this approach in producing behaviors that more closely match those that occur when seeing a real human, but also some drawbacks in overall acceptance of the restricted field of view. Second, we discuss the limited multimodal feedback provided by virtual humans in AR, present a potential improvement based on vibrotactile feedback induced via the floor to compensate for the limited augmented visual field, and report results showing that benefits of such vibrations are less visible in objective locomotion behavior than in subjective estimates of co-presence. Third, we investigate and document significant differences in the effects that real and virtual humans have on locomotion behavior in AR with respect to clearance distances, walking speed, and head motions. We discuss potential explanations for these effects related to social expectations, and analyze effects of different types of behaviors including idle standing, jumping, and walking that such real or virtual humans may exhibit in the presence of an observer.},
	address = {Los Alamitos, CA, USA},
	author = {M. Lee and G. Bruder and T. Hollerer and G. Welch},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794074},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;legged locomotion;collision avoidance;optical feedback;task analysis;vibrations},
	month = {apr},
	number = {04},
	pages = {1525-1534},
	publisher = {IEEE Computer Society},
	title = {Effects of Unaugmented Periphery and Vibrotactile Feedback on Proxemics with Virtual Humans in AR},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794074}}

@article{7383324,
	abstract = {The ray-shift phenomenon means the apparent distance shift in the display image plane between virtual and physical objects. It is caused by the difference in the refraction of virtual display and see-through optical paths derived from optical combiners that are necessary to provide a see-through capability in optical see-through head-mounted displays. In this work, through a human-subject experiment, we investigated the effects of ray-shift phenomenon induced by the optical combiner on depth perception for near-field distances (40 cm-100 cm). In our experiment, we considered three different configurations of optical combiner: horizontal-tilt and vertical-tilt configurations (using plate beamsplitters horizontally and vertically tilted by 45$\,^{\circ}$, respectively), and non-tilt configuration (using rectangular solid waveguides). Participants&#x27; depth perception errors in these configurations were compared with those in an ordinary condition (i.e., the condition where physical objects are directly shown without the displays) and theoretically estimated ones. According to the experimental results, the measured percentage depth perception errors were similar to the theoretically estimated ones, where the amount of estimated percentage depth errors was greater than 0.3%. Furthermore, the participants showed significantly larger depth perception errors in the horizontal-tilt configuration than in an ordinary condition, while no large errors were found in the vertical-tilt configuration. In the non-tilt configuration, the results were dependent on the thickness of optical combiner and target distance.},
	address = {Los Alamitos, CA, USA},
	author = {S. Lee and H. Hua},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518138},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {adaptive optics;optical imaging;optical refraction;optical distortion;optical waveguides;optical variables control;three-dimensional displays},
	month = {apr},
	number = {04},
	pages = {1432-1441},
	publisher = {IEEE Computer Society},
	title = {Effects of Configuration of Optical Combiner on Near-Field Depth Perception in Optical See-Through Head-Mounted Displays},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518138}}

@article{Lee2011:Effects-of-Viewing-Conditions,
	abstract = {We investigate the effects of viewing conditions and rotation methods on different types of collaborative tasks in a two-user colocated tabletop augmented reality (AR) environment. The viewing condition means how the manipulation of a tabletop world by one user is shown in the other users&#x27; views and the rotation method means what type of input devices is used to rotate the tabletop world for alternative orientations. Our experiment considered two viewing conditions (consistent view and inconsistent view), two rotation methods (direct turn and indirect turn), and two task types (synchronous and referring-strong type, and asynchronous and orientation-strong type). A 3D display environment called &quot;Stereoscopic Collaboration in Augmented and Projective Environments (SCAPE)'' was utilized as a test environment. According to the results, the viewing conditions had significant effects on several objective and subjective measurements. On task completion time, their effect for the synchronous and referring-strong type of task was opposite to that for the asynchronous and orientation-strong type of task. On the other hand, the rotation methods had significant effects only on the accumulated turn angle (for both task types) and the number of negotiation phrases (only in the inconsistent viewing condition for the asynchronous and orientation-strong type of task).},
	address = {Los Alamitos, CA, USA},
	author = {Sangyoon Lee and Hong Hua},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2011.49},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;augmented reality;groupware;referring-strong type;rotation methods;collaborative tabletop ar environment;augmented reality;3d display environment;stereoscopic collaboration in augmented and projective environments;scape;collaboration;three dimensional displays;mice;turning;visualization;performance evaluation;user interfaces;user interfaces.;augmented reality;collaborative computing;evaluation;human factors},
	month = {sep},
	number = {09},
	pages = {1245-1258},
	publisher = {IEEE Computer Society},
	title = {Effects of Viewing Conditions and Rotation Methods in a Collaborative Tabletop AR Environment},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2011.49}}

@article{Lee2009:Multithreaded-Hybrid-Feature,
	abstract = {We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality (AR) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking. Distinctive image features of the scene are detected and tracked frame-to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a synchronized multi-threaded manner: capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce user interaction methodology for establishing a global coordinate system and for placing virtual objects in the AR environment by tracking a user&#x27;s outstretched hand and estimating a camera pose relative to it. We evaluate the speed and accuracy of our hybrid feature tracking approach, and demonstrate a proof-of-concept application for enabling AR in unprepared tabletop environments, using bare hands for interaction.},
	address = {Los Alamitos, CA, USA},
	author = {T. Lee and T. H{\"o}llerer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2008.190},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;scene analysis},
	month = {may},
	number = {03},
	pages = {355-368},
	publisher = {IEEE Computer Society},
	title = {Multithreaded Hybrid Feature Tracking for Markerless Augmented Reality},
	volume = {15},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2008.190}}

@article{8642906,
	abstract = {In this paper, we propose a three-dimensional (3D) convolutional neural network (CNN)-based method for predicting the degree of motion sickness induced by a 360$\,^{\circ}$ stereoscopic video. We consider the user&#x27;s eye movement as a new feature, in addition to the motion velocity and depth features of a video used in previous work. For this purpose, we use saliency, optical flow, and disparity maps of an input video, which represent eye movement, velocity, and depth, respectively, as the input of the 3D CNN. To train our machine-learning model, we extend the dataset established in the previous work using two data augmentation techniques: frame shifting and pixel shifting. Consequently, our model can predict the degree of motion sickness more precisely than the previous method, and the results have a more similar correlation to the distribution of ground-truth sickness.},
	address = {Los Alamitos, CA, USA},
	author = {T. Lee and J. Yoon and I. Lee},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899186},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {videos;stereo image processing;three-dimensional displays;optical imaging;machine learning;optical saturation;optical computing},
	month = {may},
	number = {05},
	pages = {1919-1927},
	publisher = {IEEE Computer Society},
	title = {Motion Sickness Prediction in Stereoscopic Videos using 3D Convolutional Neural Networks},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899186}}

@article{7829397,
	abstract = {Recent popularity of consumer-grade virtual reality devices, such as the Oculus Rift and the HTC Vive, has enabled household users to experience highly immersive virtual environments. We take advantage of the commercial availability of these devices to provide an immersive and novel virtual reality training approach, designed to teach individuals how to survive earthquakes, in common indoor environments. Our approach makes use of virtual environments realistically populated with furniture objects for training. During a training, a virtual earthquake is simulated. The user navigates in, and manipulates with, the virtual environments to avoid getting hurt, while learning the observation and self-protection skills to survive an earthquake. We demonstrated our approach for common scene types such as offices, living rooms and dining rooms. To test the effectiveness of our approach, we conducted an evaluation by asking users to train in several rooms of a given scene type and then test in a new room of the same type. Evaluation results show that our virtual reality training approach is effective, with the participants who are trained by our approach performing better, on average, than those trained by alternative approaches in terms of the capabilities to avoid physical damage and to detect potentially dangerous objects.},
	address = {Los Alamitos, CA, USA},
	author = {C. Li and W. Liang and C. Quigley and Y. Zhao and L. Yu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2656958},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {training;earthquakes;virtual environments;solid modeling;safety;injuries},
	month = {apr},
	number = {04},
	pages = {1275-1284},
	publisher = {IEEE Computer Society},
	title = {Earthquake Safety Training through Virtual Drills},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2656958}}

@article{9382903,
	abstract = {With the rapidly increasing resolutions of 360$\,^{\circ}$ cameras, head-mounted displays, and live-streaming services, streaming high-resolution panoramic videos over limited-bandwidth networks is becoming a critical challenge. Foveated video streaming can address this rising challenge in the context of eye-tracking-equipped virtual reality head-mounted displays. However, conventional log-polar foveated rendering suffers from a number of visual artifacts such as aliasing and flickering. In this paper, we introduce a new log-rectilinear transformation that incorporates summed-area table filtering and off-the-shelf video codecs to enable foveated streaming of 360$\,^{\circ}$ videos suitable for VR headsets with built-in eye-tracking. To validate our approach, we build a client-server system prototype for streaming 360$\,^{\circ}$ videos which leverages parallel algorithms over real-time video transcoding. We conduct quantitative experiments on an existing 360$\,^{\circ}$ video dataset and observe that the log-rectilinear transformation paired with summed-area table filtering heavily reduces flickering compared to log-polar subsampling while also yielding an additional 10% reduction in bandwidth usage.},
	address = {Los Alamitos, CA, USA},
	author = {D. Li and R. Du and A. Babu and C. D. Brumar and A. Varshney},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067762},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {streaming media;rendering (computer graphics);headphones;bandwidth;pipelines;video codecs;two dimensional displays},
	month = {may},
	number = {05},
	pages = {2638-2647},
	publisher = {IEEE Computer Society},
	title = {A Log-Rectilinear Transformation for Foveated 360-degree Video Streaming},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067762}}

@article{10051634,
	abstract = {The combination of augmented reality (AR) and medicine is an important trend in current research. The powerful display and interaction capabilities of the AR system can assist doctors to perform more complex operations. Since the tooth itself is an exposed rigid body structure, dental AR is a relatively hot research direction with application potential. However, none of the existing dental AR solutions are designed for wearable AR devices such as AR glasses. At the same time, these methods rely on high-precision scanning equipment or auxiliary positioning markers, which greatly increases the operational complexity and cost of clinical AR. In this work, we propose a simple and accurate neural-implicit model-driven dental AR system, named ImTooth, and adapted for AR glasses. Based on the modeling capabilities and differentiable optimization properties of state-of-the-art neural implicit representations, our system fuses reconstruction and registration in a single network, greatly simplifying the existing dental AR solutions and enabling reconstruction, registration, and interaction. Specifically, our method learns a scale-preserving voxel-based neural implicit model from multi-view images captured from a textureless plaster model of the tooth. Apart from color and surface, we also learn the consistent edge feature inside our representation. By leveraging the depth and edge information, our system can register the model to real images without additional training. In practice, our system uses a single Microsoft HoloLens 2 as the only sensor and display device. Experiments show that our method can reconstruct high-precision models and accomplish accurate registration. It is also robust to weak, repeating and inconsistent textures. We also show that our system can be easily integrated into dental diagnostic and therapeutic procedures, such as bracket placement guidance.},
	address = {Los Alamitos, CA, USA},
	author = {H. Li and H. Zhai and X. Yang and Z. Wu and Y. Zheng and H. Wang and J. Wu and H. Bao and G. Zhang},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247459},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {dentistry;image reconstruction;teeth;three-dimensional displays;location awareness;augmented reality;glass},
	month = {may},
	number = {05},
	pages = {2837-2846},
	publisher = {IEEE Computer Society},
	title = {ImTooth: Neural Implicit Tooth for Dental Augmented Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247459}}

@article{9714120,
	abstract = {Construction industry has the largest number of preventable fatal injuries, providing effective safety training practices can play a significant role in reducing the number of fatalities. Building on recent advancements in virtual reality-based training, we devised a novel approach to synthesize construction safety training scenarios to train users on how to proficiently inspect the potential hazards on construction sites in virtual reality. Given the training specifications such as individual training preferences and target training time, we synthesize personalized VR training scenarios through an optimization approach. We validated our approach by conducting user studies where users went through our personalized guidance VR training, free exploration VR training, or slides training. Results suggest that personalized guidance VR training approach can more effectively improve users&#x27; construction hazard inspection skills.},
	address = {Los Alamitos, CA, USA},
	author = {W. Li and H. Huang and T. Solomon and B. Esmaeili and L. Yu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150510},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {hazards;training;vehicle dynamics;virtual reality;costs;task analysis;inspection},
	month = {may},
	number = {05},
	pages = {1993-2002},
	publisher = {IEEE Computer Society},
	title = {Synthesizing Personalized Construction Safety Training Scenarios for VR Training},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150510}}

@article{8642445,
	abstract = {The functionality of a workspace is one of the most important considerations in both virtual world design and interior design. To offer appropriate functionality to the user, designers usually take some general rules into account, e.g., general workflow and average stature of users, which are summarized from the population statistics. Yet, such general rules cannot reflect the personal preferences of a single individual, which vary from person to person. In this paper, we intend to optimize a functional workspace according to the personal preferences of the specific individual who will use it. We come up with an approach to learn the individual&#x27;s personal preferences from his activities while using a virtual version of the workspace via virtual reality devices. Then, we construct a cost function, which incorporates personal preferences, spatial constraints, pose assessments, and visual field. At last, the cost function is optimized to achieve an optimal layout. To evaluate the approach, we experimented with different settings. The results of the user study show that the workspaces updated in this way better fit the users.},
	address = {Los Alamitos, CA, USA},
	author = {W. Liang and J. Liu and Y. Lang and B. Ning and L. Yu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898721},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {layout;task analysis;cost function;visualization;software;three-dimensional displays},
	month = {may},
	number = {05},
	pages = {1836-1845},
	publisher = {IEEE Computer Society},
	title = {Functional Workspace Optimization via Learning Personal Preferences from Virtual Experiences},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898721}}

@article{10049655,
	abstract = {Virtual humans, including virtual agents and avatars, play an increasingly important role as VR technology advances. For example, virtual humans are used as digital bodies of users in social VR or as interfaces for AI assistants in online financing. Interpersonal trust is an essential prerequisite in real-life interactions, as well as in the virtual world. However, to date, there are no established interpersonal trust measurement tools specifically for virtual humans in virtual reality. This study fills the gap, by contributing a novel validated behavioural tool to measure interpersonal trust towards a specific virtual social interaction partner in social VR. This validated paradigm is inspired by a previously proposed virtual maze task that measures trust towards virtual characters. In the current study, a variant of this paradigm was implemented. The task of the users (the trustors) is to navigate through a maze in virtual reality, where they can interact with a virtual human (the trustee). They can choose to 1) ask for advice and 2) follow the advice from the virtual human if they want to. These measures served as behavioural measures of trust. We conducted a validation study with 70 participants in a between-subject design. The two conditions did not differ in the content of the advice but in the appearance, tone of voice and engagement of the trustees (alleged as avatars controlled by other participants). Results indicate that the experimental manipulation was successful, as participants rated the virtual human as more trustworthy in the trustworthy condition than in the untrustworthy condition. Importantly, this manipulation affected the trust behaviour of our participants, who, in the trustworthy condition, asked for advice more often and followed advice more often, indicating that the paradigm is sensitive to assessing interpersonal trust towards virtual humans. Thus, our paradigm can be used to measure differences in interpersonal trust towards virtual humans and may serve as a valuable research tool to study trust in virtual reality.},
	address = {Los Alamitos, CA, USA},
	author = {J. Lin and J. Cronje and I. Kathner and P. Pauli and M. Latoschik},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247095},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;psychology;games;particle measurements;atmospheric measurements;investment;task analysis},
	month = {may},
	number = {05},
	pages = {2401-2411},
	publisher = {IEEE Computer Society},
	title = {Measuring Interpersonal Trust towards Virtual Humans with a Virtual Maze Paradigm},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247095}}

@article{Lin2008:Real-Time-Path-Planning,
	abstract = {We present a novel approach for efficient path planning and navigation of multiple virtual agents in complex dynamic scenes. We introduce a new data structure, Multi-agent Navigation Graph (MaNG), which is constructed using first- and second-order Voronoi diagrams. The MaNG is used to perform route planning and proximity computations for each agent in real time. Moreover, we use the path information and proximity relationships for local dynamics computation of each agent by extending a social force model [Helbing05]. We compute the MaNG using graphics hardware and present culling techniques to accelerate the computation. We also address undersampling issues and present techniques to improve the accuracy of our algorithm. Our algorithm is used for real-time multi-agent planning in pursuit-evasion, terrain exploration and crowd simulation scenarios consisting of hundreds of moving agents, each with a distinct goal.},
	address = {Los Alamitos, CA, USA},
	author = {M. C. Lin and E. Andersen and A. Sud and S. Curtis and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2008.27},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {computational geometry and object modeling;geometric algorithms;languages;and systems;three-dimensional graphics and realism;animation;virtual reality},
	month = {may},
	number = {03},
	pages = {526-538},
	publisher = {IEEE Computer Society},
	title = {Real-Time Path Planning in Dynamic Virtual Environments Using Multiagent Navigation Graphs},
	volume = {14},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2008.27}}

@article{7383304,
	abstract = {We describe an augmented reality, optical see-through display based on a DMD chip with an extremely fast (16 kHz) binary update rate. We combine the techniques of post-rendering 2-D offsets and just-in-time tracking updates with a novel modulation technique for turning binary pixels into perceived gray scale. These processing elements, implemented in an FPGA, are physically mounted along with the optical display elements in a head tracked rig through which users view synthetic imagery superimposed on their real environment. The combination of mechanical tracking at near-zero latency with reconfigurable display processing has given us a measured average of 80 s of end-to-end latency (from head motion to change in photons from the display) and also a versatile test platform for extremely-low-latency display systems. We have used it to examine the trade-offs between image quality and cost (i.e. power and logical complexity) and have found that quality can be maintained with a fairly simple display modulation scheme.},
	address = {Los Alamitos, CA, USA},
	author = {P. Lincoln and A. Blate and M. Singh and T. Whitted and A. State and A. Lastra and H. Fuchs},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518038},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);field programmable gate arrays;tracking;modulation;graphics processing units;delays;optical imaging},
	month = {apr},
	number = {04},
	pages = {1367-1376},
	publisher = {IEEE Computer Society},
	title = {From Motion to Photons in 80 Microseconds: Towards Minimal Latency for Virtual and Augmented Reality},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518038}}

@article{10049725,
	abstract = {The skeleton-based human action recognition has broad application prospects in the field of virtual reality, as skeleton data is more resistant to data noise such as background interference and camera angle changes. Notably, recent works treat the human skeleton as a non-grid representation, e.g., skeleton graph, then learns the spatio-temporal pattern via graph convolution operators. Still, the stacked graph convolution plays a marginal role in modeling long-range dependences that may contain crucial action semantic cues. In this work, we introduce a skeleton large kernel attention operator (SLKA), which can enlarge the receptive field and improve channel adaptability without increasing too much computational burden. Then a spatiotemporal SLKA module (ST-SLKA) is integrated, which can aggregate long-range spatial features and learn long-distance temporal correlations. Further, we have designed a novel skeleton-based action recognition network architecture called the spatiotemporal large-kernel attention graph convolution network (LKA-GCN). In addition, large-movement frames may carry significant action information. This work proposes a joint movement modeling strategy (JMM) to focus on valuable temporal interactions. Ultimately, on the NTU-RGBD 60, NTU-RGBD 120 and Kinetics-Skeleton 400 action datasets, the performance of our LKA-GCN has achieved a state-of-the-art level.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Liu and H. Zhang and Y. Li and K. He and D. Xu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247075},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {skeleton;convolution;kernel;adaptation models;joints;topology;task analysis},
	month = {may},
	number = {05},
	pages = {2575-2585},
	publisher = {IEEE Computer Society},
	title = {Skeleton-based Human Action Recognition via Large-kernel Attention Graph Convolutional Network},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247075}}

@article{Liu2012:Online-Tracking-of-Outdoor,
	abstract = {In augmented reality, one of key tasks to achieve a convincing visual appearance consistency between virtual objects and video scenes is to have a coherent illumination along the whole sequence. As outdoor illumination is largely dependent on the weather, the lighting condition may change from frame to frame. In this paper, we propose a full image-based approach for online tracking of outdoor illumination variations from videos captured with moving cameras. Our key idea is to estimate the relative intensities of sunlight and skylight via a sparse set of planar feature-points extracted from each frame. To address the inevitable feature misalignments, a set of constraints are introduced to select the most reliable ones. Exploiting the spatial and temporal coherence of illumination, the relative intensities of sunlight and skylight are finally estimated by using an optimization process. We validate our technique on a set of real-life videos and show that the results with our estimations are visually coherent along the video sequences.},
	address = {Los Alamitos, CA, USA},
	author = {Yanli Liu and X. Granier},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.53},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {video signal processing;augmented reality;cameras;feature extraction;image sequences;lighting;object tracking;optimisation;optimization process;online tracking;outdoor lighting variation;augmented reality;moving camera;visual appearance consistency;virtual object;video scene;illumination;video sequence;lighting condition;full image-based approach;sunlight relative intensity;skylight relative intensity;planar feature point extraction;spatial coherence;temporal coherence;lighting;estimation;cameras;three dimensional displays;feature extraction;buildings;geometry;moving cameras.;augmented reality;illumination coherence},
	month = {apr},
	number = {04},
	pages = {573-580},
	publisher = {IEEE Computer Society},
	title = {Online Tracking of Outdoor Lighting Variations for Augmented Reality with Moving Cameras},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.53}}

@article{6777431,
	abstract = {The exchange of avatars, i.e. the actual fact of changing once avatar with another one, is a promising trend in multi-actor virtual environments. It provides new opportunities for users, such as controlling a different avatar for a specific action, retrieving knowledge belonging to a particular avatar, solving conflicts and deadlocks situations or even helping another user. Virtual Environments for Training are especially affected by this trend as a specific role derived from a scenario is usually assigned to a unique avatar. Despite the increasing use of avatar exchange, users&#x27; perception and understanding of this mechanism have not been studied. In this paper, we propose two complementary user-centered evaluations that aim at comparing several representations for the exchange of avatars; these are termed exchange metaphors. Our first experiment focuses on the perception of an exchange by a user who is not involved in the exchange, and the second experiment analyzes the perception of an exchange triggered by the user. Results show that the use of visual feedback globally aids better understanding of the exchange mechanism in both cases. Our first experiment suggests, however, that visual feedback is less efficient than a simple popup notification in terms of task duration. In addition, the second experiment shows that much simpler metaphors with no visual effect are generally preferred because of their efficiency.},
	address = {Los Alamitos, CA, USA},
	author = {T. Lopez and R. Bouville and E. Loup-escande and F. Nouviale and V. Gouranton and B. Arnaldi},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.22},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;visualization;virtual environments;collaboration;image color analysis;engines;wheels},
	month = {apr},
	number = {04},
	pages = {644-653},
	publisher = {IEEE Computer Society},
	title = {Exchange of Avatars: Toward a Better Perception and Understanding},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.22}}

@article{8269373,
	abstract = {This paper presents a novel approach to content delivery for video streaming services. It exploits information from connected eye-trackers embedded in the next generation of VR Head Mounted Displays (HMDs). The proposed solution aims to deliver high visual quality, in real time, around the users&#x27; fixations points while lowering the quality everywhere else. The goal of the proposed approach is to substantially reduce the overall bandwidth requirements for supporting VR video experiences while delivering high levels of user perceived quality. The prerequisites to achieve these results are: (1) mechanisms that can cope with different degrees of latency in the system and (2) solutions that support fast adaptation of video quality in different parts of a frame, without requiring a large increase in bitrate. A novel codec configuration, capable of supporting near-instantaneous video quality adaptation in specific portions of a video frame, is presented. The proposed method exploits in-built properties of HEVC encoders and while it introduces a moderate amount of error, these errors are indetectable by users. Fast adaptation is the key to enable gaze-aware streaming and its reduction in bandwidth. A testbed implementing gaze-aware streaming, together with a prototype HMD with in-built eye tracker, is presented and was used for testing with real users. The studies quantified the bandwidth savings achievable by the proposed approach and characterize the relationships between Quality of Experience (QoE) and network latency. The results showed that up to 83% less bandwidth is required to deliver high QoE levels to the users, as compared to conventional solutions.},
	address = {Los Alamitos, CA, USA},
	author = {P. Lungaro and R. Sjoberg and A. Valero and A. Mittal and K. Tollmar},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794119},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {streaming media;bandwidth;quality of experience;visualization;bit rate;rendering (computer graphics);servers},
	month = {apr},
	number = {04},
	pages = {1535-1544},
	publisher = {IEEE Computer Society},
	title = {Gaze-Aware Streaming Solutions for the Next Generation of Mobile VR Experiences},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794119}}

@article{8260916,
	abstract = {We propose a novel 360$\,^{\circ}$ scene representation for converting real scenes into stereoscopic 3D virtual reality content with head-motion parallax. Our image-based scene representation enables efficient synthesis of novel views with six degrees-of-freedom (6-DoF) by fusing motion fields at two scales: (1) disparity motion fields carry implicit depth information and are robustly estimated from multiple laterally displaced auxiliary viewpoints, and (2) pairwise motion fields enable real-time flow-based blending, which improves the visual fidelity of results by minimizing ghosting and view transition artifacts. Based on our scene representation, we present an end-to-end system that captures real scenes with a robotic camera arm, processes the recorded data, and finally renders the scene in a head-mounted display in real time (more than 40 Hz). Our approach is the first to support head-motion parallax when viewing real 360$\,^{\circ}$ scenes. We demonstrate compelling results that illustrate the enhanced visual experience --- and hence sense of immersion-achieved with our approach compared to widely-used stereoscopic panoramas.},
	address = {Los Alamitos, CA, USA},
	author = {B. Luo and F. Xu and C. Richardt and J. Yong},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794071},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;visualization;image reconstruction;rendering (computer graphics);real-time systems;videos;robustness},
	month = {apr},
	number = {04},
	pages = {1545-1553},
	publisher = {IEEE Computer Society},
	title = {Parallax360: Stereoscopic 360$\,^{\circ}$ Scene Representation for Head-Motion Parallax},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794071}}

@article{10049727,
	abstract = {360-degree video streaming has gained tremendous growth over the past years. However, the delivery of 360-degree videos over the Internet still suffers from the scarcity of network bandwidth and adverse network conditions (e.g., packet loss, delay). In this paper, we propose a practical neural-enhanced 360-degree video streaming framework called Masked360, which can significantly reduce bandwidth consumption and achieve robustness against packet loss. In Masked360, instead of transmitting the complete video frame, the video server only transmits a masked low-resolution version of each video frame to reduce bandwidth significantly. When delivering masked video frames, the video server also sends a lightweight neural network model called MaskedEncoder to clients. Upon receiving masked frames, the client can reconstruct the original 360-degree video frames and start playback. To further improve the quality of video streaming, we also propose a set of optimization techniques, such as complexity-based patch selection, quarter masking strategy, redundant patch transmission and enhanced model training methods. In addition to bandwidth savings, Masked360 is also robust to packet loss during the transmission, because packet losses can be concealed by the reconstruction operation performed by the MaskedEncoder. Finally, we implement the whole Masked360 framework and evaluate its performance using real datasets. The experimental results show that Masked360 can achieve 4K 360-degree video streaming with bandwidth as low as 2.4 Mbps. Besides, video quality of Masked360 is also improved significantly, with an improvement of 5.24-16.61% in terms of PSNR and 4.74-16.15% in terms of SSIM compared to other baselines.},
	address = {Los Alamitos, CA, USA},
	author = {Z. Luo and B. Chai and Z. Wang and M. Hu and D. Wu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247076},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {streaming media;bandwidth;computational modeling;packet loss;training;servers;visualization},
	month = {may},
	number = {05},
	pages = {2690-2699},
	publisher = {IEEE Computer Society},
	title = {Masked360: Enabling Robust 360-degree Video Streaming with Ultra Low Bandwidth Consumption},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247076}}

@article{8260976,
	abstract = {Displays that can portray environments that are perceivable from multiple views are known as multiscopic displays. Some multiscopic displays enable realistic perception of 3D environments without the need for cumbersome mounts or fragile head-tracking algorithms. These automultiscopic displays carefully control the distribution of emitted light over space, direction (angle) and time so that even a static image displayed can encode parallax across viewing directions (Iightfield). This allows simultaneous observation by multiple viewers, each perceiving 3D from their own (correct) perspective. Currently, the illusion can only be effectively maintained over a narrow range of viewing angles. In this paper, we propose and analyze a simple solution to widen the range of viewing angles for automultiscopic displays that use parallax barriers. We propose the use of a refractive medium, with a high refractive index, between the display and parallax barriers. The inserted medium warps the exitant lightfield in a way that increases the potential viewing angle. We analyze the consequences of this warp and build a prototype with a 93% increase in the effective viewing angle.},
	address = {Los Alamitos, CA, USA},
	author = {G. Lyu and X. Shen and T. Komura and K. Subr and L. Teng},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794599},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;two dimensional displays;rendering (computer graphics);prototypes;spatial resolution;refractive index;electronic mail},
	month = {apr},
	number = {04},
	pages = {1554-1563},
	publisher = {IEEE Computer Society},
	title = {Widening Viewing Angles of Automultiscopic Displays Using Refractive Inserts},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794599}}

@article{8260946,
	abstract = {360$\,^{\circ}$ images and video have become extremely popular formats for immersive displays, due in large part to the technical ease of content production. While many experiences use a single camera viewpoint, an increasing number of experiences use multiple camera locations. In such multi-view 360$\,^{\circ}$ media (MV360M) systems, a visual effect is required when the user transitions from one camera location to another. This effect can take several forms, such as a cut or an image-based warp, and the choice of effect may impact many aspects of the experience, including issues related to enjoyment and scene understanding. To investigate the effect of transition types on immersive MV360M experiences, a repeated-measures experiment was conducted with 31 participants. Wearing a head-mounted display, participants explored four static scenes, for which multiple 360$\,^{\circ}$ images and a reconstructed 3D model were available. Three transition types were examined: teleport, a linear move through a 3D model of the scene, and an image-based transition using a M{\"o}bius transformation. The metrics investigated included spatial awareness, users&#x27; movement profiles, transition preference and the subjective feeling of moving through the space. Results indicate that there was no significant difference between transition types in terms of spatial awareness, while significant differences were found for users&#x27; movement profiles, with participants taking 1.6 seconds longer to select their next location following a teleport transition. The model and M{\"o}bius transitions were significantly better in terms of creating the feeling of moving through the space. Preference was also significantly different, with model and teleport transitions being preferred over M{\"o}bius transitions. Our results indicate that trade-offs between transitions will require content creators to think carefully about what aspects they consider to be most important when producing MV360M experiences.},
	address = {Los Alamitos, CA, USA},
	author = {A. MacQuarrie and A. Steed},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793561},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cameras;media;solid modeling;three-dimensional displays;navigation;measurement;resists},
	month = {apr},
	number = {04},
	pages = {1564-1573},
	publisher = {IEEE Computer Society},
	title = {The Effect of Transition Type in Multi-View 360$\,^{\circ}$ Media},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793561}}

@article{7383340,
	abstract = {Temporal coherence of annotations is an important factor in augmented reality user interfaces and for information visualization. In this paper, we empirically evaluate four different techniques for annotation. Based on these findings, we follow up with subjective evaluations in a second experiment. Results show that presenting annotations in object space or image space leads to a significant difference in task performance. Furthermore, there is a significant interaction between rendering space and update frequency of annotations. Participants improve significantly in locating annotations, when annotations are presented in object space, and view management update rate is limited. In a follow-up experiment, participants appear to be more satisfied with limited update rate in comparison to a continuous update rate of the view management system.},
	address = {Los Alamitos, CA, USA},
	author = {J. Madsen and M. Tatzqern and C. B. Madsen and D. Schmalstieg and D. Kalkofen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518318},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {layout;three-dimensional displays;image resolution;coherence;force;animation;data visualization},
	month = {apr},
	number = {04},
	pages = {1415-1423},
	publisher = {IEEE Computer Society},
	title = {Temporal Coherence Strategies for Augmented Reality Labeling},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518318}}

@article{Majumder2011:Autocalibrating-Tiled-Projectors,
	abstract = {In this paper, we present a novel technique to calibrate multiple casually aligned projectors on fiducial-free piecewise smooth vertically extruded surfaces using a single camera. Such surfaces include cylindrical displays and CAVEs, common in immersive virtual reality systems. We impose two priors to the display surface. We assume the surface is a piecewise smooth vertically extruded surface for which the aspect ratio of the rectangle formed by the four corners of the surface is known and the boundary is visible and segmentable. Using these priors, we can estimate the display&#x27;s 3D geometry and camera extrinsic parameters using a nonlinear optimization technique from a single image without any explicit display to camera correspondences. Using the estimated camera and display properties, the intrinsic and extrinsic parameters of each projector are recovered using a single projected pattern seen by the camera. This in turn is used to register the images on the display from any arbitrary viewpoint making it appropriate for virtual reality systems. The fast convergence and robustness of this method is achieved via a novel dimension reduction technique for camera parameter estimation and a novel deterministic technique for projector property estimation. This simplicity, efficiency, and robustness of our method enable several coveted features for nonplanar projection-based displays. First, it allows fast recalibration in the face of projector, display or camera movements and even change in display shape. Second, this opens up, for the first time, the possibility of allowing multiple projectors to overlap on the corners of the CAVE---a popular immersive VR display system. Finally, this opens up the possibility of easily deploying multiprojector displays on aesthetic novel shapes for edutainment and digital signage applications.},
	address = {Los Alamitos, CA, USA},
	author = {A. Majumder and B. Sajadi},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2011.33},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {tiled displays;autocalibration;geometric registration;cylindrical displays;caves.},
	month = {sep},
	number = {09},
	pages = {1209-1222},
	publisher = {IEEE Computer Society},
	title = {Autocalibrating Tiled Projectors on Piecewise Smooth Vertically Extruded Surfaces},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2011.33}}

@article{10049676,
	abstract = {Many studies show the significance of the Proteus effect for serious virtual reality applications. The present study extends the existing knowledge by considering the relationship (congruence) between the self-embodiment (avatar) and the virtual environment. We investigated the impact of avatar and environment types and their congruence on avatar plausibility, sense of embodiment, spatial presence, and the Proteus effect. In a $2\times 2$ between-subjects design, participants embodied either an avatar in sports- or business wear in a semantic congruent or incongruent environment while performing lightweight exercises in virtual reality. The avatar-environment congruence significantly affected the avatar&#x27;s plausibility but not the sense of embodiment or spatial presence. However, a significant Proteus effect emerged only for participants who reported a high feeling of (virtual) body ownership, indicating that a strong sense of having and owning a virtual body is key to facilitating the Proteus effect. We discuss the results assuming current theories of bottom-up and top-down determinants of the Proteus effect and thus contribute to understanding its underlying mechanisms and determinants.},
	address = {Los Alamitos, CA, USA},
	author = {D. Mal and E. Wolf and N. Dollinger and C. Wienrich and M. Latoschik},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247089},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;behavioral sciences;virtual reality;semantics;business;virtual environments;coherence},
	month = {may},
	number = {05},
	pages = {2358-2368},
	publisher = {IEEE Computer Society},
	title = {The Impact of Avatar and Environment Congruence on Plausibility, Embodiment, Presence, and the Proteus Effect in Virtual Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247089}}

@article{Marchal2012:Walking-in-a-Cube:-Novel,
	abstract = {Immersive spaces such as 4-sided displays with stereo viewing and high-quality tracking provide a very engaging and realistic virtual experience. However, walking is inherently limited by the restricted physical space, both due to the screens (limited translation) and the missing back screen (limited rotation). In this paper, we propose three novel locomotion techniques that have three concurrent goals: keep the user safe from reaching the translational and rotational boundaries; increase the amount of real walking and finally, provide a more enjoyable and ecological interaction paradigm compared to traditional controller-based approaches. We notably introduce the &quot;Virtual Companion&quot;, which uses a small bird to guide the user through VEs larger than the physical space. We evaluate the three new techniques through a user study with travel-to-target and path following tasks. The study provides insight into the relative strengths of each new technique for the three aforementioned goals. Specifically, if speed and accuracy are paramount, traditional controller interfaces augmented with our novel warning techniques may be more appropriate; if physical walking is more important, two of our paradigms (extended Magic Barrier Tape and Constrained Wand) should be preferred; last, fun and ecological criteria would favor the Virtual Companion.},
	address = {Los Alamitos, CA, USA},
	author = {M. Marchal and E. Chapoulie and P. Vangorp and G. Cirio and A. Lecuyer and G. Drettakis},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.60},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;computer displays;user interfaces;constrained wand paradigm;virtual environment navigation;immersive space;4-sided display;stereo viewing;high-quality tracking;virtual experience;limited translation;limited rotation;locomotion technique;translational boundary;rotational boundary;ecological interaction paradigm;controller-based approach;virtual companion;user study;travel-to-target task;path following task;warning technique;controller interface;magic barrier tape paradigm;legged locomotion;navigation;birds;safety;virtual environments;visualization;face;restricted workspaces.;virtual reality;locomotion techniques;walking},
	month = {apr},
	number = {04},
	pages = {546-554},
	publisher = {IEEE Computer Society},
	title = {Walking in a Cube: Novel Metaphors for Safely Navigating Large Virtual Environments in Restricted Real Workspaces},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.60}}

@article{9382882,
	abstract = {Surround-view panoramic images and videos have become a popular form of media for interactive viewing on mobile devices and virtual reality headsets. Viewing such media provides a sense of immersion by allowing users to control their view direction and experience an entire environment. When using a virtual reality headset, the level of immersion can be improved by leveraging stereoscopic capabilities. Stereoscopic images are generated in pairs, one for the left eye and one for the right eye, and result in providing an important depth cue for the human visual system. For computer generated imagery, rendering proper stereo pairs is well known for a fixed view. However, it is much more difficult to create omnidirectional stereo pairs for a surround-view projection that work well when looking in any direction. One major drawback of traditional omnidirectional stereo images is that they suffer from binocular misalignment in the peripheral vision as a user&#x27;s view direction approaches the zenith / nadir (north / south pole) of the projection sphere. This paper presents a real-time geometry-based approach for omnidirectional stereo rendering that fits into the standard rendering pipeline. Our approach includes tunable parameters that enable pole merging - a reduction in the stereo effect near the poles that can minimize binocular misalignment. Results from a user study indicate that pole merging reduces visual fatigue and discomfort associated with binocular misalignment without inhibiting depth perception.},
	address = {Los Alamitos, CA, USA},
	author = {T. Marrinan and M. E. Papka},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067780},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);merging;real-time systems;visualization;image edge detection;cameras;standards},
	month = {may},
	number = {05},
	pages = {2587-2596},
	publisher = {IEEE Computer Society},
	title = {Real-Time Omnidirectional Stereo Rendering: Generating 360$\,^{\circ}$ Surround-View Panoramic Images for Comfortable Immersive Viewing},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067780}}

@article{9714046,
	abstract = {Understanding and modeling the dynamics of human gaze behavior in 360$\,^{\circ}$ environments is crucial for creating, improving, and developing emerging virtual reality applications. However, recruiting human observers and acquiring enough data to analyze their behavior when exploring virtual environments requires complex hardware and software setups, and can be time-consuming. Being able to generate virtual observers can help overcome this limitation, and thus stands as an open problem in this medium. Particularly, generative adversarial approaches could alleviate this challenge by generating a large number of scanpaths that reproduce human behavior when observing new scenes, essentially mimicking virtual observers. However, existing methods for scanpath generation do not adequately predict realistic scanpaths for 360$\,^{\circ}$ images. We present ScanGAN360, a new generative adversarial approach to address this problem. We propose a novel loss function based on dynamic time warping and tailor our network to the specifics of 360$\,^{\circ}$ images. The quality of our generated scanpaths outperforms competing approaches by a large margin, and is almost on par with the human baseline. ScanGAN360 allows fast simulation of large numbers of virtual observers, whose behavior mimics real users, enabling a better understanding of gaze behavior, facilitating experimentation, and aiding novel applications in virtual reality and beyond.},
	address = {Los Alamitos, CA, USA},
	author = {D. Martin and A. Serrano and A. W. Bergman and G. Wetzstein and B. Masia},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150502},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {observers;predictive models;solid modeling;virtual environments;visualization;mimics;task analysis},
	month = {may},
	number = {05},
	pages = {2003-2013},
	publisher = {IEEE Computer Society},
	title = {ScanGAN360: A Generative Model of Realistic Scanpaths for 360$\,^{\circ}$ Images},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150502}}

@article{10049723,
	abstract = {Human performance is poor at detecting certain changes in a scene, a phenomenon known as change blindness. Although the exact reasons of this effect are not yet completely understood, there is a consensus that it is due to our constrained attention and memory capacity: We create our own mental, structured representation of what surrounds us, but such representation is limited and imprecise. Previous efforts investigating this effect have focused on 2D images; however, there are significant differences regarding attention and memory between 2D images and the viewing conditions of daily life. In this work, we present a systematic study of change blindness using immersive 3D environments, which offer more natural viewing conditions closer to our daily visual experience. We devise two experiments; first, we focus on analyzing how different change properties (namely type, distance, complexity, and field of view) may affect change blindness. We then further explore its relation with the capacity of our visual working memory and conduct a second experiment analyzing the influence of the number of changes. Besides gaining a deeper understanding of the change blindness effect, our results may be leveraged in several VR applications such as redirected walking, games, or even studies on saliency or attention prediction.},
	address = {Los Alamitos, CA, USA},
	author = {D. Martin and X. Sun and D. Gutierrez and B. Masia},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247102},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {blindness;visualization;complexity theory;three-dimensional displays;observers;systematics;media},
	month = {may},
	number = {05},
	pages = {2446-2455},
	publisher = {IEEE Computer Society},
	title = {A Study of Change Blindness in Immersive Environments},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247102}}

@article{6479189,
	abstract = {We propose a new olfactory display system that can generate an odor distribution on a two-dimensional display screen. The proposed system has four fans on the four corners of the screen. The airflows that are generated by these fans collide multiple times to create an airflow that is directed towards the user from a certain position on the screen. By introducing odor vapor into the airflows, the odor distribution is as if an odor source had been placed onto the screen. The generated odor distribution leads the user to perceive the odor as emanating from a specific region of the screen. The position of this virtual odor source can be shifted to an arbitrary position on the screen by adjusting the balance of the airflows from the four fans. Most users do not immediately notice the odor presentation mechanism of the proposed olfactory display system because the airflow and perceived odor come from the display screen rather than the fans. The airflow velocity can even be set below the threshold for airflow sensation, such that the odor alone is perceived by the user. We present experimental results that show the airflow field and odor distribution that are generated by the proposed system. We also report sensory test results to show how the generated odor distribution is perceived by the user and the issues that must be considered in odor presentation.},
	address = {Los Alamitos, CA, USA},
	author = {H. Matsukura and T. Yoneda and H. Ishida},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.40},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {olfactory;fans;face;position measurement;gas detectors;educational institutions},
	month = {apr},
	number = {04},
	pages = {606-615},
	publisher = {IEEE Computer Society},
	title = {Smelling Screen: Development and Evaluation of an Olfactory Display System for Presenting a Virtual Odor Source},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.40}}

@article{10049712,
	abstract = {Users in a prolonged experience of virtual reality adopt a sitting position according to their task, as they do in the real world. However, inconsistencies in the haptic feedback from a chair they sit on in the real world and that which is expected in the virtual world decrease the feeling of presence. We aimed to change the perceived haptic features of a chair by shifting the position and angle of the users&#x27; viewpoints in the virtual reality environment. The targeted features in this study were seat softness and backrest flexibility. To enhance the seat softness, we shifted the virtual viewpoint using an exponential formula soon after a user&#x27;s bottom contacted the seat surface. The flexibility of the backrest was manipulated by moving the viewpoint, which followed the tilt of the virtual backrest. These shifts make users feel as if their body moves along with the viewpoint; as a result, they would perceive pseudo-softness or flexibility consistently with the body movement. Based on subjective evaluations, we confirmed that the participants perceived the seat as being softer and the backrest as being more flexible than the actual ones. These results demonstrated that only shifting the viewpoint could change the participants&#x27; perceptions of the haptic features of their seats, although significant changes created strong discomfort.},
	address = {Los Alamitos, CA, USA},
	author = {M. Matsumuro and S. Mori and Y. Kataoka and F. Igarashi and F. Shibata and A. Kimura},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247056},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;haptic interfaces;deformation;animation;task analysis;legged locomotion;propioception},
	month = {may},
	number = {05},
	pages = {2230-2238},
	publisher = {IEEE Computer Society},
	title = {Modified Egocentric Viewpoint for Softer Seated Experience in Virtual Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247056}}

@article{6777442,
	abstract = {We present an approach to model dynamic, data-driven source and listener directivity for interactive wave-based sound propagation in virtual environments and computer games. Our directional source representation is expressed as a linear combination of elementary spherical harmonic (SH) sources. In the preprocessing stage, we precompute and encode the propagated sound fields due to each SH source. At runtime, we perform the SH decomposition of the varying source directivity interactively and compute the total sound field at the listener position as a weighted sum of precomputed SH sound fields. We propose a novel plane-wave decomposition approach based on higher-order derivatives of the sound field that enables dynamic HRTF-based listener directivity at runtime. We provide a generic framework to incorporate our source and listener directivity in any offline or online frequency-domain wave-based sound propagation algorithm. We have integrated our sound propagation system in Valve&#x27;s Source game engine and use it to demonstrate realistic acoustic effects such as sound amplification, diffraction low-passing, scattering, localization, externalization, and spatial sound, generated by wave-based propagation of directional sources and listener in complex scenarios. We also present results from our preliminary user study.},
	address = {Los Alamitos, CA, USA},
	author = {R. Mehra and L. Antani and Sujeong Kim and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.38},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {runtime;mathematical model;equations;acoustics;frequency-domain analysis;computational modeling;ear},
	month = {apr},
	number = {04},
	pages = {495-503},
	publisher = {IEEE Computer Society},
	title = {Source and Listener Directivity for Interactive Wave-Based Sound Propagation},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.38}}

@article{7014276,
	abstract = {We present an interactive wave-based sound propagation system that generates accurate, realistic sound in virtual environments for dynamic (moving) sources and listeners. We propose a novel algorithm to accurately solve the wave equation for dynamic sources and listeners using a combination of precomputation techniques and GPU-based runtime evaluation. Our system can handle large environments typically used in VR applications, compute spatial sound corresponding to listener&#x27;s motion (including head tracking) and handle both omnidirectional and directional sources, all at interactive rates. As compared to prior wave-based techniques applied to large scenes with moving sources, we observe significant improvement in runtime memory. The overall sound-propagation and rendering system has been integrated with the Half-Life 2 game engine, Oculus-Rift head-mounted display, and the Xbox game controller to enable users to experience high-quality acoustic effects (e.g., amplification, diffraction low-passing, high-order scattering) and spatial audio, based on their interactions in the VR application. We provide the results of preliminary user evaluations, conducted to study the impact of wave-based acoustic effects and spatial audio on users&#x27; navigation performance in virtual environments.},
	address = {Los Alamitos, CA, USA},
	author = {R. Mehra and A. Rungta and A. Golas and M. Lin and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391858},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {runtime;acoustics;vectors;transfer functions;virtual environments;linear systems;navigation},
	month = {apr},
	number = {04},
	pages = {434-442},
	publisher = {IEEE Computer Society},
	title = {WAVE: Interactive Wave-based Sound Propagation for Virtual Environments},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391858}}

@article{9714042,
	abstract = {The visual depth perception is composed of monocular and binocular depth cues. Studies show that in absence of binocular depth cues the performance of visuomotor tasks like pointing to or grasping objects is limited. Thus, binocular depth cues are of great importance for motor control required in everyday life. However, binocular depth cues like retinal disparity (basis for stereopsis) might be influenced due to developmental disorders of the visual system. For example, amblyopia in which one eye&#x27;s visual input is not processed leads to loss of stereopsis. The primary amblyopia treatment is occlusion of the healthy eye to force the amblyopic eye to train. However, improvements in stereopsis are poor. Therefore, binocular treatments arose that equilibrate both eyes&#x27; visual input to enable binocular vision. However, most approaches rely on divided stimuli which do not account for loss of stereopsis. We created a Virtual Reality (VR) with reduced monocular depth cues in which a stereoscopic task is shown to both eyes simultaneously, consisting of two balls jumping towards the user. One ball appears closer to the user which must be identified. To evaluate the task performance the reaction time is measured. We validated our approach with 18 participants with stereopsis under three contrast settings including one leading to monocular vision. The number of correct responses reduces from 90% under binocular vision to 52% under monocular vision corresponding to random guessing. Our results indicate that it is possible to disable monocular depth cues and create a dynamic stereoscopic task inside a VR.},
	address = {Los Alamitos, CA, USA},
	author = {W. Mehringer and M. Wirth and D. Roth and G. Michelson and B. M. Eskofier},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150486},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;task analysis;games;stereo image processing;training;headphones;vision defects},
	month = {may},
	number = {05},
	pages = {2114-2124},
	publisher = {IEEE Computer Society},
	title = {Stereopsis Only: Validation of a Monocular Depth Cues Reduced Gamified Virtual Reality with Reaction Time Measurement},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150486}}

@article{9005240,
	abstract = {Optimizing rendering performance is critical for a wide variety of virtual reality (VR) applications. Foveated rendering is emerging as an indispensable technique for reconciling interactive frame rates with ever-higher head-mounted display resolutions. Here, we present a simple yet effective technique for further reducing the cost of foveated rendering by leveraging ocular dominance - the tendency of the human visual system to prefer scene perception from one eye over the other. Our new approach, eye-dominance-guided foveated rendering (EFR), renders the scene at a lower foveation level (with higher detail) for the dominant eye than the non-dominant eye. Compared with traditional foveated rendering, EFR can be expected to provide superior rendering performance while preserving the same level of perceived visual quality.},
	address = {Los Alamitos, CA, USA},
	author = {X. Meng and R. Du and A. Varshney},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973442},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);visualization;kernel;visual systems;pipelines;sensitivity;solid modeling},
	month = {may},
	number = {05},
	pages = {1972-1980},
	publisher = {IEEE Computer Society},
	title = {Eye-dominance-guided Foveated Rendering},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973442}}

@article{8642297,
	abstract = {Virtual colonoscopy (VC) is a non-invasive screening tool for colorectal polyps which employs volume visualization of a colon model reconstructed from a CT scan of the patient&#x27;s abdomen. We present an immersive analytics system for VC which enhances and improves the traditional desktop VC through the use of VR technologies. Our system, using a head-mounted display (HMD), includes all of the standard VC features, such as the volume rendered endoluminal fly-through, measurement tool, bookmark modes, electronic biopsy, and slice views. The use of VR immersion, stereo, and wider field of view and field of regard has a positive effect on polyp search and analysis tasks in our immersive VC system, a volumetric-based immersive analytics application. Navigation includes enhanced automatic speed and direction controls, based on the user&#x27;s head orientation, in conjunction with physical navigation for exploration of local proximity. In order to accommodate the resolution and frame rate requirements for HMDs, new rendering techniques have been developed, including mesh-assisted volume raycasting and a novel lighting paradigm. Feedback and further suggestions from expert radiologists show the promise of our system for immersive analysis for VC and encourage new avenues for exploring the use of VR in visualization systems for medical diagnosis.},
	address = {Los Alamitos, CA, USA},
	author = {S. Mirhosseini and I. Gutenko and S. Ojal and J. Marino and A. Kaufman},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898763},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {colon;rendering (computer graphics);tools;computed tomography;biopsy;task analysis;haptic interfaces},
	month = {may},
	number = {05},
	pages = {2011-2021},
	publisher = {IEEE Computer Society},
	title = {Immersive Virtual Colonoscopy},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898763}}

@article{8998298,
	abstract = {Virtual Reality (VR) has a great potential to improve skills of Deaf and Hard-of-Hearing (DHH) people. Most VR applications and devices are designed for persons without hearing problems. Therefore, DHH persons have many limitations when using VR. Adding special features in a VR environment, such as subtitles, or haptic devices will help them. Previously, it was necessary to design a special VR environment for DHH persons. We introduce and evaluate a new prototype called ``EarVR'' that can be mounted on any desktop or mobile VR Head-Mounted Display (HMD). EarVR analyzes 3D sounds in a VR environment and locates the direction of the sound source that is closest to a user. It notifies the user about the sound direction using two vibro-motors placed on the user&#x27;s ears. EarVR helps DHH persons to complete sound-based VR tasks in any VR application with 3D audio and a mute option for background music. Therefore, DHH persons can use all VR applications with 3D audio, not only those applications designed for them. Our user study shows that DHH participants were able to complete a simple VR task significantly faster with EarVR than without. The completion time of DHH participants was very close to participants without hearing problems. Also, it shows that DHH participants were able to finish a complex VR task with EarVR, while without it, they could not finish the task even once. Finally, our qualitative and quantitative evaluation among DHH participants indicates that they preferred to use EarVR and it encouraged them to use VR technology more.},
	address = {Los Alamitos, CA, USA},
	author = {M. Mirzaei and P. Kan and H. Kaufmann},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973441},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;auditory system;task analysis;ear;resists;hardware;three-dimensional displays},
	month = {may},
	number = {05},
	pages = {2084-2093},
	publisher = {IEEE Computer Society},
	title = {EarVR: Using Ear Haptics in Virtual Reality for Deaf and Hard-of-Hearing People},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973441}}

@article{10049736,
	abstract = {Advances in virtual reality technology have enabled the creation of virtual environments (VEs) with significantly high visual fidelity when compared to real environments (REs). In this study, we use a high-fidelity VE to examine two effects caused by alternating VE and RE experiences: ``context-dependent forgetting'' and ``source-monitoring errors.'' The former effect is that memories learned in VEs are more easily recalled in VEs than in REs, whereas memories learned in REs are more easily recalled in REs than in VEs. The source-monitoring error is that memories learned in VEs are easily confused with those learned in REs, making discriminating the source of the memory difficult. We hypothesized that the visual fidelity of VEs is responsible for these effects and conducted an experiment using two types of VEs: a high-fidelity VE created using photogrammetry techniques and low-fidelity VE created with primitive shapes and materials. The results show that the high-fidelity VE significantly improved the sense of presence. However, the level of the visual fidelity of the VEs did not show any effect on context-dependent forgetting and source-monitoring errors. Notably, the null results of the context-dependent forgetting between the VE and RE were strongly supported by Bayesian analysis. Thus, we indicate that context-dependent forgetting does not necessarily occur, which will be helpful for VR-based education and training.},
	address = {Los Alamitos, CA, USA},
	author = {T. Mizuho and T. Narumi and H. Kuzuoka},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247063},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;monitoring;avatars;virtual environments;medical treatment;forestry;cognitive science},
	month = {may},
	number = {05},
	pages = {2607-2614},
	publisher = {IEEE Computer Society},
	title = {Effects of the Visual Fidelity of Virtual Environments on Presence, Context-dependent Forgetting, and Source-monitoring Error},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247063}}

@article{Moehring2011:Natural-Interaction-Metaphors,
	abstract = {Natural Interaction in virtual environments is a key requirement for the virtual validation of functional aspects in automotive product development processes. Natural Interaction is the metaphor people encounter in reality: the direct manipulation of objects by their hands. To enable this kind of Natural Interaction, we propose a pseudophysical metaphor that is both plausible enough to provide realistic interaction and robust enough to meet the needs of industrial applications. Our analysis of the most common types of objects in typical automotive scenarios guided the development of a set of refined grasping heuristics to support robust finger-based interaction of multiple hands and users. The objects&#x27; behavior in reaction to the users&#x27; finger motions is based on pseudophysical simulations, which also take various types of constrained objects into account. In dealing with real-world scenarios, we had to introduce the concept of Normal Proxies, which extend objects with appropriate normals for improved grasp detection and grasp stability. An expert review revealed that our interaction metaphors allow for an intuitive and reliable assessment of several functionalities of objects found in a car interior. Follow-up user studies showed that overall task performance and usability are similar for CAVE and HMD environments. For larger objects and more gross manipulation, using the CAVE without employing a virtual hand representation is preferred, but for more fine-grained manipulation and smaller objects, the HMD turns out to be beneficial.},
	address = {Los Alamitos, CA, USA},
	author = {M. Moehring and B. Froehlich},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2011.36},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {user interfaces;3d graphics and realism;input/output devices;systems and software.},
	month = {sep},
	number = {09},
	pages = {1195-1208},
	publisher = {IEEE Computer Society},
	title = {Natural Interaction Metaphors for Functional Validations of Virtual Car Models},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2011.36}}

@article{9382869,
	abstract = {Hands are the most important tool to interact with virtual environments, and they should be available to perform the most critical tasks. For example, a surgeon in VR should keep his/her hands on the instruments and be able to do secondary tasks without performing a disruptive event to the operative task. In this common scenario, one can observe that hands are not available for interaction. The goal of this systematic review is to survey the literature and identify which hands-free interfaces are used, the performed interaction tasks, what metrics are used for interface evaluation, and the results of such evaluations. From 79 studies that met the eligibility criteria, the voice is the most studied interface, followed by the eye and head gaze. Some novel interfaces were brain interfaces and face expressions. System control and selection represent most of the interaction tasks studied and most studies evaluate interfaces for usability. Despite the best interface depending on the task and study, the voice was found to be versatile and showed good results amongst the studies. More research is recommended to improve the practical use of the interfaces and to evaluate the interfaces more formally.},
	address = {Los Alamitos, CA, USA},
	author = {P. Monteiro and G. Goncalves and H. Coelho and M. Melo and M. Bessa},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067687},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;three-dimensional displays;navigation;systematics;control systems;human computer interaction;two dimensional displays},
	month = {may},
	number = {05},
	pages = {2702-2713},
	publisher = {IEEE Computer Society},
	title = {Hands-free interaction in immersive virtual reality: A systematic review},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067687}}

@article{7021939,
	abstract = {With the growing availability of optical see-through (OST) head-mounted displays (HMDs) there is a present need for robust, uncomplicated, and automatic calibration methods suited for non-expert users. This work presents the results of a user study which both objectively and subjectively examines registration accuracy produced by three OST HMD calibration methods: (1) SPAAM, (2) Degraded SPAAM, and (3) Recycled INDICA, a recently developed semi-automatic calibration method. Accuracy metrics used for evaluation include subject provided quality values and error between perceived and absolute registration coordinates. Our results show all three calibration methods produce very accurate registration in the horizontal direction but caused subjects to perceive the distance of virtual objects to be closer than intended. Surprisingly, the semi-automatic calibration method produced more accurate registration vertically and in perceived object distance overall. User assessed quality values were also the highest for Recycled INDICA, particularly when objects were shown at distance. The results of this study confirm that Recycled INDICA is capable of producing equal or superior on-screen registration compared to common OST HMD calibration methods. We also identify a potential hazard in using reprojection error as a quantitative analysis technique to predict registration accuracy. We conclude with discussing the further need for examining INDICA calibration in binocular HMD systems, and the present possibility for creation of a closed-loop continuous calibration method for OST Augmented Reality.},
	address = {Los Alamitos, CA, USA},
	author = {K. Moser and Y. Itoh and K. Oshima and J. Swan and G. Klinker and C. Sandor},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391856},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {calibration;cameras;accuracy;visualization;hardware;head},
	month = {apr},
	number = {04},
	pages = {491-500},
	publisher = {IEEE Computer Society},
	title = {Subjective Evaluation of a Semi-Automatic Optical See-Through Head-Mounted Display Calibration Technique},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391856}}

@article{10049680,
	abstract = {In this paper, we investigate the use of a motorized bike to support the walk of a self-avatar in virtual reality (VR). While existing walking-in-place (WIP) techniques render compelling walking experiences, they can be judged repetitive and strenuous. Our approach consists in assisting a WIP technique so that the user does not have to actively move in order to reduce effort and fatigue. We chose to assist a technique called walking-by-cycling, which consists in mapping the cycling motion of a bike onto the walking of the user&#x27;s self-avatar, by using a motorized bike. We expected that our approach could provide participants with a compelling walking experience while reducing the effort required to navigate. We conducted a within-subjects study where we compared ``assisted walking-by-cycling'' to a traditional active walking-by-cycling implementation, and to a standard condition where the user is static. In the study, we measured embodiment, including ownership and agency, walking sensation, perceived effort and fatigue. Results showed that assisted walking-by-cycling induced more ownership, agency, and walking sensation than the static simulation. Additionally, assisted walking-by-cycling induced levels of ownership and walking sensation similar to that of active walking-by-cycling, but it induced less perceived effort. Taken together, this work promotes the use of assisted walking-by-cycling in situations where users cannot or do not want to exert much effort while walking in embodied VR such as for injured or disabled users, for prolonged uses, medical rehabilitation, or virtual visits.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Moullec and M. Cogne and J. Saint-Aubert and A. Lecuyer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247070},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;fatigue;navigation;avatars;propioception;virtual environments;user interfaces},
	month = {may},
	number = {05},
	pages = {2796-2805},
	publisher = {IEEE Computer Society},
	title = {Assisted walking-in-place: Introducing assisted motion to walking-by-cycling in embodied virtual reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247070}}

@article{10049652,
	abstract = {We present PACE, a novel method for modifying motion-captured virtual agents to interact with and move throughout dense, cluttered 3D scenes. Our approach changes a given motion sequence of a virtual agent as needed to adjust to the obstacles and objects in the environment. We first take the individual frames of the motion sequence most important for modeling interactions with the scene and pair them with the relevant scene geometry, obstacles, and semantics such that interactions in the agents motion match the affordances of the scene (e.g., standing on a floor or sitting in a chair). We then optimize the motion of the human by directly altering the high-DOF pose at each frame in the motion to better account for the unique geometric constraints of the scene. Our formulation uses novel loss functions that maintain a realistic flow and natural-looking motion. We compare our method with prior motion generating techniques and highlight the benefits of our method with a perceptual study and physical plausibility metrics. Human raters preferred our method over the prior approaches. Specifically, they preferred our method 57.1% of the time versus the state-of-the-art method using existing motions, and 81.0% of the time versus a state-of-the-art motion synthesis method. Additionally, our method performs significantly higher on established physical plausibility and interaction metrics. Specifically, we outperform competing methods by over 1.2% in terms of the non-collision metric and by over 18% in terms of the contact metric. We have integrated our interactive system with Microsoft HoloLens and demonstrate its benefits in real-world indoor scenes. Our project website is available at https://gamma.mnd.edu/pace/},
	address = {Los Alamitos, CA, USA},
	author = {J. F. Mullen and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247054},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;measurement;solid modeling;animation;videos;geometry;affordances},
	month = {may},
	number = {05},
	pages = {2536-2546},
	publisher = {IEEE Computer Society},
	title = {PACE: Data-Driven Virtual Agent Interaction in Dense and Cluttered Environments},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247054}}

@article{8260860,
	abstract = {As we explore the use of consumer virtual reality technology for training applications, there is a need to evaluate its validity compared to more traditional training formats. In this paper, we present a study that compares the effectiveness of virtual training and physical training for teaching a bimanual assembly task. In a between-subjects experiment, 60 participants were trained to solve three 3D burr puzzles in one of six conditions comprised of virtual and physical training elements. In the four physical conditions, training was delivered via paper- and video-based instructions, with or without the physical puzzles to practice with. In the two virtual conditions, participants learnt to assemble the puzzles in an interactive virtual environment, with or without 3D animations showing the assembly process. After training, we conducted immediate tests in which participants were asked to solve a physical version of the puzzles. We measured performance through success rates and assembly completion testing times. We also measured training times as well as subjective ratings on several aspects of the experience. Our results show that the performance of virtually trained participants was promising. A statistically significant difference was not found between virtual training with animated instructions and the best performing physical condition (in which physical blocks were available during training) for the last and most complex puzzle in terms of success rates and testing times. Performance in retention tests two weeks after training was generally not as good as expected for all experimental conditions. We discuss the implications of the results and highlight the validity of virtual reality systems in training.},
	address = {Los Alamitos, CA, USA},
	author = {M. Murcia-Lopez and A. Steed},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793638},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {training;testing;three-dimensional displays;virtual environments;haptic interfaces},
	month = {apr},
	number = {04},
	pages = {1574-1583},
	publisher = {IEEE Computer Society},
	title = {A Comparison of Virtual and Physical Training Transfer of Bimanual Assembly Tasks},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793638}}

@article{8260962,
	abstract = {This paper presents a novel interactive system that provides users with virtual reality (VR) experiences, wherein users feel as if they are ascending/descending stairs through passive haptic feedback. The passive haptic stimuli are provided by small bumps under the feet of users; these stimuli are provided to represent the edges of the stairs in the virtual environment. The visual stimuli of the stairs and shoes, provided by head-mounted displays, evoke a visuo-haptic interaction that modifies a user&#x27;s perception of the floor shape. Our system enables users to experience all types of stairs, such as half-turn and spiral stairs, in a VR setting. We conducted a preliminary user study and two experiments to evaluate the proposed technique. The preliminary user study investigated the effectiveness of the basic idea associated with the proposed technique for the case of a user ascending stairs. The results demonstrated that the passive haptic feedback produced by the small bumps enhanced the user&#x27;s feeling of presence and sense of ascending. We subsequently performed an experiment to investigate an improved viewpoint manipulation method and the interaction of the manipulation and haptics for both the ascending and descending cases. The experimental results demonstrated that the participants had a feeling of presence and felt a steep stair gradient under the condition of haptic feedback and viewpoint manipulation based on the characteristics of actual stair walking data. However, these results also indicated that the proposed system may not be as effective in providing a sense of descending stairs without an optimization of the haptic stimuli. We then redesigned the shape of the small bumps, and evaluated the design in a second experiment. The results indicated that the best shape to present haptic stimuli is a right triangle cross section in both the ascending and descending cases. Although it is necessary to install small protrusions in the determined direction, by using this optimized shape the users feeling of presence of the stairs and the sensation of walking up and down was enhanced.},
	address = {Los Alamitos, CA, USA},
	author = {R. Nagao and K. Matsumoto and T. Narumi and T. Tanikawa and M. Hirose},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793038},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;legged locomotion;visualization;shape;foot;actuators;virtual reality},
	month = {apr},
	number = {04},
	pages = {1584-1593},
	publisher = {IEEE Computer Society},
	title = {Ascending and Descending in Virtual Reality: Simple and Safe System Using Passive Haptics},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793038}}

@article{8998379,
	abstract = {The core idea in an XR (VR/MR/AR) application is to digitally stimulate one or more sensory systems (e.g. visual, auditory, olfactory) of the human user in an interactive way to achieve an immersive experience. Since the early 2000s biologists have been using Virtual Environments (VE) to investigate the mechanisms of behavior in non-human animals including insects, fish, and mammals. VEs have become reliable tools for studying vision, cognition, and sensory-motor control in animals. In turn, the knowledge gained from studying such behaviors can be harnessed by researchers designing biologically inspired robots, smart sensors, and rnulti-agent artificial intelligence. VE for animals is becoming a widely used application of XR technology but such applications have not previously been reported in the technical literature related to XR. Biologists and computer scientists can benefit greatly from deepening interdisciplinary research in this emerging field and together we can develop new methods for conducting fundamental research in behavioral sciences and engineering. To support our argument we present this review which provides an overview of animal behavior experiments conducted in virtual environments.},
	address = {Los Alamitos, CA, USA},
	author = {H. Naik and R. Bastien and N. Navab and I. D. Couzin},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973063},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual environments;visualization;animal behavior;robot sensing systems;insects},
	month = {may},
	number = {05},
	pages = {2073-2083},
	publisher = {IEEE Computer Society},
	title = {Animals in Virtual Environments},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973063}}

@article{8642370,
	abstract = {We present a real-time algorithm to infer the intention of a user&#x27;s avatar in a virtual environment shared with multiple human-like agents. Our algorithm applies the Bayesian Theory of Mind approach to make inferences about the avatar&#x27;s hidden intentions based on the observed proxemics and gaze-based cues. Our approach accounts for the potential irrationality in human behavior, as well as the dynamic nature of an individual&#x27;s intentions. The inferred intent is used to guide the response of the virtual agent and generate locomotion and gaze-based behaviors. Our overall approach allows the user to actively interact with tens of virtual agents from a first-person perspective in an immersive setting. We systematically evaluate our inference algorithm in controlled multi-agent simulation environments and highlight its ability to reliably and efficiently infer the hidden intent of a user&#x27;s avatar even under noisy conditions. We quantitatively demonstrate the performance benefits of our approach in terms of reducing false inferences, as compared to a prior method. The results of our user evaluation show that 68.18% of participants reported feeling more comfortable in sharing the virtual environment with agents simulated with our algorithm as compared to a prior inference method, likely as a direct result of significantly fewer false inferences and more plausible responses from the virtual agents.},
	address = {Los Alamitos, CA, USA},
	author = {S. Narang and A. Best and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898800},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;virtual environments;computational modeling;inference algorithms;two dimensional displays;bayes methods;heuristic algorithms},
	month = {may},
	number = {05},
	pages = {2113-2122},
	publisher = {IEEE Computer Society},
	title = {Inferring User Intent using Bayesian Theory of Mind in Shared Avatar-Agent Virtual Environments},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898800}}

@article{6777444,
	abstract = {Walking-In-Place (WIP) techniques make it possible to facilitate relatively natural locomotion within immersive virtual environments that are larger than the physical interaction space. However, in order to facilitate natural walking experiences one needs to know how to map steps in place to virtual motion. This paper describes two within-subjects studies performed with the intention of establishing the range of perceptually natural walking speeds for WIP locomotion. In both studies, subjects performed a series of virtual walks while exposed to visual gains (optic flow multipliers) ranging from 1.0 to 3.0. Thus, the slowest speed was equal to an estimate of the subjects normal walking speed, while the highest speed was three times greater. The perceived naturalness of the visual speed was assessed using self-reports. The first study compared four different types of movement, namely, no leg movement, walking on a treadmill, and two forms of gestural input for WIP locomotion. The results suggest that WIP locomotion is accompanied by a perceptual distortion of the speed of optic flow. The second study was performed using a 42 factorial design and compared four different display field-of-views (FOVs) and two types of movement, walking on a treadmill and WIP locomotion. The results revealed significant main effects of both movement type and field of view, but no significant interaction between the two variables. Particularly, they suggest that the size of the display FOV is inversely proportional to the degree of underestimation of the virtual speeds for both treadmill-mediated virtual walking and WIP locomotion. Combined, the results constitute a first attempt at establishing a set of guidelines specifying what virtual walking speeds WIP gestures should produce in order to facilitate a natural walking experience.},
	address = {Los Alamitos, CA, USA},
	author = {N. Nilsson and S. Serafin and R. Nordahl},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.21},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;visualization;optical distortion;optical feedback;adaptive optics;tracking;equations},
	month = {apr},
	number = {04},
	pages = {569-578},
	publisher = {IEEE Computer Society},
	title = {Establishing the Range of Perceptually Natural Visual Walking Speeds for Virtual Walking-In-Place Locomotion},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.21}}

@article{9714121,
	abstract = {Projection mapping using multiple projectors is promising for spatial augmented reality; however, it is difficult to apply it to dynamic scenes. This is because the conventional method decides all pixel intensities of multiple images simultaneously based on the global optimization method, and it is hard to reduce the latency from motion to projection. To mitigate this, we propose a novel method of controlling the intensity based on a pixel-parallel calculation for each projector in real-time with low latency. This parallel calculation leverages the insight that the projected pixels from different projectors in overlapping areas can be approximated independently if the pixel is sufficiently small relative to the surface structure. Additionally, our pixel-parallel calculation method allows a distributed system configuration, such that the number of projectors can be increased to form a network for high scalability. We demonstrate a seamless mapping into dynamic scenes at 360 fps with a 9.5-ms latency using ten cameras and four projectors.},
	address = {Los Alamitos, CA, USA},
	author = {T. Nomoto and W. Li and H. Peng and Y. Watanabe},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150488},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);low latency communication;shape;image color analysis;geometry;surface structures;real-time systems},
	month = {may},
	number = {05},
	pages = {2125-2134},
	publisher = {IEEE Computer Society},
	title = {Dynamic Multi-projection Mapping Based on Parallel Intensity Control},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150488}}

@article{Nordahl2011:Sound-Synthesis-and-Evaluation,
	abstract = {We propose a system that affords real-time sound synthesis of footsteps on different materials. The system is based on microphones, which detect real footstep sounds from subjects, from which the ground reaction force (GRF) is estimated. Such GRF is used to control a sound synthesis engine based on physical models. Two experiments were conducted. In the first experiment, the ability of subjects to recognize the surface they were exposed to was assessed. In the second experiment, the sound synthesis engine was enhanced with environmental sounds. Results show that, in some conditions, adding a soundscape significantly improves the recognition of the simulated environment.},
	address = {Los Alamitos, CA, USA},
	author = {R. Nordahl and S. Serafin and L. Turchet},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2011.30},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {sound and music computing;walking;surface simulation;soundscape rendering.},
	month = {sep},
	number = {09},
	pages = {1234-1244},
	publisher = {IEEE Computer Society},
	title = {Sound Synthesis and Evaluation of Interactive Footsteps and Environmental Sounds Rendering for Virtual Reality Applications},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2011.30}}

@article{8672601,
	abstract = {We present the results of a two-year design study to developing virtual reality (VR) flow visualization tools for the analysis of dinosaur track creation in a malleable substrate. Using Scientific Sketching methodology, we combined input from illustration artists, visualization experts, and domain scientists to create novel visualization methods. By iteratively improving visualization concepts at multiple levels of abstraction we helped domain scientists to gain insights into the relationship between dinosaur foot movements and substrate deformations. We involved over 20 art and computer science students from a VR design course in a rapid visualization sketching cycle, guided by our paleontologist collaborators through multiple critique sessions. This allowed us to explore a wide range of potential visualization methods and select the most promising methods for actual implementation. Our resulting visualization methods provide paleontologists with effective tools to analyze their data through particle, pathline and time surface visualizations. We also introduce a set of visual metaphors to compare foot motion in relation to substrate deformation by using pathsurfaces. This is one of the first large-scale projects using Scientific Sketching as a development methodology. We discuss how the research questions of our collaborators have evolved during the sketching and prototyping phases. Finally, we provide lessons learned and usage considerations for Scientific Sketching based on the experiences gathered during this project.},
	address = {Los Alamitos, CA, USA},
	author = {J. Novotny and J. Tveite and M. L. Turner and S. Gatesy and F. Drury and P. Falkingham and D. H. Laidlaw},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898796},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {data visualization;visualization;dinosaurs;three-dimensional displays;tracking;foot;substrates},
	month = {may},
	number = {05},
	pages = {2145-2154},
	publisher = {IEEE Computer Society},
	title = {Developing Virtual Reality Visualizations for Unsteady Flow Analysis of Dinosaur Track Formation using Scientific Sketching},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898796}}

@article{7829437,
	abstract = {For neurodegenerative conditions like Parkinson&#x27;s disease, early and accurate diagnosis is still a difficult task. Evaluations can be time consuming, patients must often travel to metropolitan areas or different cities to see experts, and misdiagnosis can result in improper treatment. To date, only a handful of assistive or remote methods exist to help physicians evaluate patients with suspected neurological disease in a convenient and consistent way. In this paper, we present a low-cost VR interface designed to support evaluation and diagnosis of neurodegenerative disease and test its use in a clinical setting. Using a commercially available VR display with an infrared camera integrated into the lens, we have constructed a 3D virtual environment designed to emulate common tasks used to evaluate patients, such as fixating on a point, conducting smooth pursuit of an object, or executing saccades. These virtual tasks are designed to elicit eye movements commonly associated with neurodegenerative disease, such as abnormal saccades, square wave jerks, and ocular tremor. Next, we conducted experiments with 9 patients with a diagnosis of Parkinson&#x27;s disease and 7 healthy controls to test the system&#x27;s potential to emulate tasks for clinical diagnosis. We then applied eye tracking algorithms and image enhancement to the eye recordings taken during the experiment and conducted a short follow-up study with two physicians for evaluation. Results showed that our VR interface was able to elicit five common types of movements usable for evaluation, physicians were able to confirm three out of four abnormalities, and visualizations were rated as potentially useful for diagnosis.},
	address = {Los Alamitos, CA, USA},
	author = {J. Orlosky and Y. Itoh and M. Ranchet and K. Kiyokawa and J. Morgan and H. Devos},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657018},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {diseases;gaze tracking;cameras;visualization;three-dimensional displays;lenses},
	month = {apr},
	number = {04},
	pages = {1302-1311},
	publisher = {IEEE Computer Society},
	title = {Emulation of Physician Tasks in Eye-Tracked Virtual Reality for Remote Diagnosis of Neurodegenerative Disease},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657018}}

@article{8267239,
	abstract = {Virtual reality systems are widely believed to be the next major computing platform. There are, however, some barriers to adoption that must be addressed, such as that of motion sickness --- which can lead to undesirable symptoms including postural instability, headaches, and nausea. Motion sickness in virtual reality occurs as a result of moving visual stimuli that cause users to perceive self-motion while they remain stationary in the real world. There are several contributing factors to both this perception of motion and the subsequent onset of sickness, including field of view, motion velocity, and stimulus depth. We verify first that differences in vection due to relative stimulus depth remain correlated with sickness. Then, we build a dataset of stereoscopic 3D videos and their corresponding sickness ratings in order to quantify their nauseogenicity, which we make available for future use. Using this dataset, we train a machine learning algorithm on hand-crafted features (quantifying speed, direction, and depth as functions of time) from each video, learning the contributions of these various features to the sickness ratings. Our predictor generally outperforms a na{\"\i}ve estimate, but is ultimately limited by the size of the dataset. However, our result is promising and opens the door to future work with more extensive datasets. This and further advances in this space have the potential to alleviate developer and end user concerns about motion sickness in the increasingly commonplace virtual world.},
	address = {Los Alamitos, CA, USA},
	author = {N. Padmanaban and T. Ruban and V. Sitzmann and A. M. Norcia and G. Wetzstein},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793560},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {videos;stereo image processing;three-dimensional displays;visualization;virtual environments;machine learning algorithms;trajectory},
	month = {apr},
	number = {04},
	pages = {1594-1603},
	publisher = {IEEE Computer Society},
	title = {Towards a Machine-Learning Approach for Sickness Prediction in 360$\,^{\circ}$ Stereoscopic Videos},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793560}}

@article{10049691,
	abstract = {The paper presents emotional voice puppetry, an audio-based facial animation approach to portray characters with vivid emotional changes. The lips motion and the surrounding facial areas are controlled by the contents of the audio, and the facial dynamics are established by category of the emotion and the intensity. Our approach is exclusive because it takes account of perceptual validity and geometry instead of pure geometric processes. Another highlight of our approach is the generalizability to multiple characters. The findings showed that training new secondary characters when the rig parameters are categorized as eye, eyebrows, nose, mouth, and signature wrinkles is significant in achieving better generalization results compared to joint training. User studies demonstrate the effectiveness of our approach both qualitatively and quantitatively. Our approach can be applicable in AR/VR and 3DUI, namely, virtual reality avatars/self-avatars, teleconferencing and in-game dialogue.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Pan and R. Zhang and S. Cheng and S. Tan and Y. Ding and K. Mitchell and X. Yang},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247101},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {faces;mouth;three-dimensional displays;lips;facial animation;videos;training},
	month = {may},
	number = {05},
	pages = {2527-2535},
	publisher = {IEEE Computer Society},
	title = {Emotional Voice Puppetry},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247101}}

@article{8998145,
	abstract = {We present a sensor-fusion method that exploits a depth camera and a gyroscope to track the articulation of a hand in the presence of excessive motion blur. In case of slow and smooth hand motions, the existing methods estimate the hand pose fairly accurately and robustly, despite challenges due to the high dimensionality of the problem, self-occlusions, uniform appearance of hand parts, etc. However, the accuracy of hand pose estimation drops considerably for fast-moving hands because the depth image is severely distorted due to motion blur. Moreover, when hands move fast, the actual hand pose is far from the one estimated in the previous frame, therefore the assumption of temporal continuity on which tracking methods rely, is not valid. In this paper, we track fast-moving hands with the combination of a gyroscope and a depth camera. As a first step, we calibrate a depth camera and a gyroscope attached to a hand so as to identify their time and pose offsets. Following that, we fuse the rotation information of the calibrated gyroscope with model-based hierarchical particle filter tracking. A series of quantitative and qualitative experiments demonstrate that the proposed method performs more accurately and robustly in the presence of motion blur, when compared to state of the art algorithms, especially in the case of very fast hand rotations.},
	address = {Los Alamitos, CA, USA},
	author = {G. Park and A. Argyros and J. Lee and W. Woo},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973057},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {tracking;cameras;gyroscopes;three-dimensional displays;pose estimation;robustness;solid modeling},
	month = {may},
	number = {05},
	pages = {1891-1901},
	publisher = {IEEE Computer Society},
	title = {3D Hand Tracking in the Presence of Excessive Motion Blur},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973057}}

@article{8998303,
	abstract = {In mixed reality (MR), augmenting virtual objects consistently with real-world illumination is one of the key factors that provide a realistic and immersive user experience. For this purpose, we propose a novel deep learning-based method to estimate high dynamic range (HDR) illumination from a single RGB image of a reference object. To obtain illumination of a current scene, previous approaches inserted a special camera in that scene, which may interfere with user&#x27;s immersion, or they analyzed reflected radiances from a passive light probe with a specific type of materials or a known shape. The proposed method does not require any additional gadgets or strong prior cues, and aims to predict illumination from a single image of an observed object with a wide range of homogeneous materials and shapes. To effectively solve this ill-posed inverse rendering problem, three sequential deep neural networks are employed based on a physically-inspired design. These networks perform end-to-end regression to gradually decrease dependency on the material and shape. To cover various conditions, the proposed networks are trained on a large synthetic dataset generated by physically-based rendering. Finally, the reconstructed HDR illumination enables realistic image-based lighting of virtual objects in MR. Experimental results demonstrate the effectiveness of this approach compared against state-of-the-art methods. The paper also suggests some interesting MR applications in indoor and outdoor scenes.},
	address = {Los Alamitos, CA, USA},
	author = {J. Park and H. Park and S. Yoon and W. Woo},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973050},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {lighting;shape;probes;estimation;virtual reality;image reconstruction;cameras},
	month = {may},
	number = {05},
	pages = {2002-2011},
	publisher = {IEEE Computer Society},
	title = {Physically-inspired Deep Light Estimation from a Homogeneous-Material Object for Mixed Reality Lighting},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973050}}

@article{9385924,
	abstract = {This paper proposes a novel panoramic texture mapping-based rendering system for real-time, photorealistic reproduction of large-scale urban scenes at a street level. Various image-based rendering (IBR) methods have recently been employed to synthesize high-quality novel views, although they require an excessive number of adjacent input images or detailed geometry just to render local views. While the development of global data, such as Google Street View, has accelerated interactive IBR techniques for urban scenes, such methods have hardly been aimed at high-quality street-level rendering. To provide users with free walk-through experiences in global urban streets, our system effectively covers large-scale scenes by using sparsely sampled panoramic street-view images and simplified scene models, which are easily obtainable from open databases. Our key concept is to extract semantic information from the given street-view images and to deploy it in proper intermediate steps of the suggested pipeline, which results in enhanced rendering accuracy and performance time. Furthermore, our method supports real-time semantic 3D inpainting to handle occluded and untextured areas, which appear often when the user&#x27;s viewpoint dynamically changes. Experimental results validate the effectiveness of this method in comparison with the state-of-the-art approaches. We also present real-time demos in various urban streets.},
	address = {Los Alamitos, CA, USA},
	author = {J. Park and I. Jeon and S. Yoon and W. Woo},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067768},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;semantics;geometry;real-time systems;rendering (computer graphics);image reconstruction;image color analysis},
	month = {may},
	number = {05},
	pages = {2746-2756},
	publisher = {IEEE Computer Society},
	title = {Instant Panoramic Texture Mapping with Semantic Object Matching for Large-Scale Urban Scene Reproduction},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067768}}

@article{Peck2009:Evaluation-of-Reorientation-Techniques,
	abstract = {Vir tual Environments (VEs) that use a real-walking locomotion interface have typically been restricted in size to the area of the tracked lab space. Techniques proposed to lift this size constraint, enabling real walking in VEs that are larger than the tracked lab space, all require reorientation techniques (ROTs) in the worst-case situation---when a user is close to walking out of the tracked space. We propose a new ROT using visual and audial distractors---objects in the VE that the user focuses on while the VE rotates---and compare our method to current ROTs through three user studies. ROTs using distractors were preferred and ranked more natural by users. Our findings also suggest that improving visual realism and adding sound increased a user&#x27;s feeling of presence. Users were also less aware of the rotating VE when ROTs with distractors were used.},
	address = {Los Alamitos, CA, USA},
	author = {T. C. Peck and H. Fuchs and M. C. Whitton},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2008.191},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {computer graphics;virtual reality},
	month = {may},
	number = {03},
	pages = {383-394},
	publisher = {IEEE Computer Society},
	title = {Evaluation of Reorientation Techniques and Distractors for Walking in Large Virtual Environments},
	volume = {15},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2008.191}}

@article{8260949,
	abstract = {The underrepresentation of women in technical and STEM fields is a well-known problem, and stereotype threatening situations have been linked to the inability to recruit and retain women into these fields. Virtual reality enables the unique ability to perform body-swap illusions, and research has shown that these illusions can change participant behavior. Characteristically people take on the traits of the avatar they are embodying. We hypothesized that female participants embodying male avatars when a stereotype threat was made salient would demonstrate stereotype lift. We tested our hypothesis through a between-participants user study in an immersive virtual environment by measuring working memory. Our results support that stereotype threat can be induced in an immersive virtual environment, and that stereotype lift is possible with fully-immersive body-swap illusions. Additionally, our results suggest that participants in a gender-swapped avatar without an induced stereotype threat have significantly impaired working memory; however, this impairment is lifted when a threat is made salient. We discuss possible theories as to why a body-swap illusion from a female participant into a male avatar would only increase working memory impairment when not under threat, as well as applications and future research directions. Our results offer additional insight into understanding the cognitive effects of body-swap illusions, and provide evidence that virtual reality may be an applicable tool for decreasing the gender gap in technology.},
	address = {Los Alamitos, CA, USA},
	author = {T. C. Peck and M. Doan and K. A. Bourne and J. J. Good},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793598},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;atmospheric measurements;particle measurements;electronic mail;virtual environments},
	month = {apr},
	number = {04},
	pages = {1604-1612},
	publisher = {IEEE Computer Society},
	title = {The Effect of Gender Body-Swap Illusions on Working Memory and Stereotype Threat},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793598}}

@article{8998141,
	abstract = {A common goal of human-subject experiments in virtual reality (VR) research is evaluating VR hardware and software for use by the general public. A core principle of human-subject research is that the sample included in a given study should be representative of the target population; otherwise, the conclusions drawn from the findings may be biased and may not generalize to the population of interest. In order to assess whether characteristics of participants in VR research are representative of the general public, we investigated participant demographic characteristics from human-subject experiments in the Proceedings of the IEEE Virtual Reality Conferences from 2015-2019. We also assessed the representation of female authors. In the 325 eligible manuscripts, which presented results from 365 human-subject experiments, we found evidence of significant underrepresentation of women as both participants and authors. To investigate whether this underrepresentation may bias researchers&#x27; findings, we then conducted a meta-analysis and meta-regression to assess whether demographic characteristics of study participants were associated with a common outcome evaluated in VR research: the change in simulator sickness following head-mounted display VR exposure. As expected, participants in VR studies using HMDs experienced small but significant increases in simulator sickness. However, across the included studies, the change in simulator sickness was systematically associated with the proportion of female participants. We discuss the negative implications of conducting experiments on non-representative samples and provide methodological recommendations for mitigating bias in future VR research.},
	address = {Los Alamitos, CA, USA},
	author = {T. C. Peck and L. E. Sockol and S. M. Hancock},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973498},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {sociology;statistics;virtual reality;conferences;hardware;software;task analysis},
	month = {may},
	number = {05},
	pages = {1945-1954},
	publisher = {IEEE Computer Society},
	title = {Mind the Gap: The Underrepresentation of Female Participants and Authors in Virtual Reality Research},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973498}}

@article{8998371,
	abstract = {Understanding the effects of hand proximity to objects and tasks is critical for hand-held and near-hand objects. Even though self-avatars have been shown to be beneficial for various tasks in virtual environments, little research has investigated the effect of avatar hand proximity on working memory. This paper presents a between-participants user study investigating the effects of self-avatars and physical hand proximity on a common working memory task, the Stroop interference task. Results show that participants felt embodied when a self-avatar was in the scene, and that the subjective level of embodiment decreased when a participant&#x27;s hands were not collocated with the avatar&#x27;s hands. Furthermore, a participant&#x27;s physical hand placement was significantly related to Stroop interference: proximal hands produced a significant increase in accuracy compared to non-proximal hands. Surprisingly, Stroop interference was not mediated by the existence of a self-avatar or level of embodiment.},
	address = {Los Alamitos, CA, USA},
	author = {T. C. Peck and A. Tutar},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973061},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;avatars;interference;visualization;cognition;sensors;resists},
	month = {may},
	number = {05},
	pages = {1964-1971},
	publisher = {IEEE Computer Society},
	title = {The Impact of a Self-Avatar, Hand Collocation, and Hand Proximity on Embodiment and Stroop Interference},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973061}}

@article{9382876,
	abstract = {Shooter bias is the tendency to more quickly shoot at unarmed Black suspects compared to unarmed White suspects. The primary goal of this research was to investigate the efficacy of shooter bias simulation studies in a more realistic immersive virtual scenario instead of the traditional methodologies using desktop computers. In this paper we present results from a user study (N&#x3D;99) investigating shooter and racial bias in an immersive virtual environment. Our results highlight how racial bias was observed differently in an immersive virtual environment compared to previous desktop-based simulation studies. Latency to shoot, the standard shooter bias measure, was not found to be significantly different between race or socioeconomic status in our more realistic scenarios where participants chose to raise a weapon and pull a trigger. However, more nuanced head and hand motion analysis was able to predict participants&#x27; racial shooting accuracy and implicit racism scores. Discussion of how these nuanced measures can be used for detecting behavior changes for body-swap illusions, and implications of this work related to racial justice and police brutality are discussed.},
	address = {Los Alamitos, CA, USA},
	author = {T. C. Peck and J. J. Good and K. Seitz},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067767},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {law enforcement;avatars;weapons;force;clothing;target tracking;rubber},
	month = {may},
	number = {05},
	pages = {2502-2512},
	publisher = {IEEE Computer Society},
	title = {Evidence of Racial Bias Using Immersive Virtual Reality: Analysis of Head and Hand Motions During Shooting Decisions},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067767}}

@article{9712229,
	abstract = {Current optical see-through displays in the field of augmented reality are limited in their ability to display colors with low lightness in the hue, saturation, lightness (HSL) color space, causing such colors to appear transparent. This hardware limitation may add unintended bias into scenarios with virtual humans. Humans have varying skin tones including HSL colors with low lightness. When virtual humans are displayed with optical see-through devices, people with low lightness skin tones may be displayed semi-transparently while those with high lightness skin tones will be displayed more opaquely. For example, a Black avatar may appear semi-transparent in the same scene as a White avatar who will appear more opaque. We present an exploratory user study ($\mathrm{N}&#x3D;160$) investigating whether differing opacity levels result in dehumanizing avatar and human faces. Results support that dehumanization occurs as opacity decreases. This suggests that in similar lighting, low lightness skin tones (e.g., Black faces) will be viewed as less human than high lightness skin tones (e.g., White faces). Additionally, the perceived emotionality of virtual human faces also predicts perceived humanness. Angry faces were seen overall as less human, and at lower opacity levels happy faces were seen as more human. Our results suggest that additional research is needed to understand the effects and interactions of emotionality and opacity on dehumanization. Further, we provide evidence that unintentional racial bias may be added when developing for optical see-through devices using virtual humans. We highlight the potential bias and discuss implications and directions for future research.},
	address = {Los Alamitos, CA, USA},
	author = {T. C. Peck and J. J. Good and A. Erickson and I. Bynum and G. Bruder},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150521},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {image color analysis;skin;avatars;optical imaging;lighting;faces;optical saturation},
	month = {may},
	number = {05},
	pages = {2179-2189},
	publisher = {IEEE Computer Society},
	title = {Effects of Transparency on Perceived Humanness: Implications for Rendering Skin Tones Using Optical See-Through Displays},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150521}}

@article{7383336,
	abstract = {Data annotation finds increasing use in Virtual Reality applications with the goal to support the data analysis process, such as architectural reviews. In this context, a variety of different annotation systems for application to immersive virtual environments have been presented. While many interesting interaction designs for the data annotation workflow have emerged from them, important details and evaluations are often omitted. In particular, we observe that the process of handling metadata to interactively create and manage complex annotations is often not covered in detail. In this paper, we strive to improve this situation by focusing on the design of data annotation workflows and their evaluation. We propose a workflow design that facilitates the most important annotation operations, i.e., annotation creation, review, and modification. Our workflow design is easily extensible in terms of supported annotation and metadata types as well as interaction techniques, which makes it suitable for a variety of application scenarios. To evaluate it, we have conducted a user study in a CAVE-like virtual environment in which we compared our design to two alternatives in terms of a realistic annotation creation task. Our design obtained good results in terms of task performance and user experience.},
	address = {Los Alamitos, CA, USA},
	author = {S. Pick and B. Weyers and B. Hentschel and T. W. Kuhlen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518086},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {metadata;three-dimensional displays;visualization;virtual environments;context;layout;data analysis},
	month = {apr},
	number = {04},
	pages = {1452-1461},
	publisher = {IEEE Computer Society},
	title = {Design and Evaluation of Data Annotation Workflows for CAVE-like Virtual Environments},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518086}}

@article{7012105,
	abstract = {In recent years optical see-through head-mounted displays (OST-HMDs) have moved from conceptual research to a market of mass-produced devices with new models and applications being released continuously. It remains challenging to deploy augmented reality (AR) applications that require consistent spatial visualization. Examples include maintenance, training and medical tasks, as the view of the attached scene camera is shifted from the user&#x27;s view. A calibration step can compute the relationship between the HMD-screen and the user&#x27;s eye to align the digital content. However, this alignment is only viable as long as the display does not move, an assumption that rarely holds for an extended period of time. As a consequence, continuous recalibration is necessary. Manual calibration methods are tedious and rarely support practical applications. Existing automated methods do not account for user-specific parameters and are error prone. We propose the combination of a pre-calibrated display with a per-frame estimation of the user&#x27;s cornea position to estimate the individual eye center and continuously recalibrate the system. With this, we also obtain the gaze direction, which allows for instantaneous uncalibrated eye gaze tracking, without the need for additional hardware and complex illumination. Contrary to existing methods, we use simple image processing and do not rely on iris tracking, which is typically noisy and can be ambiguous. Evaluation with simulated and real data shows that our approach achieves a more accurate and stable eye pose estimation, which results in an improved and practical calibration with a largely improved distribution of projection error.},
	address = {Los Alamitos, CA, USA},
	author = {A. Plopski and Y. Itoh and C. Nitschke and K. Kiyokawa and G. Klinker and H. Takemura},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391857},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {calibration;cornea;cameras;estimation;iris;three-dimensional displays},
	month = {apr},
	number = {04},
	pages = {481-490},
	publisher = {IEEE Computer Society},
	title = {Corneal-Imaging Calibration for Optical See-Through Head-Mounted Displays},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391857}}

@article{6479210,
	abstract = {The perception of objects, depth, and distance has been repeatedly shown to be divergent between virtual and physical environments. We hypothesize that many of these discrepancies stem from incorrect geometric viewing parameters, specifically that physical measurements of eye position are insufficiently precise to provide proper viewing parameters. In this paper, we introduce a perceptual calibration procedure derived from geometric models. While most research has used geometric models to predict perceptual errors, we instead use these models inversely to determine perceptually correct viewing parameters. We study the advantages of these new psychophysically determined viewing parameters compared to the commonly used measured viewing parameters in an experiment with 20 subjects. The perceptually calibrated viewing parameters for the subjects generally produced new virtual eye positions that were wider and deeper than standard practices would estimate. Our study shows that perceptually calibrated viewing parameters can significantly improve depth acuity, distance estimation, and the perception of shape.},
	address = {Los Alamitos, CA, USA},
	author = {K. Ponto and M. Gleicher and R. G. Radwin and Hyun Joon Shin},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.36},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {calibration;solid modeling;estimation;shape;virtual environments;cameras},
	month = {apr},
	number = {04},
	pages = {691-700},
	publisher = {IEEE Computer Society},
	title = {Perceptual Calibration for Immersive Display Environments},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.36}}

@article{10050341,
	abstract = {Deep learning has revolutionized many scene perception tasks over the past decade. Some of these improvements can be attributed to the development of large labeled datasets. The creation of such datasets can be an expensive, time-consuming, and imperfect process. To address these issues, we introduce GeoSynth, a diverse photorealistic synthetic dataset for indoor scene understanding tasks. Each GeoSynth exemplar contains rich labels including segmentation, geometry, camera parameters, surface material, lighting, and more. We demonstrate that supplementing real training data with GeoSynth can significantly improve network performance on perception tasks, like semantic segmentation. A subset of our dataset will be made publicly available at https://github.com/geomagical/GeoSynth.},
	address = {Los Alamitos, CA, USA},
	author = {B. Pugh and D. Chernak and S. Jiddi},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247087},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;annotations;three-dimensional displays;synthetic data;semantic segmentation;training;rendering (computer graphics)},
	month = {may},
	number = {05},
	pages = {2586-2595},
	publisher = {IEEE Computer Society},
	title = {GeoSynth: A Photorealistic Synthetic Indoor Dataset for Scene Understanding},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247087}}

@article{8648222,
	abstract = {Creating metrically accurate avatars is important for many applications such as virtual clothing try-on, ergonomics, medicine, immersive social media, telepresence, and gaming. Creating avatars that precisely represent a particular individual is challenging however, due to the need for expensive 3D scanners, privacy issues with photographs or videos, and difficulty in making accurate tailoring measurements. We overcome these challenges by creating ``The Virtual Caliper'', which uses VR game controllers to make simple measurements. First, we establish what body measurements users can reliably make on their own body. We find several distance measurements to be good candidates and then verify that these are linearly related to 3D body shape as represented by the SMPL body model. The Virtual Caliper enables novice users to accurately measure themselves and create an avatar with their own body shape. We evaluate the metric accuracy relative to ground truth 3D body scan data, compare the method quantitatively to other avatar creation tools, and perform extensive perceptual studies. We also provide a software application to the community that enables novices to rapidly create avatars in fewer than five minutes. Not only is our approach more rapid than existing methods, it exports a metrically accurate 3D avatar model that is rigged and skinned.},
	address = {Los Alamitos, CA, USA},
	author = {S. Pujades and B. Mohler and A. Thaler and J. Tesch and N. Mahmood and N. Hesse and H. H. Bulthoff and M. J. Black},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898748},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;shape;three-dimensional displays;shape measurement;solid modeling;tools;distance measurement},
	month = {may},
	number = {05},
	pages = {1887-1897},
	publisher = {IEEE Computer Society},
	title = {The Virtual Caliper: Rapid Creation of Metrically Accurate Avatars from 3D Measurements},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898748}}

@article{10049721,
	abstract = {Light field imaging can capture both the intensity information and the direction information of light rays. It naturally enables a six-degrees-of-freedom viewing experience and deep user engagement in virtual reality. Compared to 2D image assessment, light field image quality assessment (LFIQA) needs to consider not only the image quality in the spatial domain but also the quality consistency in the angular domain. However, there is a lack of metrics to effectively reflect the angular consistency and thus the angular quality of a light field image (LFI). Furthermore, the existing LFIQA metrics suffer from high computational costs due to the excessive data volume of LFIs. In this paper, we propose a novel concept of ``anglewise attention'' by introducing a multihead self-attention mechanism to the angular domain of an LFl. This mechanism better reflects the LFI quality. In particular, we propose three new attention kernels, including anglewise self-attention, anglewise grid attention, and anglewise central attention. These attention kernels can realize angular self-attention, extract multiangled features globally or selectively, and reduce the computational cost of feature extraction. By effectively incorporating the proposed kernels, we further propose our light field attentional convolutional neural network (LFACon) as an LFIQA metric. Our experimental results show that the proposed LFACon metric significantly outperforms the state-of-the-art LFIQA metrics. For the majority of distortion types, LFACon attains the best performance with lower complexity and less computational time.},
	address = {Los Alamitos, CA, USA},
	author = {Q. Qu and X. Chen and Y. Chung and W. Cai},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247069},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {kernel;measurement;feature extraction;light fields;image quality;convolution;computational efficiency},
	month = {may},
	number = {05},
	pages = {2239-2248},
	publisher = {IEEE Computer Society},
	title = {LFACon: Introducing Anglewise Attention to No-Reference Quality Assessment in Light Field Space},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247069}}

@article{8998293,
	abstract = {Today&#x27;s Virtual Reality (VR) displays are dramatically better than the head-worn displays offered 30 years ago, but today&#x27;s displays remain nearly as bulky as their predecessors in the 1980&#x27;s. Also, almost all consumer VR displays today provide 90-110 degrees field of view (FOV), which is much smaller than the human visual system&#x27;s FOV which extends beyond 180 degrees horizontally. In this paper, we propose ThinVR as a new approach to simultaneously address the bulk and limited FOV of head-worn VR displays. ThinVR enables a head-worn VR display to provide 180 degrees horizontal FOV in a thin, compact form factor. Our approach is to replace traditional large optics with a curved microlens array of custom-designed heterogeneous lenslets and place these in front of a curved display. We found that heterogeneous optics were crucial to make this approach work, since over a wide FOV, many lenslets are viewed off the central axis. We developed a custom optimizer for designing custom heterogeneous lenslets to ensure a sufficient eyebox while reducing distortions. The contribution includes an analysis of the design space for curved microlens arrays, implementation of physical prototypes, and an assessment of the image quality, eyebox, FOV, reduction in volume and pupil swim distortion. To our knowledge, this is the first work to demonstrate and analyze the potential for curved, heterogeneous microlens arrays to enable compact, wide FOV head-worn VR displays.},
	address = {Los Alamitos, CA, USA},
	author = {J. Ratcliff and A. Supikov and S. Alfaro and R. Azuma},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973064},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {lenses;microoptics;prototypes;optical imaging;optical diffraction;optical distortion},
	month = {may},
	number = {05},
	pages = {1981-1990},
	publisher = {IEEE Computer Society},
	title = {ThinVR: Heterogeneous microlens arrays for compact, 180 degree FOV VR near-eye displays},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973064}}

@article{Redon2007:A-Six-Degree-of-Freedom-God-Object-Method,
	abstract = {Abstract---This paper describes a generalization of the god-object method for haptic interaction between rigid bodies. Our approach separates the computation of the motion of the six degree-of-freedom god-object from the computation of the force applied to the user. The motion of the god-object is computed using continuous collision detection and constraint-based quasi-statics, which enables high-quality haptic interaction between contacting rigid bodies. The force applied to the user is computed using a novel constraint-based quasi-static approach, which allows us to suppress force artifacts typically found in previous methods. The constraint-based force applied to the user, which handles any number of simultaneous contact points, is computed within a few microseconds, while the update of the configuration of the rigid god-object is performed within a few milliseconds for rigid bodies containing up to tens of thousands of triangles. Our approach has been successfully tested on complex benchmarks. Our results show that the separation into asynchronous processes allows us to satisfy the different update rates required by the haptic and visual displays. Force shading and textures can be added and enlarge the range of haptic perception of a virtual environment. This paper is an extension of [1].},
	address = {Los Alamitos, CA, USA},
	author = {S. Redon and M. Ortega and S. Coquillart},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2007.1028},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;displays;virtual environment;computational modeling;motion detection;benchmark testing;humans;design automation;computer aided manufacturing;cadcam},
	month = {may},
	number = {03},
	pages = {458-469},
	publisher = {IEEE Computer Society},
	title = {A Six Degree-of-Freedom God-Object Method for Haptic Display of Rigid Bodies with Surface Properties},
	volume = {13},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2007.1028}}

@article{7829409,
	abstract = {Motion-to-photon latency causes images to sway from side to side in a VR/AR system, while display persistence causes smearing; both of these are undesirable artifacts. We show that once latency is reduced or eliminated, smearing due to display persistence becomes the dominant visual artifact, even with accurate tracker prediction. We investigate the human perceptual mechanisms responsible for this and we demonstrate a modified 3D rotation display controller architecture for driving a high speed digital display which minimizes latency and persistence. We simulate it in software and we built a testbench based on a very high frame rate (2880 fps 1-bit images) display system mounted on a mechanical rotation gantry which emulates display rotation during head rotation in an HMD.},
	address = {Los Alamitos, CA, USA},
	author = {M. Regan and G. P. Miller},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2656979},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {head;visualization;retina;resists;three-dimensional displays;standards;image resolution},
	month = {apr},
	number = {04},
	pages = {1295-1301},
	publisher = {IEEE Computer Society},
	title = {The Problem of Persistence with Rotating Displays},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2656979}}

@article{Reiners2011:JanusVF:-Accurate-Navigation,
	abstract = {Several critical limitations exist in the currently available tracking technologies for fully enclosed virtual reality (VR) systems. While several 6DOF tracking projects such as Hedgehog have successfully demonstrated excellent accuracy, precision, and robustness within moderate budgets, these projects still include elements of hardware that can interfere with the user&#x27;s visual experience. The objective of this project is to design a tracking solution for fully enclosed VR displays that achieves comparable performance to available commercial solutions but without any artifacts that can obscure the user&#x27;s view. JanusVF is a tracking solution involving a cooperation of both the hardware sensors and the software rendering system. A small, high-resolution camera is worn on the user&#x27;s head, but faces backward (180 degree rotation about vertical from the user&#x27;s perspective). After acquisition of the initial state, the VR rendering software draws specific fiducial markers with known size and absolute position inside the VR scene. These virtual markers are only drawn behind the user and in view of the camera. These fiducials are tracked by ARToolkitPlus and integrated by a single-constraint-at-a-time (SCAAT) filter algorithm to update the head pose. Experiments analyzing accuracy, precision, and latency in a six-sided CAVE-like system show performance that is comparable to alternative commercial technologies.},
	address = {Los Alamitos, CA, USA},
	author = {D. Reiners and M. Hutson},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2010.91},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;input devices and strategies;stereo;tracking.},
	month = {jan},
	number = {01},
	pages = {3-13},
	publisher = {IEEE Computer Society},
	title = {JanusVF: Accurate Navigation Using SCAAT and Virtual Fiducials},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2010.91}}

@article{10049734,
	abstract = {We propose a multi-sensor fusion method for capturing challenging 3D human motions with accurate consecutive local poses and global trajectories in large-scale scenarios, only using single LiDAR and 4 IMUs, which are set up conveniently and worn lightly. Specifically, to fully utilize the global geometry information captured by LiDAR and local dynamic motions captured by IMUs, we design a two-stage pose estimator in a coarse-to-fine manner, where point clouds provide the coarse body shape and IMU measurements optimize the local actions. Furthermore, considering the translation deviation caused by the view-dependent partial point cloud, we propose a pose-guided translation corrector. It predicts the offset between captured points and the real root locations, which makes the consecutive movements and trajectories more precise and natural. Moreover, we collect a LiDAR-IMU multi-modal mocap dataset, LIPD, with diverse human actions in long-range scenarios. Extensive quantitative and qualitative experiments on LIPD and other open datasets all demonstrate the capability of our approach for compelling motion capture in large-scale scenarios, which outperforms other methods by an obvious margin. We will release our code and captured dataset to stimulate future research.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Ren and C. Zhao and Y. He and P. Cong and H. Liang and J. Yu and L. Xu and Y. Ma},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247088},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {motion capture;laser radar;point cloud compression;three-dimensional displays;cameras;trajectory;optical sensors},
	month = {may},
	number = {05},
	pages = {2337-2347},
	publisher = {IEEE Computer Society},
	title = {LiDAR-aid Inertial Poser: Large-scale Human Motion Capture by Sparse Inertial and LiDAR Sensors},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247088}}

@article{6479182,
	abstract = {Accurately modeling the intrinsic material-dependent damping property for interactive sound rendering is a challenging problem. The Rayleigh damping model is commonly regarded as an adequate engineering model for interactive sound synthesis in virtual environment applications, but this assumption has never been rigorously analyzed. In this paper, we conduct a formal evaluation of this model. Our goal is to determine if auditory perception of material under Rayleigh damping assumption is &#x27;geometryinvariant&#x27;, i.e. if this approximation model is transferable across different shapes and sizes. First, audio recordings of same-material objects in various shapes and sizes are analyzed to determine if they can be approximated by the Rayleigh damping model with a single set of parameters. Next, we design and conduct a series of psychoacoustic experiments, in subjects evaluate if audio clips synthesized using the Rayleigh damping model are from the same material, when we alter the material, shape, and size parameters. Through both quantitative and qualitative evaluation, we show that the acoustic properties of the Rayleigh damping model for a single material is generally preserved across different geometries of objects consisting of homogeneous materials and is therefore a suitable, geometry-invariant sound model. Our study results also show that consistent with prior crossmodal expectations, visual perception of geometry can affect the auditory perception of materials. These findings facilitate the wide adoption of Rayleigh damping for interactive auditory systems and enable reuse of material parameters under this approximation model across different shapes and sizes, without laborious per-object parameter tuning.},
	address = {Los Alamitos, CA, USA},
	author = {Zhimin Ren and Hengchin Yeh and R. Klatzky and M. C. Lin},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.26},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {damping;shape;psychoacoustic models;geometry;glass;analytical models},
	month = {apr},
	number = {04},
	pages = {557-566},
	publisher = {IEEE Computer Society},
	title = {Auditory Perception of Geometry-Invariant Material Properties},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.26}}

@article{7829404,
	abstract = {This paper presents a novel immersive system called MR360 that provides interactive mixed reality (MR) experiences using a conventional low dynamic range (LDR) 360$\,^{\circ}$ panoramic video (360-video) shown in head mounted displays (HMDs). MR360 seamlessly composites 3D virtual objects into a live 360-video using the input panoramic video as the lighting source to illuminate the virtual objects. Image based lighting (IBL) is perceptually optimized to provide fast and believable results using the LDR 360-video as the lighting source. Regions of most salient lights in the input panoramic video are detected to optimize the number of lights used to cast perceptible shadows. Then, the areas of the detected lights adjust the penumbra of the shadow to provide realistic soft shadows. Finally, our real-time differential rendering synthesizes illumination of the virtual 3D objects into the 360-video. MR360 provides the illusion of interacting with objects in a video, which are actually 3D virtual objects seamlessly composited into the background of the 360-video. MR360 was implemented in a commercial game engine and tested using various 360-videos. Since our MR360 pipeline does not require any pre-computation, it can synthesize an interactive MR scene using a live 360-video stream while providing realistic high performance rendering suitable for HMDs.},
	address = {Los Alamitos, CA, USA},
	author = {T. Rhee and L. Petikam and B. Allen and A. Chalmers},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657178},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);videos;virtual reality;lighting;real-time systems;three-dimensional displays;visualization},
	month = {apr},
	number = {04},
	pages = {1379-1388},
	publisher = {IEEE Computer Society},
	title = {MR360: Mixed Reality Rendering for 360$\,^{\circ}$ Panoramic Videos},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657178}}

@article{8998353,
	abstract = {Telecollaboration involves the teleportation of a remote collaborator to another real-world environment where their partner is located. The fidelity of the environment plays an important role for allowing corresponding spatial references in remote collaboration. We present a novel asymmetric platform, Augmented Virtual Teleportation (AVT), which provides high-fidelity telepresence of a remote VR user (VR-Traveler) into a real-world collaboration space to interact with a local AR user (AR-Host). AVT uses a 360$\,^{\circ}$ video camera (360-camera) that captures and live-streams the omni-directional scenes over a network. The remote VR-Traveler watching the video in a VR headset experiences live presence and co-presence in the real-world collaboration space. The VR-Traveler&#x27;s movements are captured and transmitted to a 3D avatar overlaid onto the 360-camera which can be seen in the AR-Host&#x27;s display. The visual and audio cues for each collaborator are synchronized in the Mixed Reality Collaboration space (MRC-space), where they can interactively edit virtual objects and collaborate in the real environment using the real objects as a reference. High fidelity, real-time rendering of virtual objects and seamless blending into the real scene allows for unique mixed reality use-case scenarios. Our working prototype has been tested with a user study to evaluate spatial presence, co-presence, and user satisfaction during telecollaboration. Possible applications of AVT are identified and proposed to guide future usage.},
	address = {Los Alamitos, CA, USA},
	author = {T. Rhee and S. Thompson and D. Medeiros and R. dos Anjos and A. Chalmers},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973065},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {collaboration;visualization;avatars;telepresence;three-dimensional displays;teleportation},
	month = {may},
	number = {05},
	pages = {1923-1933},
	publisher = {IEEE Computer Society},
	title = {Augmented Virtual Teleportation for High-Fidelity Telecollaboration},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973065}}

@article{10049716,
	abstract = {Research on learning with and in immersive virtual reality (VR) continues to grow, yielding more insights into how immersive learning works. However, the actual use of VR learning environments in schools is still in its infancy. A major hurdle that hinders the use of immersive digital media in schools is the lack of guidelines for designing VR learning environments for practical use in schools. Such guidelines need to consider how students interact and learn in VR learning environments and how teachers can use such environments on a day-to-day basis. Using a design-based research approach, we explored the guidelines for creating VR learning content for tenth-grade students in a German secondary school and recreated a real-world, out-of-school VR learning space which can be used for hands-on instruction. This paper investigated how to maximise the experience of spatial presence by creating a VR learning environment in several microcycles. Furthermore, it took a closer look at the influence of the spatial situation model and cognitive involvement on this process. The results were evaluated with ANOVAs and path analyses, showing, for example, that involvement does not influence spatial presence in highly immersive and realistic VR learning environments.},
	address = {Los Alamitos, CA, USA},
	author = {M. Rieger and B. Risch},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247111},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;solid modeling;learning systems;media;hardware;distributed bragg reflectors;guidelines},
	month = {may},
	number = {05},
	pages = {2517-2526},
	publisher = {IEEE Computer Society},
	title = {How to Maximise Spatial Presence: Design Guidelines for a Virtual Learning Environment for School Use},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247111}}

@article{6777429,
	abstract = {We have created ``You, M.D.'', an interactive museum exhibit in which users learn about topics in public health literacy while interacting with virtual humans. You, M.D. is equipped with a weight sensor, a height sensor and a Microsoft Kinect that gather basic user information. Conceptually, You, M.D. could use this user information to dynamically select the appearance of the virtual humans in the interaction attempting to improve learning outcomes and user perception for each particular user. For this concept to be possible, a better understanding of how different elements of the visual appearance of a virtual human affects user perceptions is required. In this paper, we present the results of an initial user study with a large sample size (n &#x3D;333) ran using You, M.D. The study measured users&#x27; reactions based on the user&#x27;s gender and body-mass index (BMI) when facing virtual humans with BMI either concordant or discordant from the user&#x27;s BMI. The results of the study indicate that concordance between the users&#x27; BMI and the virtual human&#x27;s BMI affects male and female users differently. The results also show that female users rate virtual humans as more knowledgeable than male users rate the same virtual humans.},
	address = {Los Alamitos, CA, USA},
	author = {D. Rivera-Gutierrez and R. Ferdig and Jian Li and B. Lok},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.26},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {malignant tumors;visualization;public healthcare;avatars;educational institutions;tv},
	month = {apr},
	number = {04},
	pages = {636-643},
	publisher = {IEEE Computer Society},
	title = {Getting the Point Across: Exploring the Effects of Dynamic Virtual Humans in an Interactive Museum Exhibit on User Perceptions},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.26}}

@article{6479207,
	abstract = {Stressful interpersonal experiences can be difficult to prepare for. Virtual humans may be leveraged to allow learners to safely gain exposure to stressful interpersonal experiences. In this paper we present a between-subjects study exploring how the presence of a virtual human affected learners while practicing a stressful interpersonal experience. Twenty-six fourth-year medical students practiced performing a prostate exam on a prostate exam simulator. Participants in the experimental condition examined a simulator augmented with a virtual human. Other participants examined a standard unaugmented simulator. Participants reactions were assessed using self-reported, behavioral, and physiological metrics. Participants who examined the virtual human experienced significantly more stress, measured via skin conductance. Participants stress was correlated with previous experience performing real prostate exams; participants who had performed more real prostate exams were more likely to experience stress while examining the virtual human. Participants who examined the virtual human showed signs of greater engagement; non-stressed participants performed better prostate exams while stressed participants treated the virtual human more realistically. Results indicated that stress evoked by virtual humans is linked to similar previous real-world stressful experiences, implying that learners real-world experience must be taken into account when using virtual humans to prepare them for stressful interpersonal experiences.},
	address = {Los Alamitos, CA, USA},
	author = {A. Robb and R. Kopper and R. Ambani and F. Qayyum and D. Lind and Li-Ming Su and B. Lok},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.35},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {standards;stress;measurement;interviews;training;educational institutions;prostate cancer},
	month = {apr},
	number = {04},
	pages = {662-670},
	publisher = {IEEE Computer Society},
	title = {Leveraging Virtual Humans to Effectively Prepare Learners for Stressful Interpersonal Experiences},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.35}}

@article{7014272,
	abstract = {In this paper we present a study exploring whether the physical presence of another human changes how people perceive and behave with virtual teammates. We conducted a study (n &#x3D; 69) in which nurses worked with a simulated health care team to prepare a patient for surgery. The agency of participants&#x27; teammates was varied between conditions; participants either worked with a virtual surgeon and a virtual anesthesiologist, a human confederate playing a surgeon and a virtual anesthesiologist, or a virtual surgeon and a human confederate playing an anesthesiologist. While participants perceived the human confederates to have more social presence (p },
	address = {Los Alamitos, CA, USA},
	author = {A. Robb and A. Cordar and S. Lampotang and C. White and A. Wendling and B. Lok},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391855},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {surgery;training;avatars;virtual environments;virtual groups;mathematical model;speech},
	month = {apr},
	number = {04},
	pages = {511-519},
	publisher = {IEEE Computer Society},
	title = {Teaming Up with Virtual Humans: How Other People Change Our Perceptions of and Behavior with Virtual Teammates},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391855}}

@article{7383335,
	abstract = {In a group setting, it is possible for attributes of one group member to indirectly affect how other group members are perceived. In this paper, we explore whether one group member&#x27;s agency (e.g. if they are real or virtual) can indirectly affect behavior with other group members. We also consider whether variations in the agency of a group member directly affects behavior with that group member. To do so, we examined gaze behavior during a team training exercise, in which sixty-nine nurses worked with a surgeon and an anesthesiologist to prepare a simulated patient for surgery. The agency of the surgeon and the anesthesiologist were varied between conditions. Nurses&#x27; gaze behavior was coded using videos of their interactions. Agency was observed to directly affect behavior, such that participants spent more time gazing at virtual teammates than human teammates. However, participants continued to obey polite gaze norms with virtual teammates. In contrast, agency was not observed to indirectly affect gaze behavior. The presence of a second human did not affect participants&#x27; gaze behavior with virtual teammates.},
	address = {Los Alamitos, CA, USA},
	author = {A. Robb and A. Kleinsmith and A. Cordar and C. White and S. Lampotang and A. Wendling and B. Lok},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518405},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {surgery;face;training;videos;measurement;data models;robots},
	month = {apr},
	number = {04},
	pages = {1336-1345},
	publisher = {IEEE Computer Society},
	title = {Do Variations in Agency Indirectly Affect Behavior with Others? An Analysis of Gaze Behavior},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518405}}

@article{6479209,
	abstract = {The aim of our experiment is to determine if eye-gaze can be estimated from a virtuality human: to within the accuracies that underpin social interaction; and reliably across gaze poses and camera arrangements likely in every day settings. The scene is set by explaining why Immersive Virtuality Telepresence has the potential to meet the grand challenge of faithfully communicating both the appearance and the focus of attention of a remote human participant within a shared 3D computer-supported context. Within the experiment n&#x3D;22 participants rotated static 3D virtuality humans, reconstructed from surround images, until they felt most looked at. The dependent variable was absolute angular error, which was compared to that underpinning social gaze behaviour in the natural world. Independent variables were 1) relative orientations of eye, head and body of captured subject; and 2) subset of cameras used to texture the form. Analysis looked for statistical and practical significance and qualitative corroborating evidence. The analysed results tell us much about the importance and detail of the relationship between gaze pose, method of video based reconstruction, and camera arrangement. They tell us that virtuality can reproduce gaze to an accuracy useful in social interaction, but with the adopted method of Video Based Reconstruction, this is highly dependent on combination of gaze pose and camera arrangement. This suggests changes in the VBR approach in order to allow more flexible camera arrangements. The work is of interest to those wanting to support expressive meetings that are both socially and spatially situated, and particular those using or building Immersive Virtuality Telepresence to accomplish this. It is also of relevance to the use of virtuality humans in applications ranging from the study of human interactions to gaming and the crossing of the stage line in films and TV.},
	address = {Los Alamitos, CA, USA},
	author = {D. J. Roberts and J. Rae and T. W. Duckworth and C. M. Moore and R. Aspin},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.30},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cameras;estimation;accuracy;image reconstruction;face;visualization},
	month = {apr},
	number = {04},
	pages = {681-690},
	publisher = {IEEE Computer Society},
	title = {Estimating the Gaze of a Virtuality Human},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.30}}

@article{9714809,
	abstract = {Autism - also known as Autism Spectrum Disorders or Autism Spectrum Conditions - is a neurodevelopmental condition characterized by repetitive behaviours and differences in communication and social interaction. As a consequence, many autistic individuals may struggle in everyday life, which sometimes manifests in depression, unemployment, or addiction. One crucial problem in patient support and treatment is the long waiting time to diagnosis, which was approximated to thirteen months on average. Yet, the earlier an intervention can take place the better the patient can be supported, which was identified as a crucial factor. We propose a system to support the screening of Autism Spectrum Disorders based on a virtual reality social interaction, namely a shopping experience, with an embodied agent. During this everyday interaction, behavioral responses are tracked and recorded. We analyze this behavior with machine learning approaches to classify participants from an autistic participant sample in comparison to a typically developed individuals control sample with high accuracy, demonstrating the feasibility of the approach. We believe that such tools can strongly impact the way mental disorders are assessed and may help to further find objective criteria and categorization.},
	address = {Los Alamitos, CA, USA},
	author = {M. Robles and N. Namdarian and J. Otto and E. Wassiljew and N. Navab and C. Falter-Wagner and D. Roth},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150489},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {autism;avatars;tutorials;virtual environments;reliability;machine learning;three-dimensional displays},
	month = {may},
	number = {05},
	pages = {2168-2178},
	publisher = {IEEE Computer Society},
	title = {A Virtual Reality Based System for the Screening and Classification of Autism},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150489}}

@article{9714044,
	abstract = {Many quality evaluation methods are used to assess uni-modal audio or video content without considering perceptual, cognitive, and interactive aspects present in virtual reality (VR) settings. Consequently, little is known regarding the repercussions of the employed evaluation method, content, and subject behavior on the quality ratings in VR. This mixed between- and within-subjects study uses four subjective audio quality evaluation methods (viz. multiple-stimulus with and without reference for direct scaling, and rank-order elimination and pairwise comparison for indirect scaling) to investigate the contributing factors present in multi-modal 6-DoF VR on quality ratings of real-time audio rendering. For each between-subjects employed method, two sets of conditions in five VR scenes were evaluated within-subjects. The conditions targeted relevant attributes for binaural audio reproduction using scenes with various amounts of user interactivity. Our results show all referenceless methods produce similar results using both condition sets. However, rank-order elimination proved to be the fastest method, required the least amount of repetitive motion, and yielded the highest discrimination between spatial conditions. Scene complexity was found to be a main effect within results, with behavioral and task load index results implying more complex scenes and interactive aspects of 6-DoF VR can impede quality judgments.},
	address = {Los Alamitos, CA, USA},
	author = {T. Robotham and O. S. Rummukainen and M. Kurz and M. Eckert and E. P. Habets},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150491},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;visualization;virtual reality;rendering (computer graphics);real-time systems;complexity theory;testing},
	month = {may},
	number = {05},
	pages = {2091-2101},
	publisher = {IEEE Computer Society},
	title = {Comparing Direct and Indirect Methods of Audio Quality Evaluation in Virtual Reality Scenes of Varying Complexity},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150491}}

@article{7384541,
	abstract = {Recent research in sound simulation has focused on either sound synthesis or sound propagation, and many standalone algorithms have been developed for each domain. We present a novel technique for coupling sound synthesis with sound propagation to automatically generate realistic aural content for virtual environments. Our approach can generate sounds from rigid-bodies based on the vibration modes and radiation coefficients represented by the single-point multipole expansion. We present a mode-adaptive propagation algorithm that uses a perceptual Hankel function approximation technique to achieve interactive runtime performance. The overall approach allows for high degrees of dynamism - it can support dynamic sources, dynamic listeners, and dynamic directivity simultaneously. We have integrated our system with the Unity game engine and demonstrate the effectiveness of this fully-automatic technique for audio content creation in complex indoor and outdoor scenes. We conducted a preliminary, online user-study to evaluate whether our Hankel function approximation causes any perceptible loss of audio quality. The results indicate that the subjects were unable to distinguish between the audio rendered using the approximate function and audio rendered using the full Hankel function in the Cathedral, Tuscany, and the Game benchmarks.},
	address = {Los Alamitos, CA, USA},
	author = {A. Rungta and C. Schissler and R. Mehra and C. Malloy and M. Lin and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518421},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {games;vibrations;solid modeling;mathematical model;boundary conditions;acoustics;engines},
	month = {apr},
	number = {04},
	pages = {1346-1355},
	publisher = {IEEE Computer Society},
	title = {SynCoPation: Interactive Synthesis-Coupled Sound Propagation},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518421}}

@article{8307458,
	abstract = {We present a novel method to generate plausible diffraction effects for interactive sound propagation in dynamic scenes. Our approach precomputes a diffraction kernel for each dynamic object in the scene and combines them with interactive ray tracing algorithms at runtime. A diffraction kernel encapsulates the sound interaction behavior of individual objects in the free field and we present a new source placement algorithm to significantly accelerate the precomputation. Our overall propagation algorithm can handle highly-tessellated or smooth objects undergoing rigid motion. We have evaluated our algorithm&#x27;s performance on different scenarios with multiple moving objects and demonstrate the benefits over prior interactive geometric sound propagation methods. We also performed a user study to evaluate the perceived smoothness of the diffracted field and found that the auditory perception using our approach is comparable to that of a wave-based sound propagation method.},
	address = {Los Alamitos, CA, USA},
	author = {A. Rungta and C. Schissler and N. Rewkowski and R. Mehra and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794098},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {diffraction;heuristic algorithms;kernel;computational modeling;acoustics;ray tracing;runtime},
	month = {apr},
	number = {04},
	pages = {1613-1622},
	publisher = {IEEE Computer Society},
	title = {Diffraction Kernels for Interactive Sound Propagation in Dynamic Environments},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794098}}

@article{7383328,
	abstract = {Current virtual reality (VR) technologies have enormous potential to allow humans to experience computer-generated immersive virtual environments (IVEs). Many of these IVEs support near-natural audiovisual stimuli similar to the stimuli generated in our physical world. However, decades of VR research have been devoted to exploring and understand differences between perception and action in such IVEs compared to real-world perception and action. Although, significant differences have been revealed for spatiotemporal perception between IVEs and the physical world such as distance underestimation, there is still a scarcity of knowledge about the reasons for such perceptual discrepancies, in particular regarding the perception of temporal durations in IVEs. In this article, we explore the effects of manipulated zeitgebers, cognitive load and immersion on time estimation as yet unexplored factors of spatiotemporal perception in IVEs. We present an experiment in which we analyze human sensitivity to temporal durations while experiencing an immersive head-mounted display (HMO) environment. We found that manipulations of external zeitgebers caused by a natural or unnatural movement of the virtual sun had a significant effect on time judgments. Moreover, using the dual-task paradigm the results show that increased spatial and verbal cognitive load resulted in a significant shortening of judged time as well as an interaction with the external zeitgebers. Finally, we discuss the implications for the design of near-natural computer-generated virtual worlds.},
	address = {Los Alamitos, CA, USA},
	author = {C. Schatzschneider and G. Bruder and F. Steinicke},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518137},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {sun;estimation;clocks;virtual environments;games;circadian rhythm;sensitivity},
	month = {apr},
	number = {04},
	pages = {1387-1395},
	publisher = {IEEE Computer Society},
	title = {Who turned the clock? Effects of Manipulated Zeitgebers, Cognitive Load and Immersion on Time Estimation},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518137}}

@article{Schiffbauer2012:Effects-of-Immersion-on-Visual,
	abstract = {Volume visualization has been widely used for decades for analyzing datasets ranging from 3D medical images to seismic data to paleontological data. Many have proposed using immersive virtual reality (VR) systems to view volume visualizations, and there is anecdotal evidence of the benefits of VR for this purpose. However, there has been very little empirical research exploring the effects of higher levels of immersion for volume visualization, and it is not known how various components of immersion influence the effectiveness of visualization in VR. We conducted a controlled experiment in which we studied the independent and combined effects of three components of immersion (head tracking, field of regard, and stereoscopic rendering) on the effectiveness of visualization tasks with two x-ray microscopic computed tomography datasets. We report significant benefits of analyzing volume data in an environment involving those components of immersion. We find that the benefits do not necessarily require all three components simultaneously, and that the components have variable influence on different task categories. The results of our study improve our understanding of the effects of immersion on perceived and actual task performance, and provide guidance on the choice of display systems to designers seeking to maximize the effectiveness of volume visualization applications.},
	address = {Los Alamitos, CA, USA},
	author = {J. D. Schiffbauer and K. Sensharma and B. Laha and D. A. Bowman},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.42},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;computerised tomography;data analysis;data visualisation;rendering (computer graphics);immersive virtual reality systems;immersion effect;visual analysis;volume data analysis;volume visualization;3d medical image;seismic data;paleontological data;immersive vr system;head tracking component;field-of-regard component;stereoscopic rendering component;x-ray microscopic computed tomography dataset;perceived task performance;display system;three dimensional displays;data visualization;mice;visualization;rendering (computer graphics);training;head;virtual reality.;immersion;micro-ct;data analysis;volume visualization;3d visualization;cave;virtual environments},
	month = {apr},
	number = {04},
	pages = {597-606},
	publisher = {IEEE Computer Society},
	title = {Effects of Immersion on Visual Analysis of Volume Data},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.42}}

@article{7383327,
	abstract = {We present a novel spatial audio rendering technique to handle sound sources that can be represented by either an area or a volume in VR environments. As opposed to point-sampled sound sources, our approach projects the area-volumetric source to the spherical domain centered at the listener and represents this projection area compactly using the spherical harmonic (SH) basis functions. By representing the head-related transfer function (HRTF) in the same basis, we demonstrate that spatial audio which corresponds to an area-volumetric source can be efficiently computed as a dot product of the SH coefficients of the projection area and the HRTF. This results in an efficient technique whose computational complexity and memory requirements are independent of the complexity of the sound source. Our approach can support dynamic area-volumetric sound sources at interactive rates. We evaluate the performance of our technique in large complex VR environments and demonstrate significant improvement over the naive point-sampling technique. We also present results of a user evaluation, conducted to quantify the subjective preference of the user for our approach over the point-sampling approach in VR environments.},
	address = {Los Alamitos, CA, USA},
	author = {C. Schissler and A. Nicholls and R. Mehra},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518134},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {ear;harmonic analysis;rendering (computer graphics);three-dimensional displays;virtual environments;transfer functions},
	month = {apr},
	number = {04},
	pages = {1356-1366},
	publisher = {IEEE Computer Society},
	title = {Efficient HRTF-based Spatial Audio for Area and Volumetric Sources},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518134}}

@article{8260943,
	abstract = {In virtual environments, the space that can be explored by real walking is limited by the size of the tracked area. To enable unimpeded walking through large virtual spaces in small real-world surroundings, redirection techniques are used. These unnoticeably manipulate the user&#x27;s virtual walking trajectory. It is important to know how strongly such techniques can be applied without the user noticing the manipulation - or getting cybersick. Previously, this was estimated by measuring a detection threshold (DT) in highly-controlled psychophysical studies, which experimentally isolate the effect but do not aim for perceived immersion in the context of VR applications. While these studies suggest that only relatively low degrees of manipulation are tolerable, we claim that, besides establishing detection thresholds, it is important to know when the user&#x27;s immersion breaks. We hypothesize that the degree of unnoticed manipulation is significantly different from the detection threshold when the user is immersed in a task. We conducted three studies: a) to devise an experimental paradigm to measure the threshold of limited immersion (TLI), b) to measure the TLI for slowly decreasing and increasing rotation gains, and c) to establish a baseline of cybersickness for our experimental setup. For rotation gains greater than 1.0, we found that immersion breaks quite late after the gain is detectable. However, for gains lesser than 1.0, some users reported a break of immersion even before established detection thresholds were reached. Apparently, the developed metric measures an additional quality of user experience. This article contributes to the development of effective spatial compression methods by utilizing the break of immersion as a benchmark for redirection techniques.},
	address = {Los Alamitos, CA, USA},
	author = {P. Schmitz and J. Hildebrandt and A. Valdez and L. Kobbelt and M. Ziefle},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793671},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;visualization;gain measurement;rotation measurement;tracking},
	month = {apr},
	number = {04},
	pages = {1623-1632},
	publisher = {IEEE Computer Society},
	title = {You Spin my Head Right Round: Threshold of Limited Immersion for Rotation Gains in Redirected Walking},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793671}}

@article{7833188,
	abstract = {Modern virtual reality simulations require a constant high-frame rate from the rendering engine. They may also require very low latency and stereo images. Previous rendering engines for virtual reality applications have exploited spatial and temporal coherence by using image-warping to re-use previous frames or to render a stereo pair at lower cost than running the full render pipeline twice. However these previous approaches have shown artifacts or have not scaled well with image size. We present a new image-warping algorithm that has several novel contributions: an adaptive grid generation algorithm for proxy geometry for image warping; a low-pass hole-filling algorithm to address un-occlusion; and support for transparent surfaces by efficiently ray casting transparent fragments stored in per-pixel linked lists of an A-Buffer. We evaluate our algorithm with a variety of challenging test cases. The results show that it achieves better quality image-warping than state-of-the-art techniques and that it can support transparent surfaces effectively. Finally, we show that our algorithm can achieve image warping at rates suitable for practical use in a variety of applications on modern virtual reality equipment.},
	address = {Los Alamitos, CA, USA},
	author = {A. Schollmeyer and S. Schneegans and S. Beck and A. Steed and B. Froehlich},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657078},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);three-dimensional displays;casting;stereo image processing;engines;pipelines;visualization},
	month = {apr},
	number = {04},
	pages = {1332-1341},
	publisher = {IEEE Computer Society},
	title = {Efficient Hybrid Image Warping for High Frame-Rate Stereoscopic Rendering},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657078}}

@article{10049681,
	abstract = {In the embryonic human heart, complex dynamic shape changes take place in a short period of time on a microscopic scale, making this development difficult to visualize. However, spatial understanding of these processes is essential for students and future cardiologists to properly diagnose and treat congenital heart defects. Following a user centered approach, the most crucial embryological stages were identified and translated into a virtual reality learning environment (VRLE) to enable the understanding of the morphological transitions of these stages through advanced interactions. To address individual learning types, we implemented different features and evaluated the application regarding usability, perceived task load, and sense of presence in a user study. We also assessed spatial awareness and knowledge gain, and finally obtained feedback from domain experts. Overall, students and professionals rated the application positively. To minimize distraction from interactive learning content, such VRLEs should consider features for different learning types, allow for gradual habituation, and at the same time provide enough playful stimuli. Our work previews how VR can be integrated into a cardiac embryology education curriculum.},
	address = {Los Alamitos, CA, USA},
	author = {D. Schott and M. Kunz and T. Wunderling and F. Heinrich and R. Braun-Dullaeus and C. Hansen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247110},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;education;heart;embryo;solid modeling;virtual reality;visualization},
	month = {may},
	number = {05},
	pages = {2615-2625},
	publisher = {IEEE Computer Society},
	title = {CardioGenesis4D: Interactive Morphological Transitions of Embryonic Heart Development in a Virtual Learning Environment},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247110}}

@article{8661657,
	abstract = {We present a method for adding parallax and real-time playback of 360$\,^{\circ}$ videos in Virtual Reality headsets. In current video players, the playback does not respond to translational head movement, which reduces the feeling of immersion, and causes motion sickness for some viewers. Given a 360$\,^{\circ}$ video and its corresponding depth (provided by current stereo 360$\,^{\circ}$ stitching algorithms), a naive image-based rendering approach would use the depth to generate a 3D mesh around the viewer, then translate it appropriately as the viewer moves their head. However, this approach breaks at depth discontinuities, showing visible distortions, whereas cutting the mesh at such discontinuities leads to ragged silhouettes and holes at disocclusions. We address these issues by improving the given initial depth map to yield cleaner, more natural silhouettes. We rely on a three-layer scene representation, made up of a foreground layer and two static background layers, to handle disocclusions by propagating information from multiple frames for the first background layer, and then inpainting for the second one. Our system works with input from many of today&#x27;s most popular 360$\,^{\circ}$ stereo capture devices (e.g., Yi Halo or GoPro Odyssey), and works well even if the original video does not provide depth information. Our user studies confirm that our method provides a more compelling viewing experience than without parallax, increasing immersion while reducing discomfort and nausea.},
	address = {Los Alamitos, CA, USA},
	author = {A. Serrano and I. Kim and Z. Chen and S. DiVerdi and D. Gutierrez and A. Hertzmann and B. Masia},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898757},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;cameras;head;rendering (computer graphics);streaming media;virtual reality;visualization},
	month = {may},
	number = {05},
	pages = {1817-1827},
	publisher = {IEEE Computer Society},
	title = {Motion parallax for 360$\,^{\circ}$ RGBD video},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898757}}

@article{Sewall:2011:VirtualizedTraffic,
	abstract = {We present a novel concept, Virtualized Traffic, to reconstruct and visualize continuous traffic flows from discrete spatiotemporal data provided by traffic sensors or generated artificially to enhance a sense of immersion in a dynamic virtual world. Given the positions of each car at two recorded locations on a highway and the corresponding time instances, our approach can reconstruct the traffic flows (i.e., the dynamic motions of multiple cars over time) between the two locations along the highway for immersive visualization of virtual cities or other environments. Our algorithm is applicable to high-density traffic on highways with an arbitrary number of lanes and takes into account the geometric, kinematic, and dynamic constraints on the cars. Our method reconstructs the car motion that automatically minimizes the number of lane changes, respects safety distance to other cars, and computes the acceleration necessary to obtain a smooth traffic flow subject to the given constraints. Furthermore, our framework can process a continuous stream of input data in real time, enabling the users to view virtualized traffic events in a virtual world as they occur. We demonstrate our reconstruction technique with both synthetic and real-world input.},
	address = {Los Alamitos, CA, USA},
	author = {J. Sewall and J. van den Berg and M. Lin and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2010.27},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {spatiotemporal phenomena;traffic control;automated highways;data visualization;road transportation;kinematics;virtual reality;urban areas;vehicle safety},
	month = {jan},
	number = {01},
	pages = {26-37},
	publisher = {IEEE Computer Society},
	title = {Virtualized Traffic: Reconstructing Traffic Flows from Discrete Spatiotemporal Data},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2010.27}}

@article{7835276,
	abstract = {In immersive Virtual Reality systems, users tend to move in a Virtual Environment as they would in an analogous physical environment. In this work, we investigated how user behaviour is affected when the Virtual Environment differs from the physical space. We created two sets of four environments each, plus a virtual replica of the physical environment as a baseline. The first focused on aesthetic discrepancies, such as a water surface in place of solid ground. The second focused on mixing immaterial objects together with those paired to tangible objects. For example, barring an area with walls or obstacles. We designed a study where participants had to reach three waypoints laid out in such a way to prompt a decision on which path to follow based on the conflict between the mismatching visual stimuli and their awareness of the real layout of the room. We analysed their performances to determine whether their trajectories were altered significantly from the shortest route. Our results indicate that participants altered their trajectories in presence of surfaces representing higher walking difficulty (for example, water instead of grass). However, when the graphical appearance was found to be ambiguous, there was no significant trajectory alteration. The environments mixing immaterial with physical objects had the most impact on trajectories with a mean deviation from the shortest route of 60 cm against the 37 cm of environments with aesthetic alterations. The co-existance of paired and unpaired virtual objects was reported to support the idea that all objects participants saw were backed by physical props. From these results and our observations, we derive guidelines on how to alter user movement behaviour in Virtual Environments.},
	address = {Los Alamitos, CA, USA},
	author = {A. L. Simeone and I. Mavridou and W. Powell},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657038},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;trajectory;virtual environments;visualization;navigation;tracking},
	month = {apr},
	number = {04},
	pages = {1312-1321},
	publisher = {IEEE Computer Society},
	title = {Altering User Movement Behaviour in Virtual Environments},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657038}}

@article{8269807,
	abstract = {Understanding how people explore immersive virtual environments is crucial for many applications, such as designing virtual reality (VR) content, developing new compression algorithms, or learning computational models of saliency or visual attention. Whereas a body of recent work has focused on modeling saliency in desktop viewing conditions, VR is very different from these conditions in that viewing behavior is governed by stereoscopic vision and by the complex interaction of head orientation, gaze, and other kinematic constraints. To further our understanding of viewing behavior and saliency in VR, we capture and analyze gaze and head orientation data of 169 users exploring stereoscopic, static omni-directional panoramas, for a total of 1980 head and gaze trajectories for three different viewing conditions. We provide a thorough analysis of our data, which leads to several important insights, such as the existence of a particular fixation bias, which we then use to adapt existing saliency predictors to immersive VR conditions. In addition, we explore other applications of our data and analysis, including automatic alignment of VR video cuts, panorama thumbnails, panorama video synopsis, and saliency-basedcompression.},
	address = {Los Alamitos, CA, USA},
	author = {V. Sitzmann and A. Serrano and A. Pavel and M. Agrawala and D. Gutierrez and B. Masia and G. Wetzstein},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793599},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {head;visualization;magnetic heads;virtual environments;stereo image processing;predictive models;computational modeling},
	month = {apr},
	number = {04},
	pages = {1633-1642},
	publisher = {IEEE Computer Society},
	title = {Saliency in VR: How Do People Explore Virtual Environments?},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793599}}

@article{7833192,
	abstract = {We report on the design and results of an experiment investigating factors influencing Slater&#x27;s Plausibility Illusion (Psi) in virtual environments (VEs). Slater proposed Psi and Place Illusion (PI) as orthogonal components of virtual experience which contribute to realistic response in a VE. PI corresponds to the traditional conception of presence as ``being there,'' so there exists a substantial body of previous research relating to PI, but very little relating to Psi. We developed this experiment to investigate the components of plausibility illusion using subjective matching techniques similar to those used in color science. Twenty-one participants each experienced a scenario with the highest level of coherence (the extent to which a scenario matches user expectations and is internally consistent), then in eight different trials chose transitions from lower-coherence to higher-coherence scenarios with the goal of matching the level of Psi they felt in the highest-coherence scenario. At each transition, participants could change one of the following coherence characteristics: the behavior of the other virtual humans in the environment, the behavior of their own body, the physical behavior of objects, or the appearance of the environment. Participants tended to choose improvements to the virtual body before any other improvements. This indicates that having an accurate and well-behaved representation of oneself in the virtual environment is the most important contributing factor to Psi. This study is the first to our knowledge to focus specifically on coherence factors in virtual environments.},
	address = {Los Alamitos, CA, USA},
	author = {R. Skarbez and S. Neyret and F. P. Brooks and M. Slater and M. C. Whitton},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657158},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {coherence;virtual environments;avatars;animation;legged locomotion;visualization;correlation},
	month = {apr},
	number = {04},
	pages = {1369-1378},
	publisher = {IEEE Computer Society},
	title = {A Psychophysical Experiment Regarding Components of the Plausibility Illusion},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657158}}

@article{9714118,
	abstract = {One of the challenging tasks in virtual scene design for Virtual Reality (VR) is causing it to invoke a particular mood in viewers. The subjective nature of moods brings uncertainty to the purpose. We propose a novel approach to automatic adjustment of the colors of textures for objects in a virtual indoor scene, enabling it to match a target mood. A dataset of 25,000 images, including building/home interiors, was used to train a classifier with the features extracted via deep learning. It contributes to an optimization process that colorizes virtual scenes automatically according to the target mood. Our approach was tested on four different indoor scenes, and we conducted a user study demonstrating its efficacy through statistical analysis with the focus on the impact of the scenes experienced with a VR headset.},
	address = {Los Alamitos, CA, USA},
	author = {M. Solah and H. Huang and J. Sheng and T. Feng and M. Pomplun and L. Yu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150513},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {mood;image color analysis;feature extraction;three-dimensional displays;optimization;task analysis;electronic mail},
	month = {may},
	number = {05},
	pages = {2058-2068},
	publisher = {IEEE Computer Society},
	title = {Mood-Driven Colorization of Virtual Indoor Scenes},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150513}}

@article{10049651,
	abstract = {This paper investigates the effects of thermal referral and tactile masking illusions to achieve localized thermal feedback on the upper body. Two experiments are conducted. The first experiment uses a 2D array of sixteen vibrotactile actuators $(4\times 4)$ with four thermal actuators to explore the thermal distribution on the user&#x27;s back. A combination of thermal and tactile sensations is delivered to establish the distributions of thermal referral illusions with different numbers of vibrotactile cues. The result confirms that localized thermal feedback can be achieved through cross-modal thermo-tactile interaction on the user&#x27;s back of the body. The second experiment is conducted to validate our approach by comparing it with thermal-only conditions with an equal and higher number of thermal actuators in VR. The results show that our thermal referral with a tactile masking approach with a lesser number of thermal actuators achieves higher response time and better location accuracy than thermal-only conditions. Our findings can contribute to thermal-based wearable design to achieve greater user performance and experiences.},
	address = {Los Alamitos, CA, USA},
	author = {H. Son and H. Wang and Y. Singhal and J. Kim},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247068},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {actuators;temperature sensors;thermal conductivity;fingers;location awareness;temperature;tactile sensors},
	month = {may},
	number = {05},
	pages = {2211-2219},
	publisher = {IEEE Computer Society},
	title = {Upper Body Thermal Referral and Tactile Masking for Localized Feedback},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247068}}

@article{10049689,
	abstract = {Visually exploring in a real-world 4D spatiotemporal space freely in VR has been a long-term quest. The task is especially appealing when only a few or even single RGB cameras are used for capturing the dynamic scene. To this end, we present an efficient framework capable of fast reconstruction, compact modeling, and streamable rendering. First, we propose to decompose the 4D spatiotemporal space according to temporal characteristics. Points in the 4D space are associated with probabilities of belonging to three categories: static, deforming, and new areas. Each area is represented and regularized by a separate neural field. Second, we propose a hybrid representations based feature streaming scheme for efficiently modeling the neural fields. Our approach, coined NeRFPlayer, is evaluated on dynamic scenes captured by single hand-held cameras and multi-camera arrays, achieving comparable or superior rendering performance in terms of quality and speed comparable to recent state-of-the-art methods, achieving reconstruction in 10 seconds per frame and interactive rendering. Project website: https://bit.ly/nerfplayer.},
	address = {Los Alamitos, CA, USA},
	author = {L. Song and A. Chen and Z. Li and Z. Chen and L. Chen and J. Yuan and Y. Xu and A. Geiger},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247082},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);three-dimensional displays;cameras;streaming media;spatiotemporal phenomena;image reconstruction;dynamics},
	month = {may},
	number = {05},
	pages = {2732-2742},
	publisher = {IEEE Computer Society},
	title = {NeRFPlayer: A Streamable Dynamic Scene Representation with Decomposed Neural Radiance Fields},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247082}}

@article{6479191,
	abstract = {Passive haptics, also known as tactile augmentation, denotes the use of a physical counterpart to a virtual environment to provide tactile feedback. Employing passive haptics can result in more realistic touch sensations than those from active force feedback, especially for rigid contacts. However, changes in the virtual environment would necessitate modifications of the physical counterparts. In recent work space warping has been proposed as one solution to overcome this limitation. In this technique virtual space is distorted such that a variety of virtual models can be mapped onto one single physical object. In this paper, we propose as an extension adaptive space warping; we show how this technique can be employed in a mixed-reality surgical training simulator in order to map different virtual patients onto one physical anatomical model. We developed methods to warp different organ geometries onto one physical mock-up, to handle different mechanical behaviors of the virtual patients, and to allow interactive modifications of the virtual structures, while the physical counterparts remain unchanged. Various practical examples underline the wide applicability of our approach. To the best of our knowledge this is the first practical usage of such a technique in the specific context of interactive medical training.},
	address = {Los Alamitos, CA, USA},
	author = {J. Spillmann and S. Tuchschmid and M. Harders},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.23},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;avatars;bones;geometry;joints;surgery;shape},
	month = {apr},
	number = {04},
	pages = {626-633},
	publisher = {IEEE Computer Society},
	title = {Adaptive Space Warping to Enhance Passive Haptics in an Arthroscopy Surgical Simulator},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.23}}

@article{8999630,
	abstract = {Emergent in the field of head mounted display design is a desire to leverage the limitations of the human visual system to reduce the computation, communication, and display workload in power and form-factor constrained systems. Fundamental to this reduced workload is the ability to match display resolution to the acuity of the human visual system, along with a resulting need to follow the gaze of the eye as it moves, a process referred to as foveation. A display that moves its content along with the eye may be called a Foveated Display, though this term is also commonly used to describe displays with non-uniform resolution that attempt to mimic human visual acuity. We therefore recommend a definition for the term Foveated Display that accepts both of these interpretations. Furthermore, we include a simplified model for human visual Acuity Distribution Functions (ADFs) at various levels of visual acuity, across wide fields of view and propose comparison of this ADF with the Resolution Distribution Function of a foveated display for evaluation of its resolution at a particular gaze direction. We also provide a taxonomy to allow the field to meaningfully compare and contrast various aspects of foveated displays in a display and optical technology-agnostic manner.},
	address = {Los Alamitos, CA, USA},
	author = {J. Spjut and B. Boudaoud and J. Kim and T. Greer and R. Albert and M. Stengel and K. Aksit and D. Luebke},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973053},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;distribution functions;resource description framework;optical sensors;rendering (computer graphics);visual systems;spatial resolution},
	month = {may},
	number = {05},
	pages = {2126-2134},
	publisher = {IEEE Computer Society},
	title = {Toward Standardized Classification of Foveated Displays},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973053}}

@article{7383331,
	abstract = {Consumer virtual reality systems are now becoming widely available. We report on a study on presence and embodiment within virtual reality that was conducted &#x60;in the wild&#x27;, in that data was collected from devices owned by consumers in uncontrolled settings, not in a traditional laboratory setting. Users of Samsung Gear VR and Google Cardboard devices were invited by web pages and email invitation to download and run an app that presented a scenario where the participant would sit in a bar watching a singer. Each participant saw one of eight variations of the scenario: with or without a self-avatar; singer inviting the participant to tap along or not; singer looking at the participant or not. Despite the uncontrolled situation of the experiment, results from an in-app questionnaire showed tentative evidence that a self-avatar had a positive effect on self-report of presence and embodiment, and that the singer inviting the participant to tap along had a negative effect on self-report of embodiment. We discuss the limitations of the study and the platforms, and the potential for future open virtual reality experiments.},
	address = {Los Alamitos, CA, USA},
	author = {A. Steed and S. Frlston and M. Lopez and J. Drummond and Y. Pan and D. Swapp},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518135},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;data collection;rubber;gears;google;guidelines;ethics},
	month = {apr},
	number = {04},
	pages = {1406-1414},
	publisher = {IEEE Computer Society},
	title = {An `In the Wild' Experiment on Presence and Embodiment using Consumer Virtual Reality Equipment},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518135}}

@article{6479185,
	abstract = {This paper explores body ownership and control of an &#x27;extended&#x27; humanoid avatar that features a distinct and flexible tail-like appendage protruding from its coccyx. Thirty-two participants took part in a between-groups study to puppeteer the avatar in an immersive CAVETM -like system. Participantsa&#x27; body movement was tracked, and the avatara&#x27;s humanoid body synchronously reflected this motion. However, sixteen participants experienced the avatara&#x27;s tail moving around randomly and asynchronous to their own movement, while the other participants experienced a tail that they could, potentially, control accurately and synchronously through hip movement. Participants in the synchronous condition experienced a higher degree of body ownership and agency, suggesting that visuomotor synchrony enhanced the probability of ownership over the avatar body despite of its extra-human form. Participants experiencing body ownership were also more likely to be more anxious and attempt to avoid virtual threats to the tail and body. The higher task performance of participants in the synchronous condition indicates that people are able to quickly learn how to remap normal degrees of bodily freedom in order to control virtual bodies that differ from the humanoid form. We discuss the implications and applications of extended humanoid avatars as a method for exploring the plasticity of the braina&#x27;s representation of the body and for gestural human-computer interfaces.},
	address = {Los Alamitos, CA, USA},
	author = {W. Steptoe and A. Steed and M. Slater},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.32},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;games;tracking;hip;visualization;educational institutions;joints},
	month = {apr},
	number = {04},
	pages = {583-590},
	publisher = {IEEE Computer Society},
	title = {Human Tails: Ownership and Control of Extended Humanoid Avatars},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.32}}

@article{8642440,
	abstract = {Modal sound synthesis has been used to create realistic sounds from rigid-body objects, but requires accurate real-world material parameters. These material parameters can be estimated from recorded sounds of an impacted object, but external factors can interfere with accurate parameter estimation. We present a novel technique for estimating the damping parameters of materials from recorded impact sounds that probabilistically models these external factors. We represent the combined effects of material damping, support damping, and sampling inaccuracies with a probabilistic generative model, then use maximum likelihood estimation to fit a damping model to recorded data. This technique greatly reduces the human effort needed and does not require the precise object geometry or the exact hit location. We validate the effectiveness of this technique with a comprehensive analysis of a synthetic dataset and a perceptual study on object identification. We also present a study establishing human performance on the same parameter estimation task for comparison.},
	address = {Los Alamitos, CA, USA},
	author = {A. Sterling and N. Rewkowski and R. L. Klatzky and M. C. Lin},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898822},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {damping;vibrations;analytical models;geometry;probabilistic logic;real-time systems;parameter estimation},
	month = {may},
	number = {05},
	pages = {1855-1864},
	publisher = {IEEE Computer Society},
	title = {Audio-Material Reconstruction for Virtualized Reality Using a Probabilistic Damping Model},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898822}}

@article{6777452,
	abstract = {Virtual reality (VR) has been successfully applied to a broad range of training domains; however, to date there is little research investigating its benefits for sport psychology training. We hypothesized that using high-fidelity VR systems to display realistic 3D sport environments could trigger anxiety, allowing resilience-training systems to prepare athletes for real-world, high-pressure situations. In this work we investigated the feasibility and usefulness of using VR for sport psychology training. We developed a virtual soccer goalkeeping application for the Virginia Tech Visionarium VisCube (a CAVE-like display system), in which users defend against simulated penalty kicks using their own bodies. Using the application, we ran a controlled, within-subjects experiment with three independent variables: known anxiety triggers, field of regard, and simulation fidelity. The results demonstrate that a VR sport-oriented system can induce increased anxiety (physiological and subjective measures) compared to a baseline condition. There were a number of main effects and interaction effects for all three independent variables in terms of the subjective measures of anxiety. Both known anxiety triggers and simulation fidelity had a direct relationship to anxiety, while field of regard had an inverse relationship. Overall, the results demonstrate great potential for VR sport psychology training systems; however, further research is needed to determine if training in a VR environment can lead to long-term reduction in sport-induced anxiety.},
	address = {Los Alamitos, CA, USA},
	author = {C. Stinson and D. A. Bowman},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.23},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {training;psychology;solid modeling;heart rate variability;animation;three-dimensional displays},
	month = {apr},
	number = {04},
	pages = {606-615},
	publisher = {IEEE Computer Society},
	title = {Feasibility of Training Athletes for High-Pressure Situations Using Virtual Reality},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.23}}

@article{8643537,
	abstract = {Real-time 3D scene reconstruction from RGB-D sensor data, as well as the exploration of such data in VR/AR settings, has seen tremendous progress in recent years. The combination of both these components into telepresence systems, however, comes with significant technical challenges. All approaches proposed so far are extremely demanding on input and output devices, compute resources and transmission bandwidth, and they do not reach the level of immediacy required for applications such as remote collaboration. Here, we introduce what we believe is the first practical client-server system for real-time capture and many-user exploration of static 3D scenes. Our system is based on the observation that interactive frame rates are sufficient for capturing and reconstruction, and real-time performance is only required on the client site to achieve lag-free view updates when rendering the 3D model. Starting from this insight, we extend previous voxel block hashing frameworks by introducing a novel thread-safe GPU hash map data structure that is robust under massively concurrent retrieval, insertion and removal of entries on a thread level. We further propose a novel transmission scheme for volume data that is specifically targeted to Marching Cubes geometry reconstruction and enables a 90% reduction in bandwidth between server and exploration clients. The resulting system poses very moderate requirements on network bandwidth, latency and client-side computation, which enables it to rely entirely on consumer-grade hardware, including mobile devices. We demonstrate that our technique achieves state-of-the-art representation accuracy while providing, for any number of clients, an immersive and fluid lag-free viewing experience even during network outages.},
	address = {Los Alamitos, CA, USA},
	author = {P. Stotko and S. Krumpen and M. B. Hullin and M. Weinmann and R. Klein},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899231},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;real-time systems;telepresence;collaboration;bandwidth;servers;hardware},
	month = {may},
	number = {05},
	pages = {2102-2112},
	publisher = {IEEE Computer Society},
	title = {SLAMCast: Large-Scale, Real-Time 3D Reconstruction and Streaming for Immersive Multi-Client Live Telepresence},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899231}}

@article{8998570,
	abstract = {Redirected Walking (RDW) steering algorithms have traditionally relied on human-engineered logic. However, recent advances in reinforcement learning (RL) have produced systems that surpass human performance on a variety of control tasks. This paper investigates the potential of using RL to develop a novel reactive steering algorithm for RDW. Our approach uses RL to train a deep neural network that directly prescribes the rotation, translation, and curvature gains to transform a virtual environment given a user&#x27;s position and orientation in the tracked space. We compare our learned algorithm to steer-to-center using simulated and real paths. We found that our algorithm outperforms steer-to-center on simulated paths, and found no significant difference on distance traveled on real paths. We demonstrate that when modeled as a continuous control problem, RDW is a suitable domain for RL, and moving forward, our general framework provides a promising path towards an optimal RDW steering algorithm.},
	address = {Los Alamitos, CA, USA},
	author = {R. R. Strauss and R. Ramanujan and A. Becker and T. C. Peck},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973060},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {legged locomotion;learning (artificial intelligence);prediction algorithms;meters;tracking;heuristic algorithms;space exploration},
	month = {may},
	number = {05},
	pages = {1955-1963},
	publisher = {IEEE Computer Society},
	title = {A Steering Algorithm for Redirected Walking Using Reinforcement Learning},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973060}}

@article{7383338,
	abstract = {We propose a novel inter-reflection compensation technique for immersive projection displays wherein we spatially modulate the reflectance pattern on the screen to improve the compensation performance of conventional methods. As the luminance of light reflected on a projection surface is mathematically represented as the multiplication of the illuminance of incident light and the surface reflectance, we can reduce undesirable intensity elevation because of inter-reflections by decreasing surface reflectance. Based on this principle, we improve conventional inter-reflection compensation techniques by applying reflectance pattern modulation. We realize spatial reflectance modulation of a projection screen by painting it with a photochromic compound, which changes its color (i.e., the reflectance of the screen) when ultraviolet (UV) light is applied and by controlling UV irradiation with a UV LED array placed behind the screen. The main contribution of this paper is a computational model to optimize a reflectance pattern for the accurate reproduction of a target appearance by decreasing the intensity elevation caused by inter-reflection while maintaining the maximum intensity of the target appearance. Through simulation and physical experiments, we demonstrate the feasibility of the proposed model and confirm its advantage over conventional methods.},
	address = {Los Alamitos, CA, USA},
	author = {S. Takeda and D. Iwai and K. Sato},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518136},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {modulation;light emitting diodes;image color analysis;computational modeling;shape;light sources;cameras},
	month = {apr},
	number = {04},
	pages = {1424-1431},
	publisher = {IEEE Computer Society},
	title = {Inter-reflection Compensation of Immersive Projection Display by Spatio-Temporal Screen Reflectance Modulation},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518136}}

@article{7010977,
	abstract = {Interactive dexterous manipulation of virtual objects remains a complex challenge that requires both appropriate hand models and accurate physically-based simulation of interactions. In this paper, we propose an approach based on novel aggregate constraints for simulating dexterous grasping using soft fingers. Our approach aims at improving the computation of contact mechanics when many contact points are involved, by aggregating the multiple contact constraints into a minimal set of constraints. We also introduce a method for non-uniform pressure distribution over the contact surface, to adapt the response when touching sharp edges. We use the Coulomb-Contensou friction model to efficiently simulate tangential and torsional friction. We show through different use cases that our aggregate constraint formulation is well-suited for simulating interactively dexterous manipulation of virtual objects through soft fingers, and efficiently reduces the computation time of constraint solving.},
	address = {Los Alamitos, CA, USA},
	author = {A. Talvas and M. Marchal and C. Duriez and M. A. Otaduy},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391863},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {friction;aggregates;grasping;computational modeling;solid modeling;force;deformable models},
	month = {apr},
	number = {04},
	pages = {452-461},
	publisher = {IEEE Computer Society},
	title = {Aggregate Constraints for Virtual Manipulation with Soft Fingers},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391863}}

@article{8998301,
	abstract = {We present a new method to capture the acoustic characteristics of real-world rooms using commodity devices, and use the captured characteristics to generate similar sounding sources with virtual models. Given the captured audio and an approximate geometric model of a real-world room, we present a novel learning-based method to estimate its acoustic material properties. Our approach is based on deep neural networks that estimate the reverberation time and equalization of the room from recorded audio. These estimates are used to compute material properties related to room reverberation using a novel material optimization objective. We use the estimated acoustic material characteristics for audio rendering using interactive geometric sound propagation and highlight the performance on many real-world scenarios. We also perform a user study to evaluate the perceptual similarity between the recorded sounds and our rendered audio.},
	address = {Los Alamitos, CA, USA},
	author = {Z. Tang and N. J. Bryan and D. Li and T. R. Langlois and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973058},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {acoustics;optimization;rendering (computer graphics);visualization;acoustic materials;frequency estimation;estimation},
	month = {may},
	number = {05},
	pages = {1991-2001},
	publisher = {IEEE Computer Society},
	title = {Scene-Aware Audio Rendering via Deep Acoustic Analysis},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973058}}

@article{6479206,
	abstract = {In this paper we introduce novel &#x27;Camera Motions&#x27; (CMs) to improve the sensations related to locomotion in virtual environments (VE). Traditional Camera Motions are artificial oscillating motions applied to the subjective viewpoint when walking in the VE, and they are meant to evoke and reproduce the visual flow generated during a human walk. Our novel camera motions are: (1) multistate, (2) personified, and (3) they can take into account the topography of the virtual terrain. Being multistate, our CMs can account for different states of locomotion in VE namely: walking, but also running and sprinting. Being personified, our CMs can be adapted to avatars physiology such as to its size, weight or training status. They can then take into account avatars fatigue and recuperation for updating visual CMs accordingly. Last, our approach is adapted to the topography of the VE. Running over a strong positive slope would rapidly decrease the advance speed of the avatar, increase its energy loss, and eventually change the locomotion mode, influencing the visual feedback of the camera motions. Our new approach relies on a locomotion simulator partially inspired by human physiology and implemented for a real-time use in Desktop VR. We have conducted a series of experiments to evaluate the perception of our new CMs by naive participants. Results notably show that participants could discriminate and perceive transitions between the different locomotion modes, by relying exclusively on our CMs. They could also perceive some properties of the avatar being used and, overall, very well appreciated the new CMs techniques. Taken together, our results suggest that our new CMs could be introduced in Desktop VR applications involving first-person navigation, in order to enhance sensations of walking, running, and sprinting, with potentially different avatars and over uneven terrains, such as for: training, virtual visits or video games.},
	address = {Los Alamitos, CA, USA},
	author = {L. Terziman and M. Marchal and F. Multon and B. Arnaldi and A. Lecuyer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2013.38},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cameras;navigation;visualization;vibrations;mathematical model;legged locomotion;oscillators},
	month = {apr},
	number = {04},
	pages = {652-661},
	publisher = {IEEE Computer Society},
	title = {Personified and Multistate Camera Motions for First-Person Navigation in Desktop Virtual Reality},
	volume = {19},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2013.38}}

@article{10049700,
	abstract = {In this paper, we explore how virtual replicas can enhance Mixed Reality (MR) remote collaboration with a 3D reconstruction of the task space. People in different locations may need to work together remotely on complicated tasks. For example, a local user could follow a remote expert&#x27;s instructions to complete a physical task. However, it could be challenging for the local user to fully understand the remote expert&#x27;s intentions without effective spatial referencing and action demonstration. In this research, we investigate how virtual replicas can work as a spatial communication cue to improve MR remote collaboration. This approach segments the foreground manipulable objects in the local environment and creates corresponding virtual replicas of physical task objects. The remote user can then manipulate these virtual replicas to explain the task and guide their partner. This enables the local user to rapidly and accurately understand the remote expert&#x27;s intentions and instructions. Our user study with an object assembly task found that using virtual replica manipulation was more efficient than using 3D annotation drawing in an MR remote collaboration scenario. We report and discuss the findings and limitations of our system and study, and present directions for future research.},
	address = {Los Alamitos, CA, USA},
	author = {H. Tian and G. A. Lee and H. Bai and M. Billinghurst},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247113},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;collaboration;task analysis;cameras;solid modeling;annotations;resists},
	month = {may},
	number = {05},
	pages = {2785-2795},
	publisher = {IEEE Computer Society},
	title = {Using Virtual Replicas to Improve Mixed Reality Remote Collaboration},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247113}}

@article{Tiesel2011:Single-Pass-Composable-3D-Lens,
	abstract = {We present a new 3D lens rendering technique and a new spatiotemporal lens. Interactive 3D lenses, often called volumetric lenses, provide users with alternative views of data sets within 3D lens boundaries while maintaining the surrounding overview (context). In contrast to previous multipass rendering work, we discuss the strengths, limitations, and performance costs of a single-pass technique especially suited to fragment-level lens effects, such as color mapping, lighting, and clipping. Some object-level effects, such as a data set selection lens, are also incorporated, with each object&#x27;s geometry being processed once by the graphics pipeline. For a substantial range of effects, our approach supports several composable lenses at interactive frame rates without performance loss during increasing lens intersections or manipulation by a user. Other cases, for which this performance cannot be achieved, are also discussed. We illustrate possible applications of our lens system, including Time Warp lenses for exploring time-varying data sets.},
	address = {Los Alamitos, CA, USA},
	author = {J. Tiesel and E. Habib and C. W. Borst and K. Das},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2011.38},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {interaction styles;virtual reality;volumetric lens.},
	month = {sep},
	number = {09},
	pages = {1259-1272},
	publisher = {IEEE Computer Society},
	title = {Single-Pass Composable 3D Lens Rendering and Spatiotemporal 3D Lenses},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2011.38}}

@article{8998378,
	abstract = {This paper presents a novel active marker for dynamic projection mapping (PM) that emits a temporal blinking pattern of infrared (IR) light representing its ID. We used a multi-material three dimensional (3D) printer to fabricate a projection object with optical fibers that can guide IR light from LEDs attached on the bottom of the object. The aperture of an optical fiber is typically very small; thus, it is unnoticeable to human observers under projection and can be placed on a strongly curved part of a projection surface. In addition, the working range of our system can be larger than previous marker-based methods as the blinking patterns can theoretically be recognized by a camera placed at a wide range of distances from markers. We propose an automatic marker placement algorithm to spread multiple active markers over the surface of a projection object such that its pose can be robustly estimated using captured images from arbitrary directions. We also propose an optimization framework for determining the routes of the optical fibers in such a way that collisions of the fibers can be avoided while minimizing the loss of light intensity in the fibers. Through experiments conducted using three fabricated objects containing strongly curved surfaces, we confirmed that the proposed method can achieve accurate dynamic PMs in a significantly wide working range.},
	address = {Los Alamitos, CA, USA},
	author = {D. Tone and D. Iwai and S. Hiura and K. Sato},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973444},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {optical imaging;cameras;optical device fabrication;robustness;three-dimensional displays;observers;printers},
	month = {may},
	number = {05},
	pages = {2030-2040},
	publisher = {IEEE Computer Society},
	title = {FibAR: Embedding Optical Fibers in 3D Printed Objects for Active Markers in Dynamic Projection Mapping},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973444}}

@article{9714041,
	abstract = {Many haptic feedback methods have been proposed to enhance realism in virtual reality (VR). However, friction on the feet in VR, which renders feedback as if walking on different terrains or ground textures or stepping on objects is still less explored. Herein, we propose a wearable device, FrictShoes a pair of foot accessories, to provide multilevel nonuniform friction feedback to feet. This is achieved by the independent functioning of six brakes on six wheels underneath each FrictShoe, which allows the friction levels of the wheels from each to be either matched or to vary. We conducted a magnitude estimation study to understand users&#x27; distinguishability of friction force magnitudes (or levels). Based on the results, we performed an exploratory study to realize how users adjust and map the multilevel nonuniform friction patterns to common VR terrains or ground textures. Finally, a VR experience study was conducted to evaluate the performance of the proposed multilevel nonuniform friction feedback to the feet in VR experiences.},
	address = {Los Alamitos, CA, USA},
	author = {C. Tsao and T. Wu and H. Tsai and T. Wei and F. Liao and S. Chapman and B. Chen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150492},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {friction;foot;wheels;force;haptic interfaces;legged locomotion;force feedback},
	month = {may},
	number = {05},
	pages = {2026-2036},
	publisher = {IEEE Computer Society},
	title = {FrictShoes: Providing Multilevel Nonuniform Friction Feedback on Shoes in VR},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150492}}

@article{8999805,
	abstract = {Aiming at realizing novel vision augmentation experiences, this paper proposes the IlluminatedFocus technique, which spatially defocuses real-world appearances regardless of the distance from the user&#x27;s eyes to observed real objects. With the proposed technique, a part of a real object in an image appears blurred, while the fine details of the other part at the same distance remain visible. We apply Electrically Focus-Tunable Lenses (ETL) as eyeglasses and a synchronized high-speed projector as illumination for a real scene. We periodically modulate the focal lengths of the glasses (focal sweep) at more than 60 Hz so that a wearer cannot perceive the modulation. A part of the scene to appear focused is illuminated by the projector when it is in focus of the user&#x27;s eyes, while another part to appear blurred is illuminated when it is out of the focus. As the basis of our spatial focus control, we build mathematical models to predict the range of distance from the ETL within which real objects become blurred on the retina of a user. Based on the blur range, we discuss a design guideline for effective illumination timing and focal sweep range. We also model the apparent size of a real scene altered by the focal length modulation. This leads to an undesirable visible seam between focused and blurred areas. We solve this unique problem by gradually blending the two areas. Finally, we demonstrate the feasibility of our proposal by implementing various vision augmentation applications.},
	address = {Los Alamitos, CA, USA},
	author = {T. Ueda and D. Iwai and T. Hiraki and K. Sato},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973496},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {lenses;visualization;modulation;optical imaging;high-speed optical techniques;lighting;mathematical model},
	month = {may},
	number = {05},
	pages = {2051-2061},
	publisher = {IEEE Computer Society},
	title = {Illuminated Focus: Vision Augmentation using Spatial Defocusing via Focal Sweep Eyeglasses and High-Speed Projector},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973496}}

@article{8651471,
	abstract = {Emergency situations during car driving sometimes force the driver to make a sudden decision. Predicting these decisions will have important applications in updating risk analyses in insurance applications, but also can give insights for drafting autonomous vehicle guidelines. Studying such behavior in experimental settings, however, is limited by ethical issues as it would endanger peoples&#x27; lives. Here, we employed the potential of virtual reality (VR) to investigate decision-making in an extreme situation in which participants would have to sacrifice others in order to save themselves. In a VR driving simulation, participants first trained to complete a difficult course with multiple crossroads in which the wrong turn would lead the car to fall down a cliff. In the testing phase, obstacles suddenly appeared on the ``safe'' turn of a crossroad: for the control group, obstacles consisted of trees, whereas for the experimental group, they were pedestrians. In both groups, drivers had to decide between falling down the cliff or colliding with the obstacles. Results showed that differences in personality traits were able to predict this decision: in the experimental group, drivers who collided with the pedestrians had significantly higher psychopathy and impulsivity traits, whereas impulsivity alone was to some degree predictive in the control group. Other factors like heart rate differences, gender, video game expertise, and driving experience were not predictive of the emergency decision in either group. Our results show that self-interest related personality traits affect decision-making when choosing between preservation of self or others in extreme situations and showcase the potential of virtual reality in studying and modeling human decision-making.},
	address = {Los Alamitos, CA, USA},
	author = {J. Uijong and J. Kang and C. Wallraven},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2899227},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {accidents;automobiles;decision making;training;autonomous vehicles;virtual reality},
	month = {may},
	number = {05},
	pages = {1898-1907},
	publisher = {IEEE Computer Society},
	title = {You or Me? Personality Traits Predict Sacrificial Decisions in an Accident Situation},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2899227}}

@article{8642446,
	abstract = {Playing back vibrotactile signals through actuators is commonly used to simulate tactile feelings of virtual textured surfaces. However, there is often a small mismatch between the simulated tactile feelings and intended tactile feelings by tactile designers. Thus, a method of modulating the vibrotactile perception is required. We focus on fine roughness perception and we propose a method using a pseudo-haptic effect to modulate fine roughness perception of vibrotactile texture. Specifically, we visually modify the pointer&#x27;s position on the screen slightly, which indicates the touch position on textured surfaces. We hypothesized that if users receive vibrational feedback watching the pointer visually oscillating back/forth and left/right, users would believe the vibrotactile surfaces more uneven. We also hypothesized that as the size of visual oscillation is getting larger, the amount of modification of roughness perception of vibrotactile surfaces would be larger. We conducted user studies to test the hypotheses. Results of first user study suggested that users felt vibrotactile texture with our method rougher than they did without our method at a high probability. Results of second user study suggested that users felt different roughness for vibrational texture in response to the size of visual oscillation. These results confirmed our hypotheses and they suggested that our method was effective. Also, the same effect could potentially be applied to the visual movement of virtual hands or fingertips when users are interacting with virtual surfaces using their hands.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Ujitoko and Y. Ban and K. Hirota},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898820},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rough surfaces;surface roughness;visualization;surface texture;haptic interfaces;vibrations;modulation},
	month = {may},
	number = {05},
	pages = {1981-1990},
	publisher = {IEEE Computer Society},
	title = {Modulating Fine Roughness Perception of Vibrotactile Textured Surface using Pseudo-haptic Effect},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898820}}

@article{Ullrich2012:Haptic-Palpation-for-Medical,
	abstract = {Palpation is a physical examination technique where objects, e.g., organs or body parts, are touched with fingers to determine their size, shape, consistency and location. Many medical procedures utilize palpation as a supplementary interaction technique and it can be therefore considered as an essential basic method. However, palpation is mostly neglected in medical training simulators, with the exception of very specialized simulators that solely focus on palpation, e.g., for manual cancer detection. In this article we propose a novel approach to enable haptic palpation interaction for virtual reality-based medical simulators. The main contribution is an extensive user study conducted with a large group of medical experts. To provide a plausible simulation framework for this user study, we contribute a novel and detailed interaction algorithm for palpation with tissue dragging, which utilizes a multi-object force algorithm to support multiple layers of anatomy and a pulse force algorithm for simulation of an arterial pulse. Furthermore, we propose a modification for an off-the-shelf haptic device by adding a lightweight palpation pad to support a more realistic finger grip configuration for palpation tasks. The user study itself has been conducted on a medical training simulator prototype with a specific procedure from regional anesthesia, which strongly depends on palpation. The prototype utilizes a co-rotational finite-element approach for soft tissue simulation and provides bimanual interaction by combining the aforementioned techniques with needle insertion for the other hand. The results of the user study suggest reasonable face validity of the simulator prototype and in particular validate medical plausibility of the proposed palpation interaction algorithm.},
	address = {Los Alamitos, CA, USA},
	author = {S. Ullrich and T. Kuhlen},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.46},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;computer based training;finite element analysis;haptic interfaces;medical computing;user interfaces;needle insertion;haptic palpation;medical simulation;virtual environment;palpation examination technique;medical procedure;supplementary interaction technique;medical training simulator;manual cancer detection;virtual reality-based medical simulator;user study;tissue dragging;multiobject force algorithm;pulse force algorithm;arterial pulse simulation;anatomy layer support;haptic device;finger grip configuration;regional anesthesia;corotational finite-element approach;soft tissue simulation;bimanual interaction;haptic interfaces;force;skin;rendering (computer graphics);bismuth;visualization;phantoms;user studies.;medicine;physically-based simulation;haptics},
	month = {apr},
	number = {04},
	pages = {617-625},
	publisher = {IEEE Computer Society},
	title = {Haptic Palpation for Medical Simulation in Virtual Environments},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.46}}

@article{10049718,
	abstract = {This article explores the effect of one&#x27;s body representation on time perception. Time Perception is modulated by a variety of factors including, e.g., the current situation or activity, it can display significant disturbances caused by psychological disorders, and it is influenced by emotional and interoceptive states, i.e., ``the sense of the physiological condition of the body''. We investigated this relation between one&#x27;s own body and the perception of time in a novel Virtual Reality (VR) experiment explicitly fostering user activity. Forty-Eight participants randomly experienced different degrees of embodiment: i) without an avatar (low), ii) with hands (medium), and iii) with a high-quality avatar (high). Participants had to repeatedly activate a virtual lamp and estimate the duration of time intervals as well as judge the passage of time. Our results show a significant effect of embodiment on time perception: time passes slower in the low embodiment condition compared to the medium and high conditions. In contrast to prior work, the study provides missing evidence that this effect is independent of the level of activity of participants: In our task, users were prompted to repeatedly perform body actions, thereby ruling-out a potential influence of the level of activity. Importantly, duration judgements in both the millisecond and minute ranges seemed unaffected by variations in embodiment. Taken together, these results lead to a better understanding of the relationship between the body and time.},
	address = {Los Alamitos, CA, USA},
	author = {F. Unruh and D. Vogel and M. Landeck and J. Lugrin and M. Latoschik},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247040},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;psychology;task analysis;video games;pacemakers;mirrors;human computer interaction},
	month = {may},
	number = {05},
	pages = {2626-2636},
	publisher = {IEEE Computer Society},
	title = {Body and Time: Virtual Embodiment and its Effect on Time Perception},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247040}}

@article{6165137,
	abstract = {In this paper, we explore techniques that aim to improve site understanding for outdoor Augmented Reality (AR) applications. While the first person perspective in AR is a direct way of filtering and zooming on a portion of the data set, it severely narrows overview of the situation, particularly over large areas. We present two interactive techniques to overcome this problem: multi-view AR and variable perspective view. We describe in details the conceptual, visualization and interaction aspects of these techniques and their evaluation through a comparative user study. The results we have obtained strengthen the validity of our approach and the applicability of our methods to a large range of application domains.},
	address = {Los Alamitos, CA, USA},
	author = {E. Veas and R. Grasset and E. Kruijff and D. Schmalstieg},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.44},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {cameras;data visualization;three dimensional displays;mobile communication;navigation;solid modeling;context},
	month = {apr},
	number = {04},
	pages = {565-572},
	publisher = {IEEE Computer Society},
	title = {Extended Overview Techniques for Outdoor Augmented Reality},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.44}}

@article{10049656,
	abstract = {User representations are critical to the virtual experience, and involve both the input device used to support interactions as well as how the user is virtually represented in the scene. Inspired by previous work that has shown effects of user representations on the perceptions of relatively static affordances, we attempt to investigate how end-effector representations affect the perceptions of affordances that dynamically change over time. Towards this end, we empirically evaluated how different virtual hand representations affect users&#x27; perceptions of dynamic affordances in an object retrieval task wherein users were tasked with retrieving a target from a box for a number of trials while avoiding collisions with its moving doors. We employed a 3 (virtual end-effector representation) X 13 (frequency of moving doors) X 2 (target object size) multi-factorial design, manipulating the input modality and its concomitant virtual end-effector representation as a between-subjects factor across three experimental conditions: (1) Controller (using a controller represented as a virtual controller); (2) Controller-hand (using a controller represented as a virtual hand); (3) Glove (using a hand tracked hi-fidelity glove represented as a virtual hand). Results indicated that the controller-hand condition produced lower levels of performance than both the other conditions. Furthermore, users in this condition exhibited a diminished ability to calibrate their performance over trials. Overall, we find that representing the end-effector as a hand tends to increase embodiment but can also come at the cost of performance, or an increased workload due to a discordant mapping between the virtual representation and the input modality used. It follows that VR system designers should carefully consider the priorities and target requirements of the application being developed when choosing the type of end-effector representation for users to embody in immersive virtual experiences.},
	address = {Los Alamitos, CA, USA},
	author = {R. Venkatakrishnan and R. Venkatakrishnan and B. Raveendranath and C. C. Pagano and A. C. Robb and W. Lin and S. V. Babu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247041},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;end effectors;affordances;tracking;visualization;grasping;cameras},
	month = {may},
	number = {05},
	pages = {2258-2268},
	publisher = {IEEE Computer Society},
	title = {How Virtual Hand Representations Affect the Perceptions of Dynamic Affordances in Virtual Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247041}}

@article{10049690,
	abstract = {Inspired by previous works showing promise for AR self-avatarization - providing users with an augmented self avatar, we investigated whether avatarizing users&#x27; end-effectors (hands) improved their interaction performance on a near-field, obstacle avoidance, object retrieval task wherein users were tasked with retrieving a target object from a field of non-target obstacles for a number of trials. We employed a 3 (Augmented hand representation) X 2 (density of obstacles) X 2 (size of obstacles) X 2 (virtual light intensity) multi-factorial design, manipulating the presence/absence and anthropomorphic fidelity of augmented self-avatars overlaid on the user&#x27;s real hands, as a between subjects factor across three experimental conditions: (1) No-Augmented Avatar (using only real hands); (2) Iconic-Augmented Avatar; (3) Realistic Augmented Avatar. Results indicated that self-avatarization improved interaction performance and was perceived as more usable regardless of the anthropomorphic fidelity of avatar. We also found that the virtual light intensity used in illuminating holograms affects how visible one&#x27;s real hands are. Overall, our findings seem to indicate that interaction performance may improve when users are provided with a visual representation of the AR system&#x27;s interacting layer in the form of an augmented self-avatar.},
	address = {Los Alamitos, CA, USA},
	author = {R. Venkatakrishnan and R. Venkatakrishnan and B. Raveendranath and C. C. Pagano and A. C. Robb and W. Lin and S. V. Babu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247105},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;avatars;three-dimensional displays;end effectors;visualization;hardware;cameras},
	month = {may},
	number = {05},
	pages = {2412-2422},
	publisher = {IEEE Computer Society},
	title = {Give Me a Hand: Improving the Effectiveness of Near-field Augmented Reality Interactions By Avatarizing Users&#x27; End Effectors},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247105}}

@article{6777443,
	abstract = {We propose the combination of a keyframe-based monocular SLAM system and a global localization method. The SLAM system runs locally on a camera-equipped mobile client and provides continuous, relative 6DoF pose estimation as well as keyframe images with computed camera locations. As the local map expands, a server process localizes the keyframes with a pre-made, globally-registered map and returns the global registration correction to the mobile client. The localization result is updated each time a keyframe is added, and observations of global anchor points are added to the client-side bundle adjustment process to further refine the SLAM map registration and limit drift. The end result is a 6DoF tracking and mapping system which provides globally registered tracking in real-time on a mobile device, overcomes the difficulties of localization with a narrow field-of-view mobile phone camera, and is not limited to tracking only in areas covered by the offline reconstruction.},
	address = {Los Alamitos, CA, USA},
	author = {J. Ventura and C. Arth and G. Reitmayr and D. Schmalstieg},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.27},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {simultaneous localization and mapping;cameras;servers;real-time systems;global positioning system;feature extraction;mobile handsets},
	month = {apr},
	number = {04},
	pages = {531-539},
	publisher = {IEEE Computer Society},
	title = {Global Localization from Monocular SLAM on a Mobile Phone},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.27}}

@article{10053631,
	abstract = {360$\,^{\circ}$ videos provide an immersive experience, especially when watched in virtual reality (VR). Yet, even though the video data is inherently three-dimensional, interfaces to access datasets of such videos in VR almost always use two-dimensional thumbnails shown in a grid on a flat or curved plane. We claim that using spherical and cube-shaped 3D thumbnails may provide a better user experience and be more effective at conveying the high-level subject matter of a video or when searching for a specific item in it. A comparative study against the most used existing representation, that is, 2D equirectangular projections, showed that the spherical 3D thumbnails did indeed provide the best user experience, whereas traditional 2D equirectangular projections still performed better for high-level classification tasks. Yet, they were outperformed by spherical thumbnails when participants had to search for details within the videos. Our results thus confirm a potential benefit of 3D thumbnail representations for 360-degree videos in VR, especially with respect to user experience and detailed content search and suggest a mixed interface design providing both options to the users. Supplemental materials about the user study and used data are available at https://osf.io/5vk49/.},
	address = {Los Alamitos, CA, USA},
	author = {A. Vermast and W. Hurst},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247462},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {videos;three-dimensional displays;navigation;distortion;visualization;user experience;task analysis},
	month = {may},
	number = {05},
	pages = {2547-2556},
	publisher = {IEEE Computer Society},
	title = {Introducing 3D Thumbnails to Access 360-Degree Videos in Virtual Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247462}}

@article{7014383,
	abstract = {This paper proposes a fast, physically accurate method for synthesizing multimodal, acoustic and haptic, signatures of distributed fracture in quasi-brittle heterogeneous materials, such as wood, granular media, or other fiber composites. Fracture processes in these materials are challenging to simulate with existing methods, due to the prevalence of large numbers of disordered, quasi-random spatial degrees of freedom, representing the complex physical state of a sample over the geometric volume of interest. Here, I develop an algorithm for simulating such processes, building on a class of statistical lattice models of fracture that have been widely investigated in the physics literature. This algorithm is enabled through a recently published mathematical construction based on the inverse transform method of random number sampling. It yields a purely time domain stochastic jump process representing stress fluctuations in the medium. The latter can be readily extended by a mean field approximation that captures the averaged constitutive (stress-strain) behavior of the material. Numerical simulations and interactive examples demonstrate the ability of these algorithms to generate physically plausible acoustic and haptic signatures of fracture in complex, natural materials interactively at audio sampling rates.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Visell},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2015.2391865},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {load modeling;materials;stress;computational modeling;rendering (computer graphics);numerical models;mathematical model},
	month = {apr},
	number = {04},
	pages = {443-451},
	publisher = {IEEE Computer Society},
	title = {Fast Physically Accurate Rendering of Multimodal Signatures of Distributed Fracture in Heterogeneous Materials},
	volume = {21},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2015.2391865}}

@article{7383334,
	abstract = {Realistic versus stylized depictions of virtual humans in simulated inter-personal situations and their ability to elicit emotional responses in users has been an open question for artists and researchers alike. We empirically evaluated the effects of near visually realistic vs. non-realistic stylized appearance of virtual humans on the emotional response of participants in a medical virtual reality system that was designed to educate users in recognizing the signs and symptoms of patient deterioration. In a between-subjects experiment protocol, participants interacted with one of three different appearances of a virtual patient, namely visually realistic, cartoon-shaded and charcoal-sketch like conditions in a mixed reality simulation. Emotional impact were measured via a combination of quantitative objective measures were gathered using skin Electrodermal Activity (EDA) sensors, and quantitative subjective measures such as the Differential Emotion Survey (DES IV), Positive and Negative Affect Schedule (PANAS), and Social Presence questionnaire. The emotional states of the participants were analyzed across four distinct time steps during which the medical condition of the virtual patient deteriorated (an emotionally stressful interaction), and were contrasted to a baseline affective state. Objective EDA results showed that in all three conditions, male participants exhibited greater levels of arousal as compared to female participants. We found that negative affect levels were significantly lower in the visually realistic condition, as compared to the stylized appearance conditions. Furthermore, in emotional dimensions of interest-excitement, surprise, anger, fear and guilt participants in all conditions responded similarly. However, in social emotional constructs of shyness, presence, perceived personality, and enjoyment-joy, we found that participants responded differently in the visually realistic condition as compared to the cartoon and sketch conditions. Our study suggests that virtual human appearance can affect not only critical emotional reactions in affective inter-oersonal trainina scenarios. but also users&#x27; oerceotions of oersonalitv and social characteristic of the virtual interlocutors.},
	address = {Los Alamitos, CA, USA},
	author = {M. Volante and S. V. Babu and H. Chaturvedi and N. Newsome and E. Ebrahimi and T. Roy and S. B. Daily and T. Fasolino},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2016.2518158},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);visualization;electronic mail;solid modeling;training;animation;atmospheric measurements},
	month = {apr},
	number = {04},
	pages = {1326-1335},
	publisher = {IEEE Computer Society},
	title = {Effects of Virtual Human Appearance Fidelity on Emotion Contagion in Affective Inter-Personal Simulations},
	volume = {22},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2016.2518158}}

@article{9382884,
	abstract = {In this work, we evaluate two standard interaction techniques for Immersive Analytics environments: virtual hands, with actions such as grabbing and stretching, and virtual ray pointers, with actions assigned to controller buttons. We also consider a third option: seamlessly integrating both modes and allowing the user to alternate between them without explicit mode switches. Easy-to-use interaction with data visualizations in Virtual Reality enables analysts to intuitively query or filter the data, in addition to the benefit of multiple perspectives and stereoscopic 3D display. While many VR-based Immersive Analytics systems employ one of the studied interaction modes, the effect of this choice is unknown. Considering that each has different advantages, we compared the three conditions through a controlled user study in the spatio-temporal data domain. We did not find significant differences between hands and ray-casting in task performance, workload, or interactivity patterns. Yet, 60% of the participants preferred the mixed mode and benefited from it by choosing the best alternative for each low-level task. This mode significantly reduced completion times by 23% for the most demanding task, at the cost of a 5% decrease in overall success rates.},
	address = {Los Alamitos, CA, USA},
	author = {J. Wagner and W. Stuerzlinger and L. Nedel},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067759},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;three-dimensional displays;trajectory;data visualization;presses;navigation;mice},
	month = {may},
	number = {05},
	pages = {2513-2523},
	publisher = {IEEE Computer Society},
	title = {Comparing and Combining Virtual Hand and Virtual Ray Pointer Interactions for Data Manipulation in Immersive Analytics},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067759}}

@article{8263407,
	abstract = {This article reports the impact of the degree of personalization and individualization of users&#x27; avatars as well as the impact of the degree of immersion on typical psychophysical factors in embodied Virtual Environments. We investigated if and how virtual body ownership (including agency), presence, and emotional response are influenced depending on the specific look of users&#x27; avatars, which varied between (1) a generic hand-modeled version, (2) a generic scanned version, and (3) an individualized scanned version. The latter two were created using a state-of-the-art photogrammetry method providing a fast 3D-scan and post-process workflow. Users encountered their avatars in a virtual mirror metaphor using two VR setups that provided a varying degree of immersion, (a) a large screen surround projection (L-shape part of a CAVE) and (b) a head-mounted display (HMD). We found several significant as well as a number of notable effects. First, personalized avatars significantly increase body ownership, presence, and dominance compared to their generic counterparts, even if the latter were generated by the same photogrammetry process and hence could be valued as equal in terms of the degree of realism and graphical quality. Second, the degree of immersion significantly increases the body ownership, agency, as well as the feeling of presence. These results substantiate the value of personalized avatars resembling users&#x27; real-world appearances as well as the value of the deployed scanning process to generate avatars for VR-setups where the effect strength might be substantial, e.g., in social Virtual Reality (VR) or in medical VR-based therapies relying on embodied interfaces. Additionally, our results also strengthen the value of fully immersive setups which, today, are accessible for a variety of applications due to the widely available consumer HMDs.},
	address = {Los Alamitos, CA, USA},
	author = {T. Waltemate and D. Gall and D. Roth and M. Botsch and M. Latoschik},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794629},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;mirrors;rubber;three-dimensional displays;psychology;face},
	month = {apr},
	number = {04},
	pages = {1643-1652},
	publisher = {IEEE Computer Society},
	title = {The Impact of Avatar Personalization and Immersion on Virtual Body Ownership, Presence, and Emotional Response},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794629}}

@article{10049694,
	abstract = {The refresh rate of virtual reality (VR) head-mounted displays (HMDs) has been growing rapidly in recent years because of the demand to provide higher frame rate content as it is often linked with a better experience. Today&#x27;s HMDs come with different refresh rates ranging from 20Hz to 180Hz, which determines the actual maximum frame rate perceived by users&#x27; naked eyes. VR users and content developers often face a choice because having high frame rate content and the hardware that supports it comes with higher costs and other trade-offs (such as heavier and bulkier HMDs). Both VR users and developers can choose a suitable frame rate if they are aware of the benefits of different frame rates in user experience, performance, and simulator sickness (SS). To our knowledge, limited research on frame rate in VR HMDs is available. In this paper, we aim to fill this gap and report a study with two VR application scenarios that compared four of the most common and highest frame rates currently available (60, 90, 120, and 180 frames per second (fps)) to explore their effect on users&#x27; experience, performance, and SS symptoms. Our results show that 120fps is an important threshold for VR. After 120fps, users tend to feel lower SS symptoms without a significant negative effect on their experience. Higher frame rates (e.g., 120 and 180fps) can ensure better user performance than lower rates. Interestingly, we also found that at 60fps and when users are faced with fast-moving objects, they tend to adopt a strategy to compensate for the lack of visual details by predicting or filling the gaps to try to meet the performance needs. At higher fps, users do not need to follow this compensatory strategy to meet the fast response performance requirements.},
	address = {Los Alamitos, CA, USA},
	author = {J. Wang and R. Shi and W. Zheng and W. Xie and D. Kao and H. Liang},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247057},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {games;user experience;visualization;task analysis;monitoring;hardware;virtual reality},
	month = {may},
	number = {05},
	pages = {2478-2488},
	publisher = {IEEE Computer Society},
	title = {Effect of Frame Rate on User Experience, Performance, and Simulator Sickness in Virtual Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247057}}

@article{8642365,
	abstract = {Virtual Reality (VR) applications allow a user to explore a scene intuitively through a tracked head-mounted display (HMD). However, in complex scenes, occlusions make scene exploration inefficient, as the user has to navigate around occluders to gain line of sight to potential regions of interest. When a scene region proves to be of no interest, the user has to retrace their path, and such a sequential scene exploration implies significant amounts of wasted navigation. Furthermore, as the virtual world is typically much larger than the tracked physical space hosting the VR application, the intuitive one-to-one mapping between the virtual and real space has to be temporarily suspended for the user to teleport or redirect in order to conform to the physical space constraints. In this paper we introduce a method for improving VR exploration efficiency by automatically constructing a multiperspective visualization that removes occlusions. For each frame, the scene is first rendered conventionally, the z-buffer is analyzed to detect horizontal and vertical depth discontinuities, the discontinuities are used to define disocclusion portals which are 3D scene rectangles for routing rays around occluders, and the disocclusion portals are used to render a multiperpsective image that alleviates occlusions. The user controls the multiperspective disocclusion effect, deploying and retracting it with small head translations. We have quantified the VR exploration efficiency brought by our occlusion removal method in a study where participants searched for a stationary target, and chased a dynamic target. Our method showed an advantage over conventional VR exploration in terms of reducing the navigation distance, the view direction rotation, the number of redirections, and the task completion time. These advantages did not come at the cost of a reduction in depth perception or situational awareness, or of an increase in simulator sickness.},
	address = {Los Alamitos, CA, USA},
	author = {L. Wang and J. Wu and X. Yang and V. Popescu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898782},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;resists;portals;navigation;task analysis;teleportation;virtual environments},
	month = {may},
	number = {05},
	pages = {2083-2092},
	publisher = {IEEE Computer Society},
	title = {VR Exploration Assistance through Automatic Occlusion Removal},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898782}}

@article{10049669,
	abstract = {Augmented Reality (AR) and Virtual Reality (VR) are pushing from the labs towards consumers, especially with social applications. These applications require visual representations of humans and intelligent entities. However, displaying and animating photo-realistic models comes with a high technical cost while low-fidelity representations may evoke eeriness and overall could degrade an experience. Thus, it is important to carefully select what kind of avatar to display. This article investigates the effects of rendering style and visible body parts in AR and VR by adopting a systematic literature review. We analyzed 72 papers that compare various avatar representations. Our analysis includes an outline of the research published between 2015 and 2022 on the topic of avatars and agents in AR and VR displayed using head-mounted displays, covering aspects like visible body parts (e.g., hands only, hands and head, full-body) and rendering style (e.g., abstract, cartoon, realistic); an overview of collected objective and subjective measures (e.g., task performance, presence, user experience, body ownership); and a classification of tasks where avatars and agents were used into task domains (physical activity, hand interaction, communication, game-like scenarios, and education/training). We discuss and synthesize our results within the context of today&#x27;s AR and VR ecosystem, provide guidelines for practitioners, and finally identify and present promising research opportunities to encourage future research of avatars and agents in AR/VR environments.},
	address = {Los Alamitos, CA, USA},
	author = {F. Weidner and G. Boettcher and S. Arboleda and C. Diao and L. Sinani and C. Kunert and C. Gerhardt and W. Broll and A. Raake},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247072},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {avatars;rendering (computer graphics);task analysis;visualization;systematics;head-mounted displays;databases},
	month = {may},
	number = {05},
	pages = {2596-2606},
	publisher = {IEEE Computer Society},
	title = {A Systematic Review on the Visualization of Avatars and Agents in AR &amp; VR displayed using Head-Mounted Displays},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247072}}

@article{10049717,
	abstract = {Integrating taste in AR/VR applications has various promising use cases --- from social eating to the treatment of disorders. Despite many successful AR/VR applications that alter the taste of beverages and food, the relationship between olfaction, gustation, and vision during the process of multisensory integration (MSI) has not been fully explored yet. Thus, we present the results of a study in which participants were confronted with congruent and incongruent visual and olfactory stimuli while eating a tasteless food product in VR. We were interested (1) if participants integrate bi-modal congruent stimuli and (2) if vision guides MSI during congruent/incongruent conditions. Our results contain three main findings: First, and surprisingly, participants were not always able to detect congruent visual-olfactory stimuli when eating a portion of tasteless food. Second, when confronted with tri-modal incongruent cues, a majority of participants did not rely on any of the presented cues when forced to identify what they eat; this includes vision which has previously been shown to dominate MSI. Third, although research has shown that basic taste qualities like sweetness, saltiness, or sourness can be influenced by congruent cues, doing so with more complex flavors (e.g., zucchini or carrot) proved to be harder to achieve. We discuss our results in the context of multimodal integration, and within the domain of multisensory AR/VR. Our results are a necessary building block for future human-food interaction in XR that relies on smell, taste, and vision and are foundational for applied applications such as affective AR/VR.},
	address = {Los Alamitos, CA, USA},
	author = {F. Weidner and J. E. Maier and W. Broll},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247099},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {olfactory;image color analysis;visualization;multisensory integration;fans;containers;chemicals},
	month = {may},
	number = {05},
	pages = {2423-2433},
	publisher = {IEEE Computer Society},
	title = {Eating, Smelling, and Seeing: Investigating Multisensory Integration and (In)congruent Stimuli while Eating in VR},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247099}}

@article{8998307,
	abstract = {We analyzed the design space of group navigation tasks in distributed virtual environments and present a framework consisting of techniques to form groups, distribute responsibilities, navigate together, and eventually split up again. To improve joint navigation, our work focused on an extension of the Multi-Ray Jumping technique that allows adjusting the spatial formation of two distributed users as part of the target specification process. The results of a quantitative user study showed that these adjustments lead to significant improvements in joint two-user travel, which is evidenced by more efficient travel sequences and lower task loads imposed on the navigator and the passenger. In a qualitative expert review involving all four stages of group navigation, we confirmed the effective and efficient use of our technique in a more realistic use-case scenario and concluded that remote collaboration benefits from fluent transitions between individual and group navigation.},
	address = {Los Alamitos, CA, USA},
	author = {T. Weissker and P. Bimberg and B. Froehlich},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973474},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {navigation;virtual environments;collaboration;task analysis;teleportation;avatars},
	month = {may},
	number = {05},
	pages = {1860-1870},
	publisher = {IEEE Computer Society},
	title = {Getting There Together: Group Navigation in Distributed Virtual Environments},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973474}}

@article{9382870,
	abstract = {Group navigation can be an invaluable tool for performing guided tours in distributed virtual environments. Related work suggests that group navigation techniques should be comprehensible for both the guide and the attendees, assist the group in avoiding collisions with obstacles, and allow the creation of meaningful spatial arrangements with respect to objects of interest. To meet these requirements, we developed a group navigation technique based on short-distance teleportation (jumping) and evaluated its usability, comprehensibility, and scalability in an initial user study. After navigating with groups of up to 10 users through a virtual museum, participants indicated that our technique is easy to learn for guides, comprehensible also for attendees, non-nauseating for both roles, and therefore well-suited for performing guided tours.},
	address = {Los Alamitos, CA, USA},
	author = {T. Weissker and B. Froehlich},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067756},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {navigation;virtual environments;visualization;virtual reality;teleportation;legged locomotion;head-mounted displays},
	month = {may},
	number = {05},
	pages = {2524-2534},
	publisher = {IEEE Computer Society},
	title = {Group Navigation for Guided Tours in Distributed Virtual Environments},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067756}}

@article{10049698,
	abstract = {Most prior teleportation techniques in virtual reality are bound to target positions in the vicinity of selectable scene objects. In this paper, we present three adaptations of the classic teleportation metaphor that enable the user to travel to mid-air targets as well. Inspired by related work on the combination of teleports with virtual rotations, our three techniques differ in the extent to which elevation changes are integrated into the conventional target selection process. Elevation can be specified either simultaneously, as a connected second step, or separately from horizontal movements. A user study with 30 participants indicated a trade-off between the simultaneous method leading to the highest accuracy and the two-step method inducing the lowest task load as well as receiving the highest usability ratings. The separate method was least suitable on its own but could serve as a complement to one of the other approaches. Based on these findings and previous research, we define initial design guidelines for mid-air navigation techniques.},
	address = {Los Alamitos, CA, USA},
	author = {T. Weissker and P. Bimberg and A. Gokhale and T. Kuhlen and B. Froehlich},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247114},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {teleportation;navigation;avatars;visualization;task analysis;floors;virtual environments},
	month = {may},
	number = {05},
	pages = {2467-2477},
	publisher = {IEEE Computer Society},
	title = {Gaining the High Ground: Teleportation to Mid-Air Targets in Immersive Virtual Environments},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247114}}

@article{10049710,
	abstract = {Mixed Reality (MR) applications along Milgram&#x27;s Reality-Virtuality (RV) continuum motivated a number of recent theories on potential constructs and factors describing MR experiences. This paper investigates the impact of incongruencies that are processed on different information processing layers (i.e., sensation/perception and cognition layer) to provoke breaks in plausibility. It examines the effects on spatial and overall presence as prominent constructs of Virtual Reality (VR). We developed a simulated maintenance application to test virtual electrical devices. Participants performed test operations on these devices in a counterbalanced, randomized 22 between-subject design in either VR as congruent or Augmented Reality (AR) as incongruent on the sensation/perception layer. Cognitive incongruence was induced by the absence of traceable power outages, decoupling perceived cause and effect after activating potentially defective devices. Our results indicate that the effects of the power outages differ significantly in the perceived plausibility and spatial presence ratings between VR and AR. Both ratings decreased for the AR condition (incongruent sensation/perception) compared to VR (congruent sensation/perception) for the congruent cognitive case but increased for the incongruent cognitive case. The results are discussed and put into perspective in the scope of recent theories of MR experiences.},
	address = {Los Alamitos, CA, USA},
	author = {F. Westermeier and L. Brubach and M. Latoschik and C. Wienrich},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247046},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;transportation;power system reliability;x reality;virtual environments;solid modeling;psychology},
	month = {may},
	number = {05},
	pages = {2680-2689},
	publisher = {IEEE Computer Society},
	title = {Exploring Plausibility and Presence in Mixed Reality Experiences},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247046}}

@article{10049649,
	abstract = {Virtual Reality (VR) is well-known for its use in interdisciplinary applications and research. The visual representation of these applications could vary depending in their purpose and hardware limitation, and in those situations could require an accurate perception of size for task performance. However, the relationship between size perception and visual realism in VR has not yet been explored. In this contribution, we conducted an empirical evaluation using a between-subject design over four conditions of visual realism, namely Realistic, Local Lighting, Cartoon, and Sketch on size perception of target objects in the same virtual environment. Additionally, we gathered participants&#x27; size estimates in the real world via a within-subject session. We measured size perception using concurrent verbal reports and physical judgments. Our result showed that although participants&#x27; size perception was accurate in the realistic condition, surprisingly they could still tune into the invariant but meaningful information in the environment to accurately estimate the size of targets in the non-photorealistic conditions as well. We additionally found that size estimates in verbal and physical responses were generally different in real world and VR viewing and were moderated by trial presentation over time and target object widths.},
	address = {Los Alamitos, CA, USA},
	author = {I. Wijayanto and S. V. Babu and C. C. Pagano and J. Chuang},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247109},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {visualization;virtual environments;size measurement;particle measurements;atmospheric measurements;training;solid modeling},
	month = {may},
	number = {05},
	pages = {2721-2731},
	publisher = {IEEE Computer Society},
	title = {Comparing the Effects of Visual Realism on Size Perception in VR versus Real World Viewing through Physical and Verbal Judgments},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247109}}

@article{Willemsen2011:Change-Blindness-Phenomena,
	abstract = {In visual perception, change blindness describes the phenomenon that persons viewing a visual scene may apparently fail to detect significant changes in that scene. These phenomena have been observed in both computer-generated imagery and real-world scenes. Several studies have demonstrated that change blindness effects occur primarily during visual disruptions such as blinks or saccadic eye movements. However, until now the influence of stereoscopic vision on change blindness has not been studied thoroughly in the context of visual perception research. In this paper, we introduce change blindness techniques for stereoscopic virtual reality (VR) systems, providing the ability to substantially modify a virtual scene in a manner that is difficult for observers to perceive. We evaluate techniques for semiimmersive VR systems, i.e., a passive and active stereoscopic projection system as well as an immersive VR system, i.e., a head-mounted display, and compare the results to those of monoscopic viewing conditions. For stereoscopic viewing conditions, we found that change blindness phenomena occur with the same magnitude as in monoscopic viewing conditions. Furthermore, we have evaluated the potential of the presented techniques for allowing abrupt, and yet significant, changes of a stereoscopically displayed virtual reality environment.},
	address = {Los Alamitos, CA, USA},
	author = {P. Willemsen and K. Hinrichs and G. Bruder and F. Steinicke},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2011.41},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {change blindness;stereoscopic display;virtual reality.},
	month = {sep},
	number = {09},
	pages = {1223-1233},
	publisher = {IEEE Computer Society},
	title = {Change Blindness Phenomena for Virtual Reality Display Systems},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2011.41}}

@article{9382909,
	abstract = {We present a novel redirected walking controller based on alignment that allows the user to explore large and complex virtual environments, while minimizing the number of collisions with obstacles in the physical environment. Our alignment-based redirection controller, ARC, steers the user such that their proximity to obstacles in the physical environment matches the proximity to obstacles in the virtual environment as closely as possible. To quantify a controller&#x27;s performance in complex environments, we introduce a new metric, Complexity Ratio (CR), to measure the relative environment complexity and characterize the difference in navigational complexity between the physical and virtual environments. Through extensive simulation-based experiments, we show that ARC significantly outperforms current state-of-the-art controllers in its ability to steer the user on a collision-free path. We also show through quantitative and qualitative measures of performance that our controller is robust in complex environments with many obstacles. Our method is applicable to arbitrary environments and operates without any user input or parameter tweaking, aside from the layout of the environments. We have implemented our algorithm on the Oculus Quest head-mounted display and evaluated its performance in environments with varying complexity. Our project website is available at https://ganuna.umd.edu/arc/.},
	address = {Los Alamitos, CA, USA},
	author = {N. L. Williams and A. Bera and D. Manocha},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067781},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {complexity theory;legged locomotion;trajectory;virtual environments;measurement;prediction algorithms;navigation},
	month = {may},
	number = {05},
	pages = {2535-2544},
	publisher = {IEEE Computer Society},
	title = {ARC: Alignment-based Redirection Controller for Redirected Walking in Complex Environments},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067781}}

@article{10058530,
	abstract = {Standalone Virtual Reality (VR) headsets can be used when travelling in cars, trains and planes. However, the constrained spaces around transport seating can leave users with little physical space in which to interact using their hands or controllers, and can increase the risk of invading other passengers&#x27; personal space or hitting nearby objects and surfaces. This hinders transport VR users from using most commercial VR applications, which are designed for unobstructed 1-2m 360$\,^{\circ}$ home spaces. In this paper, we investigated whether three at-a-distance interaction techniques from the literature could be adapted to support common commercial VR movement inputs and so equalise the interaction capabilities of at-home and on-transport users: Linear Gain, Gaze-Supported Remote Hand, and AlphaCursor. First, we analysed commercial VR experiences to identify the most common movement inputs so that we could create gamified tasks based on them. We then investigated how well each technique could support these inputs from a constrained $50\mathrm{x}50\text{cm}$ space (representative of an economy plane seat) through a user study $(\mathrm{N}&#x3D;16)$, where participants played all three games with each technique. We measured task performance, unsafe movements (play boundary violations, total arm movement) and subjective experience and compared results to a control `at-home' condition (with unconstrained movement) to determine how similar performance and experience were. Results showed that Linear Gain was the best technique, with similar performance and user experience to the `at-home' condition, albeit at the expense of a high number of boundary violations and large arm movements. In contrast, AlphaCursor kept users within bounds and minimised arm movement, but suffered from poorer performance and experience. Based on the results, we provide eight guidelines for the use of, and research into, at-a-distance techniques and constrained spaces.},
	address = {Los Alamitos, CA, USA},
	author = {G. Wilson and M. McGill and D. Medeiros and S. Brewster},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247084},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {aerospace electronics;task analysis;automobiles;space exploration;games;x reality;weapons},
	month = {may},
	number = {05},
	pages = {2390-2400},
	publisher = {IEEE Computer Society},
	title = {A Lack of Restraint: Comparing Virtual Reality Interaction Techniques for Constrained Transport Seating},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247084}}

@article{9382892,
	abstract = {Learning an advanced skill in sports requires a huge amount of practice and players also have to overcome both physical difficulties and the dullness of repetitive training. Returning a fast spin shot in table tennis could be taken as an example, as athletes need to judge the spin type and decide the racket pose within a second, which is difficult for beginners. Therefore, in this paper, we show how to design an intuitive training system to acquire this specific skill using different cues in Virtual Reality (VR). Using VR, we can easily provide visual information, attach haptic devices, and distort the speed of time, however, it is difficult to decide which types of information could benefit the training. In an initial study, by comparing real world training with VR training, we showed the effect of VR training and obtained some insights about augmentation for training spin shots. The training system was then improved by adding three new conditions using different visualizations and temporal distortions, as well as a haptic racket for creating realistic feedback. Finally, we performed a detailed experiment, which suggest a significant improvement of skill for each condition compared to the baseline, while a qualitative evaluation indicates that both users&#x27; motivation and their understanding of spin are increased by using our system.},
	address = {Los Alamitos, CA, USA},
	author = {E. Wu and M. Piekenbrock and T. Nakumura and H. Koike},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067761},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {training;sports;visualization;haptic interfaces;distortion;trajectory;robots},
	month = {may},
	number = {05},
	pages = {2566-2576},
	publisher = {IEEE Computer Society},
	title = {SPinPong - Virtual Reality Table Tennis Skill Acquisition using Visual, Haptic and Temporal Cues},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067761}}

@article{6777428,
	abstract = {We empirically examined the impact of virtual human animation on the emotional responses of participants in a medical virtual reality system for education in the signs and symptoms of patient deterioration. Participants were presented with one of two virtual human conditions in a between-subjects experiment, static (non-animated) and dynamic (animated). Our objective measures included the use of psycho-physical Electro Dermal Activity (EDA) sensors, and subjective measures inspired by social psychology research included the Differential Emotions Survey (DES IV) and Positive and Negative Affect Survey (PANAS). We analyzed the quantitative and qualitative measures associated with participants&#x27; emotional state at four distinct time-steps in the simulated interpersonal experience as the virtual patient&#x27;s medical condition deteriorated. Results suggest that participants in the dynamic condition with animations exhibited a higher sense of co-presence and greater emotional response as compared to participants in the static condition, corresponding to the deterioration in the medical condition of the virtual patient. Negative affect of participants in the dynamic condition increased at a higher rate than for participants in the static condition. The virtual human animations elicited a stronger response in negative emotions such as anguish, fear, and anger as the virtual patient&#x27;s medical condition worsened.},
	address = {Los Alamitos, CA, USA},
	author = {Yanxiang Wu and S. V. Babu and R. Armstrong and J. W. Bertrand and Jun Luo and T. Roy and S. B. Daily and L. Cairco Dukes and L. F. Hodges and T. Fasolino},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.19},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {animation;training;educational institutions;sensors;medical services;atmospheric measurements;particle measurements},
	month = {apr},
	number = {04},
	pages = {626-635},
	publisher = {IEEE Computer Society},
	title = {Effects of Virtual Human Animation on Emotion Contagion in Simulated Inter-Personal Experiences},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.19}}

@article{8263123,
	abstract = {We present MRTouch, a novel multitouch input solution for head-mounted mixed reality systems. Our system enables users to reach out and directly manipulate virtual interfaces affixed to surfaces in their environment, as though they were touchscreens. Touch input offers precise, tactile and comfortable user input, and naturally complements existing popular modalities, such as voice and hand gesture. Our research prototype combines both depth and infrared camera streams together with real-time detection and tracking of surface planes to enable robust finger-tracking even when both the hand and head are in motion. Our technique is implemented on a commercial Microsoft HoloLens without requiring any additional hardware nor any user or environmental calibration. Through our performance evaluation, we demonstrate high input accuracy with an average positional error of 5.4 mm and 95% button size of 16 mm, across 17 participants, 2 surface orientations and 4 surface materials. Finally, we demonstrate the potential of our technique to enable on-world touch interactions through 5 example applications.},
	address = {Los Alamitos, CA, USA},
	author = {R. Xiao and J. Schwarz and N. Throm and A. D. Wilson and H. Benko},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794222},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;thumb;cameras;tracking;engines;sensors},
	month = {apr},
	number = {04},
	pages = {1653-1660},
	publisher = {IEEE Computer Society},
	title = {MRTouch: Adding Touch Input to Head-Mounted Mixed Reality},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794222}}

@article{8260971,
	abstract = {Games and experiences designed for virtual or augmented reality usually require the player to move physically to play. This poses substantial challenge for level designers because the player&#x27;s physical experience in a level will need to be considered, otherwise the level may turn out to be too exhausting or not challenging enough. This paper presents a novel approach to optimize level designs by considering the physical challenge imposed upon the player in completing a level of motion-based games. A game level is represented as an assembly of chunks characterized by the exercise intensity levels they impose on players. We formulate game level synthesis as an optimization problem, where the chunks are assembled in a way to achieve an optimized level of intensity. To allow the synthesis of game levels of varying lengths, we solve the trans-dimensional optimization problem with a Reversible-jump Markov chain Monte Carlo technique. We demonstrate that our approach can be applied to generate game levels for s of motion-based virtual reality games. A user evaluation validates the effectiveness of our approach in generating levels with the desired amount of physical challenge.},
	address = {Los Alamitos, CA, USA},
	author = {B. Xie and Y. Zhang and H. Huang and E. Ogawa and T. You and L. Yu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793618},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {games;optimization;headphones;measurement;augmented reality;computational modeling},
	month = {apr},
	number = {04},
	pages = {1661-1670},
	publisher = {IEEE Computer Society},
	title = {Exercise Intensity-Driven Level Design},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793618}}

@article{8642443,
	abstract = {In this paper, we present a case for text entry using a circular keyboard layout for mobile head-mounted displays (HMDs) that is dwell-free and does not require users to hold a dedicated input device for letter selection. To support the case, we have implemented RingText whose design is based on a circular layout with two concentric circles. The outer circle is subdivided into regions containing letters. Selection is made by using a virtual cursor controlled by the user&#x27;s head movements-entering a letter region triggers a selection and moving back into the inner circle resets the selection. The design of RingText follows an iterative process, where we initially conduct one first study to investigate the optimal number of letters per region, inner circle size, and alphabet starting location. We then optimize its design by selecting the most suitable features from the first study: one letter per region, narrowing the trigger area to lower error rates, and creating candidate regions that incorporate two suggested words to appear next to the current letter region (close to the cursor) using a dynamic (rather than fixed) approach. Our second study compares text entry performance of RingText with four other hands-free techniques and the results show that RingText outperforms them. Finally, we run a third study lasting four consecutive days with 10 participants (5 novice users and 5 expert users) doing two daily sessions and the results show that RingText is quite efficient and yields a low error rate. At the end of the eighth session, the novice users can achieve a text entry speed of 11.30 WPM after 60 minutes of training while the expert (more experienced) users can reach an average text entry speed of 13.24 WPM after 90 minutes of training.},
	address = {Los Alamitos, CA, USA},
	author = {W. Xu and H. Liang and Y. Zhao and T. Zhang and D. Yu and D. Monteiro},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898736},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {layout;keyboards;error analysis;training;head-mounted displays;resists;dynamics},
	month = {may},
	number = {05},
	pages = {1991-2001},
	publisher = {IEEE Computer Society},
	title = {RingText: Dwell-free and hands-free Text Entry for Mobile Head-Mounted Displays using Head Motions},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898736}}

@article{8643070,
	abstract = {We propose the first real-time system for the egocentric estimation of 3D human body pose in a wide range of unconstrained everyday activities. This setting has a unique set of challenges, such as mobility of the hardware setup, and robustness to long capture sessions with fast recovery from tracking failures. We tackle these challenges based on a novel lightweight setup that converts a standard baseball cap to a device for high-quality pose estimation based on a single cap-mounted fisheye camera. From the captured egocentric live stream, our CNN based 3D pose estimation approach runs at 60 Hz on a consumer-level GPU. In addition to the lightweight hardware setup, our other main contributions are: 1) a large ground truth training corpus of top-down fisheye images and 2) a disentangled 3D pose estimation approach that takes the unique properties of the egocentric viewpoint into account. As shown by our evaluation, we achieve lower 3D joint error as well as better 2D overlay than the existing baselines.},
	address = {Los Alamitos, CA, USA},
	author = {W. Xu and A. Chatterjee and M. Zollhofer and H. Rhodin and P. Fua and H. Seidel and C. Theobalt},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898650},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;cameras;pose estimation;real-time systems;two dimensional displays;hardware;distortion},
	month = {may},
	number = {05},
	pages = {2093-2101},
	publisher = {IEEE Computer Society},
	title = {Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898650}}

@article{10049645,
	abstract = {3D sketching in virtual reality (VR) provides an immersive drawing experience for designs. However, due to the lack of depth perception cues in VR, scaffolding surfaces that constrain strokes to 2D are usually used as visual guides to reduce the difficulty of drawing accurate strokes. When the dominant hand is occupied by the pen tool, the efficiency of scaffolding-based sketching can be improved by using gesture input to reduce the idleness of the non-dominant hand. This paper presents GestureSurface, a bi-manual interface that uses non-dominant hand performing gestures to operate scaffolding and the other hand drawing with controller. We designed a set of non-dominant gestures to create and manipulate scaffolding surfaces, which are assembled by automatic combination based on five predefined primitive surfaces. We evaluated GestureSurface through a 20-person user study and found that the method of scaffolding-based sketching using non-dominant hand has the advantages of high efficiency and low fatigue.},
	address = {Los Alamitos, CA, USA},
	author = {X. Xu and Y. Zhou and B. Shao and G. Feng and C. Yu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247059},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;shape;software;visualization;task analysis;virtual reality;tracking},
	month = {may},
	number = {05},
	pages = {2499-2507},
	publisher = {IEEE Computer Society},
	title = {GestureSurface: VR Sketching through Assembling Scaffold Surface with Non-Dominant Hand},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247059}}

@article{Yan2012:Scanning-3D-Full-Human,
	abstract = {Depth camera such as Microsoft Kinect, is much cheaper than conventional 3D scanning devices, and thus it can be acquired for everyday users easily. However, the depth data captured by Kinect over a certain distance is of extreme low quality. In this paper, we present a novel scanning system for capturing 3D full human body models by using multiple Kinects. To avoid the interference phenomena, we use two Kinects to capture the upper part and lower part of a human body respectively without overlapping region. A third Kinect is used to capture the middle part of the human body from the opposite direction. We propose a practical approach for registering the various body parts of different views under non-rigid deformation. First, a rough mesh template is constructed and used to deform successive frames pairwisely. Second, global alignment is performed to distribute errors in the deformation space, which can solve the loop closure problem efficiently. Misalignment caused by complex occlusion can also be handled reasonably by our global alignment algorithm. The experimental results have shown the efficiency and applicability of our system. Our system obtains impressive results in a few minutes with low price devices, thus is practically useful for generating personalized avatars for everyday users. Our system has been used for 3D human animation and virtual try on, and can further facilitate a range of home-oriented virtual reality (VR) applications.},
	address = {Los Alamitos, CA, USA},
	author = {Hao Yan and Zhigeng Pan and Ligang Liu and Jin Zhou and Jing Tong},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2012.56},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {solid modelling;avatars;computer animation;interactive devices;home-oriented virtual reality applications;3d full human body model scanning;microsoft kinect;3d scanning devices;depth camera;nonrigid deformation;rough mesh template;successive frame deformation;error distribution;loop closure problem;global alignment algorithm;personalized avatars;3d human animation;three dimensional displays;biological system modeling;image reconstruction;shape;humans;computational modeling;geometry;microsoft kinect;3d body scanning;global non-igid registration},
	month = {apr},
	number = {04},
	pages = {643-650},
	publisher = {IEEE Computer Society},
	title = {Scanning 3D Full Human Bodies Using Kinects},
	volume = {18},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2012.56}}

@article{10049667,
	abstract = {Remote communication is essential for efficient collaboration among people at different locations. We present ConeSpeech, a virtual reality (VR) based multi-user remote communication technique, which enables users to selectively speak to target listeners without distracting bystanders. With ConeSpeech, the user looks at the target listener and only in a cone-shaped area in the direction can the listeners hear the speech. This manner alleviates the disturbance to and avoids overhearing from surrounding irrelevant people. Three featured functions are supported, directional speech delivery, size-adjustable delivery range, and multiple delivery areas, to facilitate speaking to more than one listener and to listeners spatially mixed up with bystanders. We conducted a user study to determine the modality to control the cone-shaped delivery area. Then we implemented the technique and evaluated its performance in three typical multi-user communication tasks by comparing it to two baseline methods. Results show that ConeSpeech balanced the convenience and flexibility of voice communication.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Yan and H. Liu and Y. Shi and J. Wang and R. Guo and Z. Li and X. Xu and C. Yu and Y. Wang and Y. Shi},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247085},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {task analysis;headphones;collaboration;avatars;torso;target tracking;speech},
	month = {may},
	number = {05},
	pages = {2647-2657},
	publisher = {IEEE Computer Society},
	title = {ConeSpeech: Exploring Directional Speech Interaction for Multi-Person Remote Communication in Virtual Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247085}}

@article{Yapo2011:A-Spatially-Augmented-Reality,
	abstract = {We present an application of interactive global illumination and spatially augmented reality to architectural daylight modeling that allows designers to explore alternative designs and new technologies for improving the sustainability of their buildings. Images of a model in the real world, captured by a camera above the scene, are processed to construct a virtual 3D model. To achieve interactive rendering rates, we use a hybrid rendering technique, leveraging radiosity to simulate the interreflectance between diffuse patches and shadow volumes to generate per-pixel direct illumination. The rendered images are then projected on the real model by four calibrated projectors to help users study the daylighting illumination. The virtual heliodon is a physical design environment in which multiple designers, a designer and a client, or a teacher and students can gather to experience animated visualizations of the natural illumination within a proposed design by controlling the time of day, season, and climate. Furthermore, participants may interactively redesign the geometry and materials of the space by manipulating physical design elements and see the updated lighting simulation.},
	address = {Los Alamitos, CA, USA},
	author = {T. C. Yapo and C. Young and B. Cutler and Y. Sheng},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2009.209},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {spatially augmented reality;global illumination;radiosity;and daylighting design.},
	month = {jan},
	number = {01},
	pages = {38-50},
	publisher = {IEEE Computer Society},
	title = {A Spatially Augmented Reality Sketching Interface for Architectural Daylighting Design},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2009.209}}

@article{7829434,
	abstract = {This paper examines how humans adapt to novel physical situations with unknown gravitational acceleration in immersive virtual environments. We designed four virtual reality experiments with different tasks for participants to complete: strike a ball to hit a target, trigger a ball to hit a target, predict the landing location of a projectile, and estimate the flight duration of a projectile. The first two experiments compared human behavior in the virtual environment with real-world performance reported in the literature. The last two experiments aimed to test the human ability to adapt to novel gravity fields by measuring their performance in trajectory prediction and time estimation tasks. The experiment results show that: 1) based on brief observation of a projectile&#x27;s initial trajectory, humans are accurate at predicting the landing location even under novel gravity fields, and 2) humans&#x27; time estimation in a familiar earth environment fluctuates around the ground truth flight duration, although the time estimation in unknown gravity fields indicates a bias toward earth&#x27;s gravity.},
	address = {Los Alamitos, CA, USA},
	author = {T. Ye and S. Qi and J. Kubricht and Y. Zhu and H. Lu and S. Zhu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2657235},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {gravity;virtual environments;trajectory;earth;cognition;acceleration},
	month = {apr},
	number = {04},
	pages = {1399-1408},
	publisher = {IEEE Computer Society},
	title = {The Martian: Examining Human Physical Judgments across Virtual Gravity Fields},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2657235}}

@article{9714119,
	abstract = {Crowd motion data is fundamental for understanding and simulating realistic crowd behaviours. Such data is usually collected through controlled experiments to ensure that both desired individual interactions and collective behaviours can be observed. It is however scarce, due to ethical concerns and logistical difficulties involved in its gathering, and only covers a few typical crowd scenarios. In this work, we propose and evaluate a novel Virtual Reality based approach lifting the limitations of real-world experiments for the acquisition of crowd motion data. Our approach immerses a single user in virtual scenarios where he/she successively acts each crowd member. By recording the past trajectories and body movements of the user, and displaying them on virtual characters, the user progressively builds the overall crowd behaviour by him/herself. We validate the feasibility of our approach by replicating three real experiments, and compare both the resulting emergent phenomena and the individual interactions to existing real datasets. Our results suggest that realistic collective behaviours can naturally emerge from virtual crowd data generated using our approach, even though the variety in behaviours is lower than in real situations. These results provide valuable insights to the building of virtual crowd experiences, and reveal key directions for further improvements.},
	address = {Los Alamitos, CA, USA},
	author = {T. Yin and L. Hoyet and M. Christie and M. Cani and J. Pettre},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150507},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {data models;solid modeling;context modeling;trajectory;legged locomotion;ethics;decision making},
	month = {may},
	number = {05},
	pages = {2245-2255},
	publisher = {IEEE Computer Society},
	title = {The One-Man-Crowd: Single User Generation of Crowd Motions Using Virtual Reality},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150507}}

@article{8642375,
	abstract = {The mobility and ubiquity of mobile head-mounted displays make them a promising platform for telepresence research as they allow for spontaneous and remote use cases not possible with stationary hardware. In this work we present a system that provides immersive telepresence and remote collaboration on mobile and wearable devices by building a live spherical panoramic representation of a user&#x27;s environment that can be viewed in real time by a remote user who can independently choose the viewing direction. The remote user can then interact with this environment as if they were actually there through intuitive gesture-based interaction. Each user can obtain independent views within this environment by rotating their device, and their current field of view is shared to allow for simple coordination of viewpoints. We present several different approaches to create this shared live environment and discuss their implementation details, individual challenges, and performance on modern mobile hardware; by doing so we provide key insights into the design and implementation of next generation mobile telepresence systems, guiding future research in this domain. The results of a preliminary user study confirm the ability of our system to induce the desired sense of presence in its users.},
	address = {Los Alamitos, CA, USA},
	author = {J. Young and T. Langlotz and M. Cook and S. Mills and H. Regenbrecht},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898737},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {telepresence;collaboration;task analysis;cameras;hardware;mobile handsets;resists},
	month = {may},
	number = {05},
	pages = {1908-1918},
	publisher = {IEEE Computer Society},
	title = {Immersive Telepresence and Remote Collaboration using Mobile and Wearable Devices},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898737}}

@article{10050833,
	abstract = {Though virtual reality has repeatedly seen usability improvements through higher fidelity headsets, interacting with small objects has remained an issue due to a reduction in visual acuity. Given the current uptake of virtual reality platforms and the range of real world applications that they may be used for, it is worth considering how such interactions can be accounted for. We propose three techniques for improving the usability of small objects in virtual environments: i) expanding them in place, ii) showing a zoomed-in twin above the original object, and iii) showing a large readout of the object&#x27;s current state. We conducted a study comparing each technique&#x27;s usability, induced presence, and effect on short-term knowledge retention in a VR training scenario that simulated the common geoscience exercise of measuring strike and dip. Participant feedback highlighted the need for this research, however simply scaling the area of interest may not be enough to improve the usability of information-bearing objects, while displaying this information in large text format can make tasks faster to complete at the cost of reducing the user&#x27;s ability to transfer knowledge they&#x27;ve learned to the real world. We discuss these results and their implications for the design of future virtual reality experiences.},
	address = {Los Alamitos, CA, USA},
	author = {J. Young and N. Pantidi and M. Wood},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247468},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	month = {may},
	number = {05},
	pages = {2567-2574},
	publisher = {IEEE Computer Society},
	title = {I Can&#x27;t See That! Considering the Readability of Small Objects in Virtual Environments},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247468}}

@article{8643583,
	abstract = {Multi-focal plane and multi-layered light-field displays are promising solutions for addressing all visual cues observed in the real world. Unfortunately, these devices usually require expensive optimizations to compute a suitable decomposition of the input light field or focal stack to drive individual display layers. Although these methods provide near-correct image reconstruction, a significant computational cost prevents real-time applications. A simple alternative is a linear blending strategy which decomposes a single 2D image using depth information. This method provides real-time performance, but it generates inaccurate results at occlusion boundaries and on glossy surfaces. This paper proposes a perception-based hybrid decomposition technique which combines the advantages of the above strategies and achieves both real-time performance and high-fidelity results. The fundamental idea is to apply expensive optimizations only in regions where it is perceptually superior, e.g., depth discontinuities at the fovea, and fall back to less costly linear blending otherwise. We present a complete, perception-informed analysis and model that locally determine which of the two strategies should be applied. The prediction is later utilized by our new synthesis method which performs the image decomposition. The results are analyzed and validated in user experiments on a custom multi-plane display.},
	address = {Los Alamitos, CA, USA},
	author = {H. Yu and M. Bemana and M. Wernikowski and M. Chwesiuk and O. Tursun and G. Singh and K. Myszkowski and R. Mantiuk and H. Seidel and P. Didyk},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898821},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);three-dimensional displays;optimization;real-time systems;computational efficiency;computer architecture;visualization},
	month = {may},
	number = {05},
	pages = {1940-1950},
	publisher = {IEEE Computer Society},
	title = {A Perception-driven Hybrid Decomposition for Multi-layer Accommodative Displays},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898821}}

@article{9712238,
	abstract = {When two or more users attempt to collaborate in the same space with Augmented Reality, they often encounter conflicting intentions regarding the occupation of the same working area and self-positioning around such without mutual interference. Augmented Reality is a powerful tool for communicating ideas and intentions during a co-assisting task that requires multi-disciplinary expertise. To relax the constraint of physical co-location, we propose the concept of Duplicated Reality, where a digital copy of a 3D region of interest of the users&#x27; environment is reconstructed in real-time and visualized in-situ through an Augmented Reality user interface. This enables users to remotely annotate the region of interest while being co-located with others in Augmented Reality. We perform a user study to gain an in-depth understanding of the proposed method compared to an in-situ augmentation, including collaboration, effort, awareness, usability, and the quality of the task. The result indicates almost identical objective and subjective results, except a decrease in the consulting user&#x27;s awareness of co-located users when using our method. The added benefit from duplicating the working area into a designated consulting area opens up new interaction paradigms to be further investigated for future co-located Augmented Reality collaboration systems.},
	address = {Los Alamitos, CA, USA},
	author = {K. Yu and U. Eck and F. Pankratz and M. Lazarovici and D. Wilhelm and N. Navab},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150520},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {collaboration;task analysis;three-dimensional displays;real-time systems;surgery;annotations;image reconstruction},
	month = {may},
	number = {05},
	pages = {2190-2200},
	publisher = {IEEE Computer Society},
	title = {Duplicated Reality for Co-located Augmented Reality Collaboration},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150520}}

@article{10049677,
	abstract = {Inserting 3D virtual objects into real-world images has many applications in photo editing and augmented reality. One key issue to ensure the reality of the composite whole scene is to generate consistent shadows between virtual and real objects. However, it is challenging to synthesize visually realistic shadows for virtual and real objects without any explicit geometric information of the real scene or manual intervention, especially for the shadows on the virtual objects projected by real objects. In view of this challenge, we present, to our knowledge, the first end-to-end solution to fully automatically project real shadows onto virtual objects for outdoor scenes. In our method, we introduce the Shifted Shadow Map, a new shadow representation that encodes the binary mask of shifted real shadows after inserting virtual objects in an image. Based on the shifted shadow map, we propose a CNN-based shadow generation model named ShadowMover which first predicts the shifted shadow map for an input image and then automatically generates plausible shadows on any inserted virtual object. A large-scale dataset is constructed to train the model. Our ShadowMover is robust to various scene configurations without relying on any geometric information of the real scene and is free of manual intervention. Extensive experiments validate the effectiveness of our method.},
	address = {Los Alamitos, CA, USA},
	author = {P. Yu and J. Guo and F. Huang and Z. Chen and C. Wang and Y. Zhang and Y. Guo},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247066},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {lighting;three-dimensional displays;training;manuals;geometry;rendering (computer graphics);estimation},
	month = {may},
	number = {05},
	pages = {2379-2389},
	publisher = {IEEE Computer Society},
	title = {ShadowMover: Automatically Projecting Real Shadows onto Virtual Object},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247066}}

@article{8998563,
	abstract = {We propose and evaluate novel pseudo-haptic techniques to display mass and mass distribution for proxy-based object manipulation in virtual reality. These techniques are specifically designed to generate haptic effects during the object&#x27;s rotation. They rely on manipulating the mapping between visual cues of motion and kinesthetic cues of force to generate a sense of heaviness, which alters the perception of the object&#x27;s mass-related properties without changing the physical proxy. First we present a technique to display an object&#x27;s mass by scaling its rotational motion relative to its mass. A psycho-physical experiment demonstrates that this technique effectively generates correct perceptions of relative mass between two virtual objects. We then present two pseudo-haptic techniques designed to display an object&#x27;s mass distribution. One of them relies on manipulating the pivot point of rotation, while the other adjusts rotational motion based on the real-time dynamics of the moving object. An empirical study shows that both techniques can influence perception of mass distribution, with the second technique being significantly more effective.},
	address = {Los Alamitos, CA, USA},
	author = {R. Yu and D. A. Bowman},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973056},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;visualization;force;dynamics;kinematics;virtual reality;real-time systems},
	month = {may},
	number = {05},
	pages = {2094-2103},
	publisher = {IEEE Computer Society},
	title = {Pseudo-Haptic Display of Mass and Mass Distribution During Object Rotation in Virtual Reality},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973056}}

@article{10050417,
	abstract = {An ideal Virtual reality (VR) device should simultaneously provide retina-level resolution, wide field-of-view (FOV), and high refresh rate display, thereby bringing users into a deeply immersive virtual world. However, directly providing such high-quality display poses great challenges for display panel fabrication, real-time rendering, and data transfer. To address this issue, we introduce a dual-mode virtual reality system based on the spatio-temporal perception characteristics of human vision. The proposed VR system has a novel optical architecture. It can switch display modes according to the user&#x27;s perceptual requirements for different display scenes to adaptively adjust the display spatial and temporal resolution based on a given display budget, thus providing users with the optimal visual perception quality. In this work, a complete design pipeline for the dual-mode VR optical system is proposed, and a bench-top prototype is built with only off-the-shelf hardware and components to verify its capability. Compared to the conventional VR system, our proposed scheme is more efficient and flexible in utilizing the display budget, and this work is expected to facilitate the development of the VR device based on the human visual system.},
	address = {Los Alamitos, CA, USA},
	author = {H. Zeng and R. Zhao},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247097},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {spatial resolution;optical switches;high-speed optical techniques;visualization;sensitivity;rendering (computer graphics);optical imaging},
	month = {may},
	number = {05},
	pages = {2249-2257},
	publisher = {IEEE Computer Society},
	title = {Perceptually-guided Dual-mode Virtual Reality System For Motion-adaptive Display},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247097}}

@article{7833030,
	abstract = {We define the concept of Dynamic Passive Haptic Feedback (DPHF) for virtual reality by introducing the weight-shifting physical DPHF proxy object Shifty. This concept combines actuators known from active haptics and physical proxies known from passive haptics to construct proxies that automatically adapt their passive haptic feedback. We describe the concept behind our ungrounded weight-shifting DPHF proxy Shifty and the implementation of our prototype. We then investigate how Shifty can, by automatically changing its internal weight distribution, enhance the user&#x27;s perception of virtual objects interacted with in two experiments. In a first experiment, we show that Shifty can enhance the perception of virtual objects changing in shape, especially in length and thickness. Here, Shifty was shown to increase the user&#x27;s fun and perceived realism significantly, compared to an equivalent passive haptic proxy. In a second experiment, Shifty is used to pick up virtual objects of different virtual weights. The results show that Shifty enhances the perception of weight and thus the perceived realism by adapting its kinesthetic feedback to the picked-up virtual object. In the same experiment, we additionally show that specific combinations of haptic, visual and auditory feedback during the pick-up interaction help to compensate for visual-haptic mismatch perceived during the shifting process.},
	address = {Los Alamitos, CA, USA},
	author = {A. Zenner and A. Kruger},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2017.2656978},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;visualization;actuators;shape;augmented reality;augmented virtuality},
	month = {apr},
	number = {04},
	pages = {1285-1294},
	publisher = {IEEE Computer Society},
	title = {Shifty: A Weight-Shifting Dynamic Passive Haptic Proxy to Enhance Object Perception in Virtual Reality},
	volume = {23},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2017.2656978}}

@article{8998292,
	abstract = {In many professional domains, relevant processes are documented as abstract process models, such as event-driven process chains (EPCs). EPCs are traditionally visualized as 2D graphs and their size varies with the complexity of the process. While process modeling experts are used to interpreting complex 2D EPCs, in certain scenarios such as, for example, professional training or education, also novice users inexperienced in interpreting 2D EPC data are facing the challenge of learning and understanding complex process models. To communicate process knowledge in an effective yet motivating and interesting way, we propose a novel virtual reality (VR) interface for non-expert users. Our proposed system turns the exploration of arbitrarily complex EPCs into an interactive and multi-sensory VR experience. It automatically generates a virtual 3D environment from a process model and lets users explore processes through a combination of natural walking and teleportation. Our immersive interface leverages basic gamification in the form of a logical walkthrough mode to motivate users to interact with the virtual process. The generated user experience is entirely novel in the field of immersive data exploration and supported by a combination of visual, auditory, vibrotactile and passive haptic feedback. In a user study with $\mathrm{N}&#x3D;27$ novice users, we evaluate the effect of our proposed system on process model understandability and user experience, while comparing it to a traditional 2D interface on a tablet device. The results indicate a tradeoff between efficiency and user interest as assessed by the UEQ novelty subscale, while no significant decrease in model understanding performance was found using the proposed VR interface. Our investigation highlights the potential of multi-sensory VR for less time-critical professional application domains, such as employee training, communication, education, and related scenarios focusing on user interest.},
	address = {Los Alamitos, CA, USA},
	author = {A. Zenner and A. Makhsadov and S. Klingner and D. Liebemann and A. Kruger},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973476},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;solid modeling;two dimensional displays;data models;three-dimensional displays;virtual reality;business},
	month = {may},
	number = {05},
	pages = {2104-2114},
	publisher = {IEEE Computer Society},
	title = {Immersive Process Model Exploration in Virtual Reality},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973476}}

@article{9382898,
	abstract = {To provide immersive haptic experiences, proxy-based haptic feedback systems for virtual reality (VR) face two central challenges: (1) similarity, and (2) colocation. While to solve challenge (1), physical proxy objects need to be sufficiently similar to their virtual counterparts in terms of haptic properties, for challenge (2), proxies and virtual counterparts need to be sufficiently colocated to allow for seamless interactions. To solve these challenges, past research introduced, among others, two successful techniques: (a) Dynamic Passive Haptic Feedback (DPHF), a hardware-based technique that leverages actuated props adapting their physical state during the VR experience, and (b) Haptic Retargeting, a software-based technique leveraging hand redirection to bridge spatial offsets between real and virtual objects. Both concepts have, up to now, not ever been studied in combination. This paper proposes to combine both techniques and reports on the results of a perceptual and a psychophysical experiment situated in a proof-of-concept scenario focused on the perception of virtual weight distribution. We show that users in VR overestimate weight shifts and that, when DPHF and HR are combined, significantly greater shifts can be rendered, compared to using only a weight-shifting prop or unnoticeable hand redirection. Moreover, we find the combination of DPHF and HR to let significantly larger spatial dislocations of proxy and virtual counterpart go unnoticed by users. Our investigation is the first to show the value of combining DPHF and HR in practice, validating that their combination can better solve the challenges of similarity and colocation than the individual techniques can do alone.},
	address = {Los Alamitos, CA, USA},
	author = {A. Zenner and K. Ullmann and A. Kruger},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067777},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;rendering (computer graphics);measurement;visualization;virtual reality;shape;hardware},
	month = {may},
	number = {05},
	pages = {2627-2637},
	publisher = {IEEE Computer Society},
	title = {Combining Dynamic Passive Haptics and Haptic Retargeting for Enhanced Haptic Feedback in Virtual Reality},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067777}}

@article{8314105,
	abstract = {Telepresence systems have the potential to overcome limits and distance constraints of the real-world by enabling people to remotely visit and interact with each other. However, current telepresence systems usually lack natural ways of supporting interaction and exploration of remote environments (REs). In particular, single webcams for capturing the RE provide only a limited illusion of spatial presence, and movement control of mobile platforms in today&#x27;s telepresence systems are often restricted to simple interaction devices. One of the main challenges of telepresence systems is to allow users to explore a RE in an immersive, intuitive and natural way, e.g., by real walking in the user&#x27;s local environment (LE), and thus controlling motions of the robot platform in the RE. However, the LE in which the user&#x27;s motions are tracked usually provides a much smaller interaction space than the RE. In this context, redirected walking (RDW) is a very suitable approach to solve this problem. However, so far there is no previous work, which explored if and how RDW can be used in video-based 360$\,^{\circ}$ telepresence systems. In this article, we conducted two psychophysical experiments in which we have quantified how much humans can be unknowingly redirected on virtual paths in the RE, which are different from the physical paths that they actually walk in the LE. Experiment 1 introduces a discrimination task between local and remote translations, and in Experiment 2 we analyzed the discrimination between local and remote rotations. In Experiment 1 participants performed straightforward translations in the LE that were mapped to straightforward translations in the RE shown as 360$\,^{\circ}$ videos, which were manipulated by different gains. Then, participants had to estimate if the remotely perceived translation was faster or slower than the actual physically performed translation. Similarly, in Experiment 2 participants performed rotations in the LE that were mapped to the virtual rotations in a 360$\,^{\circ}$ video-based RE to which we applied different gains. Again, participants had to estimate whether the remotely perceived rotation was smaller or larger than the actual physically performed rotation. Our results show that participants are not able to reliably discriminate the difference between physical motion in the LE and the virtual motion from the 360$\,^{\circ}$ video RE when virtual translations are down-scaled by 5.8% and up-scaled by 9.7%, and virtual rotations are about 12.3% less or 9.2% more than the corresponding physical rotations in the LE.},
	address = {Los Alamitos, CA, USA},
	author = {J. Zhang and E. Langbehn and D. Krupke and N. Katzakis and F. Steinicke},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2793679},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {telepresence;legged locomotion;cameras;aerospace electronics;resists;task analysis},
	month = {apr},
	number = {04},
	pages = {1671-1680},
	publisher = {IEEE Computer Society},
	title = {Detection Thresholds for Rotation and Translation Gains in 360$\,^{\circ}$ Video-Based Telepresence Systems},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2793679}}

@article{9714049,
	abstract = {In this paper, we present the design, implementation, and evaluation of SEAR, a collaborative framework for Scaling Experiences in multi-user Augmented Reality (AR). Most AR systems benefit from computer vision (CV) algorithms to detect, classify, or recognize physical objects for augmentation. A widely used acceleration method for mobile AR is to offload the compute-intensive tasks (e.g., CV algorithms) to the network edge. However, we show that the end-to-end latency, an important metric of mobile AR, may dramatically increase when offloading AR tasks from a large number of concurrent users to the edge. SEAR tackles this scalability issue through the innovation of a lightweight collaborative local caching scheme. Our key observation is that nearby AR users may share some common interests, and may even have overlapped views to augment (e.g., when playing a multi-user AR game). Thus, SEAR opportunistically exchanges the results of offloaded AR tasks among users when feasible and leverages compute resources on mobile devices to relieve, if necessary, the edge workload by intelligently reusing these results. We build a prototype of SEAR to demonstrate its efficacy in scaling AR experiences. We conduct extensive evaluations through both real-world experiments and trace-driven simulations. We observe that SEAR not only reduces the end-to-end latency, by up to 130, compared to the state-of-the-art adaptive edge offloading scheme, but also achieves high object-recognition accuracy for mobile AR.},
	address = {Los Alamitos, CA, USA},
	author = {W. Zhang and B. Han and P. Hui},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150467},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {scalability;task analysis;collaboration;servers;image edge detection;mobile handsets;annotations},
	month = {may},
	number = {05},
	pages = {1982-1992},
	publisher = {IEEE Computer Society},
	title = {SEAR: Scaling Experiences in Multi-user Augmented Reality},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150467}}

@article{9714050,
	abstract = {The VirtualCube system is a 3D video conference system that attempts to overcome some limitations of conventional technologies. The key ingredient is VirtualCube, an abstract representation of a real-world cubicle instrumented with RGBD cameras for capturing the user&#x27;s 3D geometry and texture. We design VirtualCube so that the task of data capturing is standardized and significantly simplified, and everything can be built using off-the-shelf hardware. We use VirtualCubes as the basic building blocks of a virtual conferencing environment, and we provide each VirtualCube user with a surrounding display showing life-size videos of remote participants. To achieve real-time rendering of remote participants, we develop the V-Cube View algorithm, which uses multi-view stereo for more accurate depth estimation and Lumi-Net rendering for better rendering quality. The VirtualCube system correctly preserves the mutual eye gaze between participants, allowing them to establish eye contact and be aware of who is visually paying attention to them. The system also allows a participant to have side discussions with remote participants as if they were in the same room. Finally, the system sheds lights on how to support the shared space of work items (e.g., documents and applications) and track participants&#x27; visual attention to work items.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Zhang and J. Yang and Z. Liu and R. Wang and G. Chen and X. Tong and B. Guo},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2022.3150512},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {rendering (computer graphics);videoconferences;three-dimensional displays;virtual environments;real-time systems;geometry;cameras},
	month = {may},
	number = {05},
	pages = {2146-2156},
	publisher = {IEEE Computer Society},
	title = {VirtualCube: An Immersive 3D Video Communication System},
	volume = {28},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2022.3150512}}

@article{10050791,
	abstract = {The occlusion-capable optical see-through head-mounted display (OC-OSTHMD) is actively developed in recent years since it allows mutual occlusion between virtual objects and the physical world to be correctly presented in augmented reality (AR). However, implementing occlusion with the special type of OSTHMDs prevents the appealing feature from the wide application. In this paper, a novel approach for realizing mutual occlusion for common OSTHMDs is proposed. A wearable device with per-pixel occlusion capability is designed. OSTHMD devices are upgraded to be occlusion-capable by attaching the device before optical combiners. A prototype with HoloLens 1 is built. The virtual display with mutual occlusion is demonstrated in real-time. A color correction algorithm is proposed to mitigate the color aberration caused by the occlusion device. Potential applications, including the texture replacement of real objects and the more realistic semi-transparent objects display, are demonstrated. The proposed system is expected to realize a universal implementation of mutual occlusion in AR.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Zhang and X. Hu and K. Kiyokawa and X. Yang},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247064},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {optical imaging;lenses;adaptive optics;mirrors;image color analysis;optical polarization;holography},
	month = {may},
	number = {05},
	pages = {2700-2709},
	publisher = {IEEE Computer Society},
	title = {Add-on Occlusion: Turning Off-the-Shelf Optical See-through Head-mounted Displays Occlusion-capable},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247064}}

@article{8998337,
	abstract = {Point clouds-based 3D human pose estimation that aims to recover the 3D locations of human skeleton joints plays an important role in many AR/VR applications. The success of existing methods is generally built upon large scale data annotated with 3D human joints. However, it is a labor-intensive and error-prone process to annotate 3D human joints from input depth images or point clouds, due to the self-occlusion between body parts as well as the tedious annotation process on 3D point clouds. Meanwhile, it is easier to construct human pose datasets with 2D human joint annotations on depth images. To address this problem, we present a weakly supervised adversarial learning framework for 3D human pose estimation from point clouds. Compared to existing 3D human pose estimation methods from depth images or point clouds, we exploit both the weakly supervised data with only annotations of 2D human joints and fully supervised data with annotations of 3D human joints. In order to relieve the human pose ambiguity due to weak supervision, we adopt adversarial learning to ensure the recovered human pose is valid. Instead of using either 2D or 3D representations of depth images in previous methods, we exploit both point clouds and the input depth image. We adopt 2D CNN to extract 2D human joints from the input depth image, 2D human joints aid us in obtaining the initial 3D human joints and selecting effective sampling points that could reduce the computation cost of 3D human pose regression using point clouds network. The used point clouds network can narrow down the domain gap between the network input i.e. point clouds and 3D joints. Thanks to weakly supervised adversarial learning framework, our method can achieve accurate 3D human pose from point clouds. Experiments on the ITOP dataset and EVAL dataset demonstrate that our method can achieve state-of-the-art performance efficiently.},
	address = {Los Alamitos, CA, USA},
	author = {Z. Zhang and L. Hu and X. Deng and S. Xia},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2020.2973076},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;two dimensional displays;pose estimation;heating systems;proposals;training data;computers},
	month = {may},
	number = {05},
	pages = {1851-1859},
	publisher = {IEEE Computer Society},
	title = {Weakly Supervised Adversarial Learning for 3D Human Pose Estimation from Point Clouds},
	volume = {26},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2020.2973076}}

@article{9382899,
	abstract = {Haptic sensation plays an important role in providing physical information to users in both real environments and virtual environments. To produce high-fidelity haptic feedback, various haptic devices and tactile rendering methods have been explored in myriad scenarios, and perception deviation between a virtual environment and a real environment has been investigated. However, the tactile sensitivity for touch perception in a virtual environment has not been fully studied; thus, the necessary guidance to design haptic feedback quantitatively for virtual reality systems is lacking. This paper aims to investigate users&#x27; tactile sensitivity and explore the perceptual thresholds when users are immersed in a virtual environment by utilizing electrovibration tactile feedback and by generating tactile stimuli with different waveform, frequency and amplitude characteristics. Hence, two psychophysical experiments were designed, and the experimental results were analyzed. We believe that the significance and potential of our study on tactile perceptual thresholds can promote future research that focuses on creating a favorable haptic experience for VR applications.},
	address = {Los Alamitos, CA, USA},
	author = {L. Zhao and Y. Liu and W. Song},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2021.3067778},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;sensitivity;virtual environments;tactile sensors;visualization;correlation;estimation},
	month = {may},
	number = {05},
	pages = {2618-2626},
	publisher = {IEEE Computer Society},
	title = {Tactile Perceptual Thresholds of Electrovibration in VR},
	volume = {27},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2021.3067778}}

@article{10049687,
	abstract = {Daily travel usually demands navigation on foot across a variety of different application domains, including tasks like search and rescue or commuting. Head-mounted augmented reality (AR) displays provide a preview of future navigation systems on foot, but designing them is still an open problem. In this paper, we look at two choices that such AR systems can make for navigation: 1) whether to denote landmarks with AR cues and 2) how to convey navigation instructions. Specifically, instructions can be given via a head-referenced display (screen-fixed frame of reference) or by giving directions fixed to global positions in the world (world-fixed frame of reference). Given limitations with the tracking stability, field of view, and brightness of most currently available head-mounted AR displays for lengthy routes outdoors, we decided to simulate these conditions in virtual reality. In the current study, participants navigated an urban virtual environment and their spatial knowledge acquisition was assessed. We experimented with whether or not landmarks in the environment were cued, as well as how navigation instructions were displayed (i.e., via screen-fixed or world-fixed directions). We found that the world-fixed frame of reference resulted in better spatial learning when there were no landmarks cued; adding AR landmark cues marginally improved spatial learning in the screen-fixed condition. These benefits in learning were also correlated with participants&#x27; reported sense of direction. Our findings have implications for the design of future cognition-driven navigation systems.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Zhao and J. Stefanucci and S. Creem-Regehr and B. Bodenheimer},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247078},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {navigation;task analysis;three-dimensional displays;knowledge acquisition;internet;glass;visualization},
	month = {may},
	number = {05},
	pages = {2710-2720},
	publisher = {IEEE Computer Society},
	title = {Evaluating Augmented Reality Landmark Cues and Frame of Reference Displays with Virtual Reality},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247078}}

@article{10049714,
	abstract = {As the most common idiopathic inflammatory myopathy in children, juvenile dermatomyositis (JDM) is characterized by skin rashes and muscle weakness. The childhood myositis assessment scale (CMAS) is commonly used to measure the degree of muscle involvement for diagnosis or rehabilitation monitoring. On the one hand, human diagnosis is not scalable and may be subject to personal bias. On the other hand, automatic action quality assessment (AQA) algorithms cannot guarantee 100% accuracy, making them not suitable for biomedical applications. As a solution, we propose a video-based augmented reality system for human-in-the-loop muscle strength assessment of children with JDM. We first propose an AQA algorithm for muscle strength assessment of JDM using contrastive regression trained by a JDM dataset. Our core insight is to visualize the AQA results as a virtual character facilitated by a 3D animation dataset, so that users can compare the real-world patient and the virtual character to understand and verify the AQA results. To allow effective comparisons, we propose a video-based augmented reality system. Given a feed, we adapt computer vision algorithms for scene understanding, evaluate the optimal way of augmenting the virtual character into the scene, and highlight important parts for effective human verification. The experimental results confirm the effectiveness of our AQA algorithm, and the results of the user study demonstrate that humans can more accurately and quickly assess the muscle strength of children using our system.},
	address = {Los Alamitos, CA, USA},
	author = {K. Zhou and R. Cai and Y. Ma and Q. Tan and X. Wang and J. Li and H. H. Shum and F. B. Li and S. Jin and X. Liang},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247092},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {muscles;medical services;animation;visualization;pediatrics;medical diagnostic imaging;human in the loop},
	month = {may},
	number = {05},
	pages = {2456-2466},
	publisher = {IEEE Computer Society},
	title = {A Video-Based Augmented Reality System for Human-in-the-Loop Muscle Strength Assessment of Juvenile Dermatomyositis},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247092}}

@article{8642347,
	abstract = {Fish Tank Virtual Reality (FTVR) displays create a compelling 3D spatial effect by rendering to the perspective of the viewer with head-tracking. Combining FTVR with a spherical display enhances the 3D experience with unique properties of the spherical screen such as the enclosing shape, consistent curved surface, and borderless views from all angles around the display. The ability to generate a strong 3D effect on a spherical display with head-tracked rendering is promising for increasing user&#x27;s performance in 3D tasks. An unanswered question is whether these natural affordances of spherical FTVR displays can improve spatial perception in comparison to traditional flat FTVR displays. To investigate this question, we conducted an experiment to see whether users can perceive the depth and size of virtual objects better on a spherical FTVR display compared to a flat FTVR display on two tasks. Using the spherical display, we found significantly that users had 1cm depth accuracy compared to 6.5cm accuracy using the flat display on a depth-ranking task. Likewise, their performance on a size-matching task was also significantly better with the size error of 2.3mm on the spherical display compared to 3.1mm on the flat display. Furthermore, the perception of size-constancy is stronger on the spherical display than the flat display. This study indicates that the natural affordances provided by the spherical form factor improve depth and size perception in 3D compared to a flat display. We believe that spherical FTVR displays have potential as a 3D virtual environment to provide better task performance for various 3D applications such as 3D designs, scientific visualizations, and virtual surgery.},
	address = {Los Alamitos, CA, USA},
	author = {Q. Zhou and G. Hagemann and D. Fafard and I. Stavness and S. Fels},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2019.2898742},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {three-dimensional displays;task analysis;shape;virtual reality;fish;rendering (computer graphics);visualization},
	month = {may},
	number = {05},
	pages = {2040-2049},
	publisher = {IEEE Computer Society},
	title = {An Evaluation of Depth and Size Perception on a Spherical Fish Tank Virtual Reality Display},
	volume = {25},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2019.2898742}}

@article{10049705,
	abstract = {This paper proposes a general handheld stick haptic redirection method that allows the user to experience complex shapes with haptic feedback through both tapping and extended contact, such as in contour tracing. As the user extends the stick to make contact with a virtual object, the contact point with the virtual object and the targeted contact point with the physical object are continually updated, and the virtual stick is redirected to synchronize the virtual and real contacts. Redirection is applied either just to the virtual stick, or to both the virtual stick and hand. A user study (N &#x3D; 26) confirms the effectiveness of the proposed redirection method. A first experiment following a two-interval forced-choice design reveals that the offset detection thresholds are [15cm, +15cm]. A second experiment asks participants to guess the shape of an invisible virtual object by tapping it and by tracing its contour with the handheld stick, using a real world disk as a source of passive haptic feedback. The experiment reveals that using our haptic redirection method participants can identify the invisible object with 78% accuracy.},
	address = {Los Alamitos, CA, USA},
	author = {Y. Zhou and V. Popescu},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247047},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {haptic interfaces;shape;three-dimensional displays;electronic mail;virtual reality;synchronization;safety},
	month = {may},
	number = {05},
	pages = {2753-2762},
	publisher = {IEEE Computer Society},
	title = {Dynamic Redirection for VR Haptics with a Handheld Stick},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247047}}

@article{8267290,
	abstract = {Virtual characters that appear almost photo-realistic have been shown to induce negative responses from viewers in traditional media, such as film and video games. This effect, described as the uncanny valley, is the reason why realism is often avoided when the aim is to create an appealing virtual character. In Virtual Reality, there have been few attempts to investigate this phenomenon and the implications of rendering virtual characters with high levels of realism on user enjoyment. In this paper, we conducted a large-scale experiment on over one thousand members of the public in order to gather information on how virtual characters are perceived in interactive virtual reality games. We were particularly interested in whether different render styles (realistic, cartoon, etc.) would directly influence appeal, or if a character&#x27;s personality was the most important indicator of appeal. We used a number of perceptual metrics such as subjective ratings, proximity, and attribution bias in order to test our hypothesis. Our main result shows that affinity towards virtual characters is a complex interaction between the character&#x27;s appearance and personality, and that realism is in fact a positive choice for virtual characters in virtual reality.},
	address = {Los Alamitos, CA, USA},
	author = {K. Zibrek and E. Kokkinara and R. Mcdonnell},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2018.2794638},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual reality;visualization;shape;solid modeling;skin;games;geometry},
	month = {apr},
	number = {04},
	pages = {1681-1690},
	publisher = {IEEE Computer Society},
	title = {The Effect of Realistic Appearance of Virtual Characters in Immersive Environments - Does the Character&#x27;s Personality Play a Role?},
	volume = {24},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2018.2794638}}

@article{Ziemer2011:An-Immersive-Virtual-Peer,
	abstract = {The goal of our work is to develop a programmatically controlled peer to bicycle with a human subject for the purpose of studying how social interactions influence road-crossing behavior. The peer is controlled through a combination of reactive controllers that determine the gross motion of the virtual bicycle, action-based controllers that animate the virtual bicyclist and generate verbal behaviors, and a keyboard interface that allows an experimenter to initiate the virtual bicyclist&#x27;s actions during the course of an experiment. The virtual bicyclist&#x27;s repertoire of behaviors includes road following, riding alongside the human rider, stopping at intersections, and crossing intersections through specified gaps in traffic. The virtual cyclist engages the human subject through gaze, gesture, and verbal interactions. We describe the structure of the behavior code and report the results of a study examining how 10- and 12-year-old children interact with a peer cyclist that makes either risky or safe choices in selecting gaps in traffic. Results of our study revealed that children who rode with a risky peer were more likely to cross intermediate-sized gaps than children who rode with a safe peer. In addition, children were significantly less likely to stop at the last six intersections after the experience of riding with the risky than the safe peer during the first six intersections. The results of the study and children&#x27;s reactions to the virtual peer indicate that our virtual peer framework is a promising platform for future behavioral studies of peer influences on children&#x27;s bicycle riding behavior.},
	address = {Los Alamitos, CA, USA},
	author = {C. Ziemer and J. F. Cremer and T. Y. Grechkin and S. V. Babu and J. M. Plumert and B. Chihak and J. K. Kearney},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2009.211},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {virtual humans;virtual reality;applied perception;3d human-computer interaction.},
	month = {jan},
	number = {01},
	pages = {14-25},
	publisher = {IEEE Computer Society},
	title = {An Immersive Virtual Peer for Studying Social Influences on Child Cyclists&#x27; Road-Crossing Behavior},
	volume = {17},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2009.211}}

@article{6777462,
	abstract = {Micro aerial vehicles equipped with high-resolution cameras can be used to create aerial reconstructions of an area of interest. In that context automatic flight path planning and autonomous flying is often applied but so far cannot fully replace the human in the loop, supervising the flight on-site to assure that there are no collisions with obstacles. Unfortunately, this workflow yields several issues, such as the need to mentally transfer the aerial vehicle&#x27;s position between 2D map positions and the physical environment, and the complicated depth perception of objects flying in the distance. Augmented Reality can address these issues by bringing the flight planning process on-site and visualizing the spatial relationship between the planned or current positions of the vehicle and the physical environment. In this paper, we present Augmented Reality supported navigation and flight planning of micro aerial vehicles by augmenting the user&#x27;s view with relevant information for flight planning and live feedback for flight supervision. Furthermore, we introduce additional depth hints supporting the user in understanding the spatial relationship of virtual waypoints in the physical world and investigate the effect of these visualization techniques on the spatial understanding.},
	address = {Los Alamitos, CA, USA},
	author = {S. Zollmann and C. Hoppe and T. Langlotz and G. Reitmayr},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2014.24},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	keywords = {vehicles;cameras;three-dimensional displays;visualization;navigation;data visualization;robots},
	month = {apr},
	number = {04},
	pages = {560-568},
	publisher = {IEEE Computer Society},
	title = {FlyAR: Augmented Reality Supported Micro Aerial Vehicle Navigation},
	volume = {20},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2014.24}}

@article{10049740,
	abstract = {This paper presents a participatory design study about how consent to interaction and observation of other users can be supported in social VR. We use emerging VR dating applications, colloquially called the dating metaverse, as context for study of harm-mitigative design structures in social VR given the evidence of harms that occur through dating apps and general social VR applications individually, and the harms that may occur through their convergence. Through design workshops with potential dating metaverse users in the Midwest United States (n&#x3D;18) we elucidate nonconsensual experiences that should be prevented and participant-created designs for informing and exchanging consent in VR. We position consent as a valuable lens for which to design preventative solutions to harm in social VR by reframing harm as unwanted experiences that happen because of the absence of mechanics to support users in giving and denying agreement to a virtual experience before it occurs.},
	address = {Los Alamitos, CA, USA},
	author = {D. Zytko and J. Chan},
	date-added = {2024-03-18 02:47:10 -0400},
	date-modified = {2024-03-18 02:47:10 -0400},
	doi = {10.1109/TVCG.2023.3247065},
	issn = {1941-0506},
	journal = {IEEE Transactions on Visualization &amp; Computer Graphics},
	month = {may},
	number = {05},
	pages = {2489-2498},
	publisher = {IEEE Computer Society},
	title = {The Dating Metaverse: Why We Need to Design for Consent in Social VR},
	volume = {29},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/TVCG.2023.3247065}}

@inproceedings{512473,
	abstract = {Describes a network software architecture for solving the problem of scaling very large distributed simulations. The fundamental idea is to logically partition virtual environments by associating spatial, temporal and functionally-related entity classes with network multicast groups. We exploit the actual characteristics of the real-world large-scale environments that are simulated by focusing or restricting an entity's processing and network resources to its area of interest via a local 'area of interest manager' (AOIM). Finally, we present an example of how we would implement this concept for ground vehicles. We have begun the design and construction of the AOIM for use with the NPSNET 3D vehicle simulator. NPSNET is currently the only distributed interactive simulation (DIS) protocol compliant simulator using IP multicast communications and is suitable for operation over the Internet.},
	author = {Macedomia, M.R. and Zyda, M.J. and Pratt, D.R. and Brutzman, D.P. and Barham, P.T.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512473},
	keywords = {Computer architecture;Software architecture;Virtual environment;Large-scale systems;Environmental management;Resource management;Land vehicles;Multicast protocols;Multicast communication;Internet},
	month = {March},
	pages = {2-10},
	title = {Exploiting reality with multicast groups: a network architecture for large-scale virtual environments},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512473}}

@inproceedings{512474,
	abstract = {The Environment Manager (EM) is a high-level tool for constructing both single-user and multi-user virtual environments. A script file is used to initialize and run virtual worlds. Independent applications can share information and cooperate with each other across the Internet. EM reduces the effort required to produce a networked virtual world by providing high-level support for application replication, network configuration, communication management and concurrency control. This paper describes the architecture and implementation of EM.},
	author = {Qunjie Wang and Green, M. and Shaw, C.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512474},
	keywords = {Environmental management;Virtual reality;Graphics;Solid modeling;Animation;Geometry;User interfaces;Packaging;Object oriented modeling;Buildings},
	month = {March},
	pages = {11-18},
	title = {EM-an environment manager for building networked virtual environments},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512474}}

@inproceedings{512475,
	abstract = {In a majority of networked virtual worlds, object sharing is limited to object geometries only. The BrickNet toolkit extends the sharing of objects to include dynamic object behaviors. This is achieved by combining a structured organizational paradigm for virtual worlds with an interpreted language. Sharing in virtual worlds is handled by transferring the program code that builds the structure and executes the behavior. The range of behaviors that can be shared in BrickNet include simple behaviors, virtual world dependent behaviors, reactive behaviors and capability-based behaviors.},
	author = {Singh, G. and Serra, L. and Png, W. and Wong, A. and Ng, H.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512475},
	keywords = {File servers;Network servers;Virtual reality;Animation;Collaborative work;Object oriented modeling;Collaboration;Geometry;Computer networks;Computer network management},
	month = {March},
	pages = {19-25},
	title = {BrickNet: sharing object behaviors on the Net},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512475}}

@inproceedings{512476,
	abstract = {Reviews several significant human factors issues that could stand in the way of virtual reality realizing its full potential. These issues involve maximizing human performance efficiency in virtual environments, minimizing health and safety issues, and circumventing potential social issues through proactive assessment.},
	author = {Stanney, K.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512476},
	keywords = {Virtual reality;Human factors;Virtual environment;Health and safety;Physiology;Three dimensional displays;Industrial engineering;Engineering management;Educational technology;Biomedical imaging},
	month = {March},
	pages = {28-34},
	title = {Realizing the full potential of virtual reality: human factors issues that could stand in the way},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512476}}

@inproceedings{512477,
	abstract = {As we move our heads and bodies, and travel though our real world, the human organs of equilibrium (the vestibular system) integrate the information from the visual and proprioceptive systems and compare them. A useful analogue is the fire control system for a gun mounted on a moving vehicle such as a battleship. It is our opinion that many virtual reality (VR) systems, like many ground based flight simulators, alter the natural correspondences between these sensory inputs and when the exposure to the VR environment is protracted, the sensory systems are recalibrated to accommodate the new relationships. These recalibrations, when they involve the vestibular system, can result in balance disturbances, and these latter can outlast the period that an individual remains under the control of the person or entity that exposed that individual to the VR system. If a person should trip when leaving the building, or later when driving home, safety could be compromised and product liability could be incurred. We review our experiences with balance disturbances in flight trainers and describe recent findings with an automated postural equilibrium assessment system which can be employed before and after exposure in order to certify that no observable changes are evident in a subject or user.},
	author = {Kennedy, R.S. and Lilienthal, M.G.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512477},
	keywords = {Virtual reality;Control systems;Humans;Fires;Vehicles;Aerospace simulation;Product safety;Domestic safety;Product liability;Central nervous system},
	month = {March},
	pages = {35-39},
	title = {Implications of balance disturbances following exposure to virtual reality systems},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512477}}

@inproceedings{512478,
	abstract = {Cognitive maps are mental models of the relative locations and attributes of phenomena in spatial environments. Understanding how people form cognitive maps of virtual environments is vital to effective virtual world design. Unfortunately, such an understanding is hampered by the difficulty of cognitive map measurement. The present study tests the validity of using sketch maps to examine aspects of virtual world cognitive maps. We predict that subjects who report feeling oriented within the virtual world will produce better sketch maps and so sketch-map accuracy can be used as an external measure of subject orientation and world knowledge. Results show a high positive correlation between subjective ratings of orientation, world knowledge and sketch-map accuracy, supporting our hypothesis that sketch maps provide a valid measure of internal cognitive maps of virtual environments. Results across different worlds also suggest that sketch maps can be used to find an absolute measure for goodness of world design.},
	author = {Billinghurst, M. and Weghorst, S.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512478},
	keywords = {Virtual environment;Humans;Cognitive science;Information processing;Testing;Psychology;Decoding;Navigation;Cities and towns;Information resources},
	month = {March},
	pages = {40-47},
	title = {The use of sketch maps to measure cognitive maps of virtual environments},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512478}}

@inproceedings{512479,
	abstract = {Investigates whether subjects could separate memories of events experienced in virtual reality from real and imagined events: a decision process we term 'virtual-reality monitoring'. Participants studied 8 separate spatial configurations of red geometric objects arranged on a life-sized chessboard, 8 configurations in virtual reality (an immersive, computer-simulated world), and imagined objects in 8 other configurations. On a later source identification memory test, subjects were generally able to correctly identify the sources of the events. A 'memory characteristics questionnaire' was administered to assess differences in qualitative characteristics of memories for virtual, real and imagined events. Differences were found that could potentially serve as cues to help people decide where their memories originated. Results are interpreted within the Johnson-Raye (1981) theoretical framework.},
	author = {Hoffman, H.G. and Hullfish, K.C. and Houston, S.J.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512479},
	keywords = {Virtual reality;Laboratories;Computerized monitoring;Humans;Testing;Feedback;Navigation;Computational modeling;Computer simulation;Oceans},
	month = {March},
	pages = {48-54},
	title = {Virtual-reality monitoring},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512479}}

@inproceedings{512480,
	abstract = {An experiment was conducted on the effect of a prototype see-thru head-mounted display (HMD) on visuo-motor adaptation. When wearing video see-thru HMDs in augmented reality systems, subjects see the world around them through a pair of head-mounted video cameras. The study looked at the effects of sensory rearrangement caused by a HMD design that displaces the user's "virtual" eye position forward (165 mm) and up (62 mm) toward the spatial position of the cameras. Measures of hand-eye coordination and speed on a manual task revealed substantial perceptual costs of the eye displacement, but also evidence of adaptation. Upon first wearing the video see-thru HMD, subjects' pointing errors increased significantly along the spatial dimensions displaced (the y and z dimensions). Speed of performance on a manual task decreased by 43% compared to baseline performance. Pointing accuracy improved by about a 1/3 as subjects adapted to the sensory rearrangement but did not reach baseline performance. When subjects removed the see-thru HMD there was evidence that their hand-eye coordination had been altered by the see-thru HMD. Negative aftereffects were observed in the form of greater errors in pointing accuracy compared to baseline. Although these effects are temporary, the results may have serious practical implications for the use of see-thru HMDs by user populations who depend on accurate hand-eye coordination such as surgeons.},
	author = {Rolland, J.P. and Biocca, F.A. and Barlow, T. and Kancherla, A.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512480},
	keywords = {Cameras;Computer displays;Computer science;Augmented reality;USA Councils;Humans;Layout;Psychology;Prototypes;Displacement measurement},
	month = {March},
	pages = {56-66},
	title = {Quantification of adaptation to virtual-eye location in see-thru head-mounted displays},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512480}}

@inproceedings{512481,
	abstract = {A series of experiments is reported in which subjects performed a search-and-act spatial task in conditions of reduced resolution and exploratory freedom. Images were produced using miniature cameras, comparing static camera position, passive camera movement, and head-coupled immersive VR/teleoperation conditions. By using cameras and real light, time lags could be avoided. Video processors were used to artificially reduce spatial, and temporal resolutions. Results show that although spatial and intensity resolutions are very important in static viewing conditions, like those of traditional image-producing computer graphics, subjects can complete the puzzle in head-mounted (VR-like) conditions with resolutions as little as 18/spl times/15 pixels. Furthermore results show that animation of the image viewpoint does not always improve spatial performance when the animation is not user-controlled; in some conditions performance actually got worse by adding passive movement.},
	author = {Smets, G.J.F. and Overbeeke, K.J.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512481},
	keywords = {Spatial resolution;Cameras;Image resolution;Virtual reality;Pixel;Head;Psychology;Displays;Humans;Animation},
	month = {March},
	pages = {67-73},
	title = {Visual resolution and spatial performance: the trade-off between resolution and interactivity},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512481}}

@inproceedings{512482,
	abstract = {The paper reports the results of two experiments each investigating the sense of presence within visual and auditory virtual environments. The variables for the studies included the presence or absence of head tracking, the presence or absence of stereoscopic cues, the geometric field of view (GFOV) used to design the visual display, the presence or absence of spatialized sound and the addition of spatialized versus non-spatialized sound to a stereoscopic display. In both studies, subjects were required to navigate a virtual environment and to complete a questionnaire designed to ascertain the level of presence experienced by the participant within the virtual world. The results indicated that the reported level of presence was significantly higher when head tracking and stereoscopic cues were provided, with more presence associated with a 50 and 90 degree GFOV when compared to a narrower 10 degree GFOV. Further, the addition of spatialized sound did significantly increase ones sense of presence in the virtual environment, on the other hand, the addition of spatialized sound did not increase the apparent realism of that environment.},
	author = {Hendrix, C. and Barfield, W.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512482},
	keywords = {Virtual environment;Auditory displays;Computer displays;Human factors;Computer graphics;Industrial engineering;Navigation;Software measurement;Software design;Design engineering},
	month = {March},
	pages = {74-82},
	title = {Presence in virtual environments as a function of visual and auditory cues},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512482}}

@inproceedings{512483,
	abstract = {A new head mounted display (HMD) that provides a large field of view with a high-resolution insert is proposed and designed. Previously, this type of HMD has been designed using mechanical or sequential scanning devices, which are bulky and expensive. The proposed high-resolution insert HMD (HRI-HMD) is innovative and it uses only electronic devices that can be easily integrated with the optical components. The potential benefit of the HRI-HMD comes not only from its improved visual quality via a high-resolution insert but also from its increased human-computer interaction capability via eye tracking. The design principles and envisioned applications of the HRI-HMD are described, and the feasibility of the HRI-HMD is demonstrated by designing a prototype model.},
	author = {Yoshida, A. and Rolland, J.P. and Reif, J.H.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512483},
	keywords = {Image resolution;Application software;Virtual reality;Humans;Computer science;Computer displays;Retina;Optical devices;Prototypes;Control systems},
	month = {March},
	pages = {84-93},
	title = {Design and applications of a high-resolution insert head-mounted-display},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512483}}

@inproceedings{512484,
	abstract = {A practical and robust head-position tracking method using computer vision is presented. By combining two simple image processing techniques, this tracker can, report the position of the user's head in real time. Whole image processing is performed by software running on normal mid-range workstations. This tracker can support desk top virtual reality (also referred to as "fish tank VR"), thereby enabling a user to use a wide range of 3D systems Without having to the on any equipment. An experiment conducted by the author suggests this tracker can improve the human's ability in understanding complex 3D structures presented on the display.},
	author = {Rekimoto, J.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512484},
	keywords = {Virtual reality;Marine animals;Gears;Head;Image processing;Computer vision;Workstations;Displays;Switches;Humans},
	month = {March},
	pages = {94-100},
	title = {A vision-based head tracker for fish tank virtual reality-VR without head gear},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512484}}

@inproceedings{512485,
	abstract = {It is important to assist doctors operating intravascular surgical tools such as a catheter that is designed for minimum invasive surgery inside complex and narrow brain blood vessels. We propose a intelligent medical assistance system for operation of the intravascular surgical tool that is teleoperated by the doctor seeing a 2D X-ray image. We built a prototype of a virtual simulator system consisting of a joystick and a 3D-computer graphics display. The joystick is used for the controller of catheter head direction and the force display. We evaluated the effectiveness of the proposed visual and force assistance methods through extensive experiments.},
	author = {Arai, F. and Ito, M. and Fukuda, T. and Negoro, M. and Naito, T.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512485},
	keywords = {Minimally invasive surgery;Catheters;Biomedical imaging;Displays;Blood vessels;Intelligent systems;X-ray imaging;Virtual prototyping;Brain modeling;Medical simulation},
	month = {March},
	pages = {101-107},
	title = {Intelligent assistance for intravascular tele-surgery and experiments on virtual simulator},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512485}}

@inproceedings{512486,
	abstract = {Task definition methods for robotic systems are often difficult to use. The "on-line" programming methods are often time expensive or risky for the human operator or the robot itself. On the other hand, "off-line" techniques are tedious and complex. In addition operator training is costly and time consuming. In a Virtual Reality Robotics Environment (VRRE), users are not asked to write down complicated functions, but can operate complex robotic systems in an intuitive and cost-effective way. However a VRRE is only effective if all the environment changes and object movements are fed-back to the virtual manipulating system. The paper describes the use of a VRRE for a semi-autonomous robot system comprising an industrial 5-axis robot, its virtual equivalent and a model based vision system used as feed-back. The user is immersed in a 3-D space built out of models of the robot's environment. He directly interacts with the virtual "components", defining tasks and dynamically optimizing them. A model based vision system locates objects in the real workspace to update the VRRE through a bi-directional communication link. In order to enhance the capabilities of the VRRE, a reflex-type behavior based on vision has been implemented. By locally (independently of the VRRE) controlling the real robot, the operator is discharged of small environmental changes due to transmission delays. Thus once the tasks have been optimized on the VRRE, they are sent to the real robot and a semi autonomous process ensures their correct execution thanks to a camera directly mounted on the robot's end effector. On the other hand if the environmental changes are too important, the robot stops, re-actualizes the VRRE with the new environmental configuration, and waits for task redesign. Because the operator interacts with the robotic system at a task oriented high level, VRRE systems are easily portable to other robotics environments (mobile robotics and micro assembly).},
	author = {Natonek, E. and Zimmerman, T. and Fluckiger, L.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512486},
	keywords = {Feedback;Virtual reality;Robot vision systems;Service robots;Robotic assembly;Orbital robotics;Mobile robots;Machine vision;Robot programming;Humans},
	month = {March},
	pages = {110-117},
	title = {Model based vision as feedback for virtual reality robotics environments},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512486}}

@inproceedings{512487,
	abstract = {Human figure animation is it widely researched area with many applications. This paper addresses specific issues that deal with the synthesis, animation and environmental interaction of human figures within a virtual space teleconferencing system. A layered representation of the human figure is adopted. Skeletal posture is determined from magnetic sensors on the body, using heuristics and inverse kinematics. This paper describes the use of implicit function techniques in the synthesis and animation of a polymesh geometric skin over the skeletal structure. Implicit functions perform detection and handling of collisions with an optimal worst case time complexity that is linear in the number polymesh vertices. Body deformations resulting from auto-collisions are handled elegantly and homogeneously as part of the environment. Further, implicit functions generate precise collision contact surfaces and have the capability to model the physical characteristics of muscles in systems that employ force feedback. The real time implementation within a virtual space teleconferencing system, illustrates this new approach, coupling polymesh and implicit surface based modeling and animation techniques.},
	author = {Singh, K. and Ohya, J. and Parent, R.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512487},
	keywords = {Humans;Animation;Teleconferencing;Magnetic sensors;Kinematics;Skin;Character generation;Muscles;Force feedback;Real time systems},
	month = {March},
	pages = {118-126},
	title = {Human figure synthesis and animation for virtual space teleconferencing},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512487}}

@inproceedings{512488,
	abstract = {We describe a system for off-line production and real-time playback of motion for articulated human figures in 3D virtual environments. The key notions are (1) the logical storage of full-body motion in posture graphs, which provides a simple motion access method for playback and (2) mapping the motions of higher DOF figures to lower DOF figures using slaving to provide human models at several levels of detail, both to geometry and articulation, for later playback. We present our system in the context of a simple problem: animating human figures in a distributed simulation, using DIS protocols for communicating the human state information. We also discuss several related techniques for real-time animation of articulated figures in visual simulation.},
	author = {Granieri, J.P. and Crabtree, J. and Badler, N.I.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512488},
	keywords = {Production;Humans;Protocols;Virtual environment;Real time systems;Animation;Computational modeling;Weapons;Legged locomotion;Computational Intelligence Society},
	month = {March},
	pages = {127-135},
	title = {Production and playback of human figure motion for 3D virtual environments},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512488}}

@inproceedings{512489,
	abstract = {We propose an accurate collision detection algorithm for use in virtual reality applications. The algorithm works for three-dimensional graphical environments where multiple objects, represented as polyhedra (boundary representation), are undergoing arbitrary motion (translation and rotation). The algorithm can be used directly for both convex and concave objects and objects can be deformed (non-rigid) during motion. The algorithm works efficiently by first reducing the number of face pairs that need to be checked accurately for interference by first localizing possible collision regions using bounding box and spatial subdivision techniques; face pairs that remain after this pruning stage are then accurately checked for interference. The algorithm is efficient, simple to implement, and does not require any memory intensive auxiliary data structures to be precomputed and updated. Since polyhedral shape representation is one of the most common shape representation schemes, this algorithm should be useful to a wide audience. Performance results are given to show the efficiency of the proposed method.},
	author = {Smith, A. and Kitamura, Y. and Takemura, H. and Kishino, F.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512489},
	keywords = {Object detection;Testing;Face detection;Detection algorithms;Motion detection;Interference;Shape;Performance evaluation;Laboratories;Virtual reality},
	month = {March},
	pages = {136-145},
	title = {A simple and efficient method for accurate collision detection among deformable polyhedral objects in arbitrary motion},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512489}}

@inproceedings{512490,
	abstract = {Virtual reality toolkits and systems for computer supported cooperative work are often treated separately. However, combining them offers new possibilities for remote cooperative (or collaborative) group working. We review existing distribution models of virtual environments and propose a new method of concurrent interaction management. We examine the different types of communication layers, which are needed by collaborative virtual reality (VR) applications to achieve complex user interaction. Finally we propose a model for handling the different requirements of such applications, depending on the connection strategies used within a distributed VR system.},
	author = {Broll, W.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512490},
	keywords = {Collaboration;Virtual reality;Collaborative work;Virtual environment;Application software;Information technology;Computer science;Human computer interaction;Workstations;Joining processes},
	month = {March},
	pages = {148-155},
	title = {Interacting in distributed collaborative virtual environments},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512490}}

@inproceedings{512491,
	abstract = {This paper presents current research being undertaken at Sandia National Laboratories to develop a distributed, shared virtual reality simulation system. The architecture of the system is presented within the framework of an initial application: situational training of inspectors and escorts under programs to verify compliance with nuclear non-proliferation treaties.},
	author = {Stansfield, S. and Shawver, D. and Miner, N. and Rogers, D.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512491},
	keywords = {Virtual reality;Personnel;Inspection;Laboratories;Aerospace simulation;Machine intelligence;Aircraft;Humans;Arm;Magnetic heads},
	month = {March},
	pages = {156-161},
	title = {An application of shared virtual reality to situational training},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512491}}

@inproceedings{512492,
	abstract = {This paper presents a distributed virtual environment that supports collaboration among members of a geographically dispersed multidisciplinary team engaged in concurrent product development. The distributed virtual environment maintains a shared information space that contains product data in a standard ISO STEP compliant format. It supports a user configurable virtual environment and the integration of different CAE applications to support different engineering perspectives. The realistic manipulation of assembly models within the distributed virtual environment is supported by constraint-based 3D manipulation techniques developed at Leeds. The initial implementation of this architecture supports accurate assembly modelling and kinematic simulation for virtual prototypes and runs on a network of SGI Indy workstations over an ATM network.},
	author = {Maxfield, J. and Fernando, T. and Dew, P.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512492},
	keywords = {Virtual environment;Concurrent engineering;ISO standards;Assembly;Collaboration;Product development;Computer aided engineering;Maintenance engineering;Design engineering;Kinematics},
	month = {March},
	pages = {162-170},
	title = {A distributed virtual environment for concurrent engineering},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512492}}

@inproceedings{512493,
	abstract = {This paper describes a fast method of correcting for optical distortion in head-mounted displays (HMDs). Since the distorted display surface in an HMD is not rectilinear, the shape and location of the graphics window used with the display must be chosen carefully, and some corrections made to the predistortion model. A distortion correction might be performed with optics that reverse the distortion caused by HMD lenses, but such optics can be expensive and offer a correction for only one specific HMD. Integer incremental methods or a lookup table might be used to calculate the correction, but an I/O bottleneck makes this impractical in software. Instead, a texture map may be defined that approximates the required optical correction. Recent equipment advances allow undistorted images to be input into texture mapping hardware at interactive rates. Built in filtering handles predistortion aliasing artifacts.},
	author = {Watson, B.A. and Hodges, L.F.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512493},
	keywords = {Optical distortion;Optical filters;Displays;Predistortion;Shape;Graphics;Lenses;Table lookup;Hardware;Filtering},
	month = {March},
	pages = {172-178},
	title = {Using texture maps to correct for optical distortion in head-mounted displays},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512493}}

@inproceedings{512494,
	abstract = {This paper describes a system for calibrating the position component of a 6-degree-of-freedom magnetic tracker by comparing the output with a custom-built ultrasonic measuring system. A look-up table, created from the collected difference data, is used to interpolate for corrected values. The error of the resulting corrected magnetic tracker position is measured to be less than 5% over the calibrated range.},
	author = {Ghazisaedy, M. and Adamczyk, D. and Sandin, D.J. and Kenyon, R.V. and DeFanti, T.A.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512494},
	keywords = {Calibration;Virtual reality;Transmitters;Position measurement;Couplings;Pulse measurements;Coils;Ultrasonic variables measurement;Extraterrestrial measurements;Visualization},
	month = {March},
	pages = {179-188},
	title = {Ultrasonic calibration of a magnetic tracker in a virtual reality space},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512494}}

@inproceedings{512495,
	abstract = {This paper addresses the problem of correcting visual registration errors in video-based augmented-reality systems. Accurate visual registration between real and computer-generated objects in combined images is critically important for conveying the perception that both types of object occupy the same 3-dimensional (3D) space. To date, augmented-reality systems have concentrated on simply improving 3D coordinate system registration in order to improve apparent (image) registration error. This paper introduces the idea of dynamically measuring registration error in combined images (2D error) and using that information to correct 3D coordinate system registration error which in turn improves registration in the combined images. Registration can be made exact in every combined image if a small video delay can be tolerated. Our experimental augmented-reality system achieves improved image registration, stability, and error tolerance from tracking system drift and jitter over current augmented-reality systems. No additional tracking hardware or other devices are needed on the user's head-mounted display. Computer-generated objects can be "nailed" to real-world reference points in every image the user sees with an easily-implemented algorithm. Dynamic error correction as demonstrated here will likely be a key component of future augmented-reality systems.},
	author = {Bajura, M. and Neumann, U.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512495},
	keywords = {Augmented reality;Error correction;Computer errors;Coordinate measuring machines;Delay;Image registration;Stability;Jitter;Hardware;Computer displays},
	month = {March},
	pages = {189-196},
	title = {Dynamic registration correction in augmented-reality systems},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512495}}

@inproceedings{512496,
	abstract = {A novel compact hand master device with force feedback is presented. The Second Generation Rutgers Master (RM-II) integrates position-sensing and force-feedback to multiple fingers in a single structure, without the use of sensing gloves. The paper first discusses the kinematics and calibration followed by the integration of the device into a single-user, ethernet-distributed, virtual reality (VR) environment. The VR simulation features: visual feedback, force feedback, interactive sound and object interaction.},
	author = {Gomez, D. and Burdea, G. and Langrana, N.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512496},
	keywords = {Virtual reality;Force feedback;Fingers;Haptic interfaces;Shafts;Man machine systems;Laboratories;Kinematics;Calibration;Computational modeling},
	month = {March},
	pages = {198-202},
	title = {Integration of the Rutgers Master II in a virtual reality simulation},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512496}}

@inproceedings{512497,
	abstract = {A method of intermediate space for controlling haptic interfaces is characterized by updating a virtual plane at a low frequency while maintaining a high update rate at force control loop of the interface. By using the virtual plane, the detection of collisions between the tip of finger and virtual objects became independent from the control of the haptic interface. This will enable the haptic interface to display more and more complex surfaces in keeping the same sampling frequency of impedance control. It was revealed by the experiments with a haptic interface SPICE that an operator could touch and trace smoothly on a curved surface of stiff virtual object, even if the update rate of the virtual plane is relatively low.},
	author = {Adachi, Y. and Kumano, T. and Ogino, K.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512497},
	keywords = {Haptic interfaces;Surface impedance;Virtual environment;Object detection;Force control;Sampling methods;Fingers;Frequency;Displays;Virtual reality},
	month = {March},
	pages = {203-210},
	title = {Intermediate representation for stiff virtual objects},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512497}}

@inproceedings{512498,
	abstract = {Force feedback can be thought as a concept which was motivated by the phenomenon of contact. Efforts were made to describe cross sections in interfaces and methodologies to realize force feedback from differences in the cross sections. Based on this discussion. The concept of surface display was submitted and technical issues for the implementation of the concept were pointed out. A prototype device to demonstrate the concept was created and solutions for technical issues mere suggested. The concept of surface display is an idea to present the surface of a virtual object itself to the user, rather than the sensation of force or tactile caused by the contact with the virtual object. The prototype device consisted of a mechanism to form arbitrary surfaces and sensors to measure the force affected on the surface. The control and calculation methods for this device are also described.},
	author = {Hirota, K. and Hirose, M.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512498},
	keywords = {Virtual reality;Force feedback;Humans;Prototypes;Force control;Computer displays;Skin;Computational modeling;Computer simulation;Object detection},
	month = {March},
	pages = {211-216},
	title = {Simulation and presentation of curved surface in virtual reality environment through surface display},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512498}}

@inproceedings{512499,
	abstract = {We describe the structure of a force display recently implemented for precision manipulation of scaled or virtual environments. We discuss the advantages of direct-drive parallel manipulators over geared serial manipulators for human-robot interaction application and introduce the serial-parallel structure we chose for our robot which interfaces with the human operator either at the fingertip or at the tip of a freely held pen-like instrument. We derive the statics and the dynamics, and then introduce the optimization criteria that allowed us to choose the dimensional parameters for the force display. Finally we show some of the potential application for this device.},
	author = {Buttolo, P. and Hannaford, B.},
	booktitle = {Proceedings Virtual Reality Annual International Symposium '95},
	date-added = {2024-03-18 02:32:28 -0400},
	date-modified = {2024-03-18 02:32:28 -0400},
	doi = {10.1109/VRAIS.1995.512499},
	keywords = {Displays;Virtual reality;Friction;Manipulator dynamics;Virtual environment;Surges;Frequency;Bandwidth;Human robot interaction;Humanoid robots},
	month = {March},
	pages = {217-224},
	title = {Pen-based force display for precision manipulation in virtual environments},
	year = {1995},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1995.512499}}

@inproceedings{10108083,
	abstract = {The decoupled relationship between the optical and inertial information in virtual reality is commonly acknowledged as a major factor contributing to cybersickness. Based on laws of physics, we noticed that a slope naturally affords acceleration, and the gravito-inertial force we experience when we are accelerating freely on a slope has the same relative direction and approximately the same magnitude as the gravity we experience when standing on the ground. This provides the opportunity to simulate a slope by manipulating the orientation of virtual objects accordingly with the accelerating optical flow. In this paper, we present a novel space deformation technique that deforms the virtual environment to replicate the structure of a slope when the user accelerates virtually. As a result, we can restore the physical relationship between the optical and inertial information available to the user. However, the changes to the geometry of the virtual environment during space deformation remain perceptible to users. Consequently, we created two different transition effects, pinch and tilt, which provide different visual experiences of ground bending. A human subject study (N=87) was conducted to evaluate the effects of space deformation on both slope perception and cyber-sickness. The results confirmed that the proposed technique created a strong feeling of traveling on a slope, but no significant differences were found on measures of discomfort and cybersickness.},
	author = {Nie, Tongyu and Adhanom, Isayas Berhe and Rosenberg, Evan Suma},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00081},
	issn = {2642-5254},
	keywords = {Visualization;Solid modeling;Three-dimensional displays;Deformation;Cybersickness;Virtual environments;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Com-puting methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;},
	month = {March},
	pages = {658-668},
	title = {Like a Rolling Stone: Effects of Space Deformation During Linear Acceleration on Slope Perception and Cybersickness},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00081}}

@inproceedings{10108084,
	abstract = {Extended reality (XR) devices, including augmented, virtual, and mixed reality, provide a deeply immersive experience. However, practical limitations like weight, heat, and comfort put extreme constraints on the performance, power consumption, and image quality of such systems. In this paper, we study how these constraints form the tradeoff between Fixed Foveated Rendering (FFR), Gaze-Tracked Foveated Rendering (TFR), and conventional, non-foveated rendering. While existing papers have often studied these methods, we provide the first comprehensive study of their relative feasibility in practical systems with limited battery life and computational budget. We show that TFR with the added cost of the gaze-tracker can often be more expensive than FFR. Thus, we co-design a gaze-tracked foveated renderer considering its benefits in computation, power efficiency, and tradeoffs in image quality. We describe principled approximations for eye tracking which provide up to a 9x speedup in runtime performance with approximately a 20x improvement in energy efficiency when run on a mobile GPU. In isolation, these approximations appear to significantly degrade the gaze quality, but appropriate compensation in the visual pipeline can mitigate the loss. Overall, we show that with a highly optimized gaze-tracker, TFR is feasible compared to FFR, resulting in up to 1.25x faster frame times while also reducing total energy consumption by over 40%.},
	author = {Singh, Rahul and Huzaifa, Muhammad and Liu, Jeffrey and Patney, Anjul and Sharif, Hashim and Zhao, Yifan and Adve, Sarita},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00036},
	issn = {2642-5254},
	keywords = {Image quality;Performance evaluation;Visualization;Three-dimensional displays;Runtime;Power demand;Pipelines;Human-centered computing---Visualization---Visualization techniques---Treemaps;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {205-214},
	title = {Power, Performance, and Image Quality Tradeoffs in Foveated Rendering},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00036}}

@inproceedings{10108085,
	abstract = {As locomotion is an important factor in improving Virtual Reality (VR) immersion and usability, research in this area has been and continues to be a crucial aspect for the success of VR applications. In recent years, a variety of techniques have been developed and evaluated, ranging from abstract control, vehicle, and teleportation techniques to more realistic techniques such as motion, gestures, and gaze. However, when it comes to hands-free scenarios, for example to increase the overall accessibility of an application or in medical scenarios under sterile conditions, most of the announced techniques cannot be applied. This is where the use of speech as an intuitive means of navigation comes in handy. As systems become more capable of understanding and producing speech, voice interfaces become a valuable alternative for input on all types of devices. This takes the quality of hands-free interaction to a new level. However, intuitive user-assisted speech interaction is difficult to realize due to semantic ambiguities in natural language utterances as well as the high real-time requirements of these systems. In this paper, we investigate steering-based locomotion and selection-based locomotion using three speech-based, hands-free methods and compare them with leaning as an established alternative. Our results show that landmark-based locomotion is a convenient, fast, and intuitive way to move between locations in a VR scene. Furthermore, we show that in scenarios where landmarks are not available, number grid-based navigation is a successful solution. Based on this, we conclude that speech is a suitable alternative in hands-free scenar-ios, and exciting ideas are emerging for future work focused on developing hands-free ad hoc navigation systems for scenes where landmarks do not exist or are difficult to articulate or recognize.},
	author = {Hombeck, Jan and Voigt, Henrik and Heggemann, Timo and Datta, Rabi R. and Lawonn, Kai},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00028},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Navigation;Semantics;Virtual reality;Speech recognition;Teleportation;Human-centered computing-Human computer interaction (HCI);Computing methodologies-Artificial intelligence-Natural language processing Speech recognition},
	month = {March},
	pages = {123-134},
	title = {Tell Me Where To Go: Voice-Controlled Hands-Free Locomotion for Virtual Reality Systems},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00028}}

@inproceedings{10108086,
	abstract = {The expression of human emotion is integral to social interaction, and in virtual reality it is increasingly common to develop virtual avatars that attempt to convey emotions by mimicking these visual and aural cues, i.e. the facial and vocal expressions. However, errors in (or the absence of) facial tracking can result in the rendering of incorrect facial expressions on these virtual avatars. For example, a virtual avatar may speak with a happy or unhappy vocal inflection while their facial expression remains otherwise neutral. In circumstances where there is conflict between the avatar's facial and vocal expressions, it is possible that users will incorrectly interpret the avatar's emotion, which may have unintended consequences in terms of social influence or in terms of the outcome of the interaction. In this paper, we present a human-subjects study (N = 22) aimed at understanding the impact of conflicting facial and vocal emotional expressions. Specifically we explored three levels of emotional valence (unhappy, neutral, and happy) expressed in both visual (facial) and aural (vocal) forms. We also investigate three levels of head scales (down-scaled, accurate, and up-scaled) to evaluate whether head scale affects user interpretation of the conveyed emotion. We find significant effects of different multimodal expressions on happiness and trust perception, while no significant effect was observed for head scales. Evidence from our results suggest that facial expressions have a stronger impact than vocal expressions. Additionally, as the difference between the two expressions increase, the less predictable the multimodal expression becomes. For example, for the happy-looking and happy-sounding multimodal expression, we expect and see high happiness rating and high trust, however if one of the two expressions change, this mismatch makes the expression less predictable. We discuss the relationships, implications, and guidelines for social applications that aim to leverage multimodal social cues.},
	author = {Choudhary, Zubin and Norouzi, Nahal and Erickson, Austin and Schubert, Ryan and Bruder, Gerd and Welch, Gregory F.},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00072},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Avatars;Design methodology;User interfaces;Dynamic range;Rendering (computer graphics);Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods-User studies;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms-Virtual reality},
	month = {March},
	pages = {571-580},
	title = {Exploring the Social Influence of Virtual Humans Unintentionally Conveying Conflicting Emotions},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00072}}

@inproceedings{10108407,
	abstract = {Projector Depth-of-Field (DOF) refers to the projection range of projector images in focus. It is a crucial property of projectors in spatial augmented reality (SAR) applications since wide projector DOF can increase the effective projection area on the projection surfaces with large depth variances and thus reduce the number of projectors required. Existing state-of-the-art methods attempt to create all-in-focus displays by adopting either a deep deblurring network or light modulation. Unlike previous work that considers the optimization of the deblurring model and physic modulation separately, in this paper, we propose an end-to-end joint optimization method to learn a diffractive optical element (DOE) placed in front of a projector lens and a compensation network for deblurring. Using the desired image and the captured projection result image, the compensation network can directly output the compensated image for display. We evaluate the proposed method in physical simulation and with a real experimental prototype, showing that the proposed method can extend the projector DOF by a minor modification to the projector and thus superior to the normal projection with a shallow DOF. The compensation method is also compared with the state-of-the-art methods and shows the advance in radiometric compensation in terms of computational efficiency and image quality.},
	author = {Li, Yuqi and Fu, Qiang and Heidrich, Wolfgang},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00060},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Spatial augmented reality;Prototypes;Optimization methods;Virtual reality;User interfaces;Computing methodologies-Computer graphics-Image manipulation-Image processing},
	month = {March},
	pages = {449-459},
	title = {Extended Depth-of-Field Projector using Learned Diffractive Optics},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00060}}

@inproceedings{10108409,
	abstract = {Virtual reality (VR) technologies are used in a diverse range of applications. Many of these involve an embodied conversational agent (ECA), a virtual human who exchanges information with the user. Unfortunately, VR technologies remain inaccessible to many users due to the phenomenon of cybersickness: a collection of negative symptoms such as nausea and headache that can appear when immersed in a simulation. Many factors are believed to affect a user's level of cybersickness, but little is known regarding how these factors may influence a user's opinion of an ECA. In this study, we examined the effects of virtual stairs, a factor associated with increased levels of cybersickness. We recruited 39 participants to complete a simulated airport experience. This involved a simple navigation task followed by a brief conversation with a virtual airport customs agent in Spanish. Participants completed the experience twice, once walking across flat hallways, and once traversing a series of staircases. We collected self-reported ratings of cybersickness, presence, and perception of the ECA. We additionally collected physiological data on heart rate and galvanic skin response. Results indicate that the virtual staircases increased user level's of cybersickness and reduced their perceived realism of the ECA, but increased levels of presence.},
	author = {Ang, Samuel and Fernandez, Amanda and Rushforth, Michael and Quarles, John},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00071},
	issn = {2642-5254},
	keywords = {Geometry;Solid modeling;Three-dimensional displays;Cybersickness;Navigation;Stairs;User interfaces;Human-centered computing---Human computer interaction (HCI)---Empirical studies in HCI;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality},
	month = {March},
	pages = {561-570},
	title = {You Make Me Sick! The Effect of Stairs on Presence, Cybersickness, and Perception of Embodied Conversational Agents},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00071}}

@inproceedings{10108410,
	abstract = {We present CoboDeck - our proof-of-concept immersive virtual reality haptic system with free walking support. It provides prop-based encountered-type haptic feedback with a mobile robotic platform. Intended for use as a design tool for architects, it enables the user to directly and intuitively interact with virtual objects like walls, doors, or furniture. A collaborative robotic arm mounted on an omnidirectional mobile platform can present a physical prop that matches the position and orientation of a virtual counterpart anywhere in large virtual and real environments. We describe the concept, hardware, and software architecture of our system. Furthermore, we present the first behavioral algorithm tailored for the unique challenges of safe human-robot haptic interaction in VR, explicitly targeting availability and safety while the user is unaware of the robot and can change trajectory at any time. We explain our high-level state machine that controls the robot to follow a user closely and rapidly escape from him as required by the situation. We present our technical evaluation. The results suggest that our chasing approach saves time, decreases the travel distance and thus battery usage, compared to more traditional approaches for mobile platforms assuming a fixed parking position between interactions. We also show that the robot can escape from the user and prevent a possible collision within a mean time of 1.62 s. Finally, we confirm the validity of our approach in a practical validation and discuss the potential of the proposed system.},
	author = {Mortezapoor, Soroosh and Vasylevska, Khrystyna and Vonach, Emanuel and Kaufmann, Hannes},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00045},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Collaboration;Virtual reality;Haptic interfaces;Safety;Behavioral sciences;Trajectory;Computer systems organization---Embedded and cyberphysical systems---Robotics;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Human-centered computing---Human computer interaction (HCI)---Interaction devices---Haptic devices;},
	month = {March},
	pages = {297-307},
	title = {CoboDeck: A Large-Scale Haptic VR System Using a Collaborative Mobile Robot},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00045}}

@inproceedings{10108411,
	abstract = {With redirected walking (RDW) technology, people can explore large virtual worlds in smaller physical spaces. RDW controls the trajectory of the user's walking in the physical space through subtle adjustments, so as to minimize the collision between the user and the physical space. Previous predictive algorithms place constraints on the user's path according to the spatial layouts of the virtual environment and work well when applicable, while reactive algorithms are more general for scenarios involving free exploration or uncon-strained movements. However, even in relatively free environments, we can predict the user's walking to a certain extent by analyzing the user's historical walking data, which can help the decision-making of reactive algorithms. This paper proposes a novel RDW method that improves the effect of real-time unrestricted RDW by analyzing and utilizing the user's historical walking data. In this method, the physical space is discretized by considering the user's location and orientation in the physical space. Using the weighted directed graph obtained from the user's historical walking data, we dynamically update the scores of different reachable poses in the physical space during the user's walking. We rank the scores and choose the optimal target position and orientation to guide the user to the best pose. Since simulation experiments have been shown to be effective in many previous RDW studies, we also provide a method to simulate user walking trajectories and generate a dataset. Experiments show that our method outperforms multiple state-of-the-art methods in various environments of different sizes and spatial layouts.},
	author = {Fan, Cheng-Wei and Xu, Sen-Zhe and Yu, Peng and Zhang, Fang-Lue and Zhang, Song-Hai},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00021},
	issn = {2642-5254},
	keywords = {Legged locomotion;Solid modeling;Three-dimensional displays;Layout;Virtual environments;Aerospace electronics;User interfaces;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
	month = {March},
	pages = {53-62},
	title = {Redirected Walking Based on Historical User Walking Data},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00021}}

@inproceedings{10108412,
	abstract = {Scholars who study nonverbal behavior have focused an incredible amount of work on proxemics, how close people stand to one another, and mutual gaze, whether or not they are looking at one another. Moreover, many studies have demonstrated a correlation between gaze and distance, and so-called equilibrium theory posits that people modulate gaze and distance to maintain proper levels of nonverbal intimacy. Virtual reality scholars have also focused on these two constructs, both for theoretical reasons, as distance and gaze are often used as proxies for psychological constructs such as social presence, and for methodological reasons, as head orientation and body position are automatically produced by most VR tracking systems. However, to date, the studies of distance and gaze in VR have largely been conducted in laboratory settings, observing behavior of a small number of participants for short periods of time. In this experimental field study, we analyze the proxemics and gaze of 232 participants over two experimental studies who each contributed up to about 240 minutes of tracking data during eight weekly 30-minute social virtual reality sessions. Participants' non-verbal behaviors changed in conjunction with context manipulations and over time. Interpersonal distance increased with the size of the virtual room; and both mutual gaze and interpersonal distance increased over time. Overall, participants oriented their heads toward the center of walls rather than to corners of rectangularly-aligned environments. Finally, statistical models demonstrated that individual differences matter, with pairs and groups maintaining more consistent differences over time than would be predicted by chance. Implications for theory and practice are discussed.},
	author = {Miller, Mark Roman and DeVeaux, Cyan and Han, Eugy and Ram, Nilam and Bailenson, Jeremy N.},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00056},
	issn = {2642-5254},
	keywords = {Solid modeling;Social computing;Three-dimensional displays;Correlation;Affordances;Virtual environments;Psychology;Human-centered computing-Human computer interaction (HCI-Interaction paradigms-Virtual reality;Human-centered computing-Collaborative and social computing-Empirical studies in collaborative and social computing},
	month = {March},
	pages = {409-417},
	title = {A Large-Scale Study of Proxemics and Gaze in Groups},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00056}}

@inproceedings{10108413,
	abstract = {A recent surge in the application of Virtual Reality in education has made VR Learning Environments (VRLEs) prevalent in fields ranging from aviation, medicine, and skill training to teaching factual and conceptual content. In spite of multiple 3D affordances provided by VR, learning content placement in VRLEs has been mostly limited to a static placement in the environment. We conduct two studies to investigate the effect of different spatial representations of learning content in virtual environments on learning outcomes and user experience. In the first study, we studied the effects of placing content at four different places - world-anchored (TV screen placed in the environment), user-anchored (panel anchored to the wrist or head-mounted display of the user) and object-anchored (panel anchored to the object associated with current content) - in the VR environment with forty-two participants in the context of learning how to operate a laser cutting machine through an immersive tutorial. In the follow-up study, twenty-two participants from this study were given the option to choose from these four placements to understand their preferences. The effects of placements were examined on learning outcome measures - knowledge gain, knowledge transfer, cognitive load, user experience, and user preferences. We found that participants preferred user-anchored (controller condition) and object-anchored placement. While knowledge gain, knowledge transfer, and cognitive load were not found to be significantly different between the four conditions, the object-anchored placement scored significantly better than the TV screen and head-mounted display conditions on the user experience scales of attractiveness, stimulation, and novelty.},
	author = {Belani, Manshul and Singh, Harsh Vardhan and Parnami, Aman and Singh, Pushpendra},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00019},
	issn = {2642-5254},
	keywords = {Wrist;Three-dimensional displays;TV;Head-mounted displays;Virtual environments;Tutorials;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Interaction design-Empirical studies in interaction design},
	month = {March},
	pages = {33-43},
	title = {Investigating Spatial Representation of Learning Content in Virtual Reality Learning Environments},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00019}}

@inproceedings{10108414,
	abstract = {This paper presents a within-subject study to investigate the effects of leading and following behaviors on user visual attention behaviors when collaborating with a virtual agent (VA) during performing transportation tasks in immersive virtual environments. The task was to carry a target object from a location to a predefined location. There were two conditions, namely leader VA (LVA) and follower VA (FVA). The leader gave instructions to the follower to perform actions. In the FVA condition, users played the leader role, while they played the follower role in the LVA condition. The users and the VA communicated via spoken language. During the experiment, participants wore a head-mounted display and performed real walking in a room. In each condition, each participant performed 20 trials of object transportation for different types of objects. Our preliminary results revealed significant differences in the user visual attention behaviors between the follower and leader VA conditions during the transportation tasks.},
	author = {Wong, Sai-Keung and Volonte, Matias and Liu, Kuan-Yu and Ebrahimi, Elham and Babu, Sabarish V.},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00031},
	issn = {2642-5254},
	keywords = {Legged locomotion;Visualization;Solid modeling;Three-dimensional displays;Multimedia systems;Transportation;Virtual environments;Virtual Agents;Virtual Reality;Behavior Modeling;Human-Computer Interaction;Animation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Animations;Evaluation/methodology;I.3.3 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality;},
	month = {March},
	pages = {152-162},
	title = {Comparing Visual Attention with Leading and Following Virtual Agents in a Collaborative Perception-Action Task in VR},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00031}}

@inproceedings{10108417,
	abstract = {The development of methods to simulate the sensation of wind that can promote relaxation and elicit positive emotional responses has become a topic of interest with the widespread adoption of virtual and augmented reality systems. Previous studies have simulated natural wind by varying wind speed in a controlled environment or moving a large flow of air through an area. In contrast to such approaches to modulate physical airflow, the use of multisensory stimuli to alter the impression and sense of comfort provided by a simulated wind has rarely been considered in previous research. If visual and auditory stimuli affect wind comfort, a multisensory design should be considered for relaxation systems that use wind effects. Therefore, we experimentally measured wind comfort and associated emotions when participants experienced outdoor and indoor virtual environments through immersive virtual reality to investigate whether cross-modal effects of variations in audio-visual stimuli would impact the relaxation effects associated with a virtual wind. The results show that the virtual environment of an outdoor meadow and the sound of natural wind significantly improved users' subjective experience of comfort and openness associated with the wind, as well as their emotional state. Simulated natural wind reduced mental stress compared to a condition without wind, as shown by questionnaires and biometric data. The results of this study indicate that multisensory stimuli conveying natural impressions and simulated natural wind are effective for wind-based relaxation.},
	author = {Ito, Kenichi and Hosoi, Juro and Ban, Yuki and Kikuchi, Takayuki and Nakagawa, Kyosuke and Kitagawa, Hanako and Murakami, Chizuru and Imai, Yosuke and Warisawa, Shin'ichi},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00037},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Wind speed;Virtual environments;Resists;Human factors;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
	month = {March},
	pages = {215-225},
	title = {Wind comfort and emotion can be changed by the cross-modal presentation of audio-visual stimuli of indoor and outdoor environments},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00037}}

@inproceedings{10108418,
	abstract = {Virtual Reality (VR) can completely immerse users in a virtual world and provide little awareness of bystanders in the surrounding physical environment. Current technologies use predefined guardian area visualizations to set safety boundaries for VR interactions. However, bystanders cannot perceive these boundaries and may collide with VR users if they accidentally enter guardian areas. In this paper, we investigate four awareness techniques on mobile phones and smartwatches to help bystanders avoid invading guardian areas. These techniques include augmented reality boundary overlays and visual, auditory, and haptic alerts indicating bystanders' distance from guardians. Our findings suggest that the proposed techniques effectively keep participants clear of the safety boundaries. More specifically, using augmented reality overlays, participants could avoid guardians with less time, and haptic alerts caused less distraction.},
	author = {Wu, Sixuan and Li, Jiannan and Sousa, Maur{\'\i}cio and Grossman, Tovi},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00078},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Shape;User interfaces;Mobile handsets;Haptic interfaces;Safety;Human-centered computing-Visualization-Visualization techniques-Treemaps;Human-centered computing-;Visualization-Visualization design and evaluation methods},
	month = {March},
	pages = {631-640},
	title = {Investigating Guardian Awareness Techniques to Promote Safety in Virtual Reality},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00078}}

@inproceedings{10108419,
	abstract = {In this study, we aim to improve the experience of virtual reality (VR) shooting games by employing a 3D haptic guidance method using necklace-type and belt-type haptic devices. Such devices help to modulate the vibrations generated by and synchronized with musical signals according to the azimuth and height of a target in 3D space, which is expected to improve the gaming experience by providing 3D guidance and enhancing the music-listening experience. For the first step, we evaluated the method's potential by conducting an experiment in which participants were asked to shoot a randomly spawned target moving in 3D VR space. The experiment applied four conditions: the proposed method (Haptic), displaying 3D radar (Vision) to represent the visualization method, no guidance (None), and a combination of Haptic and Vision (VisHap). Outcomes related to the success rate and accomplishment time (of the shooting task), the number of head rotations, and participant responses to a follow-up questionnaire revealed that Haptic performed significantly better than None but was inferior to Vision, indicating that the proposed method succeeded in terms of effectively providing 3D guidance. VisHap performed roughly as well as Vision and was preferred to other conditions in most cases, indicating the general usefulness of the proposed method. Meanwhile, the findings from the questionnaire suggest that although the modular vibrations improved the music-listening experience during the shooting task, the impact on the overall gaming experience is unclear. This warrants further research.},
	author = {Yamazaki, Yusuke and Hasegawa, Shoichi},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00043},
	issn = {2642-5254},
	keywords = {Vibrations;Visualization;Three-dimensional displays;Music;Virtual reality;Radar;Games;Human-centered computing-Haptic devices;Human-centered computing-Virtual reality;Human-centered computing-Sound-based input / output},
	month = {March},
	pages = {276-285},
	title = {Providing 3D Guidance and Improving the Music-Listening Experience in Virtual Reality Shooting Games Using Musical Vibrotactile Feedback},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00043}}

@inproceedings{10108420,
	abstract = {Virtual Reality (VR) telepresence platforms are being challenged to support live performances, sporting events, and conferences with thousands of users across seamless virtual worlds. Current systems have struggled to meet these demands which has led to high-profile performance events with groups of users isolated in parallel sessions. The core difference in scaling VR environments compared to classic 2D video content delivery comes from the dynamic peer-to-peer spatial dependence on communication. Users have many pair-wise interactions that grow and shrink as they explore spaces. In this paper, we discuss the challenges of VR scaling and present an architecture that supports hundreds of users with spatial audio and video in a single virtual environment. We leverage the property of spatial locality with two key optimizations: (1) a Quality of Service (QoS) scheme to prioritize audio and video traffic based on users' locality, and (2) a resource manager that allocates client connections across multiple servers based on user proximity within the virtual world. Through real-world deployments and extensive evaluations under real and simulated environments, we demonstrate the scalability of our platform while showing improved QoS compared with existing approaches.},
	author = {Dasari, Mallesham and Lu, Edward and Farb, Michael W. and Pereira, Nuno and Liang, Ivan and Rowe, Anthony},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00080},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Telepresence;Scalability;Spatial audio;Virtual environments;Quality of service;User interfaces;VR.;Video.;Conferencing.},
	month = {March},
	pages = {648-657},
	title = {Scaling VR Video Conferencing},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00080}}

@inproceedings{10108421,
	abstract = {Volumetric video (VV) recently emerges as a new form of video application providing a photorealistic immersive 3D viewing experience with 6 degree-of-freedom (DoF), which empowers many applications such as VR, AR, and Metaverse. A key problem therein is how to stream the enormous size VV through the network with limited bandwidth. Existing works mostly focused on predicting the viewport for a tiling-based adaptive VV streaming, which however only has quite a limited effect on resource saving. We argue that the content repeatability in the viewport can be further leveraged, and for the first time, propose a client-side cache-assisted strategy that aims to buffer the repeatedly appearing VV tiles in the near future so as to reduce the redundant VV content transmission. The key challenges exist in three aspects, including (1) feature extraction and mining in 6 DoF VV context, (2) accurate long-term viewing pattern estimation and (3) optimal caching scheduling with limited capacity. In this paper, we propose CaV3, an integrated cache-assisted viewport adaptive VV streaming framework to address the challenges. CaV3 employs a Long-short term Sequential prediction model (LSTSP) that achieves accurate short-term, mid-term and long-term viewing pattern prediction with a multi-modal fusion model by capturing the viewer's behavior inertia, current attention, and subjective intention. Besides, CaV3 also contains a contextual MAB-based caching adaptation algorithm (CCA) to fully utilize the viewing pattern and solve the optimal caching problem with a proved upper bound regret. Compared to existing VV datasets only containing single or co-located objects, we for the first time collect a comprehensive dataset with sufficient practical unbounded 360$\,^{\circ}$ scenes. The extensive evaluation of the dataset confirms the superiority of CaV3, which outperforms the SOTA algorithm by 15.6%-43% in viewport prediction and 13%-40% in system utility.},
	author = {Liu, Junhua and Zhu, Boxiang and Wang, Fangxin and Jin, Yili and Zhang, Wenyi and Xu, Zihan and Cui, Shuguang},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00033},
	issn = {2642-5254},
	keywords = {Adaptation models;Solid modeling;Three-dimensional displays;Upper bound;Virtual reality;Streaming media;Predictive models},
	month = {March},
	pages = {173-183},
	title = {CaV3: Cache-assisted Viewport Adaptive Volumetric Video Streaming},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00033}}

@inproceedings{10108422,
	abstract = {In Virtual Reality (VR), a growing number of applications involve verbal communications with avatars, such as for teleconference, entertainment, virtual training, social networks, etc. In this context, our paper aims to investigate how tactile feedback consisting in vibrations synchronized with speech could influence aspects related to VR social interactions such as persuasion, co-presence and leadership. We conducted two experiments where participants embody a first-person avatar attending a virtual meeting in immersive VR. In the first experiment, participants were listening to two speaking virtual agents and the speech of one agent was augmented with vibrotactile feedback. Interestingly, the results show that such vibrotactile feedback could significantly improve the perceived co-presence but also the persuasiveness and leadership of the haptically-augmented agent. In the second experiment, the participants were asked to speak to two agents, and their own speech was augmented or not with vibrotactile feedback. The results show that vibrotactile feedback had again a positive effect on co-presence, and that participants perceive their speech as more persuasive in presence of haptic feedback. Taken together, our results demonstrate the strong potential of haptic feedback for supporting social interactions in VR, and pave the way to novel usages of vibrations in a wide range of applications in which verbal communication plays a prominent role.},
	author = {Saint-Aubert, Justine and Argelaguet, Ferran and Mac{\'e}, Marc and Pacchierotti, Claudio and Amedi, Amir and L{\'e}cuyer, Anatole},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00070},
	issn = {2642-5254},
	keywords = {Vibrations;Training;Leadership;Three-dimensional displays;Avatars;Tactile sensors;User interfaces;Audio;Haptic;Vibrotactile feedback;Speech;Co-Presence;Leadership;Persuasion},
	month = {March},
	pages = {552-560},
	title = {Persuasive Vibrations: Effects of Speech-Based Vibrations on Persuasion, Leadership, and Co-Presence During Verbal Communication in VR},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00070}}

@inproceedings{10108424,
	abstract = {This paper looks at how mixed reality can be used for the improvement and enhancement of Chinese acupuncture practice through the introduction of an acupuncture training simulator. A prototype system developed for our study allows practitioners to insert virtual needles using their bare hands into a full-scale 3D representation of the human body with labelled acupuncture points. This provides them with a safe and natural environment to develop their acupuncture skills simulating the actual physical process of acupuncture. It also helps them to develop their muscle memory for acupuncture and better develops their memory of acupuncture points through a more immersive learning experience. We describe some of the design decisions and technical challenges overcome in the development of our system. We also present the results of a comparative user evaluation with potential users aimed at assessing the viability of such a mixed reality system being used as part of their training and development. The results of our evaluation reveal the training system outperformed in the enhancement of spatial understanding as well as improved learning and dexterity in acupuncture practice. These results go some way to demonstrating the potential of mixed reality for improving practice in therapeutic medicine.},
	author = {Sun, Qilei and Huang, Jiayou and Zhang, Haodong and Craig, Paul and Yu, Lingyun and Lim, Eng Gee},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00042},
	issn = {2642-5254},
	keywords = {Training;Solid modeling;Three-dimensional displays;Mixed reality;Prototypes;Virtual reality;User interfaces;Mixed reality---immersive technologies---virtual environments---;Chinese acupuncture---Medical education},
	month = {March},
	pages = {265-275},
	title = {Design and Development of a Mixed Reality Acupuncture Training System},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00042}}

@inproceedings{10108425,
	abstract = {Recent research advance has significantly improved the visual real-ism of immersive 3D video communication. In this work we present a method to further enhance this immersive experience by adding the hand touch capability (``remote hand clapping''). In our system, each meeting participant sits in front of a large screen with haptic feedback. The local participant can reach his hand out to the screen and perform hand clapping with the remote participant as if the two participants were only separated by a virtual glass. A key challenge in emulating the remote hand touch is the realistic rendering of the participant's hand and arm as the hand touches the screen. When the hand is very close to the screen, the RGBD data required for realistic rendering is no longer available. To tackle this challenge, we present a dual representation of the user's hand. Our dual representation not only preserves the high-quality rendering usually found in recent image-based rendering systems but also allows the hand to reach to the screen. This is possible because the dual representation includes both an image-based model and a 3D geometry-based model, with the latter driven by a hand skeleton tracked by a side view camera. In addition, the dual representation provides a distance-based fusion of the image-based and 3D geometry-based models as the hand moves closer to the screen. The result is that the image-based and 3D geometry-based models mutually enhance each other, leading to realistic and seamless rendering. Our experiments demonstrate that our method provides consistent hand contact experience between remote users and improves the immersive experience of 3D video communication.},
	author = {Zhang, Yizhong and Li, Zhiqi and Xu, Sicheng and Li, Chong and Yang, Jiaolong and Tong, Xin and Guo, Baining},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00016},
	issn = {2642-5254},
	keywords = {Solid modeling;Visualization;Three-dimensional displays;Tracking;Immersive experience;Glass;User interfaces;Human-centered computing-Collaborative and social computing;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
	month = {March},
	pages = {1-10},
	title = {RemoteTouch: Enhancing Immersive 3D Video Communication with Hand Touch},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00016}}

@inproceedings{10108426,
	abstract = {In this study, we propose a method for an aerial display. The method uses a high-speed gaze control system and a laser display to perform projection mapping on a screen at a distance, which is suspended from a flying drone. A prototype system was developed and successfully demonstrated dynamic projection mapping on a screen attached to a flying drone at a distance of about 36 m, which indicated the effectiveness of the proposed method.},
	author = {Iuchi, Masatoshi and Hirohashi, Yuito and Oku, Hiromasa},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00075},
	issn = {2642-5254},
	keywords = {Meters;Three-dimensional displays;Image resolution;Prototypes;Virtual reality;User interfaces;Control systems;Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality;Hardware-Communication hardware;interfaces and storage-Displays and imagers},
	month = {March},
	pages = {603-608},
	title = {Proposal for an aerial display using dynamic projection mapping on a distant flying screen},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00075}}

@inproceedings{10108427,
	abstract = {This paper provided empirical knowledge of the user experience for using collaborative visualization in a distributed asymmetrical setting through controlled user studies. With the ability to access various computing devices, such as Virtual Reality (VR) head-mounted displays, scenarios emerge when collaborators have to or prefer to use different computing environments in different places. However, we still lack an understanding of using VR in an asymmetric setting for collaborative visualization. To get an initial understanding and better inform the designs for asymmetric systems, we first conducted a formative study with 12 pairs of participants. All participants collaborated in asymmetric (PC-VR) and symmetric settings (PC-PC and VR-VR). We then improved our asymmetric design based on the key findings and observations from the first study. Another ten pairs of participants collaborated with enhanced PC-VR and PC-PC conditions in a follow-up study. We found that a well-designed asymmetric collaboration system could be as effective as a symmetric system. Surprisingly, participants using PC perceived less mental demand and effort in the asymmetric setting (PC-VR) compared to the symmetric setting (PC-PC). We provided fine-grained discussions about the trade-offs between different collaboration settings.},
	author = {Tong, Wai and Xia, Meng and Wong, Kam Kwai and Bowman, Doug A. and Pong, Ting-Chuen and Qu, Huamin and Yang, Yalong},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00054},
	issn = {2642-5254},
	keywords = {Performance evaluation;Visualization;Three-dimensional displays;Collaboration;Prototypes;Virtual reality;User interfaces;asymmetric collaborative visualization;virtual reality;data visualization;problem solving},
	month = {March},
	pages = {387-397},
	title = {Towards an Understanding of Distributed Asymmetric Collaborative Visualization on Problem-solving},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00054}}

@inproceedings{10108428,
	abstract = {Standard selection techniques such as ray casting fail when virtual objects are partially or fully occluded. In this paper, we present two novel approaches that combine cone-casting, world-in-miniature, and grasping metaphors to disocclude objects in the representation local to the user. Through a within-subject study where we compared 4 selection techniques across 3 levels of object occlusion, we found that our techniques outperformed an alternative one that also focuses on maintaining the spatial relationships between objects. We discuss application scenarios and future research directions for these types of selection techniques.},
	author = {Maslych, Mykola and Hmaiti, Yahya and Ghamandi, Ryan and Leber, Paige and Kattoju, Ravi Kiran and Belga, Jacob and LaViola, Joseph J.},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00061},
	issn = {2642-5254},
	keywords = {Visualization;Casting;Three-dimensional displays;Virtual reality;Grasping;Maintenance engineering;User interfaces;visualization;object selection;virtual reality;occlusion;mini map;head mounted displays;user studies},
	month = {March},
	pages = {460-470},
	title = {Toward Intuitive Acquisition of Occluded VR Objects Through an Interactive Disocclusion Mini-map},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00061}}

@inproceedings{10108429,
	abstract = {Visual coherence between real and virtual objects is important in mixed reality (MR), and illumination consistency is one of the key aspects to achieve coherence. Apart from matching the illumination of the virtual objects with the real environments, the change of illumination on the real scenes produced by the inserted virtual objects should also be considered but is difficult to compute in real-time due to the heavy computation demands of global illumination. In this work, we propose delta path tracing (DPT), which only computes the radiance blocked by the virtual objects from the light sources at the primary hit points of Monte Carlo path tracing, then combines the blocked radiance and multi-bounce indirect illumination with the image of the real scene. Multiple importance sampling (MIS) between BRDF and environment map is performed to handle all-frequency environment maps captured by a panorama camera. Compared to conventional differential rendering methods, our method can remarkably reduce the number of times required to access the environment map and avoid rendering scenes twice. Therefore, the performance can be significantly improved. We implement our method using hardware-accelerated ray tracing on modern GPUs, and the results demonstrate that our method can render global illumination at real-time frame rates and produce plausible visual coherence between real and virtual objects in MR environments.},
	author = {Xu, Yang and Jiang, Yuanfa and Wang, Shibo and Li, Kang and Geng, Guohua},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00020},
	issn = {2642-5254},
	keywords = {Visualization;Monte Carlo methods;Three-dimensional displays;Lighting;Mixed reality;Virtual reality;Coherence;Mixed / augmented reality;Ray tracing},
	month = {March},
	pages = {44-52},
	title = {Delta Path Tracing for Real-Time Global Illumination in Mixed Reality},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00020}}

@inproceedings{10108432,
	abstract = {An emerging alternative to conventional Augmented Reality (AR) glasses designs, Beaming displays promise slim AR glasses free from challenging design trade-offs, including battery-related limits or computational budget-related issues. These beaming displays remove active components such as batteries and electronics from AR glasses and move them to a projector that projects images to a user from a distance (1--2 meters), where users wear only passive optical eyepieces. However, earlier implementations of these displays delivered poor resolutions (7 cycles per degree) without any optical focus cues and were introduced with a bulky form-factor eyepiece ($\sim 50\ mm$ thick). This paper introduces a new milestone for beaming displays, which we call HoloBeam. In this new design, a custom holographic projector populates a micro-volume located at some distance (1--2 meters) with multiple planes of images. Users view magnified copies of these images from this small volume with the help of an eyepiece that is either a Holographic Optical Element (HOE) or a set of lenses. Our HoloBeam prototypes demonstrate the thinnest AR glasses to date with submillimeter thickness (e.g., HOE film is only $120\ \mu m$ thick). In addition, HoloBeam prototypes demonstrate near retinal resolutions (24 cycles per degree) with a 70 degrees-wide field of view.},
	author = {Ak{\c s}it, Kaan and Itoh, Yuta},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00073},
	issn = {2642-5254},
	keywords = {Visualization;Image resolution;Three-dimensional displays;Prototypes;Glass;Holography;Optical imaging;Holographic Displays;Computer Generated Holography;Computational Displays;Augmented Reality;Virtual Reality;Near Eye Displays;AR glasses;VR Headsets;Optics;Learned Methods;Optimization;Machine Learning;Deep Learning},
	month = {March},
	pages = {581-591},
	title = {HoloBeam: Paper-Thin Near-Eye Displays},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00073}}

@inproceedings{10108433,
	abstract = {We present iARVis, a proof-of-concept toolkit for creating, experiencing, and sharing mobile AR-based information visualization environments. Over the past years, AR has emerged as a promising medium for information and data visualization beyond the physical media and the desktop, enabling interactivity and eliminating spatial limits. However, the creation of such environments remains difficult and frequently necessitates low-level programming expertise and lengthy hand encodings. We present a declarative approach for defining the augmented reality (AR) environment, including how information is automatically positioned, laid out, and interacted with, to improve the efficiency and flexibility of constructing AR-based information visualization environments. We provide fundamental layout and visual components such as the grid, rich text, images, and charts for the development of complex visualization widgets, as well as automatic targeting methods based on image and object tracking for the development of the AR environment. To increase design efficiency, we also provide features such as hot-reload and several creation levels for both novice and advanced users. We also investigate how the augmented reality-based visualization environment could persist and be shared through the internet and provide ways for storing, sharing, and restoring the environment to give a continuous and seamless experience. To demonstrate the viability and extensibility, we evaluate iARVis using a variety of use cases along with performance evaluation and expert reviews.},
	author = {Chen, Junjie and Li, Chenhui and Song, Sicheng and Wang, Changbo},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00017},
	issn = {2642-5254},
	keywords = {Performance evaluation;Visualization;Three-dimensional displays;Layout;Data visualization;Media;Real-time systems;Visualization systems and tools;Augmented Reality},
	month = {March},
	pages = {11-21},
	title = {iARVis: Mobile AR Based Declarative Information Visualization Authoring, Exploring and Sharing},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00017}}

@inproceedings{10108434,
	abstract = {Precise estimation of global orientation and location is critical to ensure a compelling outdoor Augmented Reality (AR) experience. We address the problem of geo-pose estimation by cross-view matching of query ground images to a geo-referenced aerial satellite image database. Recently, neural network-based methods have shown state-of-the-art performance in cross-view matching. However, most of the prior works focus only on location estimation, ignoring orientation, which cannot meet the requirements in outdoor AR applications. We propose a new transformer neural network-based model and a modified triplet ranking loss for joint location and orientation estimation. Experiments on several benchmark cross-view geo-localization datasets show that our model achieves state-of-the-art performance. Furthermore, we present an approach to extend the single image query-based geo-localization approach by utilizing temporal information from a navigation pipeline for robust continuous geo-localization. Experimentation on several large-scale real-world video sequences demonstrates that our approach enables high-precision and stable AR insertion.},
	author = {Mithun, Niluthpol Chowdhury and Minhas, Kshitij S. and Chiu, Han-Pang and Oskiper, Taragay and Sizintsev, Mikhail and Samarasekera, Supun and Kumar, Rakesh},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00064},
	issn = {2642-5254},
	keywords = {Visualization;Solid modeling;Three-dimensional displays;Satellites;Video sequences;Estimation;Benchmark testing;Cross View Visual Geo Localization;Ground Aerial Matching;Outdoor Augmented Reality;Transformer Neural Network;Visual Inertial Navigation},
	month = {March},
	pages = {493-502},
	title = {Cross-View Visual Geo-Localization for Outdoor Augmented Reality},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00064}}

@inproceedings{10108435,
	abstract = {The use of self-avatars is gaining popularity thanks to affordable VR headsets. Unfortunately, mainstream VR devices often use a small number of trackers and provide low-accuracy animations. Previous studies have shown that the Sense of Embodiment, and in particular the Sense of Agency, depends on the extent to which the avatar's movements mimic the user's movements. However, few works study such effect for tasks requiring a precise interaction with the environment, i.e., tasks that require accurate manipulation, precise foot stepping, or correct body poses. In these cases, users are likely to notice inconsistencies between their self-avatars and their actual pose. In this paper, we study the impact of the animation fidelity of the user avatar on a variety of tasks that focus on arm movement, leg movement and body posture. We compare three different animation techniques: two of them using Inverse Kinematics to reconstruct the pose from sparse input (6 trackers), and a third one using a professional motion capture system with 17 inertial sensors. We evaluate these animation techniques both quantitatively (completion time, unintentional collisions, pose accuracy) and qualitatively (Sense of Embodiment). Our results show that the animation quality affects the Sense of Embodiment. Inertial-based MoCap performs significantly better in mimicking body poses. Surprisingly, IK-based solutions using fewer sensors outperformed MoCap in tasks requiring accurate positioning, which we attribute to the higher latency and the positional drift that causes errors at the end-effectors, which are more noticeable in contact areas such as the feet.},
	author = {Yun, Haoran and Ponton, Jose Luis and Andujar, Carlos and Pelechano, Nuria},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00044},
	issn = {2642-5254},
	keywords = {Legged locomotion;Three-dimensional displays;Tracking;Inertial sensors;User interfaces;Sensor phenomena and characterization;Animation;Virtual reality;Motion capture;Inverse kinematics;Embodiment;Avatar Animation},
	month = {March},
	pages = {286-296},
	title = {Animation Fidelity in Self-Avatars: Impact on User Performance and Sense of Agency},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00044}}

@inproceedings{10108437,
	abstract = {In AR/MR applications, camera localization and object pose estimation both play crucial roles. The universality of learning techniques, often referred to as scene-independent localization and category-level pose estimation, presents challenges for both tasks. The two missions maintain close relationships due to the spatial geometry constraint, but differing task requirements result in distinct feature extraction. In this paper, we focus on simultaneous scene-independent camera localization and category-level object pose estimation with a unified learning framework. The system consists of a localization branch called SLO-LocNet, a pose estimation branch called SLO-ObjNet, a feature fusion module for feature sharing between two tasks, and two decoders for creating coordinate maps. In SLO-LocNet, localization features are produced for anticipating the relative pose between two adjusted frames using inputs of color and depth images. Furthermore, we establish an image fusion module in order to promote feature sharing in depth and color branches. With SLO-ObjNet, we take the detected depth image and its corresponding point cloud as inputs, and produce object pose features for pose estimation. A geometry fusion module is created to combine depth and point cloud information simultaneously. Between the two tasks, the image fusion module is also exploited to accomplish feature sharing. In terms of the loss function, we present a mixed optimization function that is composed of the relative camera pose, geometry constraint, absolute and relative object pose terms. To verify how well our algorithm could perform, we conduct experiments on both localization and pose estimation datasets, covering 7 Scenes, ScanNet, REAL275 and YCB-Video. All experiments demonstrate superior performance to other existing methods. We specifically train the network on ScanNet and test it on 7 Scenes to demonstrate the universality performance. Additionally, the positive effects of fusion modules and loss function are also demonstrated.},
	author = {Wang, Junyi and Qi, Yue},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00041},
	issn = {2642-5254},
	keywords = {Location awareness;Point cloud compression;Geometry;Image color analysis;Pose estimation;Cameras;Feature extraction;Scene-independent camera localization;Cagetory-level object pose estimation;Feature fusion;Multi-task learning;Geometry constraint},
	month = {March},
	pages = {254-264},
	title = {Simultaneous Scene-independent Camera Localization and Category-level Object Pose Estimation via Multi-level Feature Fusion},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00041}}

@inproceedings{10108438,
	abstract = {Dynamically adjusting the content of augmented reality (AR) applications to efficiently display information best fitting the available screen estate may be important for user performance and satisfaction. Currently, there is not a common practice for dynamically adjusting the content of AR applications based on their apparent size in the user's view of the surround environment. We present a Level-of-Detail AR mechanism to improve the usability of AR applications at any relative size. Our mechanism dynamically renders textual and interactable content based on its legibility, interactability, and viewability respectively. When tested, Level-of-Detail AR functioned as intended out-of-the-box on 44 of the 45 standard user interface Unity prefabs in Microsoft's Mixed Reality Tool Kit. We additionally evaluated impact on task performance, user distance, and subjective satisfaction through a mixed-design user study with 45 participants. Statistical analysis of our results revealed significant task-dependent differences in user performance between the modes. User satisfaction was consistently higher for the Level-of-Detail AR condition.},
	author = {Wysopal, Abby and Ross, Vivian and Passananti, Joyce and Yu, Kangyou and Huynh, Brandon and H{\"o}llerer, Tobias},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00022},
	issn = {2642-5254},
	keywords = {Measurement;Visualization;Three-dimensional displays;Statistical analysis;Mixed reality;User interfaces;Task analysis;Level of Detail;Augmented Reality;Automation;User Study;Task Performance;User Satisfaction},
	month = {March},
	pages = {63-71},
	title = {Level-of-Detail AR: Dynamically Adjusting Augmented Reality Level of Detail Based on Visual Angle},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00022}}

@inproceedings{10108439,
	abstract = {Over the past two decades self-avatars have been shown to affect the perception of both oneself and of environmental properties including the sizes and distances of elements in immersive virtual environments. However, virtual avatars that accurately match the body proportions of their users remain inaccessible to the general public. As such, most virtual experiences that represent the user have a generic avatar that does not fit the proportions of the users' body. This can negatively affect judgments involving affordances, such as passability and maneuverability, which pertain to the relationship between the properties of environmental elements relative to the properties of the user providing information about actions that can be enacted. This is especially true when the task requires the user to maneuver around moving objects like in games. Therefore, it is necessary to understand how different sized self-avatars affect the perception of affordances in dynamic virtual environments. To better understand this, we conducted an experiment investigating how a self-avatar that is either the same size, 20% shorter, or 20% taller, than the user's own body affects passability judgments in a dynamic virtual environment. Our results suggest that the presence of self-avatars results in better regulatory and safer road crossing behavior, and helps participants synchronize self-motion to external stimuli quicker than in the absence of self-avatars.},
	author = {Bhargava, Ayush and Venkatakrishnan, Roshan and Venkatakrishnan, Rohith and Solini, Hannah and Lucaites, Kathryn and Robb, Andrew C. and Pagano, Christopher C. and Babu, Sabarish V.},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00046},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Affordances;Avatars;Roads;Virtual environments;External stimuli;Games;Human-centered-computing;Empirical-studies-in-HCI},
	month = {March},
	pages = {308-317},
	title = {Empirically Evaluating the Effects of Eye Height and Self-Avatars on Dynamic Passability Affordances in Virtual Reality},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00046}}

@inproceedings{10108443,
	abstract = {DynaSLAM is the state-of-the-art visual simultaneous localization and mapping (SLAM) in dynamic environments. It adopts a convolutional neural network (CNN) for moving object detection, but usually incurs a very high computational cost because it performs semantic segmentation using the CNN model on every frame. This paper proposes SCP-SLAM, which accelerates DynaSLAM by running the CNN only on keyframes and propagating static confidence through other frames in parallel. The proposed static confidence characterizes the moving object features by the residual defined by inter-frame geometry transformation, which can be computed quickly. Our method combines the effectiveness of a CNN with the efficiency of static confidence in a tightly coupled manner. Extensive experiments on the publicly available TUM and Bonn RGB-D dynamic benchmark datasets demonstrate the efficacy of the method. Compared with DynaSLAM, it enables acceleration by a factor of ten on average, but retains comparable localization accuracy.},
	author = {Yu, Ming-Fei and Zhang, Lei and Wang, Wu-Fan and Wang, Jia-Hui},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00066},
	issn = {2642-5254},
	keywords = {Visualization;Solid modeling;Simultaneous localization and mapping;Three-dimensional displays;Semantic segmentation;Computational modeling;Virtual reality;Human-centered computing;Visualization;Visualization techniques},
	month = {March},
	pages = {509-518},
	title = {SCP-SLAM: Accelerating DynaSLAM With Static Confidence Propagation},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00066}}

@inproceedings{10108444,
	abstract = {Handheld virtual reality (VR) controllers enable users to manipulate virtual objects in VR but do not convey a virtual object's weight. This hinders users from effectively experiencing lighter and heavier objects. While previous work explored either hardware-based interfaces or software-based pseudo-haptics, in this paper, we combine two techniques to improve the virtual weight perception in VR. By adapting the trigger resistance of the VR controller when grasping a virtual object and manipulating the control-display (C/D) ratio during lifting, we create a continuous weight sensation. In a psychophysical study (N=29), we compared our combined approach against the individual rendering techniques. Our results show that participants were significantly more sensitive towards smaller weight differences in the combined weight simulations compared to the individual methods. Additionally, participants were also able to determine weight differences significantly faster with both cues present compared to the single pseudo-haptic technique. While all three techniques were generally valued to be effective, the combined approach was favoured the most. Our findings demonstrate the meaningful benefit of combining physical and virtual techniques for virtual weight rendering over previously proposed methods.},
	author = {Stellmacher, Carolin and Zenner, Andr{\'e} and Nunez, Oscar Javier Ariza and Kruijff, Ernst and Sch{\"o}ning, Johannes},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00040},
	issn = {2642-5254},
	keywords = {Resistance;Solid modeling;Adaptation models;Three-dimensional displays;Virtual reality;Grasping;User interfaces;virtual reality;adaptive trigger resistance;virtual weight;control display ratio;psychophysical experiment},
	month = {March},
	pages = {243-253},
	title = {Continuous VR Weight Illusion by Combining Adaptive Trigger Resistance and Control-Display Ratio Manipulation},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00040}}

@inproceedings{10108445,
	abstract = {Trends are changes in variables or attributes over time, often represented by line plots or scatterplot variants, with time being one of the axes. Interpreting tendencies and estimating trends require observing the lines or points behavior regarding increments, decrements, or both (reversals) in the value of the observed variable. Previous work assessed variants of scatterplots like Animation, Small Multiples, and Overlaid Trails for comparing the effectiveness of trends representation using large and small displays and found differences between them. In this work, we study how best to enable the analyst to explore and perform temporal trend tasks with these same techniques in immersive virtual environments. We designed and conducted a user study based on the approaches followed by previous works regarding visualization and interaction techniques, as well as tasks for comparisons in three-dimensional settings. Results show that Overlaid Trails are the fastest overall, followed by Animation and Small Multiples, while accuracy is task-dependent. We also report results from interaction measures and questionnaires.},
	author = {Quijano-Chavez, Carlos and Nedel, Luciana and Freitas, Carla M. D. S.},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00082},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Virtual environments;User interfaces;Market research;Animation;Behavioral sciences;Evaluation;graphical perception;immersive analytics;trend visualization;virtual reality},
	month = {March},
	pages = {669-679},
	title = {Comparing Scatterplot Variants for Temporal Trends Visualization in Immersive Virtual Environments},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00082}}

@inproceedings{10108446,
	abstract = {The paper presents an approach for handheld augmented reality in constrained industrial environments, where it might be hard or even impossible to reach certain poses within a workspace. Therefore, a user might be unable to see or interact with some digital content in applications like visual robot programming, robotic program visualizations, or workspace annotation. To overcome this limitation, we propose a temporal switching to a non-immersive virtual reality that allows the user to see the virtual counterpart of the workspace from any angle and distance, where the viewpoint is controlled using a unique combination of on-screen controls complemented by the physical motion of the handheld device. Using such a combination, the user can position the virtual camera roughly to the desired pose using the on-screen controls and then continue working just as in augmented reality. To explore how people would use it and what the benefits would be over pure augmented reality, we chose a representative task of object alignment and conducted a study. The results revealed that mainly physical demands, which is often a limiting factor for handheld augmented reality, could be reduced and that the usability and utility of the approach are rated as high. In addition, suggestions for improving the user interface were proposed and discussed.},
	author = {Bambu{\v s}ek, Daniel and Materna, Zden{\v e}k and Kapinus, Michal and Beran, V{\'\i}t{\v e}zslav and Smr{\v z}, Pavel},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00027},
	issn = {2642-5254},
	keywords = {Visualization;Limiting;Service robots;Annotations;Switches;User interfaces;Control systems;Augmented Reality;Virtual Reality;Transitional Interface;Human Robot Interaction;Constrained Industrial Environmnets},
	month = {March},
	pages = {115-122},
	title = {How Do I Get There? Overcoming Reachability Limitations of Constrained Industrial Environments in Augmented Reality Applications},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00027}}

@inproceedings{10108447,
	abstract = {With the emergence of brain-computer interface (BCI) technology and virtual reality (VR), how to improve the quality of motor imagery (MI) electroencephalogram (EEG) signal has become a key issue for MI BCI applications under VR. In this paper, we proposed to enhance the quality of MI EEG signal by using haptic stimulation training. We designed a first-person perspective and a third-person perspective scene under VR, and the experimental results showed that the left- and right-hand MI EEG quality of the participants improved significantly compared with that before training, and the mean differentiation of the left- and right-hand MI tasks was improved by 21.8% and 15.7%, respectively. We implemented a BCI application system in VR and developed a game based on MI EEG for control of ball movement, in which the average classification accuracy by the participants after training in the first-person perspective reached 93.5%, which was a significant improvement over existing study.},
	author = {Cheng, Shiwei and Tian, Jieming},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00074},
	issn = {2642-5254},
	keywords = {Training;Three-dimensional displays;Games;Virtual reality;Electroencephalography;Real-time systems;Brain-computer interfaces;Brain computer interface;Virtual reality;Haptic stimulation},
	month = {March},
	pages = {592-602},
	title = {A Haptic Stimulation-Based Training Method to Improve the Quality of Motor Imagery EEG Signal in VR},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00074}}

@inproceedings{10108448,
	abstract = {Full projector compensation is a practical task of projector-camera systems. It aims to find a projector input image, named compensation image, such that when projected it cancels the geometric and photometric distortions due to the physical environment and hardware. State-of-the-art methods use deep learning to address this problem and show promising performance for low-resolution setups. However, directly applying deep learning to high-resolution setups is impractical due to the long training time and high memory cost. To address this issue, this paper proposes a practical full compensation solution. Firstly, we design an attention-based grid refinement network to improve geometric correction quality. Secondly, we integrate a novel sampling scheme into an end-to-end compensation network to alleviate computation and introduce attention blocks to preserve key features. Finally, we construct a benchmark dataset for high-resolution projector full compensation. In experiments, our method demonstrates clear advantages in both efficiency and quality.},
	author = {Wang, Yuxi and Ling, Haibin and Huang, Bingyao},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00029},
	issn = {2642-5254},
	keywords = {Deep learning;Training;Costs;Three-dimensional displays;Spatial augmented reality;Memory management;Virtual reality;Projector compensation;Spatial augmented reality;Projector-camera system},
	month = {March},
	pages = {135-145},
	title = {CompenHR: Efficient Full Compensation for High-resolution Projector},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00029}}

@inproceedings{10108449,
	abstract = {Previous studies showed that natural walking reduces the susceptibility to VR sickness. However, many users still experience VR sickness when wearing VR headsets that allow free walking in room-scale spaces. This paper studies VR sickness and postural instability while the user walks in an immersive virtual environment using an electroencephalogram (EEG) headset and a full-body motion capture system. The experiment induced VR sickness by gradually increasing the translation gain beyond the user's detection threshold. A between-group comparison between participants with and without VR sickness symptoms found some significant differences in postural stability but found none on gait patterns during the walking. In the EEG analysis, the group with VR sickness showed a reduction of alpha power, a phenomenon previously linked to a higher workload and efforts to maintain postural control. In contrast, the group without VR sickness exhibited brain activities linked to fine cognitive-motor control. The EEG result provides new insights into the postural instability theory: participants with VR sickness could maintain their postural stability at the cost of a higher cognitive workload. Our result also indicates that the analysis of lower-frequency power could complement behavioral data for continuous VR sickness detection in both stationary and mobile VR setups.},
	author = {Cortes, Carlos Alfredo Tirado and Lin, Chin-Teng and Do, Tien-Thong Nguyen and Chen, Hsiang-Ting},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00025},
	issn = {2642-5254},
	keywords = {Legged locomotion;Headphones;Three-dimensional displays;Costs;Virtual environments;User interfaces;Electroencephalography;VR Sickness;Postural Instability;Virtual Reality;Translational Gain;EEG},
	month = {March},
	pages = {94-104},
	title = {An EEG-based Experiment on VR Sickness and Postural Instability While Walking in Virtual Environments},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00025}}

@inproceedings{10108450,
	abstract = {Cybersickness is a common ailment associated with virtual reality (VR) user experiences. Several automated methods exist based on machine learning (ML) and deep learning (DL) to detect cyber-sickness. However, most of these cybersickness detection methods are perceived as computationally intensive and black-box methods. Thus, those techniques are neither trustworthy nor practical for deploying on standalone energy-constrained VR head-mounted devices (HMDs). In this work, we present an explainable artificial intelligence (XAI)-based framework Lite VR for cybersickness detection, explaining the model's outcome, reducing the feature dimensions, and overall computational costs. First, we develop three cybersick-ness DL models based on long-term short-term memory (LSTM), gated recurrent unit (GRU), and multilayer perceptron (MLP). Then, we employed a post-hoc explanation, such as SHapley Additive Explanations (SHAP), to explain the results and extract the most dominant features of cybersickness. Finally, we retrain the DL models with the reduced number of features. Our results show that eye-tracking features are the most dominant for cybersickness detection. Furthermore, based on the XAI-based feature ranking and dimensionality reduction, we significantly reduce the model's size by up to 4.3, training time by up to 5.6, and its inference time by up to 3.8, with higher cybersickness detection accuracy and low regression error (i.e., on Fast Motion Scale (FMS)). Our proposed lite LSTM model obtained an accuracy of 94% in classifying cyber-sickness and regressing (i.e., FMS 1--10) with a Root Mean Square Error (RMSE) of 0.30, which outperforms the state-of-the-art. Our proposed Lite VR framework can help researchers and practitioners analyze, detect, and deploy their DL-based cybersickness detection models in standalone VR HMDs.},
	author = {Kundu, Ripan Kumar and Islam, Rifatul and Quarles, John and Hoque, Khaza Anuarul},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00076},
	issn = {2642-5254},
	keywords = {Training;Solid modeling;Frequency modulation;Three-dimensional displays;Cybersickness;Computational modeling;Gaze tracking;Virtual Reality;Cybersickness Detection;Explainable Artificial Intelligence;Deep Learning;Model Reduction},
	month = {March},
	pages = {609-619},
	title = {LiteVR: Interpretable and Lightweight Cybersickness Detection using Explainable AI},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00076}}

@inproceedings{10108451,
	abstract = {Teaching in optical systems design is usually performed on an optical bench. While experimentation plays an important role in education, experiments involving expensive or dangerous components are usually limited to short, heavily supervised sessions. Computer simulations, on the other hand, offer high accessibility, but suffer from reduced realism and tangibility when presented on a 2D screen. For this reason, we present the virtual optical bench, an application that lets users explore spherical lens layouts in virtual reality (VR). We implemented a numerically accurate simulation of optical systems using Nvidia OptiX, as well as a prototypical VR application, which we then evaluated in an expert review with 6 optics experts. Based on their feedback, we re-implemented our VR application in Unreal Engine 4. The re-implementation has since been actively used for teaching optical layouts, where we performed a qualitative evaluation with 18 students. We show that our virtual optical bench achieves good usability and is perceived to enhance the understanding of course contents.},
	author = {Bellgardt, Martin and Pape, Sebastian and Gilbert, David and Prochnau, Marcel and K{\"o}nig, Georg and Kuhlen, Torsten W.},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00065},
	issn = {2642-5254},
	keywords = {Solid modeling;Optical design;Optical feedback;Education;Layout;User interfaces;Adaptive optics;Applied computing---Education---Interactive learning environments},
	month = {March},
	pages = {503-508},
	title = {Virtual Optical Bench: Teaching Spherical Lens Layout in VR with Real-Time Ray Tracing},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00065}}

@inproceedings{10108453,
	abstract = {Peripheral vision plays a significant role in human perception and orientation. However, its relevance for human-computer interaction, especially head-mounted displays, has not been fully explored yet. In the past, a few specialized appliances were developed to display visual cues in the periphery, each designed for a single specific use case only. A multi-purpose headset to exclusively augment peripheral vision did not exist yet. We introduce MoPeDT: Modular Peripheral Display Toolkit, a freely available, flexible, reconfigurable, and extendable headset to conduct peripheral vision research. MoPeDT can be built with a 3D printer and off-the-shelf components. It features multiple spatially configurable near-eye display modules and full 3D tracking inside and outside the lab. With our system, researchers and designers may easily develop and prototype novel peripheral vision interaction and visualization techniques. We demonstrate the versatility of our headset with several possible applications for spatial awareness, balance, interaction, feedback, and notifications. We conducted a small study to evaluate the usability of the system. We found that participants were largely not irritated by the peripheral cues, but the headset's comfort could be further improved. We also evaluated our system based on established heuristics for human-computer interaction toolkits to show how MoPeDT adapts to changing requirements, lowers the entry barrier for peripheral vision research, and facilitates expressive power in the combination of modular building blocks.},
	author = {Albrecht, Matthias and Assl{\"a}nder, Lorenz and Reiterer, Harald and Streuber, Stephan},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00084},
	issn = {2642-5254},
	keywords = {Headphones;Visualization;Three-dimensional displays;Head-mounted displays;Prototypes;Virtual reality;Printers;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Mixed / augmented reality;Human-centered computing---Human computer interaction (HCI)---Interactive systems and tools---User interface toolkits},
	month = {March},
	pages = {691-701},
	title = {MoPeDT: A Modular Head-Mounted Display Toolkit to Conduct Peripheral Vision Research},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00084}}

@inproceedings{10108455,
	abstract = {Optimizing rendering performance improves the user's immersion in virtual scene exploration. Foveated rendering uses the features of the human visual system (HVS) to improve rendering performance without sacrificing perceptual visual quality. We collect and analyze the viewing motion of different locomotion methods, and describe the effects of these viewing motions on HVS's sensitivity, as well as the advantages of these effects that may bring to foveated rendering. Then we propose the locomotion-aware foveated rendering method (LaFR) to further accelerate foveated rendering by leveraging the advantages. In LaFR, we first introduce the framework of LaFR. Secondly, we propose an eccentricity-based shading rate controller that provides the shading rate control of the given region in foveated rendering. Thirdly, we propose a locomotion-aware log-polar mapping method, which controls the foveal average shading rate, the peripheral shading rate decrease speed, and the overall shading quantity with the locomotion-aware coefficients based on the eccentricity-based shading rate controller. LaFR achieves similar perceptual visual quality as the conventional foveated rendering while achieving up to 1.6 speedup. Compared with the full resolution rendering, LaFR achieves up to 3.8 speedup.},
	author = {Shi, Xuehuai and Wang, Lili and Wu, Jian and Ke, Wei and Lam, Chan-Tong},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00062},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Sensitivity;Virtual reality;Visual systems;User interfaces;Rendering (computer graphics);Virtual Reality;Foveated Rendering;Gaze-contingent Rendering;Perception},
	month = {March},
	pages = {471-481},
	title = {Locomotion-aware Foveated Rendering},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00062}}

@inproceedings{10108456,
	abstract = {Body actions and head gestures are natural interfaces for interaction in virtual environments. Existing methods for in-place body action recognition often require hardware more than a head-mounted display (HMD), making body action interfaces difficult to be introduced to ordinary virtual reality (VR) users as they usually only possess an HMD. In addition, there lacks a unified solution to recognize in-place body actions and head gestures. This potentially hinders the exploration of the use of in-place body actions and head gestures for novel interaction experiences in virtual environments. We present a unified two-stream 1-D convolutional neural network (CNN) for recognition of body actions when a user performs walking-in-place (WIP) and for recognition of head gestures when a user stands still wearing only an HMD. Compared to previous approaches, our method does not require specialized hardware and/or additional tracking devices other than an HMD and can recognize a significantly larger number of body actions and head gestures than other existing methods. In total, ten in-place body actions and eight head gestures can be recognized with the proposed method, which makes this method a readily available body action interface (head gestures included) for interaction with virtual environments. We demonstrate one utility of the interface through a virtual locomotion task. Results show that the present body action interface is reliable in detecting body actions for the VR locomotion task but is physically demanding compared to a touch controller interface. The present body action interface is promising for new VR experiences and applications, especially for VR fitness applications where workouts are intended.},
	author = {Zhao, Jingbo and Shao, Mingjun and Wang, Yaojun and Xu, Ruolin},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00026},
	issn = {2642-5254},
	keywords = {Head-mounted displays;Three-dimensional displays;Virtual environments;Resists;User interfaces;Hardware;Real-time systems;Body Action Recognition;Head Gesture Recognition;Virtual Locomotion},
	month = {March},
	pages = {105-114},
	title = {Real-Time Recognition of In-Place Body Actions and Head Gestures using Only a Head-Mounted Display},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00026}}

@inproceedings{10108457,
	abstract = {In this work, we examine how Augmented Reality (AR) impacts user's situation awareness (SA) on elements secondary to an AR-assisted main task, i.e. not directly concerned by the main task. These secondary elements can still provide relevant information that we do not want the user to miss. A good understanding of user's awareness about them is therefore interesting, especially in a context of a daily use of AR, in which not all elements of user's environment are controlled. In this regard, we measured SA about secondary elements in an industrial workshop where the AR-assisted main task is a pedestrian navigation. We compared SA between three navigation guidance conditions: a paper map, a virtual path, and a virtual path with virtual cues about secondary elements. These secondary elements were either hazardous areas, for example, for mandatory helmets, or items which could be on user's path, for example, misplaced carts, boxes{\ldots} We adapted an existing SA method evaluation to a real-world environment. With this method, participants were queried about their SA on three levels: perception, comprehension and projection about different items. We found that the use of AR decreased user's SA about secondary elements, and that this degradation mainly occurs at the perception level: with AR, participants are less likely to detect secondary elements. Participants still felt the most secure with AR and virtual cues about secondary elements.},
	author = {Truong-Alli{\'e}, Camille and Herbeth, Martin and Paljic, Alexis},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00069},
	issn = {2642-5254},
	keywords = {Hazardous areas;Solid modeling;Three-dimensional displays;Limiting;Head;Navigation;Resists;Augmented Reality;Situation Awareness;Head Mounted Display;Human-centered computing---Visualization---Visualization techniques---Treemaps;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {541-551},
	title = {A study of the influence of AR on the perception, comprehension and projection levels of situation awareness},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00069}}

@inproceedings{10108458,
	abstract = {Age-related Macular Degeneration (AMD) is the leading cause of vision loss among persons over 50. We present a two-part interface consisting of a VR-based visualization for AMD patients and an interconnected doctor interface to optimize this VR view. It focuses on remapping imagery to provide customized image optimizations. The system allows doctors to generate a tailored, patient-specific VR visualization. We pilot tested the doctor interface (n=10) with eye care professionals. The results indicate the potential of VR-based eye care for doctors to help visually-impaired patients, but also show a necessary training phase to establish new technologies in vision rehabilitation.},
	author = {Nitsche, Michael and Bosley, Blaire and Primo, Susan and Park, Jisu and Carr, Daniel},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00030},
	issn = {2642-5254},
	keywords = {Training;Visualization;Three-dimensional displays;Medical services;Virtual reality;Computer applications;User interfaces;AMD;VR;Image Remapping;medical application;H.5.2 [Information Systems]: Information Interfaces and Presentation - User Interfaces;J.3 [Computer Applications]: Life and Medical Sciences},
	month = {March},
	pages = {146-151},
	title = {Remapping Control in VR for Patients with AMD},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00030}}

@inproceedings{10108459,
	abstract = {Parallel view is a technique that allows a VR user to see multiple locations at a time. It enables the user to control several remote or virtual body parts while seeing parallel views to solve synchronous tasks. However, these techniques only explored the benefits and drawbacks of a user performing different tasks. In this paper, we explored enhancements on a singular or asynchronous task by utilizing information obtained in parallel views. We developed three prototypes where parallel views are fixed, moving in symmetric order, or following the user's eye gaze. We conducted a user study to compare each prototype against traditional VR (without parallel views) in three types of tasks: object search and interaction tasks in a 1) simple environment and 2) complex environment, and 3) object distances estimation task. We found parallel views improved multi-embodiment while each technique helped different tasks. No parallel view provided a clean interface, thus improving spatial presence, mental effort, and user performance. However, participants' feedback highlighted potential usefulness and a lower physical effort by using parallel views to solve complicated tasks.},
	author = {Teo, Theophilus and Sakurada, Kuniharu and Sugimoto, Maki},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00077},
	issn = {2642-5254},
	keywords = {Legged locomotion;Three-dimensional displays;Prototypes;Estimation;Immersive experience;User interfaces;Search problems;Human-centered computing---Visualization;Human-centered computing---Virtual reality},
	month = {March},
	pages = {620-630},
	title = {Exploring Enhancements towards Gaze Oriented Parallel Views in Immersive Tasks},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00077}}

@inproceedings{10108460,
	abstract = {This paper introduces a new multiplane CGH computation method to reconstruct artifact-free high-quality holograms with natural-looking defocus blur. Our method introduces a new targeting scheme and a new loss function. While the targeting scheme accounts for defocused parts of the scene at each depth plane, the new loss function analyzes focused and defocused parts separately in reconstructed images. Our method support phase-only CGH calculations using various iterative (e.g., Gerchberg-Saxton, Gradient Descent) and non-iterative (e.g., Double Phase) CGH techniques. We achieve our best image quality using a modified gradient descent-based optimization recipe where we introduce a constraint inspired by the double phase method. We validate our method experimentally using our proof-of-concept holographic display, comparing various algorithms, including multi-depth scenes with sparse and dense contents.},
	author = {Kavakl, Koray and Itoh, Yuta and Urey, Hakan and Ak{\c s}it, Kaan},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00057},
	issn = {2642-5254},
	keywords = {Technological innovation;Three-dimensional displays;Virtual reality;Holography;User interfaces;Optical imaging;Holographic optical components;Hardware---Emerging Technologies---Emerging optical and photonic technology;Hardware---Communication hardware, interfaces and storage---Display and imagers},
	month = {March},
	pages = {418-426},
	title = {Realistic Defocus Blur for Multiplane Computer-Generated Holography},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00057}}

@inproceedings{10108461,
	abstract = {An important application of augmented reality (AR) is the design of interfaces that reveal parts of the real world to which the user does not have line of sight. The design space for such interfaces is vast, with many options for integrating the visualization of the occluded parts of the scene into the user's main view. This paper compares four AR interfaces for disocclusion: X-ray, Cutaway, Picture-in-picture, and Multiperspective. The interfaces are compared in a within-subjects study (N = 33) over four tasks: counting dynamic spheres, pointing to the direction of an occluded person, finding the closest object to a given object, and finding pairs of matching numbers. The results show that Cutaway leads to poor performance in tasks where the user needs to see both the occluder and the occludee; that Picture-in-picture and Multiperspective have a visualization comprehensiveness advantage over Cutaway and X-ray, but a disadvantage in terms of directional guidance; that X-ray has a task completion time disadvantage due to the visualization complexity; and that participants gave Cutaway and Picture-in-picture high, and Multiperspective and X-ray low usability scores.},
	author = {Liao, Shuqi and Zhou, Yuqi and Popescu, Voicu},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00068},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;User interfaces;Complexity theory;Task analysis;Usability;Augmented reality;Computing methodologies---Computer graphics---Graphics systems and interfaces---Mixed / augmented reality;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Mixed / augmented reality},
	month = {March},
	pages = {530-540},
	title = {AR Interfaces for Disocclusion---A Comparative Study},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00068}}

@inproceedings{10108464,
	abstract = {Viewpoint transitions have been shown to improve users' spatial orientation and help them build a cognitive map when they are navigating an unfamiliar virtual environment. Previous work has investigated transitions in single-scale virtual environments, focusing on trajectories and continuity. We extend this work with an in-depth investigation of transition techniques in multiscale virtual environments (MVEs). We identify challenges in navigating MVEs with nested structures and assess how different transition techniques affect spatial understanding and usability. Through two user studies, we investigated transition trajectories, interactive control of transition movement, and speed modulation in a nested MVE. We show that some types of viewpoint transitions enhance users' spatial awareness and confidence in their spatial orientation and reduce the need to revisit a target point of interest multiple times.},
	author = {Lee, Jong-In and Asente, Paul and Stuerzlinger, Wolfgang},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00083},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Navigation;Virtual environments;Modulation;Focusing;User interfaces;Trajectory;Human-centered computing;Human computer interaction (HCI);Interaction techniques},
	month = {March},
	pages = {680-690},
	title = {Designing Viewpoint Transition Techniques in Multiscale Virtual Environments},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00083}}

@inproceedings{10108465,
	abstract = {Recent research based on hand-eye coordination has shown that gaze could improve object selection and translation experience under certain scenarios in AR. However, several limitations still exist. Specifically, we investigate whether gaze could help object selection with heavy 3D occlusions and help 3D object translation in the depth dimension. In addition, we also investigate the possibility of reducing the gaze calibration burden before use. Therefore, we develop new methods with proper gaze guidance for 3D interaction in AR, and also an implicit online calibration method. We conduct two user studies to evaluate different interaction methods and the results show that our methods not only improve the effectiveness of occluded objects selection but also alleviate the arm fatigue problem significantly in the depth translation task. We also evaluate the proposed implicit online calibration method and find its accuracy comparable to standard 9 points explicit calibration, which makes a step towards practical use in the real world.},
	author = {Bao, Yiwei and Wang, Jiaxi and Wang, Zhimin and Lu, Feng},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00018},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Design methodology;Estimation;User interfaces;Fatigue;Calibration;Task analysis;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality},
	month = {March},
	pages = {22-32},
	title = {Exploring 3D Interaction with Gaze Guidance in Augmented Reality},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00018}}

@inproceedings{10108466,
	abstract = {State-of-the-art Virtual Reality (VR) and Augmented Reality (AR) headsets rely on singlefocal stereo displays. For objects away from the focal plane, such displays create a vergence-accommodation conflict (VAC), potentially degrading user interaction performance. In this paper, we study how the VAC affects pointing at targets within arm's reach with virtual hand and raycasting interaction in current stereo display systems. We use a previously proposed experimental methodology that extends the ISO 9241--411:2015 multi-directional selection task to enable fair comparisons between selecting targets in different display conditions. We conducted a user study with eighteen participants and the results indicate that participants were faster and had higher throughput in the constant VAC condition with the virtual hand. We hope that our results enable designers to choose more efficient interaction methods in virtual environments.},
	author = {Batmaz, Anil Ufuk and Mughribi, Moaaz Hudhud and Sarac, Mine and Machuca, Mayra Barrera and Stuerzlinger, Wolfgang},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00063},
	issn = {2642-5254},
	keywords = {Headphones;Three-dimensional displays;ISO Standards;Display systems;Virtual environments;User interfaces;Throughput;Human-centered computing-Human Computer Interaction (HCI);Human-centered computing-Virtual Reality;Human-centered computing-Pointing},
	month = {March},
	pages = {1-11},
	title = {Measuring the Effect of Stereo Deficiencies on Peripersonal Space Pointing},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00063}}

@inproceedings{10108467,
	abstract = {The reading experience on current augmented reality (AR) head mounted displays (HMDs) is often impeded by the devices' low perceived resolution, translucency, and small field of view, especially in situations involving lengthy text. Although many researchers have proposed methods to resolve this issue, the inherent characteristics prevent these displays from delivering a readability on par with that of more traditional displays. As a solution, we explore the use of smartphones as assistive displays to AR HMDs. To validate the feasibility of our approach, we conducted a user study in which we compared a smartphone-assisted hybrid interface against using the HMD only for two different text lengths. The results demonstrate that the hybrid interface yields a lower task load regardless of the text length, although it does not improve task performance. Furthermore, the hybrid interface provides a better experience regarding user comfort, visual fatigue, and perceived readability. Based on these results, we claim that joining the spatial output capabilities of the HMD with the high-resolution display of the smartphone is a viable solution for improving the reading experience in AR.},
	author = {Bang, Sunyoung and Woo, Woontack},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00053},
	issn = {2642-5254},
	keywords = {Process design;Visualization;Three-dimensional displays;Resists;Switches;User interfaces;Fatigue;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented re-ality;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
	month = {March},
	pages = {378-386},
	title = {Enhancing the Reading Experience on AR HMDs by Using Smartphones as Assistive Displays},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00053}}

@inproceedings{10108468,
	abstract = {Hand redirection is effective so long as the introduced offsets are not noticeably disruptive to users. In this work we investigate the use of physiological and interaction data to detect movement discrepancies between a user's real and virtual hand, pushing towards a novel approach to identify discrepancies which are too large and therefore can be noticed. We ran a study with 22 participants, collecting EEG, ECG, EDA, RSP, and interaction data. Our results suggest that EEG and interaction data can be reliably used to detect visuo-motor discrepancies, whereas ECG and RSP seem to suffer from inconsistencies. Our findings also show that participants quickly adapt to large discrepancies, and that they constantly attempt to establish a stable mental model of their environment. Together, these findings suggest that there is no absolute threshold for possible non-detectable discrepancies; instead, it depends primarily on participants' most recent experience with this kind of interaction.},
	author = {Feick, Martin and Regitz, Kora P. and Tang, Anthony and Jungbluth, Tobias and Rekrut, Maurice and Kr{\"u}ger, Antonio},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00035},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Virtual reality;Electrocardiography;User interfaces;Physiology;Electroencephalography;Cognitive science;Virtual Reality;Hand Redirection;Physiological Data;Detection Thresholds},
	month = {March},
	pages = {194-204},
	title = {Investigating Noticeable Hand Redirection in Virtual Reality using Physiological and Interaction Data},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00035}}

@inproceedings{10108469,
	abstract = {Virtual avatars are more and more often featured in Virtual Reality (VR) and Augmented Reality (AR) applications. When embodying a virtual avatar, one may desire to change of appearance over the course of the embodiment. However, switching suddenly from one appearance to another can break the continuity of the user experience and potentially impact the sense of embodiment (SoE), especially when the new appearance is very different. In this paper, we explore how applying smooth visual transitions at the moment of the change can help to maintain the SoE and benefit the general user experience. To address this, we implemented an AR system allowing users to embody a regular-shaped avatar that can be transformed into a muscular one through a visual effect. The avatar's transformation can be triggered either by the user through physical action (``active'' transition), or automatically launched by the system (``passive'' transition). We conducted a user study to evaluate the effects of these two types of transformations on the SoE by comparing them to control conditions where there was no visual feedback of the transformation. Our results show that changing the appearance of one's avatar with an active transition (with visual feedback), compared to a passive transition, helps to maintain the user's sense of agency, a component of the SoE. They also partially suggest that the Proteus effects experienced during the embodiment were enhanced by these transitions. Therefore, we conclude that visual effects controlled by the user when changing their avatar's appearance can benefit their experience by preserving the SoE and intensifying the Proteus effects.},
	author = {Otono, Riku and Genay, Ad{\'e}la{\"\i}de and Perusqu{\'\i}a-Hern{\'a}ndez, Monica and Isoyama, Naoya and Uchiyama, Hideaki and Hachet, Martin and L{\'e}cuyer, Anatole and Kiyokawa, Kiyoshi},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00024},
	issn = {2642-5254},
	keywords = {Human computer interaction;Visualization;Three-dimensional displays;Avatars;Switches;Visual effects;User experience;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality;Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI},
	month = {March},
	pages = {83-93},
	title = {I'm Transforming! Effects of Visual Transitions to Change of Avatar on the Sense of Embodiment in AR},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00024}}

@inproceedings{10108470,
	abstract = {To promote empathy with people that have disabilities, we propose a multi-sensory interactive experience that allows sighted users to embody having a visual impairment whilst using assistive technologies. The experiment involves blindfolded sighted participants interacting with a variety of sonification methods in order to locate targets and place objects in a real kitchen environment. Prior to the tests, we enquired about the perceived benefits of increasing said empathy from the blind and visually impaired (BVI) community. To test empathy, we adapted an Empathy and Sympathy Response scale to gather sighted people's self-reported and perceived empathy with the BVI community from both sighted (N = 77) and BVI people (N = 20) respectively. We re-tested sighted people's empathy after the experiment and found that their empathetic and sympathetic responses (N = 15) significantly increased. Furthermore, survey results suggest that the BVI community believes the use of these empathy-evoking embodied experiences may lead to the development of new assistive technologies.},
	author = {Guarese, Renan and Pretty, Emma and Fayek, Haytham and Zambetta, Fabio and van Schyndel, Ron},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00034},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Visual impairment;Assistive technologies;User interfaces;Sonification;Augmented reality;Human-centered computing-Accessibility-Accessibility technologies;Human-centered computing-Accessibility-Empirical studies in accessibility;Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Auditory feedback;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality},
	month = {March},
	pages = {184-193},
	title = {Evoking empathy with visually impaired people through an augmented reality embodiment experience},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00034}}

@inproceedings{10108472,
	abstract = {VR (Virtual Reality) Motion Sickness (VRMS) refers to purely visually-induced motion sickness. Not everyone is susceptible to VRMS, but if experienced, nausea will often lead users to withdraw from the ongoing VR applications. VRMS represents a serious challenge in the field of VR ergonomics and human factors. Like other neuro-ergonomics researchers did before, this paper considers VRMS as a brain state problem as various etiologies of VRMS support the claim that VRMS is caused by disagreement between the vestibular and visual sensory inputs. However, what sets this work apart from the existing literature is that it explores anti-VRMS brain patterns via electroencephalogram (EEG) in VRMS-resistant individuals. Based on existing datasets of a previous study, we found enhanced theta activity in the left parietal cortex in VRMS-resistant individuals (N= 10) compared to VRMS-susceptible individuals (N=10). Even though the sample size per se is not large, this finding achieved medium effect size. This finding offers new hypotheses regarding how to reduce VRMS by the enhancement of brain functions per se (e.g., via non-invasive transcranial electrostimulation techniques) without the need to redesign the existing VR content.},
	author = {Li, Gang and Pohlmann, Katharina and McGill, Mark and Chen, Chao Ping and Brewster, Stephen and Pollick, Frank},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00048},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Correlation;Ergonomics;Virtual reality;Human factors;Motion sickness;VR motion sickness;brain state problem;EEG;resistance to VR motion sickness;Neuro-ergonomics;vestibular system;VR motion sickness;cybersickness;EEG},
	month = {March},
	pages = {328-335},
	title = {Exploring Neural Biomarkers in Young Adults Resistant to VR Motion Sickness: A Pilot Study of EEG},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00048}}

@inproceedings{10108474,
	abstract = {We present a lightweight and efficient rain sound synthesis method for interactive virtual environments. Existing rain sound simulation methods require massive superposition of scene-specific precomputed rain sounds, which is excessive memory consumption for virtual reality systems (e.g. video games) with limited audio memory budgets. Facing this issue, we reduce the audio memory budgets by introducing a lightweight rain sound synthesis method which is only based on eight physically-inspired basic rain sounds. First, in order to generate sufficiently various rain sounds with limited sound data, we propose an exponential moving average based frequency domain additive (FDA) synthesis method to extend and modify the pre-computed basic rain sounds. Each rain sound is generated in the frequency domain before conversion back to the time domain, allowing us to extend the rain sound which is free of temporal distortions and discontinuities. Next, we introduce an efficient binaural rendering method to simulate the 3D perception that coheres with the visual scene based on a set of Near-Field Transfer Functions (NFTF). Various results demonstrate that the proposed method drastically decreases the memory cost (77 times compressed) and overcomes the limitations of existing methods in terms of interaction.},
	author = {Cheng, Haonan and Liu, Shiguang and Zhang, Jiawan},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00038},
	issn = {2642-5254},
	keywords = {Solid modeling;Visualization;Video games;Rain;Three-dimensional displays;Frequency-domain analysis;Memory management;Human-centered computing;Human computer interaction;Interaction techniques;Auditory feedback;Applied computing;Arts and humanities;Sound and music computing},
	month = {March},
	pages = {226-236},
	title = {Lightweight Scene-aware Rain Sound Simulation for Interactive Virtual Environments},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00038}}

@inproceedings{10108475,
	abstract = {Technology is disrupting the way films involving visual effects are produced. Chroma-key, LED walls, motion capture (mocap), 3D visual storyboards, and simulcams are only a few examples of the many changes introduced in the cinema industry over the last years. Although these technologies are getting commonplace, they are presenting new, unexplored challenges to the actors. In particular, when mocap is used to record the actors' movements with the aim of animating digital character models, an increase in the workload can be easily expected for people on stage. In fact, actors have to largely rely on their imagination to understand what the digitally created characters will be actually seeing and feeling. This paper focuses on this specific domain, and aims to demonstrate how Augmented Reality (AR) can be helpful for actors when shooting mocap scenes. To this purpose, we devised a system named AR-MoCap that can be used by actors for rehearsing the scene in AR on the real set before actually shooting it. Through an Optical See-Through Head-Mounted Display (OST-HMD), an actor can see, e.g., the digital characters of other actors wearing mocap suits overlapped in real-time to their bodies. Experimental results showed that, compared to the traditional approach based on physical props and other cues, the devised system can help the actors to position themselves and direct their gaze while shooting the scene, while also improving spatial and social presence, as well as perceived effectiveness.},
	author = {Cannav{\`o}, Alberto and Prattic{\`o}, Filippo Gabriele and Bruno, Alberto and Lamberti, Fabrizio},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00047},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;User interfaces;Visual effects;Optical imaging;Motion pictures;Motion capture;Collaborative virtual production;acting rehearsal and performance;motion capture;body ownership;virtual characters;visual effects;augmented reality;Human-centered computing---Human computer interaction (HCI)---Mixed / augmented reality---;Human-centered computing---Human computer interaction (HCI)---Collaborative interaction---;Computing methodologies---Computer vision---;Image and video acquisition---Motion capture;Human-centered computing---Applied computing-Arts and humanities---Performing arts;Human-centered computing---Applied computing-Arts and humanities---Media arts},
	month = {March},
	pages = {318-327},
	title = {AR-MoCap: Using Augmented Reality to Support Motion Capture Acting},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00047}}

@inproceedings{10108476,
	abstract = {RGBD cameras can capture users and their actions in the real world for reconstruction of photo-realistic volumetric avatars that allow rich interaction between spatially distributed telepresence parties in virtual environments. In this paper, we present and evaluate a system design that enables volumetric avatar reconstruction at increased frame rates. We demonstrate that we can overcome the limited capturing frame rate of commodity RGBD cameras such as the Azure Kinect by dividing a set of cameras into two spatio-temporally offset reconstruction groups and implementing a real-time reconstruction pipeline to fuse the temporally offset RGBD image streams. Comparisons of our proposed system against capture configurations possible with the same number of RGBD cameras indicate that it is beneficial to use a combination of spatially and temporally offset RGBD cameras, allowing increased reconstruction frame rates and scene coverage while producing temporally consistent volumetric avatars.},
	author = {Rendle, Gareth and Kreskowski, Adrian and Froehlich, Bernd},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00023},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Telepresence;Avatars;Virtual environments;User interfaces;Streaming media;Cameras;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems---Artificial, augmented, and virtual realities},
	month = {March},
	pages = {72-82},
	title = {Volumetric Avatar Reconstruction with Spatio-Temporally Offset RGBD Cameras},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00023}}

@inproceedings{10108478,
	abstract = {Avatars are one of the most important elements in virtual environments. Real-time facial retargeting technology is of vital importance in AR/VR interactions, the filmmaking, and the entertainment industry, and blendshapes for avatars are one of its important materials. Previous works either focused on the characters with the same topology, which cannot be generalized to universal avatars, or used optimization methods that have high demand on the dataset. In this paper, we adopt the essence of deep learning and feature transfer to realize deformation transfer, thereby generating blendshapes for target avatars based on the given sources. We proposed a Variational Autoencoder (VAE) to extract the latent space of the avatars and then use a Multilayer Perceptron (MLP) model to realize the translation between the latent spaces of the source avatar and target avatars. By decoding the latent code of different blendshapes, we can obtain the blendshapes for the target avatars with the same semantics as that of the source. We qualitatively and quantitatively compared our method with both classical and learning-based methods. The results revealed that the blendshapes generated by our method achieves higher similarity to the groundtruth blendshapes than the state-of-art methods. We also demonstrated that our method can be applied to expression transfer for stylized characters with different topologies.},
	author = {Wang, Jingying and Qiu, Yilin and Chen, Keyu and Ding, Yu and Pan, Ye},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00050},
	issn = {2642-5254},
	keywords = {Deep learning;Training;Solid modeling;Three-dimensional displays;Avatars;Semantics;Virtual environments;Avatars;Blendshapes;facial animation;stylized char-acters;Human-centered computing-Visualization-Visu-alization techniques-Treemaps;Human-centered computing-Visualization-Visualization design and evaluation methods},
	month = {March},
	pages = {347-355},
	title = {Fully Automatic Blendshape Generation for Stylized Characters},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00050}}

@inproceedings{10108479,
	abstract = {Image-based rendering (IBR) technique enables presenting real scenes interactively to viewers and hence is a key component for implementing VR telepresence. The quality of IBR results depends on the set of pre-captured views, the rendering algorithm used, and the camera parameters of the novel view to be synthesized. Numerous methods were proposed for optimizing the set of captured images and enhancing the rendering algorithms. However, from which regions IBR methods can synthesize satisfactory results is not yet well studied. In this work, we introduce the concept of renderability, which predicts the quality of IBR results at any given viewpoint and view direction. Consequently, the renderability values evaluated for the 5D camera parameter space form a field, which effectively guides viewpoint/trajectory selection for IBR, especially for challenging large-scale 3D scenes. To demonstrate this capability, we designed 2 VR applications: a path planner that allows users to navigate through sparsely captured scenes with controllable rendering quality and a view selector that provides an overview for a scene from diverse and high quality perspectives. We believe the renderability concept, the proposed evaluation method, and the suggested applications will motivate and facilitate the use of IBR in various interactive settings.},
	author = {Yi, Zimu and Xie, Ke and Lyu, Jiahui and Gong, Minglun and Huang, Hui},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00051},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Telepresence;Navigation;Virtual reality;User interfaces;Aerospace electronics;Rendering (computer graphics);Computer graphics techniques-Image-based rendering-Scene rendering;Evaluation methods-Renderability-View selection-Rendering path planning},
	month = {March},
	pages = {356-366},
	title = {Where to Render: Studying Renderability for IBR of Large-Scale Scenes},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00051}}

@inproceedings{10108481,
	abstract = {Linear hand movement in mid-air is one of the most fundamental interactions in virtual reality (e.g., when dragging/scaling/manipulating objects and drawing shapes). However, the lack of tactile feedback makes it difficult to precisely control the direction and amplitude of hand movement. In this paper, we conducted three user studies to progressively examine users' ability of fine motor control in 3D linear hand movement tasks. In Study 1, we examined participants' behavioural patterns when drawing straight lines in various directions and lengths, using both the hand and the controller. Results showed that the exhibited stroke length tended to be longer than perceived, regardless of the interaction tool. While displaying the trajectory could help reduce directional and length errors. In Study 2, we further tested the effect of different visual references and found that, compared with an empty room or cluttered scenarios, providing only a virtual table yielded higher input precision and user preference. In Study 3, we repeated Study 2 in real dragging and scaling tasks and verified the generalizability of the findings in terms of input error. Our core finding is that the user's hand moves significantly longer than the task length due to the underestimation of stroke length, yet the error of the Z-axis movement is smaller than that of the X-axis and the Y-axis, and a simple virtual desktop can effectively reduce errors.},
	author = {Yi, Xin and Wang, Xueyang and Li, Jiaqi and Li, Hewu},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00058},
	issn = {2642-5254},
	keywords = {Motor drives;Visualization;Solid modeling;Three-dimensional displays;Shape;Tactile sensors;Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User models},
	month = {March},
	pages = {427-437},
	title = {Examining the Fine Motor Control Ability of Linear Hand Movement in Virtual Reality},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00058}}

@inproceedings{10108482,
	abstract = {The recent popularity of consumer-grade virtual reality devices has enabled users to experience immersive shopping in virtual environments. As in a real-world store, the placement of products in a virtual store should appeal to shoppers, which could be time-consuming, tedious, and non-trivial to create manually. Thus, this work introduces a novel approach for automatically optimizing product placement in virtual stores. Our approach considers product exposure and spatial constraints, applying an optimizer to search for optimal product placement solutions. We conducted qualitative scene rationality and quantitative product exposure experiments to validate our approach with users. The results show that the proposed approach can synthesize reasonable product placements and increase product exposures for different virtual stores.},
	author = {Liang, Wei and Wang, Luhui and Yu, Xinzhe and Li, Changyang and Alghofaili, Rawan and Lang, Yining and Yu, Lap-Fai},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00049},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Layout;Virtual environments;Lighting;User interfaces;Optimization;Human-centered computing-Human computer interaction (HCI)-;-Computing methodologies-Virtual reality},
	month = {March},
	pages = {336-346},
	title = {Optimizing Product Placement for Virtual Stores},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00049}}

@inproceedings{10108488,
	abstract = {We present an augmented virtuality (AV) pipeline that enables the user to interact with real-world objects through stylised representations which match the VR scene and thereby preserve immersion. It consists of three stages: First, the object of interest is reconstructed from images and corresponding camera poses recorded with the VR headset, or alternatively a retrieval model finds a fitting mesh from the ShapeNet dataset. Second, a style transfer technique adapts the mesh to the VR game scene in order to preserve consistent immersion. Third, the stylised mesh is superimposed on the real object in real time to ensure interactivity even if the real object is moved. Our pipeline serves as proof of concept for style-aware AV embeddings.},
	author = {Hoster, Johannes and Ritter, Dennis and Hildebrand, Kristian},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00032},
	issn = {2642-5254},
	keywords = {Surface reconstruction;Three-dimensional displays;Augmented virtuality;Shape;Pipelines;Mixed reality;User interfaces;Mixed reality-Virtual reality-Reconstruction},
	month = {March},
	pages = {163-172},
	title = {Style-aware Augmented Virtuality Embeddings (SAVE)},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00032}}

@inproceedings{10108489,
	abstract = {Remote collaborative work has become pervasive in many settings, ranging from engineering to medical professions. Users are im-mersed in virtual environments and communicate through life-sized avatars that enable face-to-face collaboration. Within this context, users often collaboratively view and interact with virtual 3D models, for example to assist in the design of new devices such as cus-tomized prosthetics, vehicles or buildings. Discussing such shared 3D content face-to-face, however, has a variety of challenges such as ambiguities, occlusions, and different viewpoints that all decrease mutual awareness, which in turn leads to decreased task performance and increased errors. To address this challenge, we introduce MAGIC, a novel approach for understanding pointing gestures in a face-to-face shared 3D space, improving mutual understanding and awareness. Our approach distorts the remote user's gestures to correctly reflect them in the local user's reference space when face-to-face. To measure what two users perceive in common when using pointing gestures in a shared 3D space, we introduce a novel metric called pointing agreement. Results from a user study suggest that MAGIC significantly improves pointing agreement in face-to-face collaboration settings, improving co-presence and awareness of interactions performed in the shared space. We believe that MAGIC improves remote collaboration by enabling simpler communication mechanisms and better mutual awareness.},
	author = {Fidalgo, Catarina G. and Sousa, Maur{\'\i}cio and Mendes, Daniel and Dos Anjos, Rafael Kuffner and Medeiros, Daniel and Singh, Karan and Jorge, Joaquim},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00059},
	issn = {2642-5254},
	keywords = {Performance evaluation;Solid modeling;Social computing;Three-dimensional displays;Avatars;Collaboration;Virtual environments;Human-centered computing-Collaborative and social computing-Collaborative and social computing theory;con-cepts and paradigms-Computer supported cooperative work},
	month = {March},
	pages = {438-448},
	title = {MAGIC: Manipulating Avatars and Gestures to Improve Remote Collaboration},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00059}}

@inproceedings{10108490,
	abstract = {Augmented reality (AR) helps users easily accept information when they are walking by providing virtual information in front of their eyes. However, it remains unclear how to present AR notifications considering the expected user reaction to interruption. Therefore, we investigated to confirm appropriate placement methods for each type by dividing it into notification types that are handled immediately (high) or that are performed later (low). We compared two coordinate systems (display-fixed and body-fixed) and three positions (top, right, and bottom) for the notification placement. We found significant effects of notification type and placement on how notifications are perceived during the AR notification experience. Using a display-fixed coordinate system responded faster for high notification types, whereas using a body-fixed coordinate system resulted in quick walking speed for low ones. As for the position, the high types had a higher notification performance at the bottom position, but the low types had enhanced walking performance at the right position. Based on the finding of our experiment, we suggest some recommendations for the future design of AR notification while walking.},
	author = {Lee, Hyunjin and Woo, Woontack},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00067},
	issn = {2642-5254},
	keywords = {Legged locomotion;Three-dimensional displays;Design methodology;Resists;User interfaces;Time factors;Task analysis;Human-centered computing-Human computer in-teraction (HCI)-HCI design and evaluation methods-User studies;Human-centered computing-Ubiquitous and mobile computing-Ubiquitous and mobile computing design and evaluation methods},
	month = {March},
	pages = {519-529},
	title = {Exploring the Effects of Augmented Reality Notification Type and Placement in AR HMD while Walking},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00067}}

@inproceedings{10108491,
	abstract = {Occlusion is a crucial visual element in optical see-through (OST) augmented reality, however, implementing occlusion in OST displays while addressing various design trade-offs is a difficult problem. In contrast to the traditional method of using spatial light modulators (SLMs) for the occlusion mask, using photochromic materials as occlusion masks can effectively eliminate diffraction artifacts in see-through views due to the lack of electronic pixels, thus providing superior see-through image quality. However, this design requires UV illumination to activate the photochromic mate-rial, which traditionally requires multiple SLMs, resulting in a larger form factor for the system. This paper presents a compact photochromic occlusion-capable OST design using multilayer, wavelength-dependent holographic optical lenses (HOLs). Our approach employs a single digital mi-cromirror display (DMD) to form both the occlusion mask with UV light and a virtual image with visible light in a time-multiplexed man-ner. We demonstrate our proof-of-concept system on a bench-top setup and assess the appearance and contrasts of the displayed image. We also suggest potential improvements for current prototypes to encourage the community to explore this occlusion approach.},
	author = {Ooi, Chun-Wei and Hiroi, Yuichi and Itoh, Yuta},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00039},
	issn = {2642-5254},
	keywords = {Visualization;Optical diffraction;Three-dimensional displays;Optical design;Prototypes;Holography;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Communication hardware;interfaces and storage-Displays and imagers},
	month = {March},
	pages = {237-242},
	title = {A Compact Photochromic Occlusion Capable See-through Display with Holographic Lenses},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00039}}

@inproceedings{10108494,
	abstract = {Virtual reality (VR) is distinguished by the rich, multimodal, im-mersive sensory information and affordances provided to the user. However, when moving about an immersive virtual world the vi-sual display often conflicts with other sensory cues due to design, the nature of the simulation, or to system limitations (for example impoverished vestibular motion cues during acceleration in racing games). Given that conflicts between sensory cues have been as-sociated with disorientation or discomfort, and theoretically could distort spatial perception, it is important that we understand how and when they are manifested in the user experience. To this end, this set of experiments investigates the impact of mismatch between physical and virtual motion parallax on the per-ception of the depth of an apparently perpendicular dihedral angle (a fold) and its distance. We applied gain distortions between visual and kinesthetic head motion during lateral sway movements and measured the effect of gain on depth, distance and lateral space compression. We found that under monocular viewing, observers made smaller object depth and distance settings especially when the gain was greater than 1. Estimates of target distance declined with increasing gain under monocular viewing. Similarly, mean set depth decreased with increasing gain under monocular viewing, except at 6.0 m. The effect of gain was minimal when observers viewed the stimulus binocularly. Further, binocular viewing (stereopsis) improved the precision but not necessarily the accuracy of gain perception. Overall, the lateral compression of space was similar in the stereoscopic and monocular test conditions. Taken together, our results show that the use of large presentation distances (at 6 m) combined with binocular cues to depth and distance enhanced humans' tolerance to visual and kinesthetic mismatch.},
	author = {Teng, Xue and Allison, Robert S. and Wilcox, Laurie M.},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00055},
	issn = {2642-5254},
	keywords = {Visualization;Solid modeling;Three-dimensional displays;Sensitivity;Shape;Stereo image processing;Virtual reality;Depth Perception;Egocentric Distance;Motion Gain;Motion Parallax},
	month = {March},
	pages = {398-408},
	title = {Manipulation of Motion Parallax Gain Distorts Perceived Distance and Object Depth in Virtual Reality},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00055}}

@inproceedings{10108495,
	abstract = {Although a growing number of charities have used virtual reality (VR) technology for fundraising activities, with better results than ever before, little research has been undertaken on what factors make VR beneficial in supporting charitable giving. The primary goal of this study is to investigate the underlying mechanism of VR in supporting charitable giving, which extends the current literature on VR and donation behaviors. The findings of this study indicated that VR charitable appeals increase actual money donations when compared to the traditional two-dimensional (2D) format and that this effect is achieved through a serial mediating effect of vicarious experience and existential guilt. Findings also identify the need for stimulation as a boundary condition, indicating that those with a higher (vs. lower) need for stimulation were more (vs. less) affected by the mediating mechanism of VR charitable appeals on donations. This work contributes to our understanding of the relationship between VR technology and charitable giving, as well as to future research on VR and its prosocial applications.},
	author = {Li, Ou and Qiu, Han},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00079},
	issn = {2642-5254},
	keywords = {Computers;Three-dimensional displays;Two dimensional displays;Virtual reality;User interfaces;Boundary conditions;Behavioral sciences;Virtual reality;charitable giving;vicarious experience;guilt;need for stimulation;J.4 [Social and Behavioral Sciences];K.4.4 [Computers and Society]: Electronic Commerce},
	month = {March},
	pages = {641-647},
	title = {Virtual Reality in Supporting Charitable Giving: The Role of Vicarious Experience, Existential Guilt, and Need for Stimulation},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00079}}

@inproceedings{10108496,
	abstract = {Three-dimensional curve drawing in Augmented Reality (AR) enables users to create 3D curves that fit within the real-world scene. It has applications in 3D design, sculpting, and animation. However, the task complexity increases when the desirable path for the curve is obstructed by the physical environment or by what the camera can see. For example, it is difficult to draw a curve that wraps around an object or scales to out-of-reach places. We propose WARPY, an environment-aware 3D curve drawing tool for mobile AR. Our system enables users to draw freeform curves from a distance in AR by combining 2D-to-3D sketch inference with geometric proxies. Geometric Proxies can be obtained via 3D scanning or from a list of pre-defined primitives. WARPY also provides a multi-view mode to enable users to sketch a curve from multiple viewpoints, which is useful if the target curve cannot fit within the camera's field of view. We conducted two user studies and found that WARPY can be a viable tool to help users create complex and large curves in AR.},
	author = {Alghofaili, Rawan and Nguyen, Cuong and Krs, Vojt{\u e}ch and Carr, Nathan and M{\u e}ch, Radom{\'\i}r and Yu, Lap-Fai},
	booktitle = {2023 IEEE Conference Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:40 -0400},
	date-modified = {2024-03-18 02:30:40 -0400},
	doi = {10.1109/VR55154.2023.00052},
	issn = {2642-5254},
	keywords = {Geometry;Three-dimensional displays;Spirals;Shape;User interfaces;Cameras;Animation;Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality},
	month = {March},
	pages = {367-377},
	title = {WARPY: Sketching Environment-Aware 3D Curves in Mobile Augmented Reality},
	year = {2023},
	bdsk-url-1 = {https://doi.org/10.1109/VR55154.2023.00052}}

@inproceedings{9756729,
	abstract = {Owning a virtual body inside Virtual Reality (VR) offers a unique experience where, typically, users are able to control their self-avatar's body via tracked VR controllers. However, controlling a self-avatar's facial movements is harder due to the HMD being in the way for tracking. In this work we present (1) the technical pipeline of creating and rigging self-alike avatars, whose facial expressions can be then controlled by users wearing the VIVE Pro Eye and VIVE Facial Tracker, and (2) based on this setting, two within-group studies on the psychological impact of the appearance realism of self-avatars, both the level of photorealism and self-likeness. Participants were told to practise their presentation, in front of a mirror, in the body of a realistic looking avatar and a cartoon like one, both animated with body and facial mocap data. In study 1 we made two bespoke self-alike avatars for each participant and we found that although participants found the cartoon-like character more attractive, they reported higher Body Ownership with whichever the avatar they had in the first trial. In study 2 we used generic avatars with higher fidelity facial animation, and found a similar "first trial effect" where they reported the avatar from their first trial being less creepy. Our results also suggested participants found the facial expressions easier to control with the cartoon-like character. Further, our eye-tracking data suggested that although participants were mainly facing their avatar during their presentation, their eye-gaze were focused elsewhere half of the time.},
	author = {Ma, Fang and Pan, Xueni},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00023},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Tracking;Atmospheric measurements;Avatars;Pipelines;Psychology;Virtual Reality;Facial Expressions;Embodiment},
	month = {March},
	pages = {57-65},
	title = {Visual Fidelity Effects on Expressive Self-avatar in Virtual Reality: First Impressions Matter},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00023}}

@inproceedings{9756730,
	abstract = {This paper investigates how the size of virtual space and objects within it affect the threshold range of relative translation gains, a Redirected Walking (RDW) technique that scales the user's movement in virtual space in different ratios for the width and depth. While previous studies assert that a virtual room's size affects relative translation gain thresholds on account of the virtual horizon's location, additional research is needed to explore this assumption through a structured approach to visual perception in Virtual Reality (VR). We estimate the relative translation gain thresholds in six spatial conditions configured by three room sizes and the presence of virtual objects (3  2), which were set according to differing Angles of Declination (AoDs) between eye-gaze and the forward-gaze. Results show that both size and virtual objects significantly affect the threshold range, it being greater in the large-sized condition and furnished condition. This indicates that the effect of relative translation gains can be further increased by constructing a perceived virtual movable space that is even larger than the adjusted virtual movable space and placing objects in it. Our study can be applied to adjust virtual spaces in synchronizing heterogeneous spaces without coordinate distortion where real and virtual objects can be leveraged to create realistic mutual spaces.},
	author = {Kim, Dooyoung and Kim, Jinwook and Shin, Jae-Eun and Yoon, Boram and Lee, Jeongmi and Woo, Woontack},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00057},
	issn = {2642-5254},
	keywords = {Legged locomotion;Three-dimensional displays;Conferences;Virtual reality;User interfaces;Distortion;Visual perception;Virtual Reality;relative translation gains;threshold;redirected walking;angle of declination;virtual object},
	month = {March},
	pages = {379-388},
	title = {Effects of Virtual Room Size and Objects on Relative Translation Gain Thresholds in Redirected Walking},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00057}}

@inproceedings{9756731,
	abstract = {The Empathy-Effective Communication hypothesis states the better a speaker can understand their listener's emotions, the better can they transmit information; and the better a listener can understand the speaker's emotions, the better can they apprehend the information. Previous emotional sharing systems have managed to create a space of emotional understanding between collaborators on remote locations using bio-sensing, but how a context of face-to-face communication can benefit from biofeedback is still to be studied. This study introduces a new Augmented Reality communication cue from an emotion recognition neural network model, trained using electrocardiogram physiological data (AuRea). The proposed de-sign is meant to facilitate emotional state understanding, increasing cognitive empathy without compromising the existing verbal, non-verbal, and paraverbal communication cues. We conducted a study where pairs of participants (N=12) engaged in three tasks where AuRea was found to positively affect performance and emotional understanding, but negatively affect memorization.},
	author = {Valente, Andreia and Lopes, Daniel Sim{\~o}es and Nunes, Nuno and Esteves, Augusto},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00034},
	issn = {2642-5254},
	keywords = {Emotion recognition;Solid modeling;Three-dimensional displays;Neural networks;Collaboration;Physiology;Data models;Human-centered computing;Collaborative and social computing;Empirical studies in collaborative and social computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality},
	month = {March},
	pages = {158-166},
	title = {Empathic AuRea: Exploring the Effects of an Augmented Reality Cue for Emotional Sharing Across Three Face-to-Face Tasks},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00034}}

@inproceedings{9756732,
	abstract = {We examined the effects on users during collaboration with two types of virtual human (VH) agents in object transportation in an immersive virtual environment or virtual reality (VR). The two types of virtual humans we examined are leader and follower agents. The goal of the users is to interact with the agents using natural language and carry objects from initial locations to destinations. In each trial, the follower agent follows a user's instructions to perform actions to manipulate the object. The leader agent determines the appropriate actions that the agent and the user should perform. We developed a system which enabled users and virtual agents to carry objects in an intuitive manner. We conducted a within-subjects study to evaluate the user behaviors under two conditions: (LVH) interaction with a leader virtual human and (FVH) interaction with a follower virtual human. We found that the participants in LVH required a higher workload than that in FVH. However, the users' experiences, game experiences, and users' impressions between the two conditions were not significantly different.},
	author = {Liu, Kuan-Yu and Wong, Sai-Keung and Volonte, Matias and Ebrahimi, Elham and Babu, Sabarish V.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00052},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Multimedia systems;Natural languages;Collaboration;Virtual environments;Transportation;Virtual Humans;Virtual Reality;Animation;Behavior Modeling;Human Computer Interaction},
	month = {March},
	pages = {330-339},
	title = {Investigating the Effects of Leading and Following Behaviors of Virtual Humans in Collaborative Fine Motor Tasks in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00052}}

@inproceedings{9756733,
	abstract = {Casual stereoscopic photography allows ordinary users to create a stereoscopic photo using two photos taken casually by a monocular camera. The visual comfort of a casual stereoscopic photo can greatly affect its visual experience. In this paper, we present a novel visual comfort enhancement method for casual stereoscopic photography via reinforcement learning based on continuous transformation superposition. We consider the transformation, in a continuous transformation space, to transform each view as superpositions of several basic continuous transformations, enabling more subtle and flexible image transformation operations to approach better solutions. To achieve the continuous transformation superposition, we prepare a collection of continuous transformation models for translation, rotation, and perspective transformations. Then we train a policy model to determine an optimal transformation chain to recurrently handle both the geometric constraints and disparity adjustment, and thereby enhance the visual comfort of casual stereoscopic images. We further propose an attention-based stereo feature fusion module that enhances and integrates the binocular information between the left and right views. Experimental results on three datasets demonstrate that our proposed method achieves superior performance to state-of-the-art methods.},
	author = {Chen, Yuzhong and Shen, Qijin and Niu, Yuzhen and Liu, Wenxi},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00051},
	issn = {2642-5254},
	keywords = {Photography;Visualization;Solid modeling;Three-dimensional displays;Stereo image processing;Conferences;Virtual reality;Human-centered computing;3D authoring;Image and video acquisition;Computer vision},
	month = {March},
	pages = {321-329},
	title = {Continuous Transformation Superposition for Visual Comfort Enhancement of Casual Stereoscopic Photography},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00051}}

@inproceedings{9756734,
	abstract = {We propose TimeTables, a novel prototype system that aims to support data exploration, using embodiment with space-time cubes in virtual reality. TimeTables uses multiple space-time cubes on virtual tabletops, which users can manipulate by extracting time layers or individual buildings to create new tabletop views. The surrounding environment includes a large space for multiple linked tabletops and a storage wall. TimeTables presents information at different time scales by stretching layers to drill down in time. Users can also jump into tabletops to inspect data from an egocentric perspective. We present a use case scenario of energy consumption displayed on a university campus to demonstrate how our system could support data exploration and analysis over space and time. From our experience and analysis we believe the system has a high potential in assisting spatio-temporal data exploration and analysis.},
	author = {Zhang, Yidan and Ens, Barrett and Satriadi, Kadek Ananta and Prouzeau, Arnaud and Goodwin, Sarah},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00080},
	issn = {2642-5254},
	keywords = {Energy consumption;Three-dimensional displays;Data analysis;Conferences;Buildings;Prototypes;Data visualization;Human-centered computing;Visualization;Visualization application domains;Information visualization;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction techniques},
	month = {March},
	pages = {599-605},
	title = {TimeTables: Embodied Exploration of Immersive Spatio-Temporal Data},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00080}}

@inproceedings{9756736,
	abstract = {Psychological factors to be assessed by questionnaires, such as presence, may be influenced by variables like a break in presence induced by transitioning between physical and virtual reality - or time elapsed between the actual sensation and its measurement when questionnaires are used. Moreover, participants of previous experiments stated discomfort due to changing repeatedly between VR and non-VR. These aspects motivate the investigation of new ways of administering questionnaires in VR research. Previous attempts to integrate questionnaires into VR as world- or user-anchored canvases still lead to a clear separation between interaction for task and data acquisition in the virtual environment, which has been shown as problematic due to interruption of experience. Therefore, this work goes one step further by integrating the answering of the questionnaire into the actual task using the same interaction technique and metaphor. We implemented a bow and arrow game in which the player had to shoot at randomly spawning targets as fast and accurate as possible. Subsequently, the player had to answer each item of a questionnaire by actually shooting at a target that represented the rating with the same type of interaction method. No difference in presence (SUS-PQ), satisfaction (ASQ) or workload (SMEQ) ratings was found between questionnaires presented either embedded in VR (as described above), as text panel in VR or as desktop PC version. In conclusion, the embedded questionnaires may render less negative effects of transitioning between VR and non-VR without impairing the questionnaire results.},
	author = {Gr{\"u}ndling, Jan P. and Zeiler, Daniel and Weyers, Benjamin},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00089},
	issn = {2642-5254},
	keywords = {Human computer interaction;Three-dimensional displays;Design methodology;Virtual environments;Psychology;Resists;Games;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;Empirical Studies in HCI},
	month = {March},
	pages = {683-692},
	title = {Answering With Bow and Arrow: Questionnaires and VR Blend Without Distorting the Outcome},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00089}}

@inproceedings{9756737,
	abstract = {This paper investigates the influence of floor-vibration tactile feedback on immersed users. Under symmetric and asymmetric tactile sensory cue conditions, we explore how multi-user Virtual Reality (VR) experiences are impacted by these cues in terms of illusion and coherence. Based on the reported positive impact of tactile cues in solo VR experiences, we posit that if context-matched perceptual tactile feedback is exchanged between users, they will report a significantly enhanced VR experience compared to not receiving the sensory stimuli, even within the same immersive VR experience. With our custom-built, computer-controlled vibration floor, we implemented a cannonball shooting game for two physically-separated players. In the VR game, the two players shoot cannonballs to destroy their opponent's protective wall and cannon, while the programmed floor platform generates vertical vibrations depending on the experimental condition. We used a mixed-factorial design with four conditions for each pair of participants: 1) both A and B had vibration, and 2) neither A nor B had vibration (the Symmetric group), or 3) A had vibration, but B did not, and 4) B had vibration, but A did not (the Asymmetric group). We collected subjective and objective data for variables previously shown to be related to levels of illusion, coherence, and usability, including Presence, Co-Presence, Social Presence, Plausibility Illusion, Engagement, Embodiment, Coherence, Gaming Performance, and Overall Preference. A total of 39 pairs of participants were involved in the study. We found statistically significant differences for the vibration conditions on Co-Presence, Social Presence, Engagement, and Coherence, and for the symmetric conditions on the Plausibility Illusion and Coherence, but only with trivial or small effect sizes. The results indicate that vibration provided to a pair of game players in immersive VR can significantly enhance the VR experience, but sensory symmetry does not guarantee improved gaming performance.},
	author = {Jung, Sungchul and Wu, Yuanjie and McKee, Ryan and Lindeman, Robert W.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00095},
	issn = {2642-5254},
	keywords = {Vibrations;Three-dimensional displays;Conferences;Tactile sensors;Games;Coherence;Virtual reality;Floor-vibration;Whole-body tactile;tactile;vibration;VR Game;Shared VR;Multiuser VR;Competition Game;Symmetric;Asymmetric},
	month = {March},
	pages = {737-745},
	title = {All Shook Up: The Impact of Floor Vibration in Symmetric and Asymmetric Immersive Multi-user VR Gaming Experiences},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00095}}

@inproceedings{9756738,
	abstract = {Single-view depth estimation from omnidirectional images has gained popularity with its wide range of applications such as autonomous driving and scene reconstruction. Although data-driven learning-based methods demonstrate significant potential in this field, scarce training data and ineffective 360 estimation algorithms are still two key limitations hindering accurate estimation across diverse domains. In this work, we first establish a large-scale dataset with varied settings called Depth360 to tackle the training data problem. This is achieved by exploring the use of a plenteous source of data, 360 videos from the internet, using a test-time training method that leverages unique information in each omnidirectional sequence. With novel geometric and temporal constraints, our method generates consistent and convincing depth samples to facilitate single-view estimation. We then propose an end-to-end two-branch multi-task learning network, SegFuse, that mimics the human eye to effectively learn from the dataset and estimate high-quality depth maps from diverse monocular RGB images. With a peripheral branch that uses equirectangular projection for depth estimation and a foveal branch that uses cubemap projection for semantic segmentation, our method predicts consistent global depth while maintaining sharp details at local regions. Experimental results show favorable performance against the state-of-the-art methods.},
	author = {Feng, Qi and Shum, Hubert P. H. and Morishima, Shigeo},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00087},
	issn = {2642-5254},
	keywords = {Training;Three-dimensional displays;MIMICs;Semantics;Estimation;Training data;Virtual reality;Computing methodologies;Computer graphics;Image manipulation;Image-based rendering;Artificial intelligence;Computer vision;Reconstruction},
	month = {March},
	pages = {664-673},
	title = {360 Depth Estimation in the Wild - the Depth360 Dataset and the SegFuse Network},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00087}}

@inproceedings{9756739,
	abstract = {Light-based adversarial attacks use spatial augmented reality (SAR) techniques to fool image classifiers by altering the physical light condition with a controllable light source, e.g., a projector. Compared with physical attacks that place hand-crafted adversarial objects, projector-based ones obviate modifying the physical entities, and can be performed transiently and dynamically by altering the projection pattern. However, subtle light perturbations are insufficient to fool image classifiers, due to the complex environment and project-and-capture process. Thus, existing approaches focus on projecting clearly perceptible adversarial patterns, while the more interesting yet challenging goal, stealthy projector-based attack, remains open. In this paper, for the first time, we formulate this problem as an end-to-end differentiable process and propose a Stealthy Projector-based Adversarial Attack (SPAA) solution. In SPAA, we approximate the real Project-and-Capture process using a deep neural network named PCNet, then we include PCNet in the optimization of projector-based attacks such that the generated adversarial projection is physically plausible. Finally, to generate both robust and stealthy adversarial projections, we propose an algorithm that uses minimum perturbation and adversarial confidence thresholds to alternate between the adversarial loss and stealthiness loss optimization. Our experimental evaluations show that SPAA clearly outperforms other methods by achieving higher attack success rates and meanwhile being stealthier, for both targeted and untargeted attacks.},
	author = {Huang, Bingyao and Ling, Haibin},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00073},
	issn = {2642-5254},
	keywords = {Deep learning;Solid modeling;Three-dimensional displays;Sensitivity;Perturbation methods;Neural networks;Spatial augmented reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Security and privacy;Human and societal aspects of security and privacy;Privacy protections Computing methodologies;Artificial intelligence;Computer vision;Object recognition},
	month = {March},
	pages = {534-542},
	title = {SPAA: Stealthy Projector-based Adversarial Attacks on Deep Image Classifiers},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00073}}

@inproceedings{9756740,
	abstract = {Planning tactical operations on topographic maps, for rescue or military missions, is a complex process conducted by interdisciplinary experts and involves the time-consuming derivation of 3D information from 2D maps, mostly solely executed by experienced professionals. Previous research repeatedly showed that virtual reality (VR) can convey spatial relationships and complex 3D structures intuitively. In this work, we leverage the benefits of immersive head-mounted displays (HMDs) and present the design, implementation, and evaluation of a collaborative VR application for tactical resource planning on spatial data. We derived system and design requirements from consultations with domain experts and observations of a military on-site staff exercise, a simulation-based training aiming to strengthen rapid decision-making and teamwork during a time of crisis. To evaluate our prototype, we conducted semi-structured interviews with domain experts who organized and observed field tests at different military staff exercises. The interviews support the proposed design of the prototype and show general design implications for planning tools in VR. Our results show that the potential of VR-based tactical resource planning is dependent on the technical features as well as on non-technical environmental aspects, such as user attitude, prior experience, and interoperability.},
	author = {Medeiros, Marina L. and Schlager, Bettina and Kr{\"o}sl, Katharina and Fuhrmann, Anton},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00036},
	issn = {2642-5254},
	keywords = {Training;Three-dimensional displays;Prototypes;Virtual reality;User interfaces;Spatial databases;Planning;Computer Graphics [I.3.7]: Three-Dimensional Graphics and Realism;Virtual Reality},
	month = {March},
	pages = {176-185},
	title = {The Potential of VR-based Tactical Resource Planning on Spatial Data},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00036}}

@inproceedings{9756741,
	abstract = {This investigation compared the impact on the users' non-verbal behaviors elicited by interacting with a crowd of emotional virtual humans (VHs) in native and non-native language settings. In a between-subject experiment, we collected objective metrics regarding the users' behaviors during interaction with a crowd of VHs who expressed verbal and non-verbal communicative behaviors in response to users' speech-based interaction. The study presented four VH crowd conditions based on their emotional disposition, namely Positive, Negative, Neutral, and Mixed, in accordance with which VHs exhibited the appropriate behaviors. Participants were tasked with collecting items in a virtual flea market, and they interacted using natural speech-based dialogue with the VHs. The language conditions were collected in the USA and under two different conditions in Taiwan. The participants in the USA group interacted with the VHs in English (a native language for the USA setting); and two different groups in Taiwan interacted with the VHs in either a foreign (English) or native (Mandarin) language, respectively. Results reveal that in the (TMA, TEN) and (UEN, TEN) conditions, native versus non-native language communication can alter social behaviors (e.g., interaction time) of participants towards virtual interlocutors. Our results also revealed strong effects of cultural differences on participants' non-verbal social behaviors with VHs in the UEN and TMA conditions.},
	author = {Wang, Chang-Chun and Volonte, Matias and Ebrahimi, Elham and Liu, Kuan-Yu and Wong, Sai-Keung and Babu, Sabarish V.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00053},
	issn = {2642-5254},
	keywords = {Measurement;Three-dimensional displays;Multimedia systems;Conferences;Virtual reality;User interfaces;Cultural differences;Virtual Humans;Virtual Reality;Human-Computer Interaction},
	month = {March},
	pages = {340-349},
	title = {An Evaluation of Native versus Foreign Communicative Interactions on Users' Behavioral Reactions towards Affective Virtual Crowds},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00053}}

@inproceedings{9756742,
	abstract = {Single-image-based view synthesis is significant for generating a 3D scene and gains increasing attention in recent years. However, this task is challenging as it requires inferring contents beyond what is immediately visible. Previous methods directly predict the unknown views using the convolutional neural networks, but the generated views suffer from visually unpleasant holes, deformations, and artifacts. In this paper, we propose a Single-image-based view synthesis transformer (named SivsFormer) for high-quality and realistic view synthesis. In particular, a warping and occlusion handing module is designed to reduce the influence of parallax on the network. Subsequently, a disparity alignment module captures the long-range information over the scene and ensures that pixels move in a geometrically correct manner with soft probabilistic disparity maps. Moreover, we present a parallax-aware loss function to improve the quality of the synthetic images, which explicitly quantifies the magnitude of parallaxes. We conduct extensive experiments on popular KITTI and Cityscapes datasets. Benefitting from the proposed parallax-aware transformer, our approach achieves superior performance in both quantitative and qualitative evaluations.},
	author = {Zhang, Chunlan and Lin, Chunyu and Liao, Kang and Nie, Lang and Zhao, Yao},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00022},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Conferences;User interfaces;Transformers;Probabilistic logic;Motion pictures;Single-image-based view synthesis;SivsFormer;Vision Transformers;Parallax-aware Alignment},
	month = {March},
	pages = {47-56},
	title = {SivsFormer: Parallax-Aware Transformers for Single-image-based View Synthesis},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00022}}

@inproceedings{9756743,
	abstract = {Supershapes are used in Parametric Design to model, literally, thou-sands of natural and man-made shapes with a single 6 parameter formula. However, users are left to probe such a rich yet dense collection of supershapes using a set of independent 1-D sliders. Some of the formula's parameters are non-linear in nature, making them particularly difficult to grasp with conventional 1-D sliders alone. VR appears as a promising setting for Parametric Design with supershapes since it empowers users with more natural visual inspection and shape browsing techniques, with multiple solutions being displayed at once and the possibility to design more interesting forms of slider interaction. In this work, we propose VR shape widgets that allow users to probe and select supershapes from a multitude of solutions. Our designs take leverage on thumbnails, mini-maps, haptic feedback and spatial interaction, while supporting 1-D, 2-D and 3-D supershape parameter spaces. We conducted a user study (N = 18) and found that VR shape widgets are effective, more efficient, and natural than conventional VR 1-D sliders while also usable for users without prior knowledge on supershapes. We also found that the proposed VR widgets provide a quick overview of the main supershapes, and users can easily reach the desired solution without having to perform fine-grain handle manipulations.},
	author = {Nicolau, Francisco and Gielis, Johan and Simeone, Adalberto L. and Sim{\~o}es Lopes, Daniel},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00019},
	issn = {2642-5254},
	keywords = {Visualization;Solid modeling;Three-dimensional displays;Shape;Conferences;Virtual reality;User interfaces;Human-centered computing;User interface design;Virtual reality Human-centered computing;Graphical user interfaces},
	month = {March},
	pages = {21-28},
	title = {Exploring and Selecting Supershapes in Virtual Reality with Line, Quad, and Cube Shaped Widgets},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00019}}

@inproceedings{9756744,
	abstract = {Odor display has been a popular approach in virtual reality (VR) to enhance users' multi-sensory experience. The existing multi-odor presentation methods in VR are mostly based on spatiality of scent sources to produce mixed scents, which will possibly compromise users' olfactory experience because humans normally have poor ability to analyze distinct odorant components from a mixture. To tackle this problem, we present a VR multi-odor display approach that dynamically changes the intensity combinations of different scent sources in the virtual environment according to the user's attention, hence simulating a virtual cocktail party effect of smell. We acquire the user's gaze information as attention from the eye-tracking sensors embedded in the head mounted display (HMD), and increase the display intensity of the scent that the user is focusing on to simulate the cocktail party effect of smell, enabling the user to distinguish their desired scent source. We conducted a user study to validate the perception and experience of 2 ways of intensity settings in response to the user's attention shift, which were a strong level of focused scent mixed with weak levels of non-focused scents and strong focused scent only. The results showed that both of the two intensity settings were able to improve olfactory experience in VR compared to the non-dynamic odor display method. Meanwhile, only the method of presenting strengthened focused scent while maintaining the weaker mixture of background scents showed significant improvement on simulating the olfactory cocktail party effect by giving the users an enhanced sense of their own olfactory sensitivity.},
	author = {Zou, Shangyin and Hu, Xianyin and Ban, Yuki and Warisawa, Shin'ichi},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00067},
	issn = {2642-5254},
	keywords = {Sensitivity;Three-dimensional displays;Conferences;Olfactory;Virtual environments;Focusing;Resists;virtual reality;odor presentation;attention},
	month = {March},
	pages = {474-482},
	title = {Simulating Olfactory Cocktail Party Effect in VR: A Multi-odor Display Approach Based on Attention},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00067}}

@inproceedings{9756745,
	abstract = {Using the motion behavior of users in virtual reality (VR) as a biometric signature has the potential to enable continuous identification and authentication of users without compromising VR applications if traditional passwords are acquired by malicious agents. Users exhibit natural variabilities in behavior over time that influence their body motions and can alter the trajectories of VR devices such as the headset and the controllers. Behavior variabilities may negatively impact the success rate of VR biometrics. In this work, we evaluate how deep learning approaches to match input and enrollment trajectories are influenced by user behavior variation over varying time scales. We demonstrate that over short timescales on the order of seconds to minutes, no statistically significant relationship is found in the temporal placement of enrollment trajectories and their matches to input trajectories. We find that on medium-scale separation between enrollment and input trajectories, on the order of days to weeks, median accuracy is similar within users who provide input close and distant to enrollment data. Over long timescales on the order of 7 to 18 months, we obtain optimal performance for short and long enrollment/input separations by using training sets from users providing long-timescale data, as these sets encompass coarse and fine-scale changes in behavior.},
	author = {Miller, Robert and Banerjee, Natasha Kholgade and Banerjee, Sean},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00076},
	issn = {2642-5254},
	keywords = {Training;Headphones;Deep learning;Three-dimensional displays;Biometrics (access control);Impedance matching;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Security and privacy;Security services;Authentication;Biometrics},
	month = {March},
	pages = {563-572},
	title = {Temporal Effects in Motion Behavior for Virtual Reality (VR) Biometrics},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00076}}

@inproceedings{9756746,
	abstract = {Field-of-view (FOV) restriction is a common technique to reduce cybersickness in commercial virtual reality (VR) applications. However, the majority of existing FOV restriction techniques are implemented as symmetric imagery, which occludes users' views during virtual rotation. In this paper, we proposed and evaluated a novel variant of FOV restriction, referred to as a side restrictor. Side restriction uses an asymmetric mask to obscure only one side region of the periphery during virtual rotation and laterally shifts the center of restriction towards the direction of the turn. We conducted a study using a between-subjects design that compared the side restrictor, a traditional symmetric restrictor, and a control condition without FOV restriction. Participants were required to navigate through a complex maze-like environment using a controller using one of three restrictors. Compared to the control condition, the side restrictor was effective in mitigating cybersickness, reducing discomfort, improving subjective visibility, and enabling users to remain immersed for a longer period of time. Additionally, we found no empirical evidence of negative drawbacks when compared to the symmetric restrictor, which suggests that side restriction is an effective cybersickness mitigation technique for virtual environments with frequent turns.},
	author = {Wu, Fei and Rosenberg, Evan Suma},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00028},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Cybersickness;Navigation;Design methodology;Conferences;Virtual environments;Games;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;HCI design and evaluation methods;User studies},
	month = {March},
	pages = {103-111},
	title = {Asymmetric Lateral Field-of-View Restriction to Mitigate Cybersickness During Virtual Turns},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00028}}

@inproceedings{9756747,
	abstract = {In many collaborative virtual reality applications, co-located users often have their relative position in the virtual environment matching the one in the real world. The resulting spatial consistency facilitates the co-manipulation of shared tangible props and enables the users to have direct physical contact with each other. However, these applications usually exclude their individual virtual navigation capability, such as teleportation, as it may break the spatial configuration between the real and virtual world. As a result, the users can only explore the virtual environment of approximately similar size and shape compared to their physical workspace. Moreover, their individual tasks with unlimited virtual navigation capability, which often take part in a continuous workflow of a complex collaborative scenario, have to be removed due to this constraint. This work aims to help overcome these limits by allowing users to recover spatial consistency after individual teleportation in order to re-establish their position in the current context of the collaborative task. We use a virtual representation of the user's shared physical workspace and develop two different techniques to position it in the virtual environment. The first technique allows one user to fully position the virtual workspace, and the second approach enables concurrent positioning by equally integrating the input from all the users. We compared these two techniques in a controlled experiment in a virtual assembly task. The results show that allowing two users to manipulate the workspace significantly reduced the time they spent negotiating the position of the future workspace. However, the inevitable conflicts in simultaneous co-manipulation were also a little confusing to them.},
	author = {Zhang, Yiran and Nguyen, Huyen and Ladev{\`e}ze, Nicolas and Fleury, C{\'e}dric and Bourdot, Patrick},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00088},
	issn = {2642-5254},
	keywords = {Training;Three-dimensional displays;Navigation;Shape;Avatars;Collaboration;Virtual environments;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Collaborative interaction},
	month = {March},
	pages = {674-682},
	title = {Virtual Workspace Positioning Techniques during Teleportation for Co-located Collaboration in Virtual Reality using HMDs},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00088}}

@inproceedings{9756748,
	abstract = {The Nanophotonic Phased Array (NPA) is an emerging holographic display technology. With chip-scaled sizes, high refresh rates, and integrated light sources, a large-scale NPA can enable high-resolution real-time dynamic holographic displays. However, one of the critical challenges impeding the development of such large-scale NPAs is the high electrical power consumption required to modulate the amplitude and phase of each of the pixel elements. We argue that the modulation of all the elements on the array is, in fact, not necessary to produce a high-quality image. We propose a simple method that outputs the configuration of a sparse NPA, along with the amplitude and the phase required at each active pixel to generate the desired image at the observation plane. We identify the set of active pixels according to their optimized intensities. We observe that the brighter pixels have a greater influence on the target image, and it is these that we must focus on in image formation. Using as few as 10% of the total pixels from a dense 2D array of light-emitting elements, we show that a perceptually acceptable holographic image can be generated. We compare various sparse sampling methods through computational simulations and show that our proposed method gives superior qualitative and quantitative results. We believe our study will help advance research on sparse NPAs and facilitate the use of large-scale NPAs to display high-resolution 3D holographic images.},
	author = {Jabbireddy, Susmija and Zhang, Yang and Peckerar, Martin and Dagenais, Mario and Varshney, Amitabh},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00075},
	issn = {2642-5254},
	keywords = {Phased arrays;Solid modeling;Three-dimensional displays;Power demand;Modulation;Virtual reality;User interfaces;Computing methodologies;Image processing;Hardware;Displays and imagers;Mixed / augmented reality;Virtual reality},
	month = {March},
	pages = {553-562},
	title = {Sparse Nanophotonic Phased Arrays for Energy-Efficient Holographic Displays},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00075}}

@inproceedings{9756749,
	abstract = {When playing sports in virtual reality foot interaction is crucial for many disciplines. We investigated how the visibility of the foot influences penalty shooting in soccer. In a between-group experiment, we asked 28 players to hit eight targets with a virtual ball. We measured the performance, task load, presence, ball control, and body ownership of inexperienced to advanced soccer players. In one condition, the players saw a visual representation of their tracked foot which improved the accuracy of the shots significantly. Players with invisible foot needed 58% more attempts. Further, with foot visibility the self-reported body ownership was higher.},
	author = {Bonfert, Michael and Lemke, Stella and Porzel, Robert and Malaka, Rainer},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00092},
	issn = {2642-5254},
	keywords = {Human computer interaction;Visualization;Three-dimensional displays;Target tracking;Sensitivity;Conferences;Virtual reality;Human-centered computing---Virtual reality;Human-centered computing---Empirical studies in HCI},
	month = {March},
	pages = {711-718},
	title = {Kicking in Virtual Reality: The Influence of Foot Visibility on the Shooting Experience and Accuracy},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00092}}

@inproceedings{9756750,
	abstract = {This paper investigates providing grounded passive haptic feedback to a user of a VR application through a handheld stick with which the user taps virtual objects. Such an investigation benefits VR applications beyond those where the stick interaction is actually an integral part of the narrative. Providing passive haptic feedback through a handheld stick as opposed to directly through the user's hand has the potential for more believable and more frequent feedback opportunities. The stick is likely to dull the user's haptics perception and proprioception, potentially avoiding a haptics perception uncanny valley and increasing the redirection detection thresholds. Two haptics redirection methods are proposed: the DriftingHand method, which alters the position of the user's virtual hand, and the Vari-Stick method, which alters the length of the virtual stick. Detection thresholds were measured in a user study (N = 60) by testing the two methods for a range of offsets between the virtual and the real object, for multiple stick lengths, and multiple distances from the user to the real object. Overall, the study reveals that VariStick and DriftingHand provide an undetectable range of offsets of [-20cm, +13cm] and [-11cm, +11cm], respectively.},
	author = {Zhou, Yuqi and Popescu, Voicu},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00026},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Design methodology;Conferences;Virtual reality;User interfaces;Length measurement;Grounded passive haptics redirection;haptic retargeting;detection thresholds;handheld prop;virtual reality;Human-centered computing---Visualization---Visualization techniques---Treemaps;Human-centered computing--- Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {83-92},
	title = {Tapping with a Handheld Stick in VR: Redirection Detection Thresholds for Passive Haptic Feedback},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00026}}

@inproceedings{9756751,
	abstract = {Overt redirection is a class of virtual reality locomotion that uses perceptible transformations to enable the user to naturally walk through a virtual environment larger than the physical tracking space. In this research, we propose Foldable Spaces, a novel redirection approach based on the idea of dynamically `folding' the geometry of the virtual environment to reveal new locations depending on the trajectory of the virtual reality user. Based on this approach, we developed three distinct techniques for overt redirection: (1) Horizontal, which folds and reveals layers of virtual space like the pages in a book; (2) Vertical, which rotates virtual space towards the user along a vertical axis; and (3) Accordion, which corrugates and flattens virtual space to bring faraway places closer to the user. In a within-subjects user study, we compared our proposed foldable techniques against each other along with a similarly situated redirection technique, Stop & Reset. Our findings show that Accordion was the most well-received by participants in terms of providing a smooth, continuous, and `natural' experience of walking that does not involve shifts in orientation and provides an overarching view through the virtual environment.},
	author = {Han, Jihae and Moere, Andrew Vande and Simeone, Adalberto L.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00035},
	issn = {2642-5254},
	keywords = {Legged locomotion;Geometry;Three-dimensional displays;Conferences;Virtual environments;Medical treatment;User interfaces;Human-centered computing;Interaction Paradigms;Virtual Reality},
	month = {March},
	pages = {167-175},
	title = {Foldable Spaces: An Overt Redirection Approach for Natural Walking in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00035}}

@inproceedings{9756752,
	abstract = {Social priming is the idea that observations of a virtual human (VH) engaged in short social interactions with a real or virtual human bystander can positively influence users' subsequent interactions with that VH. In this paper we investigate the question of whether the positive effects of social priming are limited to interactions with humanoid entities. For instance, virtual dogs offer an attractive candidate for non-humanoid entities, as previous research suggests multiple positive effects. In particular, real human dog owners receive more positive attention from strangers than non-dog owners.To examine the influence of such social priming we carried out a human-subjects experiment with four conditions: three social priming conditions where a participant initially observed a VH interacting with one of three virtual entities (another VH, a virtual pet dog, or a virtual personal robot), and a non-social priming condition where a VH (alone) was intently looking at her phone as if reading something. We recruited 24 participants and conducted a mixed-methods analysis. We found that a VH's prior social interactions with another VH and a virtual dog significantly increased participants' perceptions of the VHs' affective attraction. Also, participants felt more inclined to interact with the VH in the future in all of the social priming conditions. Qualitatively, we found that the social priming conditions resulted in a more positive user experience than the non-social priming condition. Also, the virtual dog and the virtual robot were perceived as a source of positive surprise, with participants appreciating the non-humanoid interactions for various reasons, such as the avoidance of social anxieties sometimes associated with humans.},
	author = {Norouzi, Nahal and Gottsacker, Matthew and Bruder, Gerd and Wisniewski, Pamela J. and Bailenson, Jeremy and Welch, Greg},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00050},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Design methodology;Conferences;Anxiety disorders;Humanoid robots;Dogs;User interfaces;Human-centered computing;Interaction paradigms;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality},
	month = {March},
	pages = {311-320},
	title = {Virtual Humans with Pets and Robots: Exploring the Influence of Social Priming on One's Perception of a Virtual Human},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00050}}

@inproceedings{9756753,
	abstract = {Surgical procedures requiring needle navigation assistance suffer from complicated hand-eye coordination and are mentally demanding. Augmented reality (AR) can help overcome these issues. How-ever, only an insufficient amount of fundamental research has focused on the design and hardware selection of such AR needle navigation systems. This work contributes to this research area by presenting a user study (n=24) comparing three state-of-the-art navigation concepts displayed by an optical see-through head-mounted display and a stereoscopic projection system. A two-dimensional glyph visualization resulted in higher targeting accuracy but required more needle insertion time. In contrast, punctures guided by a three-dimensional see-through vision concept were less accurate but faster and were favored in a qualitative interview. The third concept, a static representation of the correctly positioned needle, showed too high target errors for clinical accuracy needs. This concept per-formed worse when displayed by the projection system. Besides that, no meaningful differences between the evaluated AR display devices were detected. User preferences and use case restrictions, e.g., sterility requirements, seem to be more crucial selection criteria. Future work should focus on improving the accuracy of the see-through vision concept. Until then, the glyph visualization is recommended.},
	author = {Heinrich, Florian and Schwenderling, Lovis and Joeres, Fabian and Hansen, Christian},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00045},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Target tracking;Navigation;Stereo image processing;Surgery;User interfaces;Human-centered computing---Visualization---Empirical studies in visualization;Computing methodologies---Computer graphics---Graphics systems and interfaces---Mixed / augmented reality;Hardware---Communication hardware, interfaces and storage---Displays and imagers;Applied computing---Life and medical sciences---Health informatics},
	month = {March},
	pages = {260-269},
	title = {2D versus 3D: A Comparison of Needle Navigation Concepts between Augmented Reality Display Devices},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00045}}

@inproceedings{9756754,
	abstract = {This work investigates the use of Virtual Reality (VR) to present forensic evidence to the jury in a courtroom trial. The findings of a between-participant user study on comprehension of an expert statement are presented, examining the benefits and issues of using VR compared to traditional courtroom presentation (being still images). Participants listened to a forensic scientist explain bloodstain spatter patterns while viewing a mock crime scene in either VR or as still images in video format. Under these conditions, we compared understanding of the expert domain, mental effort and content recall. We found that VR significantly improves the understanding of spatial information and knowledge acquisition. We also identify different patterns of user behaviour depending on the display method. We conclude with suggestions on how to best adapt evidence presentation to VR.},
	author = {Reichherzer, Carolin and Cunningham, Andrew and Barr, Jason and Coleman, Tracey and McManus, Kurt and Sheppard, Dion and Coussens, Scott and Kohler, Mark and Billinghurst, Mark and Thomas, Bruce H.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00082},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Forensics;Knowledge acquisition;Conferences;Virtual environments;Information processing;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Interaction paradigms;Virtual reality},
	month = {March},
	pages = {615-624},
	title = {Supporting Jury Understanding of Expert Evidence in a Virtual Environment},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00082}}

@inproceedings{9756755,
	abstract = {Embodied locomotion, especially leaning, has one major problem. Effectively the regular functionality of the utilized body parts is overwritten. Thus, in this work, we propose 6 different status control methods that seamlessly switch off (brake) a seated leaning locomotion interface. Different input modalities, such as a physical button, voice, and gestures/metaphors, are used and evaluated against a baseline condition and a leaning interface with a bilateral transfer function. In a user study, participants were encouraged, but not forced to use these interfaces. They did, and the most diegetic interface, a hover-board metaphor, was the most preferred. However, the overall results are more heterogeneous and the interfaces vary in their suitability for different applications/scenarios. Therefore, we summarized the results in a guideline.},
	author = {Flemming, Carlo and Weyers, Benjamin and Zielasko, Daniel},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00094},
	issn = {2642-5254},
	keywords = {Human computer interaction;Three-dimensional displays;Conferences;Transfer functions;Switches;Virtual reality;Muscles;Human-centered computing;Mixed/augmented reality;Interaction techniques;Empirical studies in HCI},
	month = {March},
	pages = {728-736},
	title = {How to Take a Brake from Embodied Locomotion -- Seamless Status Control Methods for Seated Leaning Interfaces},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00094}}

@inproceedings{9756756,
	abstract = {Virtual reality (VR) researchers have had a long-standing interest in studying locomotion for developing new techniques, improving upon prior ones, and analyzing their effects on users. To help organize prior work, several researchers have presented taxonomies for categorizing locomotion techniques in general. More recently, researchers have begun to conduct systematic reviews to better understand what locomotion techniques have been investigated. In this paper, we present our own systematic review of locomotion techniques based on a well-established taxonomy, and we use k-means clustering to identify to what extent locomotion techniques have been explored. Our results indicate that selection-based, walking-based, and steering-based locomotion techniques have been moderately to highly explored while manipulation-based and automated locomotion techniques have been less explored. We also present results on what types of metrics have been used to evaluate locomotion techniques. While usability, discomfort, and travel performance metrics have been moderately to highly explored, other metrics, such as biometrics, user experience, and emotions, have been less explored.},
	author = {Martinez, Esteban Segarra and Wu, Annie S. and McMahan, Ryan P.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00046},
	issn = {2642-5254},
	keywords = {Measurement;Systematics;Three-dimensional displays;Biometrics (access control);Taxonomy;Virtual reality;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	month = {March},
	pages = {270-280},
	title = {Research Trends in Virtual Reality Locomotion Techniques},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00046}}

@inproceedings{9756757,
	abstract = {Current solutions for providing navigation instructions to users who are walking are mostly limited to 2D maps on smartphones and voice-based instructions. Mixed Reality (MR) holds the promise of integrating navigation instructions directly in users' visual field, potentially making them less obtrusive and more expressive. Current MR navigation systems, however, largely focus on using conventional designs such as arrows, and do not fully leverage the technological possibilities. While MR could present users with more sophisticated navigation visualizations, such as in-situ virtual signage, or visually modifying the physical world to highlight a target, it is unclear how such interventions would be perceived by users. We conducted two experiments to evaluate a set of navigation instructions and the impact of different contexts such as environment or task. In a remote survey (n = 50), we collected preference data with ten different designs in twelve different scenarios. Results indicate that while familiar designs such as arrows are well-rated, methods such as avatars or desaturation of non-target areas are viable alternatives. We confirmed and expanded our findings in an in-person virtual reality (VR) study (n = 16), comparing the highest-ranked designs from the initial study. Our findings serve as guidelines for MR content creators, and future MR navigation systems that can automatically choose the most appropriate navigation visualization based on users' contexts.},
	author = {Lee, Jaewook and Jin, Fanjie and Kim, Younsoo and Lindlbauer, David},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00102},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Navigation;Avatars;Multimedia systems;Mixed reality;User interfaces;H.5.1 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems---Artificial;augmented;and virtual realities},
	month = {March},
	pages = {802-811},
	title = {User Preference for Navigation Instructions in Mixed Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00102}}

@inproceedings{9756758,
	abstract = {Virtual Reality (VR) allows users to perform natural movements such as hand movements, turning the head and natural walking in virtual environments. While such movements enable seamless natural interaction, they come with the need for a large tracking space, particularly in the case of walking. To optimise use of the available physical space, prediction models for upcoming behavior are helpful. In this study, we examined whether a user's eye movements tracked by current VR hardware can improve such predictions. Eighteen participants walked through a virtual environment while performing different tasks, including walking in curved paths, avoiding or approaching objects, and conducting a search. The recorded position, orientation and eye-tracking features from 2.5 s segments of the data were used to train an LSTM model to predict the user's position 2.5 s into the future. We found that future positions can be predicted with an average error of 65 cm. The benefit of eye movement data depended on the task and environment. In particular, situations with changes in walking speed benefited from the inclusion of eye data. We conclude that a model utilizing eye tracking data can improve VR applications in which path predictions are helpful.},
	author = {Stein, Niklas and Bremer, Gianni and Lappe, Markus},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00069},
	issn = {2642-5254},
	keywords = {Legged locomotion;Deep learning;Solid modeling;Tracking;Virtual environments;Gaze tracking;Predictive models;Virtual Reality;Eye Tracking;Locomotion;LSTM;Path Prediction;Machine Learning;Gaze},
	month = {March},
	pages = {493-503},
	title = {Eye Tracking-based LSTM for Locomotion Prediction in VR},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00069}}

@inproceedings{9756759,
	abstract = {Since for most consumers the Virtual Reality (VR) experience exceeds that of desktop applications, an increasing number of applications is being transferred from desktop to VR. Industrial and entertainment applications primarily expect for a richer consumer experience, while others, such as surgical applications, seek for improved precision over their desktop counterparts. One way to improve the performance of precision-based VR applications is to provide suitable visualizations. Today, these "suitable" visualizations are mostly transferred from desktop to VR without considering their spatial and temporal performance might change in VR. This may not lead to an optimal solution, which can be crucial for precision-based tasks. Misinterpretation of shape or distance in a surgical or pre-operative simulation can affect the chosen treatment and thus directly impact the outcome. Therefore, we evaluate the performance differences of multiple visualizations for 3D surfaces based on their shape and distance estimation for desktop and VR applications. We conducted a quantitative user study with 56 participants evaluating seven visualizations (Phong, Toon, Fresnel, Pseudo-Chromadepth, Heatmap, Isolines, and Arrow Glyphs). Our results show that the performance of each visualization varies depending on the task, system, and surface type, with VR generally providing improved results. While Isolines are able to improve distance estimation, Phong and Heatmaps are beneficial for shape estimation.},
	author = {Hombeck, Jan and Meuschke, Monique and Zyla, Lennert and Heuser, Andr{\'e}-Joel and Toader, Justus and Popp, Felix and Bruns, Christiane J. and Hansen, Christian and Datta, Rabi R. and Lawonn, Kai},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00071},
	issn = {2642-5254},
	keywords = {Heating systems;Visualization;Solid modeling;Three-dimensional displays;Shape;Estimation;Surgery;Computer Graphics;Distance and Shape Estimation;Virtual Reality;Visualization},
	month = {March},
	pages = {514-523},
	title = {Evaluating Perceptional Tasks for Medicine: A Comparative User Study Between a Virtual Reality and a Desktop Application},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00071}}

@inproceedings{9756760,
	abstract = {As a result of urban concentration, colonisation, increased physical distance from tribal homelands, and globalisation, many indigenous people, including M{\=a}ori, are seeking digital solutions to connect to their culture and identity. Through co-design, close collaboration with our indigenous partners, and careful consideration of cultural context, we show that mixed reality experiences can be an effective mechanism for the growing diaspora of M{\=a}ori to access and experience their language, genealogy, families, histories and knowledge. Inductive analysis of semi-structured interviews highlights the importance of cultural values and context in this experience, and confirms that this approach can support connections to community and culture. Our work is deeply embedded in a particular indigenous group's context and culture, but we believe that it holds important lessons that can generalise to other groups. In particular, collaborative co-design and recognition of cultural values throughout the project are essential for producing experiences that meet the needs of specific communities, and that reflect and respect their culture and worldview.},
	author = {Park, Noel and Regenbrecht, Holger and Duncan, Stuart and Mills, Steven and Lindeman, Robert W. and Pantidi, Nadia and Whaanga, H{\=e}mi},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00033},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Mixed reality;Collaboration;Globalization;Virtual reality;User interfaces;Cultural differences;Mixed Reality;Virtual Reality;Application;Field Studies;Presence;Co-presence;Qualitative;Indigenous},
	month = {March},
	pages = {149-157},
	title = {Mixed Reality Co-Design for Indigenous Culture Preservation & Continuation},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00033}}

@inproceedings{9756762,
	abstract = {Simulating real-world experiences in a safe environment has made virtual human medical simulations a common use case for research and interpersonal communication training. Despite the benefits virtual human medical simulations provide, previous work suggests that users struggle to notice when virtual humans make potentially life-threatening verbal communication mistakes inside virtual human medical simulations. In this work, we performed a 2x2 mixed design user study that had learners (n = 80) attempt to identify verbal communication mistakes made by a virtual human acting as a nurse in a virtual desktop environment. A virtual desktop environment was used instead of a head-mounted virtual reality environment due to Covid-19 limitations. The virtual desktop environment experience allowed us to explore how frequently learners identify verbal communication mistakes in virtual human medical simulations and how perceptions of credibility, reliability, and trustworthiness in the virtual human affect learner error recognition rates. We found that learners struggle to identify infrequent virtual human verbal communication mistakes. Additionally, learners with lower initial trustworthiness ratings are more likely to overlook potentially life-threatening mistakes, and virtual human mistakes temporarily lower learner credibility, reliability, and trustworthiness ratings of virtual humans. From these findings, we provide insights on improving virtual human medical simulation design. Developers can use these insights to design virtual simulations for error identification training using virtual humans.},
	author = {Stuart, Jacob and Aul, Karen and Bumbach, Michael D. and Stephen, Anita and de Siqueira, Alexandre Gomes and Lok, Benjamin},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00065},
	issn = {2642-5254},
	keywords = {Training;Human computer interaction;Solid modeling;Three-dimensional displays;Medical simulation;Conferences;Computational modeling;Human-centered computing;Empirical studies in HCI},
	month = {March},
	pages = {455-463},
	title = {The Effect of Virtual Humans Making Verbal Communication Mistakes on Learners' Perspectives of their Credibility, Reliability, and Trustworthiness},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00065}}

@inproceedings{9756763,
	abstract = {Gaze-only input techniques in VR face the challenge of avoiding false triggering due to continuous eye tracking while maintaining interaction performance. In this paper, we proposed GazeDock, a technique for enabling fast and robust gaze-based menu selection in VR. GazeDock features a view-fixed peripheral menu layout that automatically triggers appearing and selection when the user's gaze approaches and leaves the menu zone, thus facilitating interaction speed and minimizing the false triggering rate. We built a dataset of 12 participants' natural gaze movements in typical VR applications. By analyzing their gaze movement patterns, we designed the menu UI personalization and optimized selection detection algorithm of GazeDock. We also examined users' gaze selection precision for targets on the peripheral menu and found that 4--8 menu items yield the highest throughput when considering both speed and accuracy. Finally, we validated the usability of GazeDock in a VR navigation game that contains both scene exploration and menu selection. Results showed that GazeDock achieved an average selection time of 471ms and a false triggering rate of 3.6%. And it received higher user preference ratings compared with dwell-based and pursuit-based techniques.},
	author = {Yi, Xin and Lu, Yiqin and Cai, Ziyin and Wu, Zihan and Wang, Yuntao and Shi, Yuanchun},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00105},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Navigation;Layout;Virtual reality;Gaze tracking;Games;User interfaces;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies;Human-centered computing---Human computer interaction (HCI)--- Interaction paradigms---Virtual reality},
	month = {March},
	pages = {832-842},
	title = {GazeDock: Gaze-Only Menu Selection in Virtual Reality using Auto-Triggering Peripheral Menu},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00105}}

@inproceedings{9756764,
	abstract = {Redirected Walking (RDW) is a common technique leveraged to allow real walking for exploring large virtual environments in constrained physical tracking spaces. Effective RDW is challenging due to its complexity and disturbance factors (e.g., spontaneous user behavior). Existing techniques range from combinations of simple motion scaling to more elaborate curvature injections and reactive, predictive, or scripted steering concepts. However, many of these approaches were evaluated in simulation only, and researchers argued that the findings would translate to real scenarios to motivate the effectiveness of their algorithms. Using the Redirected Walking Toolkit and its virtual path generator, a randomized waypoint-based path generator has been common practice, although its built-in simplifications assume sequential user behavior regarding translation and rotation.In this paper, we argue that pure simulation-based evaluations employing such simplified path generators require critical reflection. We demonstrate RDW simulations that show the chaotic process fundamental to RDW, in which altering the initial user's position by mere millimeters can drastically change the resulting steering behavior. This insight suggests that RDW is more sensitive to the underlying data than previously assumed. Thus, we rigorously analyze the influence of commonly used synthetically generated paths on multiple state-of-the-art steering concepts and compare them against previously recorded real paths.},
	author = {Hirt, Christian and Kompis, Yves and Holz, Christian and Kunz, Andreas},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00072},
	issn = {2642-5254},
	keywords = {Legged locomotion;Solid modeling;Three-dimensional displays;Tracking;Conferences;Virtual environments;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	month = {March},
	pages = {524-533},
	title = {The Chaotic Behavior of Redirection -- Revisiting Simulations in Redirected Walking},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00072}}

@inproceedings{9756765,
	abstract = {Jumping onto steps is a promising action for creating an instant height movement in VR, but installing physical steps is impractical. We propose PseudoJumpOn, a novel locomotion technique using a common VR setup that allows the user to experience virtual step-up jumping motion by applying two types of viewpoint-manipulation methods to a physical jump on a flat floor. The core idea is to replicate the physical characteristics of ascending jumps, and thus we designed two viewpoint-manipulation methods: gain manipulation, which differentiates the ascent and descent height, and peak shifting, which delays the peak timing. We conducted a user study asking participants (N = 20) to experience two-legged step-up jumps onto 0.2--0.8-m heights in VR as the two methods were applied in combination (gain manipulation: five conditions where the ascending gain was 1.0--5.0; peak shifting: four conditions where the peak timing in VR was delayed by 0--1.0 ratios of the original timing). The results showed that the participants in most conditions felt positively in terms of reality and naturalness of actually jumping onto steps, even though knowing no physical steps existed. In addition, subsequent analyses also derived practical guidelines for determining the appropriate gains and the potential use of peak shifting to achieve a natural step-up jumping experience.},
	author = {Ogawa, Kumpei and Fujita, Kazuyuki and Takashima, Kazuki and Kitamura, Yoshifumi},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00084},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Conferences;Buildings;Virtual reality;User interfaces;Prediction algorithms;Delays;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality},
	month = {March},
	pages = {635-643},
	title = {PseudoJumpOn: Jumping onto Steps in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00084}}

@inproceedings{9756766,
	abstract = {Enabling users to push the physical world's limits, augmented and virtual reality platforms opened a new chapter in perception. Novel immersive experiences resulted in the emergence of new interaction methods for virtual environments, which came with unprecedented security and privacy risks. This paper presents a keylogging inference attack to infer user inputs typed with in-air tapping keyboards. We observe that hands follow specific patterns when typing in the air and exploit this observation to carry out our attack. Starting with three plausible attack scenarios where the adversary obtains the hand trace patterns of the victim, we build a pipeline to reconstruct the user input. Our attack pipeline takes the hand traces of the victim as an input and outputs a set of input inferences ordered from the best to worst. Through various experiments, we showed that our inference attack achieves a pinpoint accuracy ranging from 40% to 87% within at most the top-500 candidate reconstructions. Finally, we discuss countermeasures, while the results presented provide a cautionary tale of the security and privacy risk of the immersive mobile technology.},
	author = {Meteriz-Yldran, {\"U}lk{\"u} and Yldran, Necip Fazl and Awad, Amro and Mohaisen, David},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00098},
	issn = {2642-5254},
	keywords = {Solid modeling;Privacy;Three-dimensional displays;Pipelines;Keyboards;Virtual environments;User interfaces;Security and privacy---Privacy protections;Human-centered computing---Text input;Human-centered computing---Ubiquitous and mobile devices},
	month = {March},
	pages = {765-774},
	title = {A Keylogging Inference Attack on Air-Tapping Keyboards in Virtual Environments},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00098}}

@inproceedings{9756767,
	abstract = {Reconstructing 3D virtual face from a single image has a wide range of applications in virtual reality. Existing approaches synthesize plausible reconstructed virtual faces, however, eye gaze information is usually ignored, which is critical in human-computer interaction. In this paper, we propose to reconstruct 3D virtual face with eye gaze information from a single image. The main challenges lie in two aspects, one is the low reconstruction quality in the eye region, the other one is the lack of an efficient method to obtain precise eye gaze information. To address these problems, we decompose this task into two key steps, i.e., 3D face reconstruction with precise eye region and eye contact guided facial-rotation for eye gaze information. The first step is designed for precise eye region reconstruction through joint optimization on 3D face/eye shapes and textures. The second step consists of two parts: eye contact discriminator and automatic eye contact search algorithm via gradient-based optimization to perform both eye contact and gaze estimation simultaneously. Extensive experiments on different tasks demonstrate the significant gain of the proposed approach, achieving an MSE of (30%), an SSIM of (17.85%), and a PSNR of (8.4%). It also produces lower angular errors (63.01%) in the gaze estimation task compared with human annotations.},
	author = {Liang, Jiadong and Liu, Yunfei and Lu, Feng},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00056},
	issn = {2642-5254},
	keywords = {Human computer interaction;Three-dimensional displays;Shape;Conferences;Estimation;Virtual reality;Task analysis;3D face reconstruction;eye contact;gaze estimation},
	month = {March},
	pages = {370-378},
	title = {Reconstructing 3D Virtual Face with Eye Gaze from a Single Image},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00056}}

@inproceedings{9756768,
	abstract = {The pairing of Virtual Reality technology with Physiological Sensing has gained much interest in clinical settings and beyond: from developing novel methods for diagnosis of perception and cognition impairments, biofeedback for anxiety treatment, to enhancing everyday practices such as self-guided meditation. However, conducting this type of research does not come without challenges. For example, accessing the equipment for recording data from the user and synchronizing physiological response data with the stimuli or interactive environment are not trivial tasks, and generating virtual content in response to the user's real-time data is costly and complex. This paper presents Galea, a device for multi-modal signal acquisition able to measure the physiological response of a user when experiencing virtual content, enabling behavioral, affective computing , and human-computer interaction research and applications to access data from the Parasympathetic nervous system and Sympathetic nervous system simultaneously. We present a primer on detectable human physiology as an input source for Physiological Computing from the perspective of the signals available through our device. We describe the primary design considerations and circuit characterization results of in-vivo recordings from the wearer's brain, eyes, heart, skin, and muscles. We also present an example to help contextualize how these signals can be used in a virtual reality setting. Galea makes working with physiological sensors in virtual reality more accessible and can offer a standard for inter and intra experiment data comparisons. Lastly, we discuss the importance and contributions of this work as well as future challenges that need to be considered.},
	author = {Bernal, Guillermo and Hidalgo, Nelson and Russomanno, Conor and Maes, Pattie},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00024},
	issn = {2642-5254},
	keywords = {Heart;Visualization;Virtual environments;Sensor phenomena and characterization;Physiology;Skin;Real-time systems;Human-centered computing;Human Computer Interaction (HCI);Input devices;Tracking and sensing;Toolkits;User/Machine Systems;Human factors;Brain-Computer Interfaces;Steady State Visual Evoked Potentials (SSVEP);Virtual Reality (VR)},
	month = {March},
	pages = {66-76},
	title = {Galea: A physiological sensing system for behavioral research in Virtual Environments},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00024}}

@inproceedings{9756769,
	abstract = {We present a novel metric to analyze the similarity between the physical environment and the virtual environment for natural walking in virtual reality. Our approach is general and can be applied to any pair of physical and virtual environments. We use geometric techniques based on conforming constrained Delaunay triangulations and visibility polygons to compute the Environment Navigation Incompatibility (ENI) metric that can be used to measure the complexity of performing simultaneous navigation. We demonstrate applications of ENI for highlighting regions of incompatibility for a pair of environments, guiding the design of the virtual environments to make them more compatible with a fixed physical environment, and evaluating the performance of different redirected walking controllers. We validate the ENI metric using simulations and two user studies. Results of our simulations and user studies show that in the environment pair that our metric identified as more navigable, users were able to walk for longer before colliding with objects in the physical environment. Overall, ENI is the first general metric that can automatically identify regions of high and low compatibility in physical and virtual environments. Our project website is available at https://gamma.umd.edu/eni/.},
	author = {Williams, Niall L. and Bera, Aniket and Manocha, Dinesh},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00061},
	issn = {2642-5254},
	keywords = {Measurement;Legged locomotion;Solid modeling;Three-dimensional displays;Navigation;Conferences;Virtual environments;Navigability Metric;Locomotion Interfaces;Visibility Polygon;Isovist},
	month = {March},
	pages = {419-427},
	title = {ENI: Quantifying Environment Compatibility for Natural Walking in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00061}}

@inproceedings{9756770,
	abstract = {Projector-camera systems have long been used in measuring three-dimensional shapes. Most projector-camera systems can only be used in dark rooms because frame-based cameras are not robust against strong ambient light and are difficult to obtain correspondence to the image pixels of the projector. Recently, event cameras, which can detect the direction of luminance change, have received attention in the field of computer vision. When considering the many advantages of event cameras, this study focuses on their wide dynamic range (120 vs. 40 dB of a frame-based camera) and their ability to detect fast luminance changes. Our objective is to realize a projector-camera system that combines the event camera with a projector under the strong ambient light. Specifically, this study proposes a new structured light that combines different frequencies of flickers to acquire the correspondence between the image pixels of the event camera and the projector. This method does not rely on the co-axial frame-based measurement and synchronization mechanism between projector and camera and is thus applicable to most general event cameras. Experiments confirm that the proposed method obtains the correspondence robustly with reasonable accuracy in a bright room (up to 2,600 lux) under general indoor lighting and additional light projection.},
	author = {Fujimoto, Yuichiro and Sawabe, Taishi and Kanbara, Masayuki and Kato, Hirokazu},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00078},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Shape;Shape measurement;Lighting;Virtual reality;User interfaces;Dynamic range;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed/augmented reality;Artificial intelligence;Computer vision;Computer vision problems},
	month = {March},
	pages = {582-588},
	title = {Structured Light of Flickering Patterns Having Different Frequencies for a Projector-Event-Camera System},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00078}}

@inproceedings{9756771,
	abstract = {When using personal computing services in mixed reality (MR) such as online payment and social media, sensitive information and account passwords must be typed in MR. To design secure MR systems and build up user trust, it is imperative to first understand the security threat to the sensitive MR input. Although keystroke inference attacks by analyzing human-computer interaction in videos or via wireless signals have been successful, they require placing extra hardware near the user which is easily noticeable in practice. In this paper, we expose a more dangerous malware-based attack through the vulnerability that no permission is required for accessing MR motion data. We aim to monitor MR headset motion and infer the user input through a benign App. Realizing the attack system requires addressing unique challenges in MR such as six-degree-of-freedom (6DoF) device motion and no explicit motion signal for keystroke identification. To this end, we present HoloLogger, the first malware-based keystroke inference attack system on HoloLens. HoloLogger is empowered by a 6DoF-head-motion-driven key tracking scheme and an air-tap-pattern-based keystroke inference framework. Extensive evaluations with 25 users and 750 inference trials of passwords consisting of 4--8 lowercase English letters demonstrate that HoloLogger successfully achieves a top-5 accuracy of 93%. HoloLogger is also robust in various environments such as different user positions and input categories.},
	author = {Luo, Shiqing and Hu, Xinyu and Yan, Zhisheng},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00064},
	issn = {2642-5254},
	keywords = {Wireless communication;Wireless sensor networks;Solid modeling;Three-dimensional displays;Tracking;Social networking (online);Mixed reality;Human-centered computing;Human computer interaction;Mixed/augmented reality;Security and privacy;Human and societal aspects of security and privacy;Privacy protection},
	month = {March},
	pages = {445-454},
	title = {HoloLogger: Keystroke Inference on Mixed Reality Head Mounted Displays},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00064}}

@inproceedings{9756772,
	abstract = {Currently, ray tracing and image-based lighting (IBL) have shortcomings when rendering the metallic virtual object displayed in the holographic pyramid in mixed reality. Ray tracing can hardly achieve the interactive frame rates, and IBL cannot accurately render the reflection result of the foreground near the virtual object. In this paper, we propose a mixed reality rendering method to render glossy and specular reflection effects on metallic virtual objects displayed in the holographic pyramid based on the surrounding real environment at interactive frame rates. First, we acquire the real environment data with four RGBD cameras and a panoramic camera; then, we introduce a foreground point cloud generation method to extract a temporally stable foreground point cloud from RGBD videos captured in real time; after this, we propose an efficient ray tracing method to render the dynamic glossy and specular reflections on the virtual objects that are displayed in the holographic pyramid. We test our method on several real and synthetic scenes. Compared with IBL and screen-space ray tracing, our method can generate the rendering results closer to the ground truth at the same time cost. Compared with Monte Carlo path tracing, our method is 2.5-4.5 faster in generating rendering results of the comparable quality.},
	author = {Dai, Danqing and Shi, Xuehuai and Wang, Lili and Li, Xiangyu},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00068},
	issn = {2642-5254},
	keywords = {Point cloud compression;Three-dimensional displays;Mixed reality;Virtual reality;Ray tracing;User interfaces;Rendering (computer graphics);Mixed Reality;Virtual Reality;Reflection Rendering},
	month = {March},
	pages = {483-492},
	title = {Interactive Mixed Reality Rendering on Holographic Pyramid},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00068}}

@inproceedings{9756773,
	abstract = {Cybersickness (i.e., visually induced motion sickness) serves as a significant obstacle to the usage and broader adoption of virtual reality (VR) technologies. This collection of symptoms akin to motion sickness can be impacted by different characteristics of a virtual experience, such as visual realism and optical flow. However, relatively little is known regarding how cybersickness is influenced by traversing uneven virtual terrain. In this study, we aim to better understand the impacts of different virtual terrain types on cybersickness in VR. We recruited 38 participants to navigate a virtual forest environment with three terrain variants: flat surface, terrain with regular bumps, and irregular terrain generated from Perlin noise. We collected cybersickness data using the Fast Motion Sickness Scale (FMSS) and Simulator Sickness Questionnaire (SSQ) in addition to galvanic skin response data. Our results indicate that users felt greater levels of cybersickness in the presence of regular bumps and irregular terrain than they did when traversing flat geometry. We recommend that designers exercise caution when incorporating uneven terrain into their virtual experiences, and maintain awareness of the risks carried by these design decisions.},
	author = {Ang, Samuel and Quarles, John},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00062},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Cybersickness;Navigation;Multimedia systems;Virtual environments;User interfaces;H.5.1 [INFORMATION INTERFACES AND PRESENTATION (e.g., HCI)]: Multimedia Information Systems;Artificial, augmented, and virtual realities},
	month = {March},
	pages = {428-435},
	title = {You're in for a Bumpy Ride! Uneven Terrain Increases Cybersickness While Navigating with Head Mounted Displays},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00062}}

@inproceedings{9756774,
	abstract = {There are decades of work investigating how to support users in inhabiting avatars' bodies in immersive experiences, however we are still learning about how Virtual Reality (VR) can support people in more deeply inhabiting their own bodies. Over the course of five action research workshops, we explored the design space for how VR can support feeling embodied and generating movement within a dance context. We explored design factors and participants' experiences within: games intended to stimulate physical movement, single player and multiplayer 3D painting, 360 live video, and custom real-time audio and visual feedback based on motion capture. We found that participants spontaneously explored collaboration including synchronous, asynchronous, remote and collocated. Where there were mismatches between visual and kinesthetic perceptions, participants explored how to recalibrate, alternate between, and integrate perceptions. Participants were compelled to explore their control of real-time effects, which led to rich movement generation as we iterated through the effects' parameters. We present a design space that encapsulates the insights of our action research, with axes for control, collaboration, auditory and visual feedback. We discuss implications for the support of immersive embodiment in dance.},
	author = {Lottridge, Danielle and Weber, Rebecca and McLean, Eva-Rae and Williams, Hazel and Cook, Joanna and Bai, Huidong},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00027},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Conferences;Collaboration;Virtual reality;Games;Aerospace electronics;virtual reality;embodiment;creativity support;dance},
	month = {March},
	pages = {93-102},
	title = {Exploring the Design Space for Immersive Embodiment in Dance},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00027}}

@inproceedings{9756775,
	abstract = {The embodiment of avatars in virtual reality (VR) is a promising tool for enhancing the user's mental health. A great example is the treatment of body image disturbances, where eliciting a full-body illusion can help identify, visualize, and modulate persisting misperceptions. Augmented reality (AR) could complement recent advances in the field by incorporating real elements, such as the therapist or the user's real body, into therapeutic scenarios. However, research on the use of AR in this context is very sparse. Therefore, we present a holographic AR mirror system based on an optical see-through (OST) device and markerless body tracking, collect valuable qualitative feedback regarding its user experience, and compare quantitative results regarding presence, embodiment, and body weight perception to similar systems using video see-through (VST) AR and VR. For our OST AR system, a total of 27 normal-weight female participants provided predominantly positive feedback on display properties (field of view, luminosity, and transparency of virtual objects), body tracking, and the perception of the avatar's appearance and movements. In the quantitative comparison to the VST AR and VR systems, participants reported significantly lower feelings of presence, while they estimated the body weight of the generic avatar significantly higher when using our OST AR system. For virtual body ownership and agency, we found only partially significant differences. In summary, our study shows the general applicability of OST AR in the given context offering huge potential in future therapeutic scenarios. However, the comparative evaluation between OST AR, VST AR, and VR also revealed significant differences in relevant measures. Future work is mandatory to corroborate our findings and to classify the significance in a therapeutic context.},
	author = {Wolf, Erik and Fiedler, Marie Luisa and D{\"o}llinger, Nina and Wienrich, Carolin and Latoschik, Marc Erich},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00054},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Tracking;Avatars;Optical feedback;User interfaces;Optical imaging;Virtual reality;virtual human;virtual body ownership;agency;body image distortion;body weight perception},
	month = {March},
	pages = {350-359},
	title = {Exploring Presence, Avatar Embodiment, and Body Perception with a Holographic Augmented Reality Mirror},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00054}}

@inproceedings{9756776,
	abstract = {Typical VR interactions can be tiring, including standing up, walking, and mid-air gestures. Such interactions result in decreased comfort and session duration compared with traditional non-VR interfaces, which may, in turn, reduce productivity. Nevertheless, current approaches often neglect this aspect, making the VR experience not as promising as it can be. As we see it, desk VR experiences provide the convenience and comfort of a desktop experience and the benefits of VR immersion, being a good compromise between the overall experience and ergonomics. In this work, we explore navigation techniques targeted at desk VR users, using both controllers and a large multi-touch surface. We address travel and orientation techniques independently, considering only continuous approaches for travel as these are better suited for exploration and both continuous and discrete approaches for orientation. Results revealed advantages for a continuous controller-based travel method and a trend for a dragging-based orientation technique. Also, we identified possible trends towards task focus affecting overall cybersickness symptomatology.},
	author = {Amaro, Guilherme and Mendes, Daniel and Rodrigues, Rui},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00041},
	issn = {2642-5254},
	keywords = {Productivity;Legged locomotion;Three-dimensional displays;Navigation;Cybersickness;Ergonomics;Conferences;Human-centered computing---Human computer interaction (HCI)---Interaction techniques;Computing methodologies---Computer graphics---Graphics systems and interfaces---Virtual reality},
	month = {March},
	pages = {222-231},
	title = {Design and Evaluation of Travel and Orientation Techniques for Desk VR},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00041}}

@inproceedings{9756777,
	abstract = {Computer-Generated Holography (CGH) offers the potential for genuine, high-quality three-dimensional visuals. However, fulfilling this potential remains a practical challenge due to computational complexity and visual quality issues. We propose a new CGH method that exploits gaze-contingency and perceptual graphics to accelerate the development of practical holographic display systems. Firstly, our method infers the user's focal depth and generates images only at their focus plane without using any moving parts. Second, the images displayed are metamers; in the user's peripheral vision, they need only be statistically correct and blend with the fovea seamlessly. Unlike previous methods, our method prioritises and improves foveal visual quality without causing perceptually visible distortions at the periphery. To enable our method, we introduce a novel metameric loss function that robustly compares the statistics of two given images for a known gaze location. In parallel, we implement a model representing the relation between holograms and their image reconstructions. We couple our differentiable loss function and model to metameric varifocal holograms using a stochastic gradient descent solver. We evaluate our method with an actual proof-of-concept holographic display, and we show that our CGH method leads to practical and perceptually three-dimensional image reconstructions.},
	author = {Walton, David R. and Kavakl, Koray and Dos Anjos, Rafael Kuffner and Swapp, David and Weyrich, Tim and Urey, Hakan and Steed, Anthony and Ritschel, Tobias and Ak{\c s}it, Kaan},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00096},
	issn = {2642-5254},
	keywords = {Visualization;Solid modeling;Three-dimensional displays;Stochastic processes;Virtual reality;Holography;User interfaces;Computer-Generated Holography;Foveated Rendering;Metamerisation;Varifocal Near-Eye Displays;Virtual Reality;Augmented Reality},
	month = {March},
	pages = {746-755},
	title = {Metameric Varifocal Holograms},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00096}}

@inproceedings{9756778,
	abstract = {When we move through a real environment, egocentric location representations are effortlessly and automatically updated. While moving in synthetic environments, this effortless, continuous spatial updating is often disrupted or incomplete due to a lack of sensory, especially body-based, movement information. To prevent disorientation in virtual reality caused by missing body-based information, the support of spatial updating via other sensory movement cues is necessary. In the presented experiment, participants performed a spatial updating task in a sparse virtual scene presented inside a CAVE (Cave Automatic Virtual Environment). The task was to navigate back to a starting position after simulated movements with either no orientation cues, three visible distant landmarks or one continuous auditory cue present. The focus was not to compare visual and auditory cues but to explore the viability of auditory cueing with visual cues as a reference. Overall, the data showed improved task performance when an orientation cue was present, with auditory cues providing at least as much improvement as visual cues. Our results indicate that auditory cues in virtual environments can support spatial updating when body-based information is missing.},
	author = {Breitkreutz, Christiane and Brade, Jennifer and Winkler, Sven and Bendixen, Alexandra and Klimant, Philipp and Jahn, Georg},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00093},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Navigation;Conferences;Virtual environments;User interfaces;Task analysis;Spatial updating;virtual reality;auditory and visual cues;triangle completion task;CAVE;spatial orientation},
	month = {March},
	pages = {719-727},
	title = {Spatial Updating in Virtual Reality -- Auditory and Visual Cues in a Cave Automatic Virtual Environment},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00093}}

@inproceedings{9756779,
	abstract = {Virtual Reality (VR) users often experience postural instability, i.e., balance problems, which could be a major barrier to universal usability and accessibility for all, especially for persons with balance impairments. Prior research has confirmed the imbalance effect, but minimal research has been conducted to reduce this effect. We recruited 42 participants (with balance impairments: 21, without balance impairments: 21) to investigate the impact of several auditory techniques on balance in VR, specifically spatial audio, static rest frame audio, rhythmic audio, and audio mapped to the center of pressure (CoP). Participants performed two types of tasks - standing visual exploration and standing reach and grasp. Within-subject results showed that each auditory technique improved balance in VR for both persons with and without balance impairments. Spatial and CoP audio improved balance significantly more than other auditory conditions. The techniques presented in this research could be used in future virtual environments to improve standing balance and help push VR closer to universal usability.},
	author = {Mahmud, M. Rasel and Stewart, Michael and Cordova, Alberto and Quarles, John},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00100},
	issn = {2642-5254},
	keywords = {Human computer interaction;Visualization;Three-dimensional displays;Design methodology;Conferences;Spatial audio;Virtual environments;Virtual Reality;balance;postural stability;auditory feedback;VR accessibility;VR usability;Head-Mounted Display},
	month = {March},
	pages = {782-791},
	title = {Auditory Feedback for Standing Balance Improvement in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00100}}

@inproceedings{9756780,
	abstract = {As the boundary between real and virtual life is becoming increasingly blurred, researchers and practitioners are looking for ways to integrate the two intending to improve human lives in a plethora of domains. A cutting-edge concept is the design of Digital Twins (DT), having a broad range of implications and applications, spanning from education, training, as well as safety and productivity in the workplace. An emergent approach for implementing DTs is the usage of mixed reality (MR) and augmented reality (AR), which are well aligned with merging real and virtual objects to enhance the human's ability to interact with and manage DTs. Yet, this is still a novel area of research and, as such, a grounded understanding of the current state, challenges, and open questions is still lacking. Towards this, we conducted a PRISMA-based literature review of scientific articles and book chapters dealing with the use of MR and AR for digital twins. After a thorough screening phase and eligibility check, 25 papers were analyzed, sorted and compared by different categories like research topic (e.g., visualization, guidance), domain (e.g., manufacturing, education), paper type (e.g., design study, evaluation), evaluation type (user study, case study or none), used hardware (e.g., Microsoft HoloLens, mobile devices) as well as the different outcomes (result type and topic, problems, outlook). The major finding of this research survey is the predominant focus of the reviewed papers on the technology itself and the neglect of factors regarding the users. We, therefore, encourage researchers in this area to keep the importance of ease and joy of use in mind and include users in multiple stages of their work.},
	author = {K{\"u}nz, Andreas and Rosmann, Sabrina and Loria, Enrica and Pirker, Johanna},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00058},
	issn = {2642-5254},
	keywords = {Training;Three-dimensional displays;Systematics;Digital twin;Bibliographies;Mixed reality;Manufacturing;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed/augmented reality;Computer systems organization;Embedded and cyber-physical systems;General and reference;Document types;Surveys and overviews},
	month = {March},
	pages = {389-398},
	title = {The Potential of Augmented Reality for Digital Twins: A Literature Review},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00058}}

@inproceedings{9756781,
	abstract = {We present and evaluate methods to redirect desktop inputs such as eye gaze and mouse pointing to a VR-embedded avatar. We use these methods to build a novel interface that allows a desktop user to give presentations in remote VR meetings such as conferences or classrooms. Recent work on such VR meetings suggests a substantial number of users continue to use desktop interfaces due to ergonomic or technical factors. Our approach enables desk-top and immersed users to better share virtual worlds, by allowing desktop-based users to have more engaging or present "cross-reality" avatars. The described redirection methods consider mouse pointing and drawing for a presentation, eye-tracked gaze towards audience members, hand tracking for gesturing, and associated avatar motions such as head and torso movement. A study compared different levels of desktop avatar control and headset-based control. Study results suggest that users consider the enhanced desktop avatar to be human-like and lively and draw more attention than a conventionally animated desktop avatar, implying that our interface and methods could be useful for future cross-reality remote learning tools.},
	author = {Woodworth, Jason W. and Broussard, David and Borst, Christoph W.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00106},
	issn = {2642-5254},
	keywords = {Torso;Three-dimensional displays;Tracking;Distance learning;Avatars;Design methodology;Ergonomics;Human-centered computing;Human computer interaction (HCI);Interaction Paradigms;Virtual Reality;Human-centered computing;Interaction design;Interaction design pro-cess and methods;User interface design},
	month = {March},
	pages = {843-851},
	title = {Redirecting Desktop Interface Input to Animate Cross-Reality Avatars},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00106}}

@inproceedings{9756782,
	abstract = {Time-anchored on-screen comments, as known as bullet comments, are a popular feature for online video streaming. Bullet comments reflect audiences' feelings and opinions at specific video timings, which have been shown to be beneficial to video content understanding and social connection level. In this paper, we for the first time investigate the problem of bullet comment display and insertion for 360$\,^{\circ}$ video via head-mounted display and controller. We design four bullet comment display methods and evaluate their effects on 360$\,^{\circ}$ video experiences. We further propose two controller-based methods for bullet comment insertion. Combining the display and insertion methods, the user can experience 360$\,^{\circ}$ videos with bullet comments, and interactively post new ones by selecting among existing comments. User study results revealed how the factors of display and insertion methods affect 360$\,^{\circ}$ video experience. With the experiment findings, we also discuss useful design insights for 360$\,^{\circ}$ video bullet comments.},
	author = {Li, Yi-Jun and Shi, Jinchuan and Zhang, Fang-Lue and Wang, Miao},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00017},
	issn = {2642-5254},
	keywords = {Headphones;Visualization;Three-dimensional displays;Head-mounted displays;Conferences;Virtual reality;Streaming media;Bullet Comments;360$\,^{\circ}$ Video;Danmaku;Dan Mu},
	month = {March},
	pages = {1-10},
	title = {Bullet Comments for 360$\,^{\circ}$Video},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00017}}

@inproceedings{9756783,
	abstract = {This paper analyses methodological and conceptual shifts in filmmaking brought about by the integrated use of VR technology, game engines, and LED walls in a Virtual Production Studio (VPS). The paper focuses not only on the components of an integrated VPS system, but also on what makes it qualitatively different from traditional filmmaking technologies. The VPS concept transforms the relationships among display technology, live action, image production, and postproduction (VFX) by causing a qualitative and performative shift in filmmaking. The paper analyses the current state of VP research from both conceptual and technological perspectives, global VP facilities, and significant trends in Australia by presenting two business case studies. We have observed three major trends: Globalisation, Collaboration and Industrialisation. The paper concludes by stating that there is a need for an integrated VPS network through shared infrastructure and resources to foster high quality research and to solve industry-identified problems through industry-led collaborative research partnerships.},
	author = {Kavakli, Manolya and Cremona, Cinzia},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00020},
	issn = {2642-5254},
	keywords = {Tracking;Collaboration;Globalization;Entertainment industry;Production;Games;Cameras;Cinematography;Virtual reality;Virtual production studio;LED walls;filmmaking;in camera VFX;Film;K.6.1 [Management of Computing and Information Systems];Project and People Management---Life Cycle; K.7.m [The Computing Profession];Miscellaneous---Ethics},
	month = {March},
	pages = {29-37},
	title = {The Virtual Production Studio Concept -- An Emerging Game Changer in Filmmaking},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00020}}

@inproceedings{9756784,
	abstract = {Augmented reality (AR) allows us to wear virtual displays that are registered to our bodies and devices. Such virtually extendable displays, or AR extended displays (AREDs), provide personal display space and are free from physical restrictions. Existing work has explored the new design space to improve user experience and efficiency. Contrary to this direction, we focus on the weight that the user perceives from AREDs, even though they are virtual and have no physical weight. Our user study results show evidence that AREDs can be a source of pseudo-weight, in addition to that of a handheld physical display device. We also systematically evaluate the perceived weight changes depending on the layout and delay in the visualization system. These findings are similar to those in existing pseudo-haptics research. However, we found such behavior in pseudo-weight for a real device and virtual visual stimuli in the air, which differentiates our research from previous work.},
	author = {Mori, Shohei and Kataoka, Yuta and Hashiguchi, Satoshi},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00091},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Handheld computers;Conferences;Layout;User interfaces;User experience;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Perception},
	month = {March},
	pages = {703-710},
	title = {Exploring Pseudo-Weight in Augmented Reality Extended Displays},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00091}}

@inproceedings{9756785,
	abstract = {In situ evaluations of novel authentication systems, where the system is evaluated in its intended usage context, are often infeasible due to ethical and legal constraints. Consequently, researchers evaluate their authentication systems in the lab, which questions the eco-logical validity. In this work, we explore how VR can overcome the shortcomings of authentication studies conducted in the lab and contribute towards more realistic authentication research. We built a highly realistic automated teller machine (ATM) and a VR replica to investigate through a user study (N=20) the impact of in situ evaluations on an authentication system`s usability results. We evaluated and compared: Lab studies in the real world, lab studies in VR, in situ studies in the real world, and in situ studies in VR. Our findings highlight 1) VR`s great potential to circumvent potential restrictions researchers experience when evaluating authentication schemes and 2) the impact of the context on an authentication system`s usability evaluation results. In situ ATM authentications took longer (+24.71% in the real world, +14.17% in VR) than authentications in a traditional (VR) lab environment and elicited a higher sense of being part of an ATM authentication scenario compared to a real-world and VR-based evaluation in the lab. Our quantitative findings, along with participants` qualitative feedback, provide first evidence of increased authentication realism when using VR for in situ authentication research. We provide researchers with a novel research approach to conduct (simulated) in situ authentication re-search, discuss our findings in the light of prior works, and conclude with three key lessons to support researchers in deciding when to use VR for in situ authentication research.},
	author = {Mathis, Florian and Vaniea, Kami and Khamis, Mohamed},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00049},
	issn = {2642-5254},
	keywords = {Human computer interaction;Ethics;Three-dimensional displays;Online banking;Law;Conferences;Authentication;Virtual Reality;Authentication;In Situ Research},
	month = {March},
	pages = {301-310},
	title = {Can I Borrow Your ATM? Using Virtual Reality for (Simulated) In Situ Authentication Research},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00049}}

@inproceedings{9756786,
	abstract = {Distance compression, which refers to the underestimation of ego-centric distance to objects, is a common problem in immersive virtual environments. Besides visually compensating the compressed distance, several studies have shown that auditory information can be an alternative solution for this problem. In particular, reverberation time (RT) has been proven to be an effective method to compensate distance compression. To further explore the feasibility of applying audio information to improve distance perception, we investigate whether users' egocentric distance perception can be calibrated, and whether the calibrated effect can be carried over and even sustain for a longer duration. We conducted a study to understand the perceptual learning and carryover effects by using RT as stimuli for users to perceive distance in IVEs. The results show that the carryover effect exists after calibration, which indicates people can learn to perceive distances by attuning reverberation time, and the accuracy even remains a constant level after 6 months. Our findings could potentially be utilized to improve the distance perception in VR systems as the calibration of auditory distance perception in VR could sustain for several months. This could eventually avoid the burden of frequent training regimens.},
	author = {Lin, Wan-Yi and Wang, Ying-Chu and Wu, Dai-Rong and Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Ebrahimi, Elham and Pagano, Christopher and Babu, Sabarish V. and Lin, Wen-Chieh},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00042},
	issn = {2642-5254},
	keywords = {Human computer interaction;Training;Solid modeling;Visualization;Three-dimensional displays;Virtual environments;Calibration;Depth Perception;Auditory Reverberation;Calibration;Perceptual Learning;Computing methodologies [Computer Graphics]: Graphics systems and interfaces---Perception Human-centered computing [Human computer interaction (HCI)]: Interaction paradigms--- Virtual reality},
	month = {March},
	pages = {232-240},
	title = {Empirical Evaluation of Calibration and Long-term Carryover Effects of Reverberation on Egocentric Auditory Depth Perception in VR},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00042}}

@inproceedings{9756787,
	abstract = {In this paper, we present a 360$\,^{\circ}$ panoramic Mixed Reality (MR) sys-tem that visualises shared gaze cues using contextual speech input to improve task coordination. We conducted two studies to evaluate the design of the MR gaze-speech interface exploring the combinations of visualisation style and context control level. Findings from the first study suggest that an explicit visual form that directly connects the collaborators' shared gaze to the contextual conversation is preferred. The second study indicates that the gaze-speech modality shortens the coordination time to attend to the shared interest, making the communication more natural and the collaboration more effective. Qualitative feedback also suggest that having a constant joint gaze indicator provides a consistent bi-directional view while establishing a sense of co-presence during task collaboration. We discuss the implications for the design of collaborative MR systems and directions for future research.},
	author = {Jing, Allison and Lee, Gun and Billinghurst, Mark},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00044},
	issn = {2642-5254},
	keywords = {Manufacturing industries;Visualization;Three-dimensional displays;Collaboration;Mixed reality;Virtual reality;Bidirectional control;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
	month = {March},
	pages = {250-259},
	title = {Using Speech to Visualise Shared Gaze Cues in MR Remote Collaboration},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00044}}

@inproceedings{9756788,
	abstract = {This work explores the interaction between Augmented Reality (AR) and eye accommodation for airborne surveillance by simulating AR environments in Virtual Reality (VR). We simulate the AR display as displays with the capabilities needed for airborne surveillance are limited and because it would be hazardous to experiment directly on surveillance aircraft. While there is precedent for simulating AR in a VR environment, our study account for two of the physical and physiological aspects of AR: we factor in the focal plane of the AR technology and simulate the eye accommodation reflex of the user to provide focus. We ran a study with 24 participants examining AR cues to support visual search. We also compare the effects of having secondary tasks (that surveillance operators are normally responsible for) directly on the observation window using AR. Our results show that the effectiveness of the AR cues is dependent on the modality of the secondary task. We also found that, under certain situations, operators' performances for the search task are improved if the focal plane of the AR display is at the same distance as subsequent search targets.},
	author = {Barbotin, Nicolas and Baumeister, James and Cunningham, Andrew and Duval, Thierry and Grisvard, Olivier and Thomas, Bruce H.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00040},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Surveillance;User interfaces;Physiology;Time factors;Sea level;Human-centered computing---Human computer interaction (HCI)---Empirical studies in HCI;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {213-221},
	title = {Evaluating Visual Cues for Future Airborne Surveillance Using Simulated Augmented Reality Displays},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00040}}

@inproceedings{9756789,
	abstract = {Glanceable Augmented Reality interfaces have the potential to provide fast and efficient information access for the user. However, where to place the virtual content and how to access them depend on the user context. We designed a Context-Aware AR interface that can intelligently adapt for two different contexts: solo and social. We evaluated information access using Context-Aware AR compared to current mobile phones and non-adaptive Glanceable AR interfaces. We found that in a solo scenario, compared to a mobile phone, the Context-Aware AR interface was preferred, easier, and significantly faster; it improved the user experience; and it allowed the user to better focus on their primary task. In the social scenario, we discovered that the mobile phone was slower, more intrusive, and perceived as the most difficult. Meanwhile, Context-Aware AR was faster for responding to information needs triggered by the conversation; it was preferred and perceived as the easiest for resuming conversation after information access; and it improved the user's awareness of the other person's facial expressions.},
	author = {Davari, Shakiba and Lu, Feiyu and Bowman, Doug A.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00063},
	issn = {2642-5254},
	keywords = {Human computer interaction;Three-dimensional displays;Conferences;User experience;Task analysis;Augmented reality;Smart phones;Human-centered computing;Mixed/augmented reality;Interaction techniques;Empirical Studies in HCI},
	month = {March},
	pages = {436-444},
	title = {Validating the Benefits of Glanceable and Context-Aware Augmented Reality for Everyday Information Access Tasks},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00063}}

@inproceedings{9756790,
	abstract = {Encountered-type of Haptic devices (ETHD) are robotic interfaces physically overlaying virtual counterparts prior to a user interaction in Virtual Reality. They theoretically reliably provide haptics in Virtual environments, yet they raise several intrinsic design challenges to properly display rich haptic feedback and interactions in VR applications. In this paper, we use a Failure Mode and Effects Analysis (FMEA) approach to identify, organise and analyse the failure modes and their causes in the different stages of an ETHD scenario and highlight appropriate solutions from the literature to mitigate them. We help justify these interfaces' lack of deployment, to ultimately identify guidelines for future ETHD designers.},
	author = {Bouzbib, Elodie and Bailly, Gilles},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00055},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Conferences;Virtual environments;User interfaces;Reliability theory;Reliability engineering;Hardware;Encountered-type of Haptic Devices;Haptics;Virtual Reality;Design;FMEA;Theoretical Framework;Robotic Graphics},
	month = {March},
	pages = {360-369},
	title = {"Let's Meet and Work it Out": Understanding and Mitigating Encountered-Type of Haptic Devices Failure Modes in VR},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00055}}

@inproceedings{9756791,
	abstract = {Deep networks have demonstrated enormous potential for identification and authentication using behavioral biometrics in virtual reality (VR). However, existing VR behavioral biometrics datasets have small sample sizes which can make it challenging for deep networks to automatically learn features that characterize real-world user behavior and that may enable high success, e.g., high-level spatial relationships between headset and hand controller devices and underlying smoothness of trajectories despite noise. We provide an approach to perform behavioral biometrics using deep networks while incorporating spatial and smoothing constraints on input data to represent real-world behavior. We represent the input data to neural networks as a combination of scale- and translation-invariant device-centric position and orientation features, and displacement vectors representing spatial relationships between device pairs. We assess identification and authentication by including spatial relationships and by performing Gaussian smoothing of the position features. We evaluate our approach against baseline methods that use the raw data directly and that perform a global normalization of the data. By using displacement vectors, our work shows higher success over baseline methods in 36 out of 42 cases of analysis done by varying user sets and pairings of VR systems and sessions.},
	author = {Miller, Robert and Banerjee, Natasha Kholgade and Banerjee, Sean},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00060},
	issn = {2642-5254},
	keywords = {Performance evaluation;Deep learning;Smoothing methods;Biometrics (access control);Neural networks;Authentication;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Security and privacy;Security services;Authentication;Biometrics},
	month = {March},
	pages = {409-418},
	title = {Combining Real-World Constraints on User Behavior with Deep Neural Networks for Virtual Reality (VR) Biometrics},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00060}}

@inproceedings{9756792,
	abstract = {Virtual Reality (VR) is a promising platform for home rehabilitation with the potential to completely immerse users within a playful experience. To explore this area we design, implement, and evaluate a system that uses a VR headset in conjunction with force feed-back gloves to present users with a playful experience for home rehabilitation. The system immerses the user within a virtual cat bathing simulation that allows users to practice fine motor skills by progressively completing three cat-care tasks. The study results demonstrate the positive role that playfulness may play in the user experience of VR rehabilitation.},
	author = {Wang, Qisong and Kang, Bo and Kristensson, Per Ola},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00070},
	issn = {2642-5254},
	keywords = {Headphones;Solid modeling;Three-dimensional displays;Force feedback;Force;Exoskeletons;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Interaction devices;Haptic devices},
	month = {March},
	pages = {504-513},
	title = {Supporting Playful Rehabilitation in the Home using Virtual Reality Headsets and Force Feedback Gloves},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00070}}

@inproceedings{9756794,
	abstract = {We present a study of the effects of field of view (FOV), target movement, and number of targets on visual search performance in virtual reality. We compared visual search tasks in two FOVs (~65$\,^{\circ}$, ~32.5$\,^{\circ}$) under two target movement speeds (static, dynamic) while varying the visible target count, with targets potentially out of the user's view. We examined the expected linear relationship between search time and number of items, to explore how moving and/or out-of-view targets affected this relationship. Overall, search performance increased with a wide FOV, but decreased when targets were moving and with more visible targets. FOV more strongly influenced search performance than target movement. Neither FOV nor target movement meaningfully altered the linear relationship between visual search time and number of items. Participants also rated perceived workload for each condition; FOV and target movement both negatively affected the perceived workload, with target movement being a more significant factor.},
	author = {Grinyer, Kristen and Teather, Robert J.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00032},
	issn = {2642-5254},
	keywords = {Visualization;Solid modeling;Three-dimensional displays;Conferences;Virtual environments;User interfaces;Search problems;Mobile VR;field of view;moving targets;search},
	month = {March},
	pages = {139-148},
	title = {Effects of Field of View on Dynamic Out-of-View Target Search in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00032}}

@inproceedings{9756795,
	abstract = {In contrast to the real world, users are not able to perceive bystanders in virtual reality (VR). Bystanders may distract users and influence their cognitive load. This involves users to feel discomfort at the thought of unintentionally touching or even bumping into a physical bystander while interacting with the virtual environment. Not knowing the intentions of a bystander or whether one is present can unsettle the user. We investigate how a bystander affects a user's cognitive load since it has a decisive impact on applications such as VR training. In a between-subjects lab study (N = 42), three conditions were compared: 1) no bystander, 2) an invisible bystander, and 3) a visible bystander (as an avatar). Over a series of iterations, the participants were asked to memorize four pairs of letters, perform a mental rotation task and then recall the pairs of letters. The results of our study demonstrate that a bystander acting as an avatar in the virtual environment increases the user's cognitive load more than an invisible bystander. Moreover, the cognitive load of a VR user is significantly increased by a bystander. Therefore, our work suggests that either the examiner must be separated from the participant or the examiner's influence (as a bystander) must be included in the analysis.},
	author = {Rettinger, Maximilian and Schmaderer, Christoph and Rigoll, Gerhard},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00025},
	issn = {2642-5254},
	keywords = {Training;Social computing;Three-dimensional displays;Avatars;Virtual environments;User interfaces;Particle measurements;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Collaborative and social computing;Empirical studies in collaborative and social computing},
	month = {March},
	pages = {77-82},
	title = {Do You Notice Me? How Bystanders Affect the Cognitive Load in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00025}}

@inproceedings{9756796,
	abstract = {Gaze tracking is increasingly becoming an essential component in Augmented and Virtual Reality. Modern gaze tracking algorithms are heavyweight; they operate at most 5 Hz on mobile processors despite that near-eye cameras comfortably operate at a real-time rate (> 30 Hz). This paper presents a real-time eye tracking algorithm that, on average, operates at 30 Hz on a mobile processor, achieves 0.1$\,^{\circ}$--0.5$\,^{\circ}$ gaze accuracies, all the while requiring only 30K parameters, one to two orders of magnitude smaller than state-of-the-art eye tracking algorithms. The crux of our algorithm is an Auto ROI mode, which continuously predicts the Regions of Interest (ROIs) of near-eye images and judiciously processes only the ROIs for gaze estimation. To that end, we introduce a novel, lightweight ROI prediction algorithm by emulating an event camera. We discuss how a software emulation of events enables accurate ROI prediction without requiring special hardware. The code of our paper is available at https://github.com/horizon-research/edgaze.},
	author = {Feng, Yu and Goulding-Hotta, Nathan and Khan, Asif and Reyserhove, Hans and Zhu, Yuhao},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00059},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Software algorithms;Gaze tracking;Virtual reality;User interfaces;Prediction algorithms;Gaze;eye tracking;event camera;segmentation},
	month = {March},
	pages = {399-408},
	title = {Real-Time Gaze Tracking with Event-Driven Eye Segmentation},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00059}}

@inproceedings{9756797,
	abstract = {Redirected walking (RDW) aims to reduce the collisions in the physical space for VR applications. However, most of the previous RDW methods do not consider future possibilities of collisions after imperceptibly redirecting users. In this paper, we combine the subtle RDW methods and reset strategy in our method design and propose a novel solution for RDW that can make better use of physical space and trigger fewer resets. The key idea of our method is to discretize the representation of possible user positions and orientations by a series of standard poses and rate them based on the possibilities of hitting obstacles of their reachable poses. A transfer path algorithm is proposed to measure the accessibility among standard poses and is used to support the calculation of the scores of standard poses. Using our method, the user can be redirected imperceptibly to the optimal pose with the best score among all the reachable poses from the user's current pose during walking. Experiments demonstrate that our method outperforms state-of-the-art methods in various environment sizes and obstacle layouts.},
	author = {Xu, Sen-Zhe and Lv, Tian and He, Guangrong and Chen, Chia-Hao and Zhang, Fang-Lue and Zhang, Song-Hai},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00086},
	issn = {2642-5254},
	keywords = {Legged locomotion;Three-dimensional displays;Design methodology;Current measurement;Conferences;Layout;Virtual reality;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
	month = {March},
	pages = {655-663},
	title = {Optimal Pose Guided Redirected Walking with Pose Score Precomputation},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00086}}

@inproceedings{9756800,
	abstract = {Virtual reality applications for industrial training have widespread benefits for simulating various scenarios and conditions. We present an empirical evaluation of VR training approach using kinesthetics learning strategy in industrial maintenance training, specifically the hydraulic manufacturing industry. Through our collaboration with a leading industry partner, a remote multi-user training platform using head-mounted display was designed and implemented. We present the evaluation of the platform with two diverse cohorts of novice users and industry contractors, in comparison to traditional training using slides, photos, and videos. The results show that VR training is engaging and effective in boosting trainee's confidence, especially for novice users. Our studies highlight the impact of virtual reality training on trainee experience, performance, and skills transfer, with reflections on the differences between novice and industry trainees.},
	author = {Hoang, Thuong and Greuter, Stefan and Taylor, Simeon},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00077},
	issn = {2642-5254},
	keywords = {Training;Three-dimensional displays;Collaboration;Virtual environments;Virtual reality;Hydraulic systems;Maintenance engineering;Virtual Reality;Industrial Training;Multimedia Learning in Virtual Reality;Kinesthetics Learning},
	month = {March},
	pages = {573-581},
	title = {An Evaluation of Virtual Reality Maintenance Training for Industrial Hydraulic Machines},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00077}}

@inproceedings{9756802,
	abstract = {We propose RedirectedDoors, a novel space-efficient technique for redirection in VR focused on door-opening behavior. This technique manipulates the user's walking direction by rotating the entire virtual environment (VE) at a certain angular ratio of the door being opened. This ratio is called door rotation gain. At the same time, the virtual door's position is kept unmanipulated so that a realistic door-opening user experience can be ensured. We designed and implemented the rotational manipulation algorithm and two types of door-opening interfaces; with and without a doorknob-type passive haptic prop. We then conducted a user study (N = 12) to investigate redirection performance and user feedback as we examined three independent variables: door rotation gain, door-opening interface, and door-opening direction (push/pull). From the results, the estimated detection thresholds generally showed a higher space efficiency of redirection with our technique. Our results also showed that providing the haptic feedback led to a higher noticeability of redirection, but at the same time supported a higher subjective sense of realism and less discomfort. Following our results, we discuss which combinations of gain and door-opening direction can jointly provide lower noticeability and higher acceptability.},
	author = {Hoshikawa, Yukai and Fujita, Kazuyuki and Takashima, Kazuki and Fjeld, Morten and Kitamura, Yoshifumi},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00066},
	issn = {2642-5254},
	keywords = {Resistance;Legged locomotion;Three-dimensional displays;Conferences;Force feedback;Virtual environments;User interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality},
	month = {March},
	pages = {464-473},
	title = {RedirectedDoors: Redirection While Opening Doors in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00066}}

@inproceedings{9756803,
	abstract = {In augmented reality, the insertion of virtual objects into the real scene needs to meet the requirements of visual consistency. The virtual objects rendered by the augmented reality system should be consistent with the illumination of the real scene. However, for complex scenes, it is not enough to just complete the illumination estimation. When there are transparent objects in the real scene, the difference in refractive index and roughness of transparent objects will influence the effect of the virtual and real fusion. To tackle this problem, this paper proposes a new approach to jointly estimate the illumination and transparent material for inserting virtual objects into the real scene. We solve for the material parameters of objects and illumination simultaneously by nesting microfacet model and hemispherical area illumination model into inverse path tracing. Although there is no geometry model of light sources in the recovered geometry model, the proposed hemispherical area illumination model can be used to recover scene appearance. Multiple experiments on both virtual and real-world datasets verify that the proposed approach subjectively and objectively performs better than the state-of-the-art method.},
	author = {Zhang, Aijia and Zhao, Yan and Wang, Shigang and Wei, Jian},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00021},
	issn = {2642-5254},
	keywords = {Geometry;Solid modeling;Visualization;Three-dimensional displays;Lighting;Refractive index;Estimation;Computing methodologies---Computer graphics---Graphics systems and interfaces---Mixed / augmented reality;Artificial intelligence---Computer vision---Computer vision representations---Appearance and texture representations},
	month = {March},
	pages = {38-46},
	title = {An improved augmented-reality method of inserting virtual objects into the scene with transparent objects},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00021}}

@inproceedings{9756804,
	abstract = {Music videos are short films that integrate songs and imagery and are produced for artistic and promotional purposes. Modern music videos apply various media capture techniques and creative postproduction technologies to provide a myriad of stimulating and artistic approaches to audience entertainment and engagement for viewing across multiple devices. Within this domain, volumetric technologies are becoming a popular means of recording and reproducing musical performances for new audiences to access via traditional 2D screens and emergent virtual reality platforms. However, the precise impact of volumetric video in virtual reality music video entertainment has yet to be fully explored from a user's perspective. Here we show how users responded to volumetric representations of music performance in virtual reality. Our results preliminarily demonstrate how audiences are likely to respond to music videos and offer insight into how future music videos may be developed for different user types. We anticipate our essay as a formative starting point for more sophisticated, interactive music videos that can be accessed and presented via extended-reality technologies.},
	author = {Young, Gareth W. and O'Dwyer, N{\'e}ill and Moynihan, Matthew and Smolic, Aljosa},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00099},
	issn = {2642-5254},
	keywords = {Human computer interaction;Three-dimensional displays;Music;Entertainment industry;Media;User experience;Real-time systems;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI},
	month = {March},
	pages = {775-781},
	title = {Audience Experiences of a Volumetric Virtual Reality Music Video},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00099}}

@inproceedings{9756806,
	abstract = {This paper presents Shape Aware Haptic Retargeting, an extension of "state-of-the-art" haptic retargeting that is the first to support retargeted interaction between any part of the user's hand and any part of the target object. In previous haptic retargeting algorithms, the maximum retargeting is applied only when the hand position aligns with the target position. Shape Aware Haptic Retargeting generalizes the distance computation process to instead consider the hand and target geometry. The shortest hand-target distance is then used to calculate the applied retargeting offset. This ensures the full amount of haptic retargeting is applied at the point of contact with the passive haptic regardless of contact position on the hand or target. We leverage existing geometry algorithms to implement three distance computation methods: Multi-Point, Primitive and Mesh Geometry, in addition to conventional single position approaches. These are evaluated through a set of simulated interactions instead of the single position representation used in previous haptic retargeting systems. The evaluation demonstrated all three approaches can provide improved interaction accuracy over a Point distance computation method, with Mesh Geometry being the most accurate and Primitive being the preferred method for combined performance and interaction accuracy.},
	author = {Matthews, Brandon J. and Thomas, Bruce H. and Von Itzstein, G. Stewart and Smith, Ross T.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00083},
	issn = {2642-5254},
	keywords = {Geometry;Visualization;Solid modeling;Three-dimensional displays;Target tracking;Shape;Conferences;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Computer graphics;Shape modeling;Shape analysis},
	month = {March},
	pages = {625-634},
	title = {Shape Aware Haptic Retargeting for Accurate Hand Interactions},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00083}}

@inproceedings{9756808,
	abstract = {Interaction with tangible user interfaces (TUIs) in virtual reality (VR) is known to offer several benefits in terms of user experience. Incorporating identical-formed tangible objects for foot-enabled embodied interaction in VR is not a well-researched area. To address this gap, in this study, we explored foot-enabled embodied interaction in VR through a room-scale tangible soccer game (Tangiball). Users interacted with a physical ball with their feet in real time by seeing its virtual counterpart inside a VR head mounted display (HMD). Tangiball included a custom-built transparent physical ball, inside which motion trackers were secured using custom 3D-printed attachments. A between-subjects user study was performed with 40 participants, in which Tangiball was compared with the control condition of foot-enabled embodied interaction with a purely virtual ball. The results revealed that tangible interaction improved user performance and presence significantly, while no difference in terms of motion sickness was detected between the tangible and virtual versions. This paper discusses the development and evaluation of Tangiball along with implications of the user study results.},
	author = {Bozgeyikli, Lal Lila and Bozgeyikli, Evren},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00103},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Tracking;Virtual reality;Resists;Games;User interfaces;Motion sickness;User experience;Real-time systems;Sports;Virtual reality;tangible user interfaces;foot-enabled interaction;embodied interaction;direct manipulation;user experience;evaluation},
	month = {March},
	pages = {812-820},
	title = {Tangiball: Foot-Enabled Embodied Tangible Interaction with a Ball in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00103}}

@inproceedings{9756810,
	abstract = {Virtual reality (VR) and augmented reality (AR) have continued to increase in popularity over the past decade. However, there are still issues with how much space is required for room-scale VR and experiences are still lacking from haptic feedback. We present LevelEd SR, a substitutional reality level design workflow that combines AR and VR systems and is built for consumer devices. The system enables passive haptics through the inclusion of physical objects from within a space into a virtual world. A validation study (17 participants) has produced quantitative data that suggests players benefit from passive haptics in entertainment VR games with an improved game experience and increased levels of presence. Including objects, such as real-world furniture that is paired with a digital proxy in the virtual world, also opens up more spaces to be used for room-scale VR. We evaluated the workflow and found that participants were accepting of the system, rating it positively using the System Usability Scale questionnaire and would want to use it again to experience substitutional reality.},
	author = {Beever, Lee and John, Nigel W.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00031},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Entertainment industry;Virtual environments;Games;Organizations;User interfaces;Software;Substitutional reality;Virtual reality;Augmented reality;Level design;Level editor;User generated content},
	month = {March},
	pages = {130-138},
	title = {LevelEd SR: A Substitutional Reality Level Design Workflow},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00031}}

@inproceedings{9756811,
	abstract = {Increasing vehicle automation presents challenges as drivers of automated vehicles become more disengaged from the primary driving task, as there will still be activities that require interfaces for vehicle-passenger interactions. Windshield displays provide large-content areas supporting drivers in non-driving related tasks. This work addresses user preferences as well as task and safety aspects for 3D augmented reality (AR) windshield displays in automated driving. Participants of a user study (N = 24) were presented with two modes of content presentation (multiple content-specific windows vs. one main window), and could freely choose their preferred positions, content types, as well as size, and transparency levels for these content windows using a simulated "ideal" windshield display in a virtual reality driving simulator. We found that using one main content window resulted in better task performance and lower take-over times, however, subjective user experience was higher for the multi-window user interface. These insights help designers of in-vehicle applications to provide a rich user experience in automated vehicles.},
	author = {Riegler, Andreas and Riener, Andreas and Holzmann, Clemens},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00074},
	issn = {2642-5254},
	keywords = {Productivity;Three-dimensional displays;Automation;User interfaces;User experience;Safety;Task analysis;Human-centered computing;Visualization;Visualization techniques;Visualization design and evaluation methods;User Interfaces;Graphical user interfaces (GUI);Information Interfaces and Presentation;Miscellaneous},
	month = {March},
	pages = {543-552},
	title = {Content Presentation on 3D Augmented Reality Windshield Displays in the Context of Automated Driving},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00074}}

@inproceedings{9756815,
	abstract = {The portrayed personality of virtual characters and agents is understood to influence how we perceive and engage with digital applications. Understanding how the features of speech and animation drive portrayed personality allows us to intentionally design characters to be more personalized and engaging. In this study, we use performance capture data of unscripted conversations from a variety of actors to explore the perceptual outcomes associated with the modalities of speech and motion. Specifically, we contrast full performance-driven characters to those portrayed by generated gestures and synthesized speech, analysing how the features of each influence portrayed personality according to the Big Five personality traits. We find that processing speech and motion can have mixed effects on such traits, with our results highlighting motion as the dominant modality for portraying extraversion and speech as dominant for communicating agreeableness and emotional stability. Our results can support the Extended Reality (XR) community in development of virtual characters, social agents and 3D User Interface (3DUI) agents portraying a range of targeted personalities.},
	author = {Thomas, Sean and Ferstl, Ylva and McDonnell, Rachel and Ennis, Cathy},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00018},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Extended reality;Conferences;User interfaces;Animation;Stability analysis;Cognition;Embodied agents;virtual humans and (self-)avatars;Perception and cognition},
	month = {March},
	pages = {11-20},
	title = {Investigating how speech and animation realism influence the perceived personality of virtual characters and agents},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00018}}

@inproceedings{9756816,
	abstract = {When designing virtual embodiment studies, one of the key choices is the nature of the experimental factors, either between-subjects or within-subjects. However, it is well known that each design has ad-vantages and disadvantages in terms of statistical power, sample size requirements and confounding factors. This paper reports a within-subjects experiment with 92 participants comparing self-reported embodiment scores under a visuomotor task with two conditions: synchronous motions and asynchronous motions with a latency of 300 ms. With the gathered data, using a Monte-Carlo method, we created numerous simulations of within- and between-subjects experiments by selecting subsets of the data. In particular, we explored the impact of the number of participants on the replicability of the results from the 92 within-subjects experiment. For the between-subjects simulations, only the first condition for each user was considered to create the simulations. The results showed that while the replicability of the results increased as the number of participants increased for the within-subjects simulations, no matter the number of participants, between-subjects simulations were not able to replicate the initial results. We discuss the potential reasons that could have led to this surprising result and potential methodological practices to mitigate them.},
	author = {Richard, Gr{\'e}goire and Pietrzak, Thomas and Argelaguet, Ferran and L{\'e}cuyer, Anatole and Casiez, G{\'e}ry},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00037},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Monte Carlo methods;Conferences;Virtual reality;User interfaces;Particle measurements;Data models;Virtual Embodiment;Methodology;Latency},
	month = {March},
	pages = {186-195},
	title = {Within or Between? Comparing Experimental Designs for Virtual Embodiment Studies},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00037}}

@inproceedings{9756817,
	abstract = {Continuous virtual rotation is likely one of the biggest contributors to cybersickness, while simultaneously being necessary for many VR scenarios where the user, for instance, is sitting in a bus, at an office desk, or on a couch and therefore, limited in physical body rotation. A possible solution is discrete virtual rotation, such as already broadly accepted in translational movements (teleportation). In this work, we want to help increase the knowledge about discrete virtual rotations. We classify existing work and systematically investigate the two dimensionstarget(rotation)acquisition(selectionvs. directional)and body-based (yes vs. no) regarding their impact on the performance in a naive and a primed rotational search task, spatial orientation, and usability. We do find the novel virtual rotation selection most successful in both search tasks and no difference in the factor bodybased on spatial orientation.},
	author = {Zielasko, Daniel and Heib, Jonas and Weyers, Benjamin},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00090},
	issn = {2642-5254},
	keywords = {Human computer interaction;Three-dimensional displays;Systematics;Cybersickness;Conferences;Teleportation;Space exploration;Human-centered computing;Mixed / augmented reality;Interaction techniques;Empirical studies in HCI},
	month = {March},
	pages = {693-702},
	title = {Systematic Design Space Exploration of Discrete Virtual Rotations in VR},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00090}}

@inproceedings{9756818,
	abstract = {We present STROE, a new ungrounded string-based weight simulation device. STROE is worn as an add-on to a shoe that in turn is connected to the user's hand via a controllable string. A motor is pulling the string with a force according to the weight to be simulated. The design of STROE allows the users to move more freely than other state-of-the-art devices for weight simulation. It is also quieter than other devices, and is comparatively cheap. We conducted a user study that empirically shows that STROE is able to simulate the weight of various objects and, in doing so, increases users' perceived realism and immersion of VR scenes.},
	author = {Achberger, Alexander and Arulrajah, Pirathipan and Sedlmair, Michael and Vidackovic, Kresimir},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00029},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Conferences;Computational modeling;Force;Virtual reality;Footwear;Human-centered computing;Haptic devices},
	month = {March},
	pages = {112-120},
	title = {STROE: An Ungrounded String-Based Weight Simulation Device},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00029}}

@inproceedings{9756819,
	abstract = {Mimicking physical odor sensations virtually can present users with a real - time odor synthesis that approximates what users would smell in a virtual environment, e.g., as they walk around in virtual reality. To this end, we devise a Smell Engine that includes: (i) a Smell Composer framework that allows developers to configure odor sources in virtual space, (ii) a Smell Mixer that dynamically estimates the odor mix that the user would smell, based on diffusion models and relative odor source distances, and (iii) a Smell Controller that coordinates an olfactometer to physically present an approximation of the odor mix to the user's mask from a set of odorants channeled through controllable flow valves. Through a three - part user study, we found that the Smell Engine can help measure a subject's olfactory detection threshold and improve their ability to precisely localize odors in the virtual environment, as compared to existing trigger - based solutions.},
	author = {Bahremand, Alireza and Manetta, Mason and Lai, Jessica and Lahey, Byron and Spackman, Christy and Smith, Brian H. and Gerkin, Richard C. and LiKamWa, Robert},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00043},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Conferences;Olfactory;Virtual environments;User interfaces;Aerospace electronics;Human-centered computing;Interactive systems and tools;Computer systems organization;Sensors and actuators},
	month = {March},
	pages = {241-249},
	title = {The Smell Engine: A system for artificial odor synthesis in virtual environments},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00043}}

@inproceedings{9756822,
	abstract = {Nurses in intensive care units are exposed to permanent stress. Emotional stress contributes to psycho-physiological exhaustion symptoms such as chronic sleep disturbances, restlessness or burnout, which negatively affects the quality of care and interaction with patients. A promising method for learning coping strategies to deal with stress is Stress Inoculation Training, which involves practicing stressors in a controlled environment at increasing intensity. In this work, using virtual reality, the stressor of having to comply with the patient's or family's wishes, even if one does not agree with them, was selected. The stressor was defined based on a literature review and interviews with experts which was then implemented in three intensity levels. In a Wizard-of-Oz trial, participants interacted with virtual characters using natural speech. The stress response of the induced stress was measured using objective (heart rate, time between R peaks, skin conductance, and respiratory rate) and subjective measures (Perceived Stress Questionnaire and a 7-point Likert item). While objective results do not show significant differences between intensity levels, a significant increase is detected in the subjective measures. We show that emotional stress can be induced by increasing the intensity of a stressor in VR using virtual characters.},
	author = {Wei{\ss}, Sebastian and Busse, Steffen and Heuten, Wilko},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00038},
	issn = {2642-5254},
	keywords = {Training;Solid modeling;Three-dimensional displays;Atmospheric measurements;Anxiety disorders;Virtual reality;User interfaces;Human-centered computing;User Studies Human-centered computing;Virtual Reality Human-centered computing;HCI theory;concepts and models},
	month = {March},
	pages = {196-204},
	title = {Inducing Emotional Stress From The Intensive Care Context Using Storytelling In VR},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00038}}

@inproceedings{9756823,
	abstract = {Spatial Augmented Reality (SAR) is a useful tool for procedural tasks as virtual instructions can be co-located with the physical task. Predictive cue annotations have been utilized to further enhance user performance through SAR. These predictive cues have only been measured under ordinary circumstances. This paper aims to investigate predictive cues under a sub-optimal scenario by depriving users of sleep. Sleep deprivation is a common form of fatigue, which is known to have serious detriments on user performance. Based upon existing sleep literature, we expected predictive cue performance to degrade over time with sharp declines during the early hours of the morning. Despite these drops in performance, we hypothesized that having a predictive cue would benefit user performance in comparison to no cue. Results from a 62-hour sleep deprivation experiment indicated that providing SAR predictive cues was beneficial throughout sleep deprivation. Furthermore, having no predictive cue caused accuracy to decline earlier in the sleep deprivation period. From the predictive cues outlined in this paper, the line cue maintained the fastest response time and was least impacted by early morning performance declines.},
	author = {Volmer, Benjamin and Baumeister, James and Matthews, Raymond and Grosser, Linda and Von Itzstein, Stewart and Banks, Siobhan and Thomas, Bruce H.},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00079},
	issn = {2642-5254},
	keywords = {Performance evaluation;Uncertainty;Spatial augmented reality;Resists;Virtual reality;User interfaces;Fatigue;Spatial augmented reality;sleep deprivation;circadian rhythm;predictive cues;procedural task},
	month = {March},
	pages = {589-598},
	title = {A Comparison of Spatial Augmented Reality Predictive Cues and their Effects on Sleep Deprived Users},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00079}}

@inproceedings{9756824,
	abstract = {Although it is commonly accepted that depth perception in augmented reality (AR) displays is distorted, we have yet to isolate which properties of AR affect people's ability to correctly perceive virtual objects in real spaces. From prior research on depth perception in commercial virtual reality, it is likely that ergonomic properties and graphical limitations impact visual perception in head-mounted displays (HMDs). However, an insufficient amount of research has been conducted in augmented reality HMDs for us to begin isolating pertinent factors in this family of displays. To this end, in the current research, we evaluate absolute measures of distance perception in the Microsoft HoloLens 2, an optical see-through AR display, and the Varjo XR-3, a video see-through AR display. The current work is the first to evaluate either device using absolute distance perception as a measure. For each display, we asked participants to verbally report distance judgments to both grounded and floating targets that were rendered either with or without a cast shadow along the ground. Our findings suggest that currently available video see-through displays may induce more distance underestimation than their optical see-through counterparts. We also find that the vertical position of an object and the presence of a cast shadow influence depth perception.},
	author = {Adams, Haley and Stefanucci, Jeanine and Creem-Regehr, Sarah and Bodenheimer, Bobby},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00101},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Atmospheric measurements;Current measurement;Optical distortion;Optical variables measurement;User interfaces;Particle measurements;OST AR;VST AR;distance;perception;shadow;depth;surface contact},
	month = {March},
	pages = {792-801},
	title = {Depth Perception in Augmented Reality: The Effects of Display, Shadow, and Position},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00101}}

@inproceedings{9756825,
	abstract = {Consumer level virtual experiences almost always occur when physical space is limited, either by the constraints of an indoor space or of a tracked area. This observation coupled with the need for movement through large virtual spaces has resulted in a proliferation of research into locomotion interfaces that decouples movement through the virtual environment from movement in the real world. While many locomotion interfaces support movement of some kind in the real world, some do not. This paper examines the effect of the amount of physical space used in the real world on one popular locomotion interface, resetting, when compared to a locomotion interface that requires minimal physical space, walking in place. The metric used to compare the two locomotion interfaces was navigation performance, specifically, the acquisition of survey knowledge. We find that, while there are trade-offs between the two methods, walking in place is preferable in small spaces.},
	author = {Paris, Richard A. and Buck, Lauren E. and McNamara, Timothy P. and Bodenheimer, Bobby},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00104},
	issn = {2642-5254},
	keywords = {Legged locomotion;Measurement;Costs;Three-dimensional displays;Navigation;Tracking;Sociology;Virtual Reality;Locomotion Methods;Walking in Place;Resetting},
	month = {March},
	pages = {821-831},
	title = {Evaluating the Impact of Limited Physical Space on the Navigation Performance of Two Locomotion Methods in Immersive Virtual Environments},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00104}}

@inproceedings{9756826,
	abstract = {Given the difficulties of studying the shoulder surfing resistance of authentication systems in a live setting, researchers often ask study participants to shoulder surf authentications by watching two-dimensional (2D) video recordings of a user authenticating. How-ever, these video recordings do not provide participants with a realistic shoulder surfing experience, creating uncertainty in the value and validity of lab-based shoulder surfing experiments. In this work, we exploit the unique characteristics of virtual reality (VR) and study the use of non-immersive/immersive VR recordings for shoulder surfing research. We conducted a user study (N=18) to explore the strengths and weaknesses of such a VR-based shoulder surfing research approach. Our results suggest that immersive VR observations result in a more realistic shoulder surfing experience, in a significantly higher sense of being part of the authentication environment, in a greater feeling of spatial presence, and in a higher level of involvement than 2D video observations without impacting participants' observation performance. This suggests that studying shoulder surfing in VR is advantageous in many ways compared to currently used approaches, e.g., participants can freely choose their observation angle rather than being limited to a fixed observation angle as done in current methods. We discuss the strengths and weaknesses of using VR for shoulder surfing research and conclude with four recommendations to help researchers decide when (and when not) to employ VR for shoulder surfing research in the authentication research domain.},
	author = {Mathis, Florian and O'Hagan, Joseph and Khamis, Mohamed and Vaniea, Kami},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00048},
	issn = {2642-5254},
	keywords = {Resistance;Privacy;Uncertainty;Three-dimensional displays;Two dimensional displays;Authentication;Virtual reality;Virtual Reality;Shoulder Surfing;Authentication},
	month = {March},
	pages = {291-300},
	title = {Virtual Reality Observations: Using Virtual Reality to Augment Lab-Based Shoulder Surfing Research},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00048}}

@inproceedings{9756827,
	abstract = {Group exercise is more effective for gaining motivation than exercising alone, but it can be difficult to always find such partners. In this paper, we explore the experiences that joggers have with a virtual partner instead of a human partner and report on the results of two controlled experiments evaluating our approach. In Study 1, we investigated how participants felt and how their behav-ior changed when they jogged indoors with a human partner or with a virtual partner compared to solitary jogging. The virtual partner was represented either as a full-body, limb-only, or a point-light avatar displayed on smartglasses. In Study 2, we investigated the differences between the three representations as virtual partners for casual joggers in an outdoor setting. Based on our results, we propose implications for the design of virtual runners as casual jogging partners and speculate on their relationship with human users.},
	author = {Hamada, Takeo and Hautasaari, Ari and Kitazaki, Michiteru and Koshizuka, Noboru},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00085},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Statistical analysis;Avatars;Design methodology;Conferences;User interfaces;Safety;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Applied computing;Life and medical sciences;Consumer health},
	month = {March},
	pages = {644-654},
	title = {Solitary Jogging with A Virtual Runner using Smartglasses},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00085}}

@inproceedings{9756828,
	abstract = {When using tangible props as proxies for virtual objects, it is important that these haptic proxies are similar to and co-located with their virtual counterparts. This makes it challenging to scale virtual scenarios because more proxies are needed as scenarios grow more complex. Haptic retargeting, or virtual remapping, makes it possible to repurpose the same physical prop as a proxy for multiple virtual objects. This paper details a user study comparing two techniques for repurposing haptic proxies; namely haptic retargeting based on body warping and change blindness remapping. Participants performed a simple button-pressing task, and 24 virtual buttons were mapped onto four haptic proxies with varying degrees of misalignment. Body warping and change blindness remapping were used to realign the real and virtual buttons, and the results indicate that users failed to reliably detect realignment of up to 7.9 cm for body warping and up to 9.7 cm for change blindness remapping. Moreover, change blindness remapping yielded significantly higher self-reported agency, and marginally higher ownership. Taken together these results suggest that this less explored technique has potential when it comes to repurposing haptic proxies for virtual reality.},
	author = {Patras, Cristian and Cibulskis, Mantas and Nilsson, Niels Christian},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00039},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Conferences;Blindness;Virtual reality;User interfaces;Haptic interfaces;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	month = {March},
	pages = {205-212},
	title = {Body Warping Versus Change Blindness Remapping: A Comparison of Two Approaches to Repurposing Haptic Proxies for Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00039}}

@inproceedings{9756829,
	abstract = {Nonverbal cues are paramount in real-world interactions. Among these cues, gaze has received much attention in the literature. In particular, previous work has shown a search asymmetry between directed and averted gaze towards the observer using photographic stimuli, with faster detection and longer fixation towards directed gaze by the observer. This is known as the stare-in-the-crowd effect. In this study, we investigate whether stare-in-the crowd effect is preserved in Virtual Reality (VR). To this end, we designed a within-subject experiment where 30 human users were immersed in a virtual environment in front of an audience of 11 virtual agents following 4 different gaze behaviours. We analysed the user's gaze behaviour when observing the audience, computing fixations and dwell time. We also collected the users' social anxiety score using a post-experiment questionnaire to control for some potential influencing factors. Results show that the stare-in-the-crowd effect is preserved in VR, as demonstrated by the significant differences between gaze behaviours, similarly to what was found in previous studies using photographic stimuli. Additionally, we found a negative correlation between dwell time towards directed gazes and users' social anxiety scores. Such results are encouraging for the development of expressive and reactive virtual humans, which can be animated to express natural interactive behaviour.},
	author = {Raimbaud, Pierre and Jovane, Alberto and Zibrek, Katja and Pacchierotti, Claudio and Christie, Marc and Hoyet, Ludovic and Pettr{\'e}, Julien and Olivier, Anne-H{\'e}l{\`e}ne},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00047},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Correlation;Design methodology;Conferences;Anxiety disorders;Virtual environments;Observers;Human-centered computing;Visualization;Visualization design and evaluation methods;Human-centered computing;User studies;Applied computing;Law;social and behavioral sciences},
	month = {March},
	pages = {281-290},
	title = {The Stare-in-the-Crowd Effect in Virtual Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00047}}

@inproceedings{9756830,
	abstract = {Optical see-through head-mounted displays (OST HMDs) are becoming increasingly popular as they get better and smaller. One application area is interaction with virtual content, which is more intuitive when using physical objects as tangibles. Since it is not possible to use a matching replica for each virtual object, it is necessary to identify physical objects that can represent several different virtual objects. As a first step, we investigated to what extent a physical object can differ in size from its virtual counterpart.Since the perception of content in optical see-through Augmented Reality (OST AR) is strongly influenced by the ambient lighting, the illumination intensity was considered in our study. We investigated three indoor lighting conditions and their effects on the perception of seven different size variations between the physical object and its virtual overlay.The results of the study show that there is a decrease in usability and presence with increasing illuminance. However, this cannot be avoided when applications are run under realistic interior lighting conditions. Furthermore, the results demonstrate that the size ranges in which a physical object can deviate from its virtual counterpart without having a strong negative impact on usability, presence and performance increase with increasing environmental illumination. Therefore, it is possible to interact with even smaller and even larger physical props to manipulate the associated virtual content under brighter lighting conditions.},
	author = {Kahl, Denise and Ruble, Marc and Kr{\"u}ger, Antonio},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00030},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Head-mounted displays;Shape;Conferences;Lighting;User interfaces;Object recognition;Tangible augmented reality;optical see-through augmented reality;tangible interaction;illumination},
	month = {March},
	pages = {121-129},
	title = {The Influence of Environmental Lighting on Size Variations in Optical See-through Tangible Augmented Reality},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00030}}

@inproceedings{9756831,
	abstract = {With the speedy increase of display resolution and the demand for interactive frame rate, rendering acceleration is becoming more critical for a wide range of virtual reality applications. Foveated rendering addresses this challenge by rendering with a non-uniform resolution for the display. Motivated by the non-linear optical lens equation, we present rectangular mapping-based foveated rendering (RMFR), a simple yet effective implementation of foveated rendering framework. RMFR supports varying level of foveation according to the eccentricity and the scene complexity. Compared with traditional foveated rendering methods, rectangular mapping-based foveated rendering provides a superior level of perceived visual quality while consuming minimal rendering cost.},
	author = {Ye, Jiannan and Xie, Anqi and Jabbireddy, Susmija and Li, Yunchuan and Yang, Xubo and Meng, Xiaoxu},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00097},
	issn = {2642-5254},
	keywords = {Visualization;Costs;Three-dimensional displays;Pipelines;Optical buffering;Virtual reality;User interfaces;Computing methodologies;Computer graphics;Rendering;Visibility;Human-centered computing;Visualization;Visualization design and evaluation methods},
	month = {March},
	pages = {756-764},
	title = {Rectangular Mapping-based Foveated Rendering},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00097}}

@inproceedings{9756834,
	abstract = {In this study, we propose an approach to display a distortion-free mid-air image inside a transparent refractive object and on a curved reflective surface. We compensate for the distortion by generating a light source image that cancels the distortions in the mid-air image caused by refraction and reflection. The light source image is generated via ray-tracing simulation by transmitting the desired view image to the mid-air imaging system, which includes distortive surfaces, and by receiving the transmitted image at the light source position. The proposed methods can be applied to dynamic images using a light source image as a UV map in texture mapping. Finally, we present the results of an evaluation of our method performed in an actual optical system using the generated light source image, which visually demonstrate the effectiveness of the proposed approach.},
	author = {Kiuchi, Shunji and Koizumi, Naoya},
	booktitle = {2022 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:34 -0400},
	date-modified = {2024-03-18 02:30:34 -0400},
	doi = {10.1109/VR51125.2022.00081},
	issn = {2642-5254},
	keywords = {Solid modeling;Optical distortion;Virtual reality;Ray tracing;User interfaces;Distortion;Optical imaging;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Rendering;Ray tracing},
	month = {March},
	pages = {606-614},
	title = {Distortion-free Mid-air Image Inside Refractive Surface and on Reflective Surface},
	year = {2022},
	bdsk-url-1 = {https://doi.org/10.1109/VR51125.2022.00081}}

@inproceedings{9417634,
	abstract = {We explore the concept of a Spherical World in Miniature (SWIM) for discrete locomotion in Virtual Reality (VR). A SWIM wraps a planar WIM around a physically embodied sphere and thereby implements the metaphor of a tangible Tiny Planet that can be rotated and moved, enabling scrolling, scaling, and avatar teleportation. The scaling factor is set according to the sphere&#x0027;s distance from the head-mounted display (HMD), while rotation moves the current viewing window. Teleportation is triggered with a dwell time when looking at the sphere and keeping it still. In a lab study (N&#x003D;20), we compare our SWIM implementation to a planar WIM with an established VR controller technique using physical buttons. We test both concepts in a navigation task and also investigate the effects of two different screen sizes. Our results show that the SWIM, despite its less direct geometrical transformation, performed superior in most evaluations. It outperformed the planar WIM not only in terms of task completion time (TCT) and accuracy but also in subjective ratings.},
	author = {Englmeier, David and Sajko, Wanja and Butz, Andreas},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00057},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Head-mounted displays;Planets;Navigation;Avatars;Resists;Teleportation;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Haptic devices;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
	month = {March},
	pages = {345-352},
	title = {Spherical World in Miniature: Exploring the Tiny Planets Metaphor for Discrete Locomotion in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00057}}

@inproceedings{9417636,
	abstract = {We present online survey results on social virtual reality (social VR) users' activities and usage motives. Based on content analysis of users' free-text responses, we found that most users, in fact, use these applications for social activities and to satisfy diverse social needs. The second most frequently mentioned categories of activities and motives relate to experiential aspects such as entertainment activities. Another important category of motives, which has only recently been described in related work, relates to the self, such as personal growth. Our results indicate that while social VR provides a superior social experience than traditional digital social spaces, like games or social media, users still desire better and affordable tracking technology, increased sensory immersion, and further improvement concerning social features. These findings complement related work as they come from a comparatively large sample (N= 273) and summarize a general user view on social VR. Besides confirming an intuitive assumption, they help identify use cases and opportunities for further research on social VR.},
	author = {Sykownik, Philipp and Graf, Linda and Zils, Christoph and Masuch, Maic},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00079},
	issn = {2642-5254},
	keywords = {COVID-19;Three-dimensional displays;Social networking (online);Pandemics;Entertainment industry;Virtual reality;Games;Social VR;online social worlds;user motives;virtual reality},
	month = {March},
	pages = {546-554},
	title = {The Most Social Platform Ever? A Survey about Activities & Motives of Social VR Users},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00079}}

@inproceedings{9417638,
	abstract = {When users create hand-drawn annotations in Virtual Reality they often reach their physical limits in terms of precision, especially if the region to be annotated is small. One intuitive solution employs magnification beyond natural scale. However, scaling the whole environment results in wrong assumptions about the coherence between physical and virtual space. In this paper, we introduce Mag-noramas, a novel interaction method for selecting and extracting a region of interest that the user can subsequently scale and transform inside the virtual space. Our technique enhances the user's capabilities to perform supernaturally precise virtual annotations on virtual objects. We explored our technique in a user study within asimplified clinical scenario of a teleconsultation-supported craniectomy procedure that requires accurate annotations on a human head. Teleconsultation was performed asymmetrically between a remote expert in Virtual Reality that collaborated with a local user through Augmented Reality. The remote expert operates inside a reconstructed environment, captured from RGB-D sensors at the local site, and is embodied by an avatar to establish co-presence. The results show that Magnoramas significantly improve the precision of annotations while preserving usability and perceived presence measures compared to the baseline method. By hiding the 3D reconstruction while keeping the Magnorama, users can intentionally choose to lower their perceived social presence and focus on their tasks.},
	author = {Yu, Kevin and Winkler, Alexander and Pankratz, Frieder and Lazarovici, Marc and Wilhelm, Dirk and Eck, Ulrich and Roth, Daniel and Navab, Nassir},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00062},
	issn = {2642-5254},
	keywords = {Manifolds;Three-dimensional displays;Telepresence;Annotations;Transforms;User interfaces;Real-time systems;Interaction Techniques;Medical Information System;Virtual Reality},
	month = {March},
	pages = {392-401},
	title = {Magnoramas: Magnifying Dioramas for Precise Annotations in Asymmetric 3D Teleconsultation},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00062}}

@inproceedings{9417639,
	abstract = {Immersive Virtual Environments (IVEs) incorporating tangibles are becoming more accessible. The success of applications combining 3D printed tangibles and VR often depends on how accurately size is perceived. Research has shown that visuo-haptic perceptual information is important in the perception of size. However, it is unclear how these sensory-perceptual channels are affected by immersive virtual environments that incorporate tangible objects. Towards understanding the effects of different sensory information channels in the near field size perception of tangibles of graspable sizes in IVEs, we conducted a between-subjects study evaluating the accuracy of size perception across three experimental conditions (Vision-only, Haptics-only, Vision and Haptics). We found that overall, participants consistently over-estimated the size of the dials regardless of the type of perceptual information that was presented. Participants in the haptics only condition overestimated diameters to a larger degree as compared to other conditions. Participants were most accurate in the vision only condition and least accurate in the haptics only condition. Our results also revealed that increased efficiency in reporting size over time was most pronounced in the visuo- haptic condition.},
	author = {de Siqueira, Alexandre Gomes and Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Bhargava, Ayush and Lucaites, Kathryn and Solini, Hannah and Nasiri, Moloud and Robb, Andrew and Pagano, Christopher and Ullmer, Brygg and Babu, Sabarish V.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00086},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Virtual environments;User interfaces;Haptic interfaces;Human-centered computing&#x2014;Virtual reality;Human-centered computing&#x2014;Haptic devices;Computing methodologies&#x2014;Perception},
	month = {March},
	pages = {1-10},
	title = {Empirically Evaluating the Effects of Perceptual Information Channels on the Size Perception of Tangibles in Near-Field Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00086}}

@inproceedings{9417640,
	abstract = {Learning to move the hands in particular ways is essential in many training andleisure virtual reality applications, yet challenging. Existing techniques that support learning of motor movement in virtual reality rely on external cues such as arrows showing where to move or transparent hands showing the target movement. We propose a technique where the avatar's hand movement is corrected to be closer to the target movement. This embeds guidance in the user's avatar, instead of in external cues and minimizes visual distraction. Through two experiments, we found that such movement guidance improves the short-term retention of the target movement when compared to a control condition without guidance.},
	author = {Lilija, Klemen and Kyllingsb{\ae}k, S{\o}ren and Hornb{\ae}k, Kasper},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00069},
	issn = {2642-5254},
	keywords = {Training;Visualization;Three-dimensional displays;Avatars;User interfaces;Human-centered computing---Empirical studies in HCI;Human-centered computing---Virtual reality},
	month = {March},
	pages = {1-8},
	title = {Correction of Avatar Hand Movements Supports Learning of a Motor Skill},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00069}}

@inproceedings{9417642,
	abstract = {Conventional swept-volume displays can provide accurate physical cues for depth perception. However, the corresponding texture reproduction does not have high quality because such displays employ high-speed projectors with low bit-depth and low resolution. In this study, to address the limitation of swept-volume displays while retaining their advantages, a novel swept-volume three-dimensional (3D) display is proposed by incorporating physical materials as screens. Physical materials such as wool, felt, and so on are directly used for reproducing textures on a displayed 3D surface. Furthermore, we introduce the adaptive pattern generation based on real-time viewpoint tracking to perform the hidden-surface removal. Our algorithm leverages the ray-tracing concept and can run at high speed on GPU.},
	author = {Asahina, Ray and Nomoto, Takashi and Yoshida, Takatoshi and Watanabe, Yoshihiro},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00032},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Shape;Graphics processing units;Virtual reality;User interfaces;Ray tracing;Observers;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Displays and imagers},
	month = {March},
	pages = {113-121},
	title = {Realistic 3D Swept-Volume Display with Hidden-Surface Removal Using Physical Materials},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00032}}

@inproceedings{9417643,
	abstract = {Holographic displays and computer-generated holography offer a unique opportunity in improving optical resolutions and depth characteristics of near-eye displays. The thermally-modulated Nanopho-tonic Phased Array (NPA), a new type of holographic display, affords several advantages, including integrated light source and higher refresh rates, over other holographic display technologies. However, the thermal phase modulation of the NPA makes it susceptible to the thermal proximity effect where heating one pixel affects the temperature of nearby pixels. Proximity effect correction (PEC) methods have been proposed for 2D Fourier holograms in the far field but not for Fresnel holograms at user-specified depths. Here we extend an existing PEC method for the NPA to Fresnel holograms with phase-only hologram optimization and validate it through computational simulations. Our method is not only effective in correcting the proximity effect for the Fresnel holograms of 2D images at desired depths but can also leverage the fast refresh rate of the NPA to display 3D scenes with time-division multiplexing.},
	author = {Sun, Xuetong and Zhang, Yang and Huang, Po-Chun and Acharjee, Niloy and Dagenais, Mario and Peckerar, Martin and Varshney, Amitabh},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00058},
	issn = {2642-5254},
	keywords = {Phased arrays;Multiplexing;Solid modeling;Three-dimensional displays;Phase modulation;Proximity effects;Holography;Nanophotonic phased array;proximity effect correction;proximal algorithms;phase-only hologram;Fresnel hologram;Computing methodologies;Image processing;Computing methodologies-Mixed / augmented reality Computing methodologies;Virtual reality;Hardware-Displays and imagers},
	month = {March},
	pages = {353-362},
	title = {Proximity Effect Correction for Fresnel Holograms on Nanophotonic Phased Arrays},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00058}}

@inproceedings{9417644,
	abstract = {We present methods used to produce and study a first version of an attempt to reconstruct a 1983 live rock concert in virtual reality. An approximately 10 minute performance by the rock band Dire Straits was rendered in virtual reality, based on the use of computer vision techniques to extract the appearance and movements of the band, and crowd simulation for the audience. An online pilot study was conducted where participants experienced the scenario and freely wrote about their experience. The documents produced were analyzed using sentiment analysis, and groups of responses with similar sentiment scores were found and compared. The results showed that some participants were disturbed not by the band performance but by the accompanying virtual audience that surrounded them. The results point to a profound level of plausibility of the experience, though not in the way that the authors expected. The findings add to our understanding of plausibility of virtual environments.},
	author = {Beacco, Alejandro and Oliva, Ramon and Cabreira, Carlos and Gallego, Jaime and Slater, Mel},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00078},
	issn = {2642-5254},
	keywords = {Solid modeling;Sentiment analysis;Computer vision;Three-dimensional displays;Computational modeling;Virtual environments;User interfaces;concert performance;computer vision;virtual reality;historical reconstruction;plausibility;presence;1.3.7;Computer Graphics;Three-Dimensional Graphics and Realism},
	month = {March},
	pages = {538-545},
	title = {Disturbance and Plausibility in a Virtual Rock Concert: A Pilot Study},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00078}}

@inproceedings{9417645,
	abstract = {Remote sensing is currently the primary method of obtaining knowledge about the composition and physical properties of the surface of other planets. In a commonly used technique, visible and near-infrared (VNIR) spectrometers onboard orbiting satellites capture reflectance data at different wavelengths, which in turn gives insight about the minerals present and the overall composition of the terrain. In select locations on Mars, rovers have also conducted up close in-situ investigation of the same terrains examined by orbiters, allowing direct comparisons at different spatial scales. In this work, we build Planetary Visor, a virtual reality tool to visualize orbital and ground data around NASA's Mars Science Laboratory Curiosity rover's ongoing traverse in Gale Crater. We have built a 3D terrain along Curiosity's traverse using rover images, and within it we visualize satellite data as polyhedrons, superimposed on that terrain. This system provides perspectives of VNIR spectroscopic data from a satellite aligned with ground images from the rover, allowing the user to explore both the physical aspects of the terrain and their relation to the mineral composition. The result is a system that provides seamless rendering of datasets at vastly different scales. We conduct a user study with subject matter experts to evaluate the success and potential of our tool. The results indicate that Visor assists with geometric understanding of spectral data, improved geological context, a better sense of scale while navigating terrain, and new insights into spectral data. The result is not only an immersive environment in a scientifically interesting area on Mars, but a robust tool for analysis and visualization of data that can yield improved scientific discovery. This technology is relevant to the ongoing operations of the Curiosity rover and will directly be able to represent the data collected in the upcoming Mars 2020 Perseverance rover mission.},
	author = {Gold, Lauren and Bahremand, Alireza and Richards, Connor and Hertzberg, Justin and Sese, Kyle and Gonzalez, Alexander and Purcell, Zoe and Powell, Kathryn and LiKamWa, Robert},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00066},
	issn = {2642-5254},
	keywords = {Space vehicles;Mars;Satellites;Three-dimensional displays;Data visualization;Virtual reality;Tools;Spectroscopy;Remote Sensing;Vitrual reality},
	month = {March},
	pages = {428-437},
	title = {Visualizing Planetary Spectroscopy through Immersive On-site Rendering},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00066}}

@inproceedings{9417646,
	abstract = {We present a VR system that supports the restoration of broken cultural artifacts. As a case study, we demonstrate this approach for the restoration of a funerary monument. Among the challenges of this monument are a large number of 415 fragments, an unknown amount belongs to another artifact, missing pieces prevent a full reconstruction and the preserved fragments vary strongly in size. Our VR system supports the workflow of digital restoration by offering a configurable self-arranging fragment wall. It supports the user to organize all fragments in an overview representation and to identify relevant fragments quickly. For assembly, we implemented a jigsaw approach comprising two sets of manipulation techniques that allow the user to roughly align fragments first in sub-puzzles and precisely assemble them in a second step. The iterative development and assembling process was accompanied by a professional restorer. We report about the insights we gained from this process and how we optimized the VR system according to her requirements and feedback. Within 14 sessions that took 21 hours, the virtual reconstruction was finalized.},
	author = {Saalfeld, Patrick and B{\"o}ttcher, Claudia and Klink, Fabian and Preim, Bernhard},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00101},
	issn = {2642-5254},
	keywords = {Surface reconstruction;Three-dimensional displays;Virtual reality;User interfaces;Three-dimensional printing;Surface fitting;Systems support;Human-centered computing-Human computer interaction (HCI);Human-centered computing-Interaction design Human-centered computing-Visualization Applied computing-Arts and humanities Applied computing-Digital libraries and archives},
	month = {March},
	pages = {739-748},
	title = {VR System for the Restoration of Broken Cultural Artifacts on the Example of a Funerary Monument},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00101}}

@inproceedings{9417647,
	abstract = {We present Text2Gestures, a transformer-based learning method to interactively generate emotive full-body gestures for virtual agents aligned with natural language text inputs. Our method generates emotionally expressive gestures by utilizing the relevant biomechanical features for body expressions, also known as affective features. We also consider the intended task corresponding to the text and the target virtual agents' intended gender and handedness in our generation pipeline. We train and evaluate our network on the MPI Emotional Body Expressions Database and observe that our network produces state-of-the-art performance in generating gestures for virtual agents aligned with the text for narration or conversation. Our network can generate these gestures at interactive rates on a commodity GPU. We conduct a web-based user study and observe that around 91% of participants indicated our generated gestures to be at least plausible on a five-point Likert Scale. The emotions perceived by the participants from the gestures are also strongly positively correlated with the corresponding intended emotions, with a minimum Pearson coefficient of 0.77 in the valence dimension.},
	author = {Bhattacharya, Uttaran and Rewkowski, Nicholas and Banerjee, Abhishek and Guhan, Pooja and Bera, Aniket and Manocha, Dinesh},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00037},
	issn = {2642-5254},
	keywords = {Learning systems;Three-dimensional displays;Correlation;Databases;Natural languages;Pipelines;Graphics processing units;Computing methodologies-Virtual reality;Computing methodologies-Intelligent agents;Computer systems organization-Neural networks},
	month = {March},
	pages = {1-10},
	title = {Text2Gestures: A Transformer-Based Network for Generating Emotive Body Gestures for Virtual Agents},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00037}}

@inproceedings{9417649,
	abstract = {In the near future, augmented reality (AR) glasses are envisioned to become the next-generation personal computing platform. They could be always on and worn all day, delivering continuous and pervasive AR experiences for general-purpose everyday use cases. However, it remains unclear how we could enable unobtrusive and easy information access without distracting users, while being acceptable to use at the same time. To address this question, we implemented two prototypes based on the Glanceable AR paradigm, a promising way of managing and acquiring information through glancing at the periphery of AR head-worn displays (HWDs). We conducted two separate studies to evaluate our designs. In the first study, we obtained feedback from a large sample of participants of varied age and background about a video prototype that showcased some envisioned scenarios of using Glanceable AR for everyday tasks. In the second study, we asked participants to use a working prototype during authentic real-world activities for three days. We found that users appreciated the Glanceable AR approach. They found it less distracting or intrusive than existing devices in authentic everyday use cases, and would like to use the interface on a daily basis if the form factor of the AR headset was more like eyeglasses.},
	author = {Lu, Feiyu and Bowman, Doug A.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00104},
	issn = {2642-5254},
	keywords = {Headphones;Three-dimensional displays;Head-mounted displays;Prototypes;Glass;User interfaces;Task analysis;Human-centered computing-Mixed / augmented reality;Human-centered computing-User interface design},
	month = {March},
	pages = {768-777},
	title = {Evaluating the Potential of Glanceable AR Interfaces for Authentic Everyday Uses},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00104}}

@inproceedings{9417651,
	abstract = {Virtual reality (VR) experiences currently tend to focus on full body interactions. However, fine motor control in actions such as writing and drawing are seldom studied. Challenges include the inability to perceive fine details due to the low resolution of head mounted displays, the difficulty in simulating fine motor actions in virtual environments, tracking instabilities, latency issues, etc. State of the art VR has managed to address a host of such concerns, supporting a variety of input mechanisms for activities such as writing, sketching, immersive modeling, etc. With VR increasingly being applied in education and medical contexts where writing and note taking is a crucial, it is important to study how well humans can perform these tasks in VR. In a between-subjects empirical evaluation, we studied participants' fine motor coordination with several digital input based writing and artistic tasks performed both in virtual and real world settings, further examining the effects of providing a virtual self avatar on task performance. We integrated multiple tracking systems and applied inverse kinematics to animate the virtual body and simulate hand motions. We went on to compare how different the outputs of these digital input metaphors are to a real world pen and paper approach in an effort to ascertain where we currently stand in being able to support writing and note taking in virtual world contexts. Overall, it seems to be the case that while writing and artistic activities can be successfully supported in VR applications using specialized input devices, the accuracy with which users perform such tasks is significantly higher in the real world, highlighting the need for developments that support such fine motor tasks in VR.},
	author = {Hsu, Chi-Hsuan and Chung, Chi-Han and Venkatakrishnan, Rohith and Venkatakrlshnan, Roshan and Wang, Yu-Shuen and Babu, Sabarish V.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00089},
	issn = {2642-5254},
	keywords = {Performance evaluation;Motor drives;Solid modeling;Art;Three-dimensional displays;Tracking;Virtual environments;Writing and Art in Virtual Reality;Perception-Action Coordination;Fine Motor Control},
	month = {March},
	pages = {1-10},
	title = {Comparative Evaluation of Digital Writing and Art in Real and Immersive Virtual Environments},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00089}}

@inproceedings{9417652,
	abstract = {Today's availability of sophisticated consumer-grade Virtual Reality (VR) hardware provides affordable access to training simulation technology. In contrast to more traditional high fidelity training simulations utilizing physical replicas of control panels that allow natural interaction, users of virtual training often rely on handheld VR controllers to manipulate simulated controls. Therefore, control manipulations of virtual buttons and sliders must be mapped to approximations of real hand manipulations. For instance, the turning of a physical knob with thumb and index finger can be mapped to a gross motor forearm rotation, or a fine motor joystick manipulation when manipulating a controller. In this paper, we present an exploration of hand input approximations of real hands to manipulate the buttons, toggles, knobs and sliders using typical handheld VR controllers. We use common Oculus Quest controllers that rely on capacitive sensing to create basic, approximate hand gestures. We demonstrate the potential of our designs by performing an experiment in which we compare approximate hand gestures against using the controller's joystick that allows fine motor control using thumb input, and a baseline ray-casting interaction that is commonly used in VR applications. We provide a detailed analysis of our interaction designs using the Framework for Interactive Fidelity Analysis (FIFA), which allows us to discuss differences between input approximations and the real-world hand manipulations.},
	author = {Tatzgern, Markus and Birgmann, Christoph},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00092},
	issn = {2642-5254},
	keywords = {Training;Solid modeling;Motor drives;Three-dimensional displays;Thumb;Virtual reality;User interfaces;Human-centered computing-Interaction paradigms-Virtual reality;Human-centered computing-Human-centered Interaction (HCI)-Empirical studies in HCI;Applied computing-Education-Interactive learning environments},
	month = {March},
	pages = {1-9},
	title = {Exploring Input Approximations for Control Panels in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00092}}

@inproceedings{9417655,
	abstract = {We introduce the disocclusion headlight, a method for VR selection assistance based on alleviating occlusions at the center of the user's field of view. The user's visualization of the VE is modified to reduce overlap between objects. This way, selection candidate objects have larger image footprints, which facilitates selection. The modification is confined to the center of the frame, with continuity to the periphery of the frame which is rendered conventionally. The selection assistance is provided automatically, without any interaction from the user. Furthermore, our method disoccludes without destroying the local spatial relationships between selection candidates, which allows solving complex selection queries based on the relative position of objects. We have tested our method on three selection tasks, where we compared it to two state-of-the-art VR selection techniques, i.e., the alpha cursor and the flower cone. Our method showed significant advantages in terms of shorter task completion times, and of fewer selection errors.},
	author = {Wang, Lili and Chen, Jianjun and Ma, Qixiang and Popescu, Voicu},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00043},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Virtual reality;User interfaces;Rendering (computer graphics);Task analysis;Virtual reality-Pointing and selection-Disocclusion-Multiperspective rendering},
	month = {March},
	pages = {216-225},
	title = {Disocclusion Headlight for Selection Assistance in VR},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00043}}

@inproceedings{9417656,
	abstract = {Understanding the extent to which, and conditions under which, scene detail affects spatial perception accuracy can inform the responsible use of sketch-like rendering styles in applications such as immersive architectural design walkthroughs using 3D concept drawings. This paper reports the results of an experiment that provides important new insight into this question using a custom-built, portable video-see-through (VST) conversion of an optical-see-through head-mounted display (HMD). Participants made egocentric distance judgments by blind walking to the perceived location of a real physical target in a real-world outdoor environment under three different conditions of HMD-mediated scene detail reduction: full detail (raw camera view), partial detail (Sobel-filtered camera view), and no detail (complete background subtraction), and in a control condition of unmediated real world viewing through the same HMD. Despite the significant differences in participants' ratings of visual and experiential realism between the three different video-see-through rendering conditions, we found no significant difference in the distances walked between these conditions. Consistent with prior findings, participants underestimated distances to a significantly greater extent in each of the three VST conditions than in the real world condition. The lack of any clear penalty to task performance accuracy not only from the removal of scene detail, but also from the removal of all contextual cues to the target location, suggests that participants may be relying nearly exclusively on context - independent information such as angular declination when performing the blind-walking task. This observation highlights the limitations in using blind walking to the perceived location of a target on the ground to make inferences about people's understanding of the 3D space of the virtual environment surrounding the target. For applications like immersive architectural design, where we seek to verify the equivalence of the 3D spatial understanding derived from virtual immersion and real world experience, additional measures of spatial understanding should be considered.},
	author = {Vaziri, Koorosh and Bondy, Maria and Bui, Amanda and Interrante, Victoria},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00056},
	issn = {2642-5254},
	keywords = {Legged locomotion;Visualization;Three-dimensional displays;Virtual environments;Resists;User interfaces;Rendering (computer graphics);Virtual reality;spatial perception;non-photorealistic rendering},
	month = {March},
	pages = {1-9},
	title = {Egocentric Distance Judgments in Full-Cue Video-See-Through VR Conditions are No Better than Distance Judgments to Targets in a Void},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00056}}

@inproceedings{9417658,
	abstract = {Computer Generated Holography (CGH) is a promising technique for synthesizing 3D images on-demand. CGH systems sculpt light by computing a 2D modulation pattern to shape the phase or the amplitude of a coherent light source. Yet, static wave modulation restricts feasible holograms to a very small subset of all the possible illumination patterns where speckle noise is omnipresent. Here, we introduce Dynamic Computer Generated Holography (DCGH), a novel light sculpting technique that modulates light both spatially and temporally. DCGH has many more degrees of control to eliminate unwanted wave correlations and yields high fidelity, speckle- free 3D holograms. Our technique relies on an algorithm that simultaneously computes a set of engineered coherent wavefronts by optimizing them as a whole to best render the desired 3D illumination pattern when their contributions are rapidly superimposed and averaged in time. We have implemented DCGH with a Digital Micromirror Device (DMD) and synthesized 3D holograms made of up to 50 mutually optimized waves with refresh rates of 190 3D images per second. Our experimental results indicate that DCGH yields 3D images with improved resolution and contrast, successfully addressing the shortcomings of single-frame holography. Our technique can be implemented with inexpensive, low-power hardware, as a compact, portable 3D display that is perfectly suited for virtual and augmented reality applications.},
	author = {Curtis, Vincent R. and Caira, Nicholas W. and Xu, Jiayi and Sata, Asha Gowda and P{\'e}gard, Nicolas C.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00097},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Image resolution;Shape;Phase modulation;Lighting;Holography;User interfaces;Computer Generated Holography-3D image synthesis-virtual reality-3D displays sculpted light},
	month = {March},
	pages = {1-9},
	title = {DCGH: Dynamic Computer Generated Holography for Speckle-Free, High Fidelity 3D Displays},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00097}}

@inproceedings{9417659,
	abstract = {In Virtual Reality (VR), users typically interact with the virtual world using virtual keyboard to insert keywords, surfing the webpages, or typing passwords to access online accounts. Hence, it becomes imperative to understand the security of virtual keystrokes. In this paper, we present VR-Spy, a virtual keystrokes recognition method using channel state information (CSI) of WiFi signals. To the best of our knowledge, this is the first work that uses WiFi signals to recognize virtual keystrokes in VR headsets. The key idea behind VR -Spy is that the side-channel information of fine-granular hand movements associated with each virtual keystroke has a unique gesture pattern in the CSI waveforms. Our novel pattern extraction algorithm leverages signal processing techniques to extract the patterns from the variations of CSI. We implement VR-Spy using two Commercially Off-The-Shelf (COTS) devices, a transmitter (WAVLINK router), and a receiver (Intel NUC with an IWL 5300 NIC). Finally, VR-Spy achieves a virtual keystrokes recognition accuracy of 69.75% in comparison to techniques that assume very advanced adversary models with vision and motion sensors near the victim.},
	author = {Arafat, Abdullah Al and Guo, Zhishan and Awad, Amro},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00081},
	issn = {2642-5254},
	keywords = {Headphones;Solid modeling;Transmitters;Keyboards;Virtual reality;Receivers;Side-channel attacks;Human-centered computing-Gesture Computing-Virtual Key-logging Attack-Channel State Information},
	month = {March},
	pages = {564-572},
	title = {VR-Spy: A Side-Channel Attack on Virtual Key-Logging in VR Headsets},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00081}}

@inproceedings{9417660,
	abstract = {Digital pen interaction has become a first-class input modality for precision tasks such as writing, annotating, drawing, and 2D manipulation. The key enablers of digital inking are the capacitive or resistive sensors that are integrated in contemporary tablet devices. In Virtual Reality (VR), however, users typically provide input across large regions, hence limiting the suitability of using additional tablet devices for accurate pen input. In this paper, we present Flashpen, a digital pen for VR whose sensing principle affords accurately digitizing hand writing and intricate drawing, including small and quick turns. Flashpen re-purposes an inexpensive gaming mouse sensor that digitizes extremely fine grained motions in the micrometer range at over 8 kHz when moving on a surface. We combine Flashpen's high-fidelity relative input with the absolute tracking cues from a VR headset to enable pen interaction across a variety of VR applications. In our two-block evaluation, which consists of a tracing task and a writing task, we compare Flashpen to a professional drawing tablet (Wacom). With this, we demonstrate that Flashpen's fidelity matches the performance of state-of-the-art digitizers and approaches the fidelity of analog pens, while adding the flexibility of supporting a wide range of flat surfaces.},
	author = {Romat, Hugo and Fender, Andreas and Meier, Manuel and Holz, Christian},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00053},
	issn = {2642-5254},
	keywords = {Surface reconstruction;Three-dimensional displays;Tracking;Input devices;Virtual reality;Writing;Tools;Human-centered computing;Human computer interaction (HCI);Interaction devices;Graphics input devices},
	month = {March},
	pages = {306-315},
	title = {Flashpen: A High-Fidelity and High-Precision Multi-Surface Pen for Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00053}}

@inproceedings{9417662,
	abstract = {We present a Virtual and Augmented Reality multi-user prototype of a learning environment for liver anatomy education. Our system supports various training scenarios ranging from small learning groups to classroom-size education, where students and teachers can participate in virtual reality, augmented reality, or via desktop PCs. In an iterative development process with surgeons and teachers, a virtual organ library was created. Nineteen liver data sets were used comprising 3D surface models, 2D image data, pathology information, diagnosis and treatment decisions. These data sets can interactively be sorted and investigated individually regarding their volumetric and meta information. The three participation modes were evaluated within a user study with surgery lecturers (5) and medical students (5). We assessed the usability and presence using questionnaires. Additionally, we collected qualitative data with semistructured interviews. A total of 435 individual statements were recorded and summarized to 49 statements. The results show that our prototype is usable, induces presence, and potentially support the teaching of liver anatomy and surgery in the future.},
	author = {Schott, Danny and Saalfeld, Patrick and Schmidt, Gerd and Joeres, Fabian and Boedecker, Christian and Huettl, Florentine and Lang, Hauke and Huber, Tobias and Preim, Bernhard and Hansen, Christian},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00052},
	issn = {2642-5254},
	keywords = {Training;Three-dimensional displays;Liver;Surgery;Prototypes;User interfaces;Systems support;Human-centered computing-Human computer interaction (HCI);Human-centered computing-Interactive systems and tools;Human-centered computing-Collaborative and social computing systems and tools;Applied computing-Life and medical sciences;Applied computing-Interactive learning environments},
	month = {March},
	pages = {296-305},
	title = {A VR/AR Environment for Multi-User Liver Anatomy Education},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00052}}

@inproceedings{9417663,
	abstract = {Tangible user interfaces (TUIs) have been widely studied in computer, virtual reality and augmented reality systems and are known to improve user experience in these mediums. However, there have been few evaluations of TUIs in wearable mixed reality (MR). In this study, we present the results from a comparative study evaluating three object manipulation techniques in wearable MR: (1) Space-multiplexed identical-formed TUI (i.e., a physical cube that acted as a dynamic tangible proxy with identical real and virtual forms); (2) Time-multiplexed TUI (i.e., a tangible controller that was used to manipulate virtual content); (3) Hand gesture (i.e., reaching, pinching and moving the hand to manipulate virtual content). The interaction techniques were compared with a user study with 42 participants. Results revealed that the tangible cube and the controller interaction methods were comparative to each other while both being superior to the hand gesture interaction method in terms of user experience, performance, and presence. We also present suggestions for interaction design for MR based on our findings.},
	author = {Bozgeyikli, Evren and Bozgeyikli, Lal Lila},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00105},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Mixed reality;User interfaces;User experience;Manipulator dynamics;Augmented reality;Mixed reality;augmented reality;tangible user interfaces;object manipulation;user experience;evaluation.: Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
	month = {March},
	pages = {778-787},
	title = {Evaluating Object Manipulation Interaction Techniques in Mixed Reality: Tangible User Interfaces and Gesture},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00105}}

@inproceedings{9417665,
	abstract = {Indoor asset tracking is an essential task in many areas of industry such as shipping and warehousing. Widely-used asset tracking technologies typically require supporting infrastructure (signal transponders) to communicate with active tags on the assets. Continuous asset tracking in large indoor spaces like warehouses can be costly, and reliable reference points are needed to calculate asset positions accurately. In this paper, we describe an indoor asset tracking technique for augmented reality (AR) that combines the use of (passive) fiducial markers together with flexible spatial anchors. Our approach, called SABIAT, continuously tracks the approximate location of an asset using spatial anchors and, when needed, the precise location of that asset using fiducial markers. We have applied our SABIAT technique to build a demonstrator system, AR-IPS, to show how assets can be tracked and located inside of a large, multi-level building.},
	author = {He, Wennan and Xi, Mingze and Gardner, Henry and Swift, Ben and Adcock, Matt},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00047},
	issn = {2642-5254},
	keywords = {Industries;Three-dimensional displays;Warehousing;Buildings;User interfaces;Fiducial markers;Transponders;Human-centered computing-Mixed / augmented reality;Information systems-Location based services},
	month = {March},
	pages = {255-259},
	title = {Spatial Anchor Based Indoor Asset Tracking},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00047}}

@inproceedings{9417666,
	abstract = {Hand pose estimation, which predicts the spatial location of hand joints, is a fundamental task in VR/AR applications. Although existing methods can recover hand pose competently, the tremor issue occurring in hand motion has not been completely solved. Tremor is an involuntary motion accompanied by a desired gesture or hand motion, leading to hand pose that deviates from user's intentions. Considering the characteristic of tremor motion, we present a novel Graph Neural Network for stable 3D hand pose estimation. The input is depth images. The constraint adjacency matrix is devised in Graph Neural Network for dynamically adjusting the topology of a hand graph during message passing and aggregation. Firstly, since there are rich potential constraints among hand joints, we utilize the constraint adjacency matrix to mine the suitable topology, modeling spatial-temporal constraints of joints and outputting the precise tremor hand pose as the pre-estimation result. Then, for obtaining a stable hand pose, we provide a tremor compensation module based on the constraint adjacency matrix, which exploits the constraint between control points and tremor hand pose. Concretely, the control points represented the voluntary motion are employed as constraints to edit the tremor hand pose. Our extensive quantitative and qualitative experiments show that the proposed method has achieved decent performance for 3D tremor hand pose estimation.},
	author = {Leng, Zhiying and Chen, Jiaying and Shum, Hubert P. H. and Li, Frederick W. B. and Liang, Xiaohui},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00044},
	issn = {2642-5254},
	keywords = {Training;Human computer interaction;Three-dimensional displays;Network topology;Message passing;Pose estimation;Virtual reality;Computing methodologies-Artificial intelligence-Computer vision-Computer vision problems;Human-centered computing-Human computer interaction-Interaction techniques-Gestural input},
	month = {March},
	pages = {226-234},
	title = {Stable Hand Pose Estimation under Tremor via Graph Neural Network},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00044}}

@inproceedings{9417667,
	abstract = {In this paper, we present a series of user studies to investigate the technique of gesture-amplitude-based walking-speed control for locomotion in place (LIP) in virtual reality (VR). Our 1st study suggested that compared to tapping and goose-stepping, the gesture of marching in place was significantly preferred by users across three different virtual walking speed (i.e., 1 , 3 , and 10 ) while sitting and standing, and it yielded larger motion difference across the three speed levels. With the tracker data recorded in the 1st study, we trained a Support- Vector-Machine classification model for LIP speed control based on users' leg/foot gestures in marching in place. The overall accuracy for classifying three speed levels was above 90% for sitting and standing. With the classification model, we then compared the marching-in-place speed-control technique with the controller-based teleportation approach on a target-reaching task where users were sitting and standing. We found no significant difference between the two conditions in terms of target-reaching accuracy. More importantly, the technique of marching in place yielded significantly higher user ratings in terms of naturalness, realness, and engagement than the controller-based teleportation did.},
	author = {Ke, Pinachuan and Zhu, Kenina},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00067},
	issn = {2642-5254},
	keywords = {Legged locomotion;Solid modeling;Three-dimensional displays;Tracking;Lips;Velocity control;Virtual reality;H.5 [Information Interfaces and Presentation (e.g. HCI)]: Multimedia Information Systems;Artificial;augmented;virtual realities},
	month = {March},
	pages = {438-447},
	title = {Larger Step Faster Speed: Investigating Gesture-Amplitude-based Locomotion in Place with Different Virtual Walking Speed in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00067}}

@inproceedings{9417668,
	abstract = {Using virtual reality setups, users can fade out of their surroundings and dive fully into a thrilling and appealing virtual environment. The success of such immersive experiences depends heavily on natural and engaging interactions with the virtual world. As developers tend to focus on intuitive hand controls, other aspects of the broad range of full-body capabilities are easily left vacant. One repeatedly overlooked input modality is the user&#x0027;s gait. Even though users may walk physically to explore the environment, it usually does not matter how they move. However, gait-based interactions, using the variety of information contained in human gait, could offer interesting benefits for immersive experiences. For instance, stealth VR-games could profit from this additional range of interaction fidelity in the form of a sneaking-based input modality. In our work, we explore the potential of sneaking as a playful input modality for virtual environments. Therefore, we discuss possible sneaking-based gameplay mechanisms and develop three technical approaches, including precise foot-tracking and two abstraction levels. Our evaluation reveals the potential of sneaking-based inter-actions in IVEs, offering unique challenges and thrilling gameplay. For these interactions, precise tracking of individual footsteps is unnecessary, as a more abstract approach focusing on the players&#x0027; intention offers the same experience while providing better comprehensible feedback. Based on these findings, we discuss the broader potential and individual strengths of our gait-centered interactions.},
	author = {Cmentowski, Sebastian and Krekhov, Andrey and Zenner, Andr&#x00E9; and Kucharski, Daniel and Kr&#x00FC;ger, Jens},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00071},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Virtual environments;Focusing;Games;User interfaces;Human-centered computing-Virtual reality;Software and its engineering-Interactive games},
	month = {March},
	pages = {473-482},
	title = {Towards Sneaking as a Playful Input Modality for Virtual Environments},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00071}}

@inproceedings{9417669,
	abstract = {The number of people who own a virtual reality (VR) head-mounted display (HMD) has reached a point where researchers can readily recruit HMD owners to participate remotely using their own equipment. However, HMD owners recruited online may differ from the university community members who typically participate in VR research. HMD owners (n=220) and non-owners (n=282) were recruited through two online work sites-Amazon's Mechanical Turk and Prolific-and an undergraduate participant pool. Participants completed a survey in which they provided demographic information and completed measures of HMD use, video game use, spatial ability, and motion sickness susceptibility. In the context of the populations sampled, the results provide 1) a characterization of HMD owners, 2) a snapshot of the most commonly owned HMDs, 3) a comparison between HMD owners and non-owners, and 4) a comparison among online workers and undergraduates. Significant gender differences were found: men reported lower motion sickness susceptibility and more video game hours than women, and men outperformed women on spatial tasks. Men comprised a greater proportion of HMD owners than non-owners, but after accounting for this imbalance, HMD owners did not differ appreciably from non-owners. Comparing across recruitment platform, male undergraduates outperformed male online workers on spatial tests, and female undergraduates played fewer video game hours than female online workers. The data removal rate was higher from Amazon compared to Prolific, possibly reflecting greater dishonesty. These results provide a description of HMD users that can inform researchers recruiting remote participants through online work sites. These results also signal a need for caution when comparing in-person VR research that primarily enrolls undergraduates to online VR research that enrolls online workers.},
	author = {Kelly, Jonathan W. and Cherep, Lucia A. and Lim, Alex F. and Doty, Taylor and Gilbert, Stephen B.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00095},
	issn = {2642-5254},
	keywords = {Headphones;Three-dimensional displays;Sociology;Resists;Virtual reality;Games;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
	month = {March},
	pages = {687-694},
	title = {Who Are Virtual Reality Headset Owners? A Survey and Comparison of Headset Owners and Non-Owners},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00095}}

@inproceedings{9417671,
	abstract = {With the rising popularity of Virtual Reality (VR), there is also a rising interest in co-located multiplayer experiences, as people want to play VR games together with their friends. As having multiple VR headsets is out of reach to the average consumer, we need to look into different possible ways of including multiple people in this play space. We have created a multi-modal co-located multiplayer VR game, Stuck in Space, that introduces a second player in two ways - one with a PC (the baseline that a lot of current games do), as well as a tracked Phone that can be used as a `window into the virtual world'. We have conducted a user study (n = 24) where we explore the difference in immersion and co-presence between the two versions using two questionnaires (IPQ and NMMoSP), as well as a thematic analysis of the subsequent interview data, from which 5 themes emerged. Surprisingly, we found no significant difference in co-presence or immersion based on the quantitative data. However, the qualitative analysis helps reveal one of the main reasons why that is - maintaining a mental model of the real world while also being in the virtual world makes it harder for the person wearing the headset to immerse themselves and feel co-present. From these themes and sub-themes we theorize that each of the two versions has positives and negatives that cancel each other out in the quantitative data, and for there to be a difference we would need to accentuate or change certain elements of the game. The results show that introducing a second player through a Phone is not detrimental in terms of co-presence and immersion and that it is a viable way of doing so, although certain design considerations would have to be taken into account to minimize the negatives.},
	author = {Malinov, Yoan-Daniel and Millard, David E. and Blount, Tom},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00074},
	issn = {2642-5254},
	keywords = {Headphones;Three-dimensional displays;Games;Resists;Virtual reality;User interfaces;Mobile handsets;Human-centered computing-User studies;Human-centered computing-Virtual reality;Human-centered computing-Collaborative interaction;Applied computing-Computer games},
	month = {March},
	pages = {501-510},
	title = {StuckInSpace: Exploring the Difference Between Two Different Mediums of Play in a Multi-Modal Virtual Reality Game},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00074}}

@inproceedings{9417676,
	abstract = {Engagement has been traditionally linked to presence in desktop-based virtual reality learning environments. Although several studies have been performed to determine other factors affecting cognitive engagement, the role of cognitive load as a factor of student's engagement in desktop-based virtual reality (VR) has received little attention in the literature. The main purpose of this study was to explain if individual dimensions of cognitive load (mental demand, effort, and frustration level) can be used in addition to factors like presence and self-efficacy to predict student's cognitive engagement. The results of the study confirmed presence and self-efficacy as significant predictors of student's engagement. Also, a three-step hierarchical regression analysis revealed that two of the three individual dimensions of cognitive load (effort and frustration level) were also significant predictors of student's engagement.},
	author = {Bueno-Vesga, Jhon Alexander and Xu, Xinhao and He, Hao},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00090},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Virtual reality;User interfaces;Regression analysis;Task analysis;Virtual Reality;Cognitive Engagement;Presence;Self-efficacy;Cognitive Load;Effort;Frustration},
	month = {March},
	pages = {645-652},
	title = {The Effects of Cognitive Load on Engagement in a Virtual Reality Learning Environment},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00090}}

@inproceedings{9417677,
	abstract = {Virtual Reality is increasingly used for safe evaluation and validation of autonomous vehicles by automotive engineers. However, the design and creation of virtual testing environments is a cumbersome process. Engineers are bound to utilize desktop-based authoring tools, and a high level of expertise is necessary. By performing scene authoring entirely inside VR, faster design iterations become possible. To this end, we propose a VR authoring environment that enables engineers to design road networks and traffic scenarios for automated vehicle testing based on free-hand interaction. We present a 3D interaction technique for the efficient placement and selection of virtual objects that is employed on a 2D panel. We conducted a comparative user study in which our interaction technique outperformed existing approaches regarding precision and task completion time. Furthermore, we demonstrate the effectiveness of the system by a qualitative user study with domain experts.},
	author = {Eroglu, Sevinc and Stefan, Frederic and Chevalier, Alain and Roettger, Daniel and Zielasko, Daniel and Kuhlen, Torsten W. and Weyers, Benjamin},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00020},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Roads;Design methodology;Fingers;Virtual environments;User interfaces;Usability;Human-centered computing-Human computer interaction (HCI)- Virtual Reality;Human-centered computing-Interaction design and evaluation methods-User interface design-User studies},
	month = {March},
	pages = {1-10},
	title = {Design and Evaluation of a Free-Hand VR-based Authoring Environment for Automated Vehicle Testing},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00020}}

@inproceedings{9417680,
	abstract = {Visual realism in Virtual Reality (VR) increases both immersion and development costs. Consequently, it is important to understand the cost-benefit trade-off of specific aspects of visual realism. Determining the extent of visual realism often leads to decisions on the level of visual complexity of a Virtual Environment (VE). In this paper, we investigate the impact of visual complexity on a user's spatial orientation through a user study. To do so, we created a VE containing parts of a real-world place. Participants were asked to map their location within the VE to the corresponding real-world location. They were provided by a pop-up map of the entire VE, on which they were required to choose one named location out of a predefined set of locations. We manipulated the VE's visual complexity by varying the visual elements used to provide cartographic information, namely a map overlay and 3D blocks as buildings. This results in four scene types: i) landscape contour, ii) landscape contour with 3D buildings, iii) landscape contour overlaid with a map, and iv) landscape contour with both 3D buildings and the map overlay. Each participant performed our location recall task for each scene type. Our findings provide empirical evidence that addition of each of these two visual elements individually improved spatial orientation, while their combination only adds a slight improvement.},
	author = {Handali, Joshua Peter and Schneider, Johannes and Gau, Michael and Holzwarth, Valentin and Brocke, Jan vom},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00051},
	issn = {2642-5254},
	keywords = {Performance evaluation;Visualization;Three-dimensional displays;Scalability;Buildings;Urban planning;Virtual environments;Human-centered computing-Visualization-Empirical studies in visualization;Human-centered computing-Visualization-Visualization design and evaluation methods},
	month = {March},
	pages = {286-295},
	title = {Visual Complexity and Scene Recognition: How Low Can You Go?},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00051}}

@inproceedings{9417681,
	abstract = {Previous research on distance estimation in virtual reality (VR) has well established that even for geometrically accurate virtual objects and environments users tend to systematically mis-estimate distances. This has implications for Social VR, where it introduces variables in personal space and proxemics behavior that change social behaviors compared to the real world. One yet unexplored factor is related to the trend that avatars' embodied cues in Social VR are often scaled, e.g., by making one's head bigger or one's voice louder, to make social cues more pronounced over longer distances. In this paper we investigate how the perception of avatar distance is changed based on two means for scaling embodied social cues: visual head scale and verbal volume scale. We conducted a human-subject study employing a mixed factorial design with two Social VR avatar representations (full-body, head-only) as a between factor as well as three visual head scales and three verbal volume scales (up-scaled, accurate, down-scaled) as within factors. For three distances from social to far-public space, we found that visual head scale had a significant effect on distance judgments and should be tuned for Social VR, while conflicting verbal volume scales did not, indicating that voices can be scaled in Social VR without immediate repercussions on spatial estimates. We discuss the interactions between the factors and implications for Social VR.},
	author = {Choudhary, Zubin and Gottsacker, Matthew and Kim, Kangsoo and Schubert, Ryan and Stefanucci, Jeanine and Bruder, Gerd and Welch, Gregory F.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00106},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Avatars;Design methodology;Estimation;Aerospace electronics;User interfaces;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
	month = {March},
	pages = {788-797},
	title = {Revisiting Distance Perception with Scaled Embodied Cues in Social Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00106}}

@inproceedings{9417683,
	abstract = {Virtual Reality/360$\,^{\circ}$ videos provide an immersive experience to users. Besides this, 360$\,^{\circ}$ videos may lead to an undesirable effect when consumed with Head-Mounted Displays (HMDs), referred to as simulator sickness/cybersickness. The Simulator Sickness Questionnaire (SSQ) is the most widely used questionnaire for the assessment of simulator sickness. Since the SSQ with its 16 questions was not designed for 360$\,^{\circ}$ video related studies, our research hypothesis in this paper was that it may be simplified to enable more efficient testing for 360$\,^{\circ}$ video. Hence, we evaluate the SSQ to reduce the number of questions asked from subjects, based on six different previously conducted studies. We derive the reduced set of questions from the SSQ using Principal Component Analysis (PCA) for each test. Pearson Correlation is analysed to compare the relation of all obtained reduced questionnaires as well as two further variants of SSQ reported in the literature, namely Virtual Reality Sickness Questionnaire (VRSQ) and Cybersickness Questionnaire (CSQ). Our analysis suggests that a reduced questionnaire with 9 out of 16 questions yields the best agreement with the initial SSQ, with less than 44% of the initial questions. Exploratory Factor Analysis (EFA) shows that the nine symptom-related attributes determined as relevant by PCA also appear to be sufficient to represent the three dimensions resulting from EFA, namely, Uneasiness, Visual Discomfort and Loss of Balance. The simplified version of the SSQ has the potential to be more efficiently used than the initial SSQ for 360$\,^{\circ}$ video by focusing on the questions that are most relevant for individuals, shortening the required testing time.},
	author = {Singla, Ashutosh and G{\"o}ring, Steve and Keller, Dominik and Ramachandra Rao, Rakesh Rao and Fremerey, Stephan and Raake, Alexander},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00041},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Head-mounted displays;Focusing;Immersive experience;User interfaces;Tools;360$\,^{\circ}$ video-simulator sickness-questionnaire-cybersickness;PCA-factor analysis},
	month = {March},
	pages = {198-206},
	title = {Assessment of the Simulator Sickness Questionnaire for Omnidirectional Videos},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00041}}

@inproceedings{9417686,
	abstract = {Power wheelchairs are one of the main solutions for people with reduced mobility to maintain or regain autonomy and a comfortable and fulfilling life. However, driving a power wheelchair in a safe way is a difficult task that often requires training methods based on real-life situations. Although these methods are widely used in occupational therapy, they are often too complex to implement and unsuitable for some people with major difficulties. In this context, we collaborated with clinicians to develop a Virtual Reality based power wheelchair simulator. This simulator is an innovative training tool adapted to any type of situations and impairments. In this paper, we present a clinical study in which 29 power wheelchair regular users were asked to complete a clinically validated task designed by clinicians within two conditions: driving in a virtual environment with our simulator and driving in real conditions with a real power wheelchair. The objective of this study is to compare performances between the two conditions and to evaluate the Quality of Experience provided by our simulator in terms of Sense of Presence and Cybersickness. Results show that participants complete the tasks in a similar amount of time for both real and virtual conditions, using respectively a real power wheelchair and our simulator. Results also show that our simulator provides a high level of Sense of Presence and provokes only slight to moderate Cybersickness discomforts resulting in a valuable Quality of Experience.},
	author = {Vailland, Guillaume and Devigne, Louise and Pasteau, Fran{\c c}ois and Nouviale, Florian and Fraudet, Bastien and Leblong, {\'E}milie and Babel, Marie and Gouranton, Val{\'e}rie},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00065},
	issn = {2642-5254},
	keywords = {Training;Cybersickness;Wheelchairs;Urban planning;Virtual environments;User interfaces;Tools},
	month = {March},
	pages = {420-427},
	title = {VR based Power Wheelchair Simulator: Usability Evaluation through a Clinically Validated Task with Regular Users},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00065}}

@inproceedings{9417687,
	abstract = {Most research on providing location-based content in 3D interactive virtual reality has been limited to social media content. Few studies have suggested how to represent the video clip of movies or TV shows in virtual reality. This paper investigates a video content representation method to provide a hyper-reality experience of the narrative world in virtual reality. We reflect the time and place settings of the video content in virtual reality and have participants watch the video in four different virtual reality environments. We reveal that reflecting the story's environment settings to the virtual reality environment significantly improves the spatial presence and narratives engagement. We also confirm a positive correlation between spatial presence and narrative engagement, including subscales such as emotional engagement and narrative presence. Based on the study results, we discuss how to provide the hyper-reality experience in content-adaptive virtual reality.},
	author = {Park, Hyerim and Woo, Woontack},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00064},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Correlation;TV;Social networking (online);Virtual reality;User interfaces;Motion pictures;Cinematic virtual reality;hyper-reality;video representation;narrative engagement;spatial presence: Human-centered computing-Human Computer Interaction (HCI)-Empirical studies in HCI-;Human-centered computing-Interaction design-Empirical studies in interaction design},
	month = {March},
	pages = {411-419},
	title = {Video Content Representation to Support the Hyper-reality Experience in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00064}}

@inproceedings{9417689,
	abstract = {This paper examines the impact of implicit and explicit feedback in Virtual Reality (VR) on performance and user experience during motor rehabilitation. In this work, explicit feedback consists of visual and auditory cues provided by a virtual trainer, compared to traditional feedback provided by a real physiotherapist. Implicit feedback was generated by the walking motion of the virtual trainer accompanying the patient during virtual walks. Here, the potential synchrony of movements between the trainer and trainee is intended to create an implicit visual affordance of motion adaption. We hypothesize that this will stimulate the activation of mirror neurons, thus fostering neuroadaptive processes. We conducted a clinical user study in a rehabilitation center employing a gait robot. We investigated the performance outcome and subjective experience of four resulting VR-supported rehabilitation conditions: with/without explicit feedback, and with/without implicit (synchronous motion) stimulation by a virtual trainer. We further included two baseline conditions reflecting the current NonVR procedure in the rehabilitation center. Our results show that additional feedback generally resulted in better patient performance, objectively assessed by the necessary applied support force of the robot. Additionally, our VR-supported rehabilitation procedure improved enjoyment and satisfaction, while no negative impacts could be observed. Implicit feedback and adapted motion synchrony by the virtual trainer led to higher mental demand, giving rise to hopes of increased neural activity and neuroadaptive stimulation.},
	author = {Hamzeheinejad, Negin and Roth, Daniel and Monty, Samantha and Breuer, Julian and Rodenberg, Anuschka and Latoschik, Marc Erich},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00061},
	issn = {2642-5254},
	keywords = {Legged locomotion;Visualization;Three-dimensional displays;Neurons;Virtual reality;User interfaces;Synchronous motors;Human-centered computing-Visualization-Virtual reality},
	month = {March},
	pages = {382-391},
	title = {The Impact of Implicit and Explicit Feedback on Performance and Experience during VR-Supported Motor Rehabilitation},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00061}}

@inproceedings{9417691,
	abstract = {In this paper, we present a class of mixed-reality displays which allow for the 3D exploration of content in public exhibitions. The shared experience of the exhibition and the preservation of artworks are two very important aspects of these contexts, in particular for museum exhibits. The use of display cases as a protection tool is substantially accepted. It decreases the risks of damages to artworks and cultural materials hosted in museums. In addition, the transparent panels create a reflection of the visitors inside the display case. This reflection can be used to augment and interact in 3D with the exhibited content, by coupling Spatial Augmented-Reality and Optical Combiners. We call such a combination a Revealable Volume Display (RVD). It allows visitors to reveal information placed freely inside or around protected artefacts, visible by all, using their reflection in the panel. However, it may also suffer from unfamiliar gestures and disrupted depth perception cues, making 3D exploration of content difficult. In this paper, we first discuss the implementation of RVDs, providing both projector-based and mobile versions. We then present a design space that describes the interaction possibilities that it offers. Drawing on insights from a field study during a first exhibition, we finally propose and evaluate techniques for facilitating 3D exploration with RVDs.},
	author = {Guefrech, Fatma Ben and Berthaut, Florent and Pl{\'e}nacoste, Patricia and Peter, Yvan and Grisoni, Laurent},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00023},
	issn = {2642-5254},
	keywords = {Couplings;Three-dimensional displays;Input devices;User interfaces;Tools;Reflection;Optical coupling;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Graphics input devices Human-centered computing-Human computer interaction (HCI)-Interaction devices-Displays and imagers Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality},
	month = {March},
	pages = {31-39},
	title = {Revealable Volume Displays: 3D Exploration of Mixed-Reality Public Exhibitions},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00023}}

@inproceedings{9417692,
	abstract = {Embodiment and body perception have become important research topics in the field of virtual reality (VR). VR is considered a particularly promising tool to support research and therapy in regard to distorted body weight perception. However, the influence of embodiment on body weight perception has yet to be clarified. To address this gap, we compared body weight perception of 56 female participants of normal weight using a VR application. They either (a) self-embodied a photorealistic, non-personalized virtual human and performed body movements in front of a virtual mirror or (b) only observed the virtual human as other's avatar (or agent) performing the same movements in front of them. Afterward, participants had to estimate the virtual human's body weight. Additionally, we considered the influence of the participants' body mass index (BMI) on the estimations and captured the participants' feelings of presence and embodiment. Participants estimated the body weight of the virtual human as their embodied self-avatars significantly lower compared to participants rating the virtual human as other's avatar. Furthermore, the estimations of body weight were significantly predicted by the participant's BMI with embodiment, but not without. Our results clearly highlight embodiment as an important factor influencing the perception of virtual humans' body weights in VR.},
	author = {Wolf, Erik and Merdan, Nathalie and D{\"o}linger, Nina and Mal, David and Wienrich, Carolin and Botsch, Mario and Latoschik, Marc Erich},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00027},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Avatars;Estimation;Medical treatment;User interfaces;Tools;Mirrors;Virtual human;presence;virtual body ownership;agency;body image;eating- and body weight disorders.: Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI;Human-centered computing-Human computer interaction (HCI)- Virtual reality},
	month = {March},
	pages = {65-74},
	title = {The Embodiment of Photorealistic Avatars Influences Female Body Weight Perception in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00027}}

@inproceedings{9417694,
	abstract = {Sound duration and location may influence both auditory and visual perception with consequences for the judgement of both auditory-visual event location and integration. This study investigated audio-visual integration in a virtual environment using both short- and long-duration auditory stimuli with visual stimuli temporally offset from the start of the auditory stimulus, to investigate the effects of top-down neural effects on perception. Two tasks were used, an auditory localization task and a detection task (judgement of audio-visual synchrony). Eleven participants took part in the study using a HTC Vive Pro. The short-duration auditory stimuli (35-ms spatialized sound) and long-duration auditory stimuli (600-ms non-spatialized sound followed by 35 ms of spatialized sound) were presented at -60$\,^{\circ}$, -30$\,^{\circ}$, 0$\,^{\circ}$, +30$\,^{\circ}$ and +60$\,^{\circ}$ degrees azimuth, with the visual stimulus presented synchronously or asynchronously with respect to the start of the auditory stimulus. Results showed that localization errors were larger for the longer-duration stimuli and judgements of audiovisual synchrony tended to be improved for stimuli presented at $\pm$30$\,^{\circ}$. Top-down neural processing can affect spatial localization and audio-visual processing. Auditory localization errors and audio-visual synchrony detection may reveal the effects of underlying neural feedback mechanisms that can be harnessed to optimize audio-visual experiences in virtual environments.},
	author = {Liu, Jiacheng and Drga, Vit and Yasin, Ifat},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00099},
	issn = {2642-5254},
	keywords = {Location awareness;Visualization;Solid modeling;Three-dimensional displays;Azimuth;Virtual environments;Psychology;Auditory localization;Visual;Virtual;Temporal.: H.1.2 [Models and Principles]: User/Machine Systems-Software psychology;Human factors;H.5.1 [Information and Interfaces and presentation]: Multimedia Information Systems-Artificial;augmented;virtual realities},
	month = {March},
	pages = {723-728},
	title = {Optimal Time Window for the Integration of Spatial Audio-Visual Information in Virtual Environments},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00099}}

@inproceedings{9417696,
	abstract = {Haptic information significantly improves human awareness of objects in virtual reality. One way of presenting this information is via encountered-type haptic feedback. An advantage of encounter type feedback is that it enables physical interaction with virtual environments without the need for specialized haptic devices on the hand. Additionally, encountered-type haptics are known for being able to provide high quality contact feedback to the user. However, such systems are typically designed to be grounded (i.e., fixed to the floor). As such, they typically have bounded workspace and a limited range of possible applications. In this work, we present a novel, wearable approach to presenting a user with encountered-type haptic feedback. We realize this feedback using a wearable robotic limb that holds a plate where the user might interact with their environment. An appropriate location for the plate is determined by a novel haptic solver while control of the arm is made possible using motion trackers. The system was designed to be stable, for presenting consistent haptic feedback, while also being safe and lightweight for wearability. By making the feedback system wearable, we enable the presentation of stiff feedback while maintaining the spatial freedom and unbounded workspace of natural hand interaction. Herein, we present the design of the novel system, mechanical and safety considerations when designing a wearable encountered-type system, and an evaluation of the system. A technical evaluation of the implemented system showed that the system provides a stiffness over 25 N/m and slant angle errors under 3$\,^{\circ}$. Three user studies show the limitations of haptic slant perception in humans and the quantitative and qualitative effectiveness of the current prototype system. We conclude the paper by discussing various potentialapplications and possible improvements that could be made to the system.},
	author = {Horie, Arata and Saraiji, MHD Yamen and Kashino, Zendai and Inami, Masahiko},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00048},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Tracking;Virtual environments;Tactile sensors;User interfaces;End effectors;Human-centered computing;Human computer interaction(HCI);Interaction devices;Haptic devices},
	month = {March},
	pages = {260-269},
	title = {EncounteredLimbs: A Room-scale Encountered-type Haptic Presentation using Wearable Robotic Arms},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00048}}

@inproceedings{9417697,
	abstract = {Mastering motor skills requires performing the task unconsciously with great speed and accuracy. This is acquired slowly through practice over time. Nonetheless, in domains such as surgery, the training of these skills in the field introduces safety, ethical and economic issues. In this context, immersive VR technologies offer the possibility to recreate real-world situations and allow the trainees to improve their skills in a safe and controlled environment. However, the design of such systems raises new research questions, such as how to represent the user in the virtual environment, and whether this representation can influence motor skills automaticity. In this work, we focus on how the user's hand representation can impact the training of tool-based motor skills in immersive VR. To investigate this question, we have created a VR simulator for training a tool-based pick and place task, and conducted a user study to evaluate how the user's hand visualization can influence participants' learning performance after a two-week training period. For that purpose, two groups of participants were trained in the VR simulator under one of the two experimental conditions: the presence and the absence of their virtual hands' representation, while a control group received no training. The results of the study show that training on the VR simulator improves the participants' motor task performance when compared with the control group. On the other hand, no difference was observed between the two training groups. This suggests that the user's hand visualization does not always impact tool-based motor tasks training in immersive VR simulators. Indeed, for short-term motor training there was no difference in performance between having a partial embodiment of the user's hands and only the tools representation.},
	author = {Ricca, Aylen and Chellali, Amine and Otrnane, Samir},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00031},
	issn = {2642-5254},
	keywords = {Training;Visualization;Solid modeling;Three-dimensional displays;Surgery;Prototypes;Virtual environments;Avatar;Motor-skills training;Immersive virtual reality},
	month = {March},
	pages = {103-112},
	title = {The influence of hand visualization in tool-based motor-skills training, a longitudinal study},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00031}}

@inproceedings{9417698,
	abstract = {Recent developments in computer graphics and hardware technology enable easy access to virtual reality headsets along with integrated eye trackers, leading to mass usage of such devices. The immersive experience provided by virtual reality and the possibility to control environmental factors in virtual setups may soon help to create realistic digital alternatives to conventional classrooms. The importance of such settings has become especially evident during the COVID-19 pandemic, forcing many schools and universities to provide the digital teaching. Researchers foresee that such transformations will continue in the future with virtual worlds becoming an integral part of education. Until now, however, students' behaviors in immersive virtual environments have not been investigated in depth. In this work, we study students' attention by exploiting object-of-interests using eye tracking in different classroom manipulations. More specifically, we varied sitting positions of students, visualization styles of virtual avatars, and hand-raising percentages of peer-learners. Our empirical evidence shows that such manipulations play an important role in students' attention towards virtual peer-learners, instructors, and lecture material. This research may contribute to understanding of how visual attention relates to social dynamics in the virtual classroom, including significant considerations for the design of virtual learning spaces.},
	author = {Bozkir, Efe and Stark, Philipp and Gao, Hong and Hasenbein, Lisa and Hahn, Jens-Uwe and Kasneci, Enkelejda and G{\"o}llner, Richard},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00085},
	issn = {2642-5254},
	keywords = {Headphones;Visualization;Three-dimensional displays;Pandemics;Avatars;Education;Immersive experience;Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Applied computing-Education-Interactive learning environments;Applied computing-Education-Computer -assisted instruction},
	month = {March},
	pages = {597-605},
	title = {Exploiting Object-of-Interest Information to Understand Attention in VR Classrooms},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00085}}

@inproceedings{9417699,
	abstract = {Headworn Augmented Reality (AR) and Virtual Reality (VR) displays are an exciting new medium for locative storytelling. Authors face challenges planning and testing the placement of story elements when the story is experienced in multiple locations or the environment is large or complex. We present Story CreatAR, the first locative AR/VR authoring tool that integrates spatial analysis techniques. Story CreatAR is designed to help authors think about, experiment with, and reflect upon spatial relationships between story elements, and between their story and the environment. We motivate and validate our design through developing different locative AR/VR stories with several authors.},
	author = {Singh, Abbey and Kaur, Ramanpreet and Haltner, Peter and Peachey, Matthew and Gonzalez-Franco, Mar and Malloch, Joseph and Reilly, Derek},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00098},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Design methodology;Games;Tools;User interfaces;Media;Planning;augmented reality;space syntax;storytelling;prox-emics;f-formations;authoring toolkit;head-mounted display: Human-centered computing-Human computer interaction (HCI)-Interactive systems and tools-User interface toolkits;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Interaction design-Interaction design process and methods-User centered design},
	month = {March},
	pages = {713-722},
	title = {Story CreatAR: a Toolkit for Spatially-Adaptive Augmented Reality Storytelling},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00098}}

@inproceedings{9417703,
	abstract = {Introducing olfactory display in the virtual reality (VR) system brings the immersive experience to new heights. However, it is intractable to simulate olfactory features (such as the intensity and the direction) with multiple levels. Visual stimuli have been proved to dominate human perception among multiple sensors in virtual environments. If visual stimuli can be used to guide the olfactory sense in VR, the design of the olfactory display can be simpler but still able to provide olfactory experience with more diversity. To understand the visual-olfactory effect on different olfactory characteristics, a portable olfactory display that can control the intensity and direction of odors was developed. An experimental study was conducted to investigate cross-modal human perception, i.e. how the visually virtual odor representation in VR influences human perception of real odor produced by the proposed olfactory display. The results showed that the perception of odor intensity and directionality can be modulated by visually virtual odor representation.},
	author = {Tsai, Shou-En and Tsai, Wan-Lun and Pan, Tse-Yu and Kuo, Chia-Ming and Hu, Min-Chun},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00050},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Olfactory;Virtual environments;Prototypes;Immersive experience;User interfaces;virtual reality;olfactory display: Human-centered computing-Virtual reality-},
	month = {March},
	pages = {279-285},
	title = {Does Virtual Odor Representation Influence the Perception of Olfactory Intensity and Directionality in VR?},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00050}}

@inproceedings{9417706,
	abstract = {Natural and intuitive interaction such as freehand grasping of virtual objects is still a significant challenge in VR due to the dexterous versatility of the human grasping actions. Currently, the design considerations for creating freehand grasping interactions in VR are drawn from the body of historical knowledge presented for real object grasping. While this may be suitable for some applications, recent work has shown that users grasp virtual objects differently than they grasp real objects, presenting an absence of knowledge on how users intuitively grasp virtual objects. To begin to address this, we present an elicitation study where participants (N =39) grasped 16 virtual objects categorised by shape in a mixed docking task exploring placement and rotation. We report on a Wizard of OZ methodology, extract Grasp Type and Grasp Dimension and present grasping patterns in VR. Our results are of value to be taken forward into a framework of recommendations for grasping interactions, as well as parameterizing grasp types for developing intuitive grasp models and thus begin to bridge the gap in understanding natural grasping patters for VR object interactions.},
	author = {Dalia Blaga, Andreea and Frutos-Pascual, Maite and Creed, Chris and Williams, Ian},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00102},
	issn = {2642-5254},
	keywords = {Measurement;Solid modeling;Three-dimensional displays;Shape;Grasping;Virtual reality;User interfaces;Hand Interaction;Grasping Virtual Objects;Virtual Reality Interaction},
	month = {March},
	pages = {749-758},
	title = {Freehand Grasping: An Analysis of Grasping for Docking Tasks in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00102}}

@inproceedings{9417709,
	abstract = {Four-dimensional (4D) films, which provide special physical effects to the audience with audiovisual stimuli, are gaining more popularity and acceptance. One of the most frequent 4D effects is the object-based motion effect, which refers to the vestibular stimulus generated by a motion chair to emphasize a moving object of interest, e.g., the flying iron man, displayed on the screen. In this paper, we present an algorithm for synthesizing convincing object-based motion effects automatically from a given object motion trajectory. While previous approaches use the 2D object position on the screen as input, our method takes the 3D position and orientation of the object in the camera space and computes its motion proxy that reflects both the object translation and rotation, as well as its size to the viewers' eyes. The proxy is determined based on the results of a perceptual experiment that presents an optimal additive rule of the translation and rotation information scaled by the object's visual size. The motion proxy is fed to a motion cueing algorithm (MCA) that computes the command using a washout filter or model predictive control. The most appropriate MCA for our purpose is selected from six candidates by a user study. We also consider the effects of visual perception by incorporating two types of motion field equations into the computation of the visually perceived velocity. The results of a user study indicate that our algorithm can generate compelling object-based motion effects that better enhance the 4D film viewing experience than the previous methods.},
	author = {Han, Sangyoon and Yun, Gyeore and Choi, Seungmoon},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00093},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Films;Virtual reality;User interfaces;Cameras;Prediction algorithms;Information systems-Information systems applications-Multimedia information systems-Multimedia content creation;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
	month = {March},
	pages = {670-678},
	title = {Camera Space Synthesis of Motion Effects Emphasizing a Moving Object in 4D films},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00093}}

@inproceedings{9417713,
	abstract = {This investigation compared the emotional impact caused by a crowd of affective virtual humans (VHs) that communicated in the users' native or foreign language. We evaluated the users' affective reactions to a crowd of VHs that exhibited distinct emotional expressions. A total of four emotions were presented, which were Positive, Negative, Neutral, and Mixed. The VHs performed verbal and non-behaviors accordingly. Under the Mixed condition, the VHs were divided into three groups equally and each group was uniquely assigned one of the three emotions (i.e., positive, negative, and neutral). Users collected ten items from a virtual reality market. To complete the tasks, they interacted using natural speech with the emotional VHs. Three language conditions were investigated: one condition in USA and another two conditions in Taiwan. The group of participants in USA interacted with the VHs in English; and the two groups of participants in Taiwan interacted with the VHs using a foreign (English) language and a native (Mandarin) language respectively. We discovered that the medium of communication or language familiarity had a strong influence on participants' emotional reactions. When participants interacted in a foreign language with VHs with a positive emotional disposition, we found their positive emotional reactions were subdued and negative reactions were elevated. However, this was not the case when participants interacted with VHs in their native language, as their emotional reactions were contingent on the emotional disposition of the VHs.},
	author = {Volonte, Matias and Wang, Chang-Chun and Ebrahimi, Elham and Hsu, Yu-Chun and Liu, Kuan-Yu and Wong, Sai-Keung and Babu, Sabarish V.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00040},
	issn = {2642-5254},
	keywords = {Learning systems;Visualization;Solid modeling;Three-dimensional displays;Natural languages;Virtual reality;Writing;Virtual Humans;Virtual Reality;Virtual Crowds;Emotional Contagion},
	month = {March},
	pages = {188-197},
	title = {Effects of Language Familiarity in Simulated Natural Dialogue with a Virtual Crowd of Digital Humans on Emotion Contagion in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00040}}

@inproceedings{9417714,
	abstract = {Illumination consistency has applications to modeling and rendering in virtual reality. In 3D reconstruction and Mixed Reality(MR) fusion, the appearance of a large-scale outdoor scene may change in response to lighting and seasons, for example. Since 3D reconstruction from scratch is costly, it is helpful to be able to update existing models with recently captured photographs. However, the illumination conditions of the captured photograph can be arbitrary, making it challenging to fit to the existing model. To tackle this problem, this paper proposes a novel approach that can precisely estimate the illumination of the input image. Our Deep Shadow Network (DSNet) collaboratively utilizes illumination-based data augmentation for sun position estimation, along with a dataset of illumination-based augmented renderings. Our run-time rendering and optimization strategy is also discussed. We show that accurate simulation of illumination can improve the performance of visual applications including place recognition and long-term localization. Experimental results validate the effectiveness of the proposed approach, and show its superiority over the state-of-the-art.},
	author = {Xiong, Yuan and Chen, Hongrui and Wang, Jingru and Zhu, Zhe and Zhou, Zhong},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00039},
	issn = {2642-5254},
	keywords = {Solid modeling;Visualization;Three-dimensional displays;Computational modeling;Lighting;Estimation;Virtual reality;Computing methodologies-Modeling and simulation-Model development and analysis-Modeling methodologies;Artificial intelligence-Computer vision-Computer vision problems-Reconstruction},
	month = {March},
	pages = {179-187},
	title = {DSNet: Deep Shadow Network for Illumination Estimation},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00039}}

@inproceedings{9417716,
	abstract = {Several virtual reality (VR) proxies have been developed that can emulate impact sensations by generating actual forces on the hand. Although these proxies contribute to increasing the reality of VR, they still have some limitations, such as high latency, high power consumption, and low frequency to provide impact sensations. To overcome these limitations, we first propose a method to provide an impact sensation without actual force generation by quickly changing the rotational inertia of a handheld proxy while users are swinging it. Then, we developed Unident, a handheld proxy capable of changing its rotational inertia by moving a weight along one axis at a high speed. Two experiments were conducted to evaluate the ability of Unident to provide users with impact sensations. In the first experiment, we demonstrate that Unident can physically provide an impact sensation applied to a handheld object by analyzing the pressure on the user's palm. The second experiment shows that Unident can provide an impact sensation with various magnitudes depending on the amount of rotational inertia to be changed. Finally, we present an application that can be enabled by Unident.},
	author = {Shimizu, Shuntaro and Hashimoto, Takeru and Yoshida, Shigeo and Matsumura, Reo and Narumi, Takuji and Kuzuoka, Hideaki},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00021},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Power demand;Force;Virtual reality;User interfaces;Kinetic theory;Human-centered computing-Human computer interaction-Interaction devices-Haptic devices},
	month = {March},
	pages = {11-20},
	title = {Unident: Providing Impact Sensations on Handheld Objects via High-Speed Change of the Rotational Inertia},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00021}}

@inproceedings{9417717,
	abstract = {Research is exploring novel ways of adding haptics to VR. One popular technique is haptic retargeting, where real and virtual hands are decoupled to enable the reuse of physical props. However, this technique requires the system to know the users' intended interaction target, or requires additional hardware for prediction. We explore software-based reach prediction as a means of facilitating responsive, unscripted retargeting. We trained a Long Short-Term Memory network on users' reach trajectories to predict intended targets. We achieved an accuracy of 81.1 % at approximately 65% of movement. This could enable haptic retargeting during the last 35% of movement. We discuss the implications for possible physical proxy locations.},
	author = {Clarence, Aldrich and Knibbe, Jarrod and Cordeil, Maxime and Wybrow, Michael},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00036},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Neural networks;Virtual reality;Predictive models;User interfaces;Hardware;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Computing methodologies-Machine learning-Machine learning approaches-Neural networks},
	month = {March},
	pages = {150-159},
	title = {Unscripted Retargeting: Reach Prediction for Haptic Retargeting in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00036}}

@inproceedings{9417718,
	abstract = {Head-Mounted Virtual reality (VR) systems provide full-immersive experiences to users and completely isolate them from the outside world, placing them in unsafe situations. Existing research proposed different alert-based solutions to address this. Our work builds on these studies on notification systems for VR environments from a different perspective. We focus on: (i) exploring alert systems to notify VR users about non-immersed bystanders' in socially related, non-critical interaction contexts; (ii) understanding how best to provide awareness of non-immersed bystanders while maintaining presence and immersion within the Virtual Environment(VE). To this end, we developed single and combined alert cues - leveraging proxemics, perception channels, and push/pull approaches and evaluated those via two user studies. Our findings indicate a strong preference towards maintaining immersion and combining audio and visual cues, push and pull notification techniques that evolve dynamically based on proximity.},
	author = {Medeiros, Daniel and Anjos, Rafael dos and Pantidi, Nadia and Huang, Kun and Sousa, Maur{\'\i}cio and Anslow, Craig and Jorge, Joaquim},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00022},
	issn = {2642-5254},
	keywords = {Wrist;Pervasive computing;Human computer interaction;Visualization;Three-dimensional displays;Virtual reality;Safety;Notifications;Virtual Reality;Human Computer Inter-action;Context Awareness;Reality Awareness},
	month = {March},
	pages = {21-30},
	title = {Promoting Reality Awareness in Virtual Reality through Proxemics},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00022}}

@inproceedings{9417722,
	abstract = {Augmented reality (AR) provides enormous potential to improve navigation assistance interfaces by displaying information directly into the user's field of view. In maritime contexts, such AR interfaces could supplement conventional solutions that require users to interpret spatial positions visualized on two-dimensional maps. In order to design useful navigation assistance, it is crucial to understand how egocentric distances of displayed objects are perceived and how different design attributes influence depth estimation. While previous research mainly focused on depth perception in indoor environments and rather short distances, this paper presents an investigation of the perceived egocentric distance of virtual objects in distances up to 75 meters in an open outdoor environment. In a perceptual matching task experiment using the Microsoft HoloLens 2, participants had to move objects with different (i) shape, (ii) coloration, and (iii) relation to floor to various target distances. Our results suggest that participants overestimated the distance to virtual objects across all tested distances since they significantly underproduced target distances for all investigated design factors. Based on these results, we explored potential design implications for a maritime AR navigation assistance on a ship in the local port area.},
	author = {Hertel, Julia and Steinicke, Frank},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00033},
	issn = {2642-5254},
	keywords = {Performance evaluation;Visualization;Navigation;Shape;Seaports;Rendering (computer graphics);Indoor environment;Human-centered computing-Visualization-Empirical studies in visualization-;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality},
	month = {March},
	pages = {122-130},
	title = {Augmented Reality for Maritime Navigation Assistance - Egocentric Depth Perception in Large Distance Outdoor Environments},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00033}}

@inproceedings{9417723,
	abstract = {Many interaction techniques in virtual reality break with the 1-to-1 mapping from real to virtual space. Instead, specialized techniques for 3D interaction and haptic retargeting leverage hand redirection, offsetting the virtual hand rendering from the real hand position. To achieve unnoticeable hand redirection, however, the utilization of change blindness phenomena has not been systematically explored. Inspired by recent advances in the domain of redirected walking, we present the first hand redirection technique that makes use of blink-induced visual suppression and corresponding change blindness. We introduce Blink-Suppressed Hand Redirection (BSHR) to study the feasibility and detectability of hand redirection based on blink suppression. Our technique is based on Cheng et al.'s (2017) [9] body warping algorithm and instantaneously shifts the virtual hand when the user's vision is suppressed during a blink. Additionally, it can be configured to continuously increment hand offsets when the user's eyes are opened, limited to an extent below detection thresholds. In a psychophysical experiment, we verify that unnoticeable blink-suppressed hand redirection is possible even in worst -case scenarios, and derive the corresponding conservative detection thresholds (CDTs). Moreover, our results show that the range of unnoticeable redirection can be increased by combining continuous warping and blink-suppressed instantaneous shifts. As an additional contribution, we derive the CDTs for Cheng et al.'s (2017) [9] redirection technique that does not leverage blinks.},
	author = {Zenner, Andr{\'e} and Regitz, Kora Persephone and Kr{\"u}ger, Antonio},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00028},
	issn = {2642-5254},
	keywords = {Legged locomotion;Visualization;Three-dimensional displays;Blindness;Virtual reality;User interfaces;Rendering (computer graphics);Virtual reality;hand redirection;haptic retargeting;blink suppression;detection thresholds},
	month = {March},
	pages = {75-84},
	title = {Blink-Suppressed Hand Redirection},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00028}}

@inproceedings{9417724,
	abstract = {Most virtual reality (VR) experiences are held in limited physical space; therefore, increasing the physical space's spatial efficiency is an essential task for the VR industry. Redirected walking maps a virtual path and a real path with unnoticeable distortion, enabling users to walk through a much bigger virtual space than physical space. To hide the distortion from the user, detection thresholds have been measured, entirely focusing on forward steps. However, it is not uncommon for the user to walk non-forward, that is, sideward and backward in VR. In addition to a forward step, adding options for a non-forward step can expand the VR locomotion in any direction. In this work, we measure the translation and curvature detection thresholds for non-forward steps. The results show similar translation detection thresholds with forward-step and wider detection thresholds for the curvature gain in both backward and sideward step experiments. Having sideward and backward steps in the redirected walking arsenal can add freedom to virtual world design and lead to efficient space usage.},
	author = {Cho, Yong-Hun and Min, Dae-Hong and Huh, Jin-Suk and Lee, Se-Hee and Yoon, June-Seop and Lee, In-Kwon},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00068},
	issn = {2642-5254},
	keywords = {Legged locomotion;Industries;Three-dimensional displays;Focusing;Estimation;Virtual reality;User interfaces;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Perception;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
	month = {March},
	pages = {448-454},
	title = {Walking Outside the Box: Estimation of Detection Thresholds for Non-Forward Steps},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00068}}

@inproceedings{9417729,
	abstract = {In this work, we explore if the immersion afforded by Virtual Reality can improve the cognitive integration of information in Cinematic Virtual Reality (CVR). We conducted a user study examining participants' cognitive activities (recall performance and cortical response) when consuming visual information of emotional and emotionally neutral scenes in a non-CVR environment (i.e. a monitor) versus a CVR environment (i.e. a head-mounted display). Cortical response was recorded using electroencephalography. The results showed that participants had greater early visual attention with neutral emotions in a CVR environment, and showed higher overall alpha power in a CVR environment. The use of CVR did not significantly affect participants' recall performance.},
	author = {Cao, Ruochen and Zou-Williams, Lena and Cunningham, Andrew and Walsh, James and Kohler, Mark and Thornas, Bruce H.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00100},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Head-mounted displays;Virtual reality;User interfaces;Electroencephalography;Monitoring;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Visualization},
	month = {March},
	pages = {729-738},
	title = {Comparing the Neuro-Physiological Effects of Cinematic Virtual Reality with 2D Monitors},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00100}}

@inproceedings{9417734,
	abstract = {We present a novel hybrid sound propagation algorithm for interactive applications. Our approach is designed for dynamic scenes and uses a neural network-based learned scattered field representation along with ray tracing to generate specular, diffuse, diffraction, and occlusion effects efficiently. We use geometric deep learning to approximate the acoustic scattering field using spherical harmonics. We use a large 3D dataset for training, and compare its accuracy with the ground truth generated using an accurate wave-based solver. The additional overhead of computing the learned scattered field at runtime is small and we demonstrate its interactive performance by generating plausible sound effects in dynamic scenes with diffraction and occlusion effects. We demonstrate the perceptual benefits of our approach based on an audio-visual user study.},
	author = {Tang, Zhenyu and Meng, Hsien-Yu and Manocha, Dinesh},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00111},
	issn = {2642-5254},
	keywords = {Training;Three-dimensional displays;Runtime;Diffraction;Heuristic algorithms;Acoustic scattering;Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
	month = {March},
	pages = {835-844},
	title = {Learning Acoustic Scattering Fields for Dynamic Interactive Sound Propagation},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00111}}

@inproceedings{9417736,
	abstract = {The process of sensemaking involves foraging through and extracting information from large sets of documents, and it can be a cognitively intensive task. A recent approach, the Immersive Space to Think (IST), allows analysts to browse, read, mark up documents, and use immersive 3D space to organize and label collections of documents. In this study, we observed seventeen novice analysts perform a historical analysis task in order to understand how users utilize the features of IST to extract meaning from large text-based datasets. We found three different layout strategies they employed to create meaning with the documents we provided. We further found patterns of interaction and organization that can inform future improvements to the IST approach.},
	author = {Lisle, Lee and Davidson, Kylie and Gitre, Edward J.K. and North, Chris and Bowman, Doug A.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00077},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Layout;Virtual reality;Organizations;User interfaces;Feature extraction;Human-centered computing- Visualization- Visualization techniques-;Human-centered computing-Human Computer Interaction (HCI)-Interaction Paradigms-Virtual Reality},
	month = {March},
	pages = {529-537},
	title = {Sensemaking Strategies with Immersive Space to Think},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00077}}

@inproceedings{9417768,
	abstract = {Nowadays, Virtual Reality (VR) technology can be potentially used everywhere through wearable head-mounted displays. Nevertheless, it is still uncommon to see VR devices used in public settings. In these contexts, unaware bystanders in the surroundings might influence the User Experience (UX) and create concerns about the social acceptability of this technology. The user acts in a Social Environment (SE), characterized by surrounding people's number, proximity, and behavior. Simultaneously, VR applications often require a different degree of interactivity concerning body movements and controllers interaction. In this paper, the influence of Social Environments, and degree of interactivity on User Experience and social acceptability is investigated. Four Social Environments were simulated employing 360$\,^{\circ}$ Videos, and two VR games developed with two levels of interactivity. Results showed a statistically significant influence of Social Environments on Overall UX as well as Public VR, Interaction, Isolation, Privacy and Safety acceptability, and of the degree of interactivity on Presence, Valence, Arousal, Overall UX, UX Hedonic quality, and Safety acceptability. Findings indicate that Social Environments and degree of interactivity should be taken into account while designing VR applications.},
	author = {Vergari, Maurizio and Koji{\'c}, Tanja and Vona, Francesco and Garzotto, Franca and M{\"o}ller, Sebastian and Voigt-Antons, Jan-Niklas},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00096},
	issn = {2642-5254},
	keywords = {Privacy;Three-dimensional displays;Virtual reality;Games;User interfaces;Particle measurements;User experience;Virtual Reality;Social Acceptability;User Experience;Social Environments;Interactivity;360$\,^{\circ}$ Videos},
	month = {March},
	pages = {695-704},
	title = {Influence of Interactivity and Social Environments on User Experience and Social Acceptability in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00096}}

@inproceedings{9417770,
	abstract = {Movement is one of the key elements in virtual reality (VR) and significantly influences user experience. In particular, walking-in-place is a method of supporting movement in a limited space, and many studies are being conducted on its effective support. However, most studies have focused on forward movement despite many situations in which backward movement is needed. In this paper, we present the development of a prediction model for forward/backward movement while considering a user's orientation and the verification of the model's effectiveness. We built a deep learning-based model by collecting sensor data on the movement of the user's head, waist, and feet. We developed three realistic VR scenarios that involve backward movement, set three conditions (controller-based, treadmill-based, and model-based) for movement, and evaluated user experience in each condition through a study of 36 participants. As a result, the model-based condition showed the highest sensory sensitivity, effectiveness, and satisfaction and similar cognitive burden compared with the other two conditions. The results of our study demonstrated that movement support through modeling is possible, suggesting its potential for use in many VR applications.},
	author = {Paik, Seungwon and Jeon, Youngseung and Shih, Patrick C. and Han, Kyungsik},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00072},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Sensitivity;Navigation;Virtual reality;Predictive models;User interfaces;Human-centered computing-Human computer interaction (HCI)-;Human-centered computing-Virtual reality-;Computing methodologies-Machine learning approaches-},
	month = {March},
	pages = {483-492},
	title = {I Feel More Engaged When I Move!: Deep Learning-based Backward Movement Detection and its Application},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00072}}

@inproceedings{9417771,
	abstract = {We envision a convenient telepresence system available to users anywhere, anytime. Such a system requires displays and sensors embedded in commonly worn items such as eyeglasses, wristwatches, and shoes. To that end, we present a standalone real-time system for the dynamic 3D capture of a person, relying only on cameras embedded into a head-worn device, and on Inertial Measurement Units (IMUs) worn on the wrists and ankles. Our prototype system egocentrically reconstructs the wearer's motion via learning-based pose estimation, which fuses inputs from visual and inertial sensors that complement each other, overcoming challenges such as inconsistent limb visibility in head-worn views, as well as pose ambiguity from sparse IMUs. The estimated pose is continuously re-targeted to a prescanned surface model, resulting in a high-fidelity 3D reconstruction. We demonstrate our system by reconstructing various human body movements and show that our visual-inertial learning-based method, which runs in real time, outperforms both visual-only and inertial-only approaches. We captured an egocentric visual-inertial 3D human pose dataset publicly available at https://sites.google.com/site/youngwooncha/egovip for training and evaluating similar methods.},
	author = {Cha, Young-Woon and Shaik, Husam and Zhang, Qian and Feng, Fan and State, Andrei and Ilie, Adrian and Fuchs, Henry},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00087},
	issn = {2642-5254},
	keywords = {Wrist;Three-dimensional displays;Telepresence;Inertial sensors;Pose estimation;Prototypes;Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Computing methodologies-Computer graphics-Animation-Motion capture;Computing methodologies-Artificial intelligence-Computer vision-Reconstruction;Computing methodologies-Machine learning-Machine learning approaches-Neural networks},
	month = {March},
	pages = {616-625},
	title = {Mobile. Egocentric Human Body Motion Reconstruction Using Only Eyeglasses-mounted Cameras and a Few Body-worn Inertial Sensors},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00087}}

@inproceedings{9417772,
	abstract = {Immersive virtual environments (VEs) are most useful for training and education when viewers perceive and act accurately within them. Judgments of action capabilities within a VE provide a good measure of perceptual fidelity - the notion of how closely perception and action in the VE match that in the real world - and can also assess how perception for action may be calibrated with visual feedback based on one's own actions. In the current study we tested judgments of action capabilities within a VE for two different reaching behaviors: reaching out and reaching up. Our goal was to assess whether feedback from actual reaching improves judgments and if any recalibration due to feedback differed across reaching behaviors. We first measured participants' actual reaching out and reaching up capabilities so that feedback trials could be scaled to their actual abilities. Participants then completed blocks of alternating perceptual adjustment and feedback trials. In adjustment trials, they adjusted a virtual target to a distance perceived to be just reachable. In feedback trials, they viewed targets that were farther or closer than their actual reach, decided whether the target was reachable, and then reached out to the target to receive visual feedback from a hand-held controller. The first feedback block manipulated the target distance to be 30% over or under actual reach and subsequent blocks decreased the deviation to 20%,10% and 5% of actual reach. We found that for both reaching behaviors, reach was initially overestimated, and then perceptual estimations decreased to become more accurate over feedback blocks. Accuracy in the feedback trials themselves showed that targets just beyond reach were more difficult to judge correctly. This study establishes a straightforward methodology that can be used for calibration of actions in VEs and has implications for applications that depend on accurate reaching within VEs.},
	author = {Gagnon, Holly C. and Rohovit, Taren and Finney, Hunter and Zhao, Yu and Franchak, John M. and Stefanucci, Jeanine K. and Bodenheimer, Bobby and Creem-Regehr, Sarah H.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00107},
	issn = {2642-5254},
	keywords = {Training;Visualization;Three-dimensional displays;Current measurement;Virtual environments;Estimation;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Collaborative interaction},
	month = {March},
	pages = {798-806},
	title = {The Effect of Feedback on Estimates of Reaching Ability in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00107}}

@inproceedings{9417774,
	abstract = {Virtual reality (VR) applications such as interior design typically require accurate and efficient selection and movement of indoor objects. In this paper, we present an indoor object selection and movement approach by taking into account scene contexts such as object semantics and interrelations. This provides more intelligence and guidance to the interaction, and greatly enhances user experience. We evaluate our proposals by comparing them with traditional approaches in different interaction modes based on controller, head pose, and eye gaze. Extensive user studies on a variety of selection and movement tasks are conducted to validate the advantages of our approach. We demonstrate our findings via a furniture arrangement application.},
	author = {Wang, Miao and Ye, Zi-Ming and Shi, Jin-Chuan and Yang, Yang-Liang},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00045},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Semantics;Virtual reality;User interfaces;User experience;Proposals;Task analysis;Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Pointing},
	month = {March},
	pages = {235-244},
	title = {Scene-Context-Aware Indoor Object Selection and Movement in VR},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00045}}

@inproceedings{9417775,
	abstract = {In this paper, we provide an approach on using behavioral biometrics to perform cross-system high-assurance authentication of users in virtual reality (VR) environments. VR is currently being explored as a critical tool to ensure seamless delivery of essential services, such as education, healthcare, and personal finance, while enabling users to work from home environments. Due to the sensitive nature of personal data generated, VR applications for essential services need to provide secure access. Traditional PIN or password-based credentials can be breached by malicious impostors, or be handed over by an intended user of a VR system to a confederate to assist the intended user in completing a task, e.g., an exam or a physical therapy routine. Existing approaches that use the behavior of the user in VR as a biometric signature fail when users provide enrollment and use-time data on different VR systems. We use Siamese neural networks to learn a distance function that characterizes the systematic differences between data provided across pairs of dissimilar VR systems. Our approach provides average equal error rates (EERs) ranging from 1.38% to 3.86% for authentication using a benchmark dataset that consists of 41 users performing a ball-throwing task with 3 VR systems-an Oculus Quest, an HTC Vive, and an HTC Vive Cosmos. To compare to prior approaches in VR biometrics, we also obtain average accuracies for the task of identification, where given an input user's trajectory in a use-time VR system, we use Siamese networks to return the user with the top matching trajectory in an enrollment VR system as the label. We report identification results ranging from 87.82% to 98.53% with average improvements of 29.78%$\pm$8.58% and 30.78%$\pm$3.68% over existing approaches that use generic distance matching and fully convolutional networks on the enrollment dataset respectively.},
	author = {Miller, Robert and Banerjee, Natasha Kholgade and Banerjee, Sean},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00035},
	issn = {2642-5254},
	keywords = {Systematics;Biometrics (access control);Neural networks;Authentication;Virtual reality;User interfaces;Tools;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Security and privacy-Security services-Authentication-Biometrics},
	month = {March},
	pages = {140-149},
	title = {Using Siamese Neural Networks to Perform Cross-System Behavioral Authentication in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00035}}

@inproceedings{9417776,
	abstract = {Physical monitors require space, lack flexibility, and can become expensive and less portable in large setups. Virtual monitors, on the other hand, can minimize those problems, but may be subject to technological limitations such as lower resolution and field of view. We investigate the impacts of using virtual monitors displayed on a current state-of-the-art augmented reality headset for conducting productivity work. We conducted a user study that compared physical monitors, virtual monitors, and a hybrid combination of both in terms of performance, accuracy, comfort, focus, preference, and confidence. Results show that virtual monitors are a feasible approach for performing serious productivity work, albeit currently constrained by technical limitations that lead to inferior usability and performance compared to physical monitors. We also discovered that, with current technology, the hybrid condition was a better tradeoff between the familiarity and trustworthiness of physical monitors and the extra space provided by virtual monitors. We conclude by expressing the opportunity for designing strategies for mixing virtual and physical monitors into novel hybrid interfaces.},
	author = {Pavanatto, Leonardo and North, Chris and Bowman, Doug A. and Badea, Carmen and Stoakley, Richard},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00103},
	issn = {2642-5254},
	keywords = {Productivity;Headphones;Three-dimensional displays;User interfaces;Usability;Monitoring;Augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Empirical studies in interaction design},
	month = {March},
	pages = {759-767},
	title = {Do we still need physical monitors? An evaluation of the usability of AR virtual monitors for productivity work},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00103}}

@inproceedings{9417777,
	abstract = {Fitts' law and the associated throughput measure characterize user pointing performance in virtual reality (VR) training systems and simulators well. Yet, pointing performance can be affected by the feedback users receive from a VR application. This work examines the effect of the pitch of auditory error feedback on user performance in a Fitts' task through a distributed experiment. In our first study, we used middle- and high-frequency sound feedback and demonstrated that high-pitch error feedback significantly decreases user performance in terms of time and throughput. In the second study, we used adaptive sound feedback, where we increased the frequency with the error rate, while asking subjects to execute the task ``as fast/as precise/as fast and precise as possible''. Results showed that adaptive sound feedback decreases the error rate for ``as fast as possible'' task execution without affecting the time. The results can be used to enhance and design various VR systems.},
	author = {Batmaz, Anil Ufuk and Stuerzlinger, Wolfgang},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00029},
	issn = {2642-5254},
	keywords = {Training;Three-dimensional displays;Error analysis;Virtual reality;User interfaces;Throughput;Task analysis;Human-centered computing-Human Computer Interaction (HCI);Human-centered computing-Virtual Reality;Human-centered computing-Pointing},
	month = {March},
	pages = {85-94},
	title = {The Effect of Pitch in Auditory Error Feedback for Fitts' Tasks in Virtual Reality Training Systems},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00029}}

@inproceedings{9417778,
	abstract = {Motion capture is a well-established technology for capturing actors' movements and performances within the entertainment industry. Many actors, however, witness the poor acting conditions associated with such recordings. Instead of detailed sets, costumes and props, they are forced to play in empty spaces wearing tight suits. Often, their co-actors will be imaginary, replaced by placeholder props, or they would be out of scale with their virtual counterparts. These problems do not only affect acting, they also cause an abundance of laborious post-processing clean-up work. To solve these challenges, we propose using a combination of virtual reality and motion capture technology to bring differently proportioned virtual characters into a shared collaborative virtual environment. A within-subjects user study with trained actors showed that our proposed platform enhances their feelings of body ownership and immersion. This in turn changed actors' performances which narrowed the gap between virtual performances and final intended animations.},
	author = {Kammerlander, Robin K. and Pereira, Andr{\'e} and Alexanderson, Simon},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00063},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Interactive systems;Collaboration;Virtual environments;Production;User interfaces;Tools;Collaborative virtual production;acting;motion capture;body ownership;presence},
	month = {March},
	pages = {402-410},
	title = {Using Virtual Reality to Support Acting in Motion Capture with Differently Scaled Characters},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00063}}

@inproceedings{9417779,
	abstract = {Unlike traditional object stores, Augmented Reality (AR) query workloads possess several unique characteristics, such as spatial and visual information. Such workloads are often keyed on a variety of attributes simultaneously, such as device orientation and position, the scene in view, and spatial anchors. The natural mode of user-interaction in these devices triggers queries implicitly based on the field in the user's view at any instant, generating data queries in excess of the device frame rate. Ensuring a smooth user experience in such a scenario requires a systemic solution exploiting the unique characteristics of the AR workloads. For exploration in such contexts, we are presented with a view-maintenance or cache-prefetching problem; how do we download the smallest subset from the server to the mixed reality device such that latency and device space constraints are met? We present a novel data platform - DreamStore, that considers AR queries as first-class queries, and view-maintenance and large-scale analytics infrastructure around this design choice. Through performance experiments on large-scale and query-intensive AR workloads on DreamStore, we show the advantages and the capabilities of our proposed platform.},
	author = {Khan, Meraj and Nandi, Arnab},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00080},
	issn = {2642-5254},
	keywords = {Performance evaluation;Visualization;Three-dimensional displays;Mixed reality;User interfaces;User experience;Servers},
	month = {March},
	pages = {555-563},
	title = {DreamStore: A Data Platform for Enabling Shared Augmented Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00080}}

@inproceedings{9417780,
	abstract = {Gait supervision plays an important role in the diagnosis, analysis and rehabilitation of motor impairments and neurodegenerative disorders. For example, in Parkinson's disease, gait assessment is used for progression observation and medication guidance. Previous work has presented the potential of virtual reality (VR) supported gait applications. While virtual environments and user representation strategies are used for gait applications, the influence of appearance and context cues on gait performance is not extensively researched. In this paper, we analyzed the influence of avatar appearance, environment awareness, and camera perspective on gait parameters relevant for clinical application. Four different avatar appearances, varying in abstraction, two environmental settings, as well as an egocentric and exocentric camera perspective were compared in three walking tasks on a treadmill. Our results show that variability, as an indicator for gait stability, is significantly impacted by VR exposure in comparison to a real world (in vivo) baseline. Further, our results revealed that walking tasks influence gait behavior significantly different in VR compared to in vivo. Overall, these findings suggest that particular care has to be taken when assessing gait characteristics acquired from subjects immersed in VR and that equivalence of results with in vivo may not be blindly assumed.},
	author = {Wirth, Markus and Gradl, Stefan and Prosinger, Georg and Kluge, Felix and Roth, Daniel and Eskofier, Bjoern M.},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00055},
	issn = {2642-5254},
	keywords = {Legged locomotion;In vivo;Three-dimensional displays;Avatars;Virtual environments;User interfaces;Cameras;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
	month = {March},
	pages = {326-335},
	title = {The Impact of Avatar Appearance, Perspective and Context on Gait Variability and User Experience in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00055}}

@inproceedings{9417781,
	abstract = {We propose the concept of relative translation gains, a novel Redirected Walking (RDW) method to create a mutual movable space between the Augmented Reality (AR) host's reference space and the Virtual Reality (VR) client's space. Previous RDW methods have focused on maximizing the movable space at the expense of aligning the coordinates between the AR and VR side, and could only be applied to collaborative scenarios involving sequential tasks. Our method solves these problems by adjusting the remote client's walking speed for each axis of a VR space to modify the movable area without coordinate distortion. We estimate the relative translation gain threshold, defined as the extent to which the walking speed can be altered without creating a perceived difference in distance. In order to reflect features of the reference space in generating the mutual space, we then examine how changing its size affects the threshold value. Our study showed that for remote clients connected to the larger reference space, relative translation gains can be increased to utilize a VR space bigger than their real space. Our method can be applied to create optimal mutual spaces for a wider variety of asymmetric Mixed Reality (MR) remote collaboration systems.},
	author = {Kim, Dooyoung and Shin, Jae-eun and Lee, Jeongmi and Woo, Woontack},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00091},
	issn = {2642-5254},
	keywords = {Legged locomotion;Three-dimensional displays;Mixed reality;Collaboration;User interfaces;Distortion;Task analysis;Virtual Reality;Mixed Reality;relative translation gains;redirected walking;mutual space;visual cognition;threshold},
	month = {March},
	pages = {653-660},
	title = {Adjusting Relative Translation Gains According to Space Size in Redirected Walking for Mixed Reality Mutual Space Generation},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00091}}

@inproceedings{9417782,
	abstract = {Advancing technology and higher availability of Virtual Reality (VR) devices sparked its application in various research fields. For instance, health-related research showed that simulated nature environments in VR could reduce arousal and increase valence levels. This study investigates how the amount of possible interactivity influences the presence in nature environments and consequences on arousal and valence. After inducing fear (high arousal and low valence) through a VR-horror game, it was tested how participants recovered if they played a VR-nature game with either no, limited, or extensive interaction. The horror game proved to be a valid stimulus for inducing high arousal and low valence with a successful manipulation check. Igroup presence questionnaire (IPQ) scores showed that more interaction with the virtual environment increases spatial presence. A beneficial effect of experiencing nature can also be concluded. Results from the Self-Assessment Manikin questionnaire (SAM) scores for valence indicate a significant increase in the conditions with extensive and limited interaction compared to the control group. The VR Nature experience did significantly decrease arousal and increase valence compared to the post-horror game ratings. The physiological responses support this finding. These results can increase the effectiveness of health-related VR-applications to elevate mood levels by either implementing plenty of interactions and consequently increasing spatial presence or doing the opposite and leaving out any interactions at all.},
	author = {Voigt-Antons, Jan-Niklas and Spang, Robert and Koji{\'c}, Tanja and Meier, Luis and Vergari, Maurizio and M{\"o}ller, Sebastian},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00094},
	issn = {2642-5254},
	keywords = {Heart rate;Solid modeling;Three-dimensional displays;Mood;Computational modeling;Virtual environments;Games;Virtual Reality;green environment;inducing fear;VR relaxation;emotional response;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-HCI theory;concepts and models;Human-centered computing-Interaction design},
	month = {March},
	pages = {679-686},
	title = {Don't Worry be Happy - Using virtual environments to induce emotional states measured by subjective scales and heart rate parameters},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00094}}

@inproceedings{9417783,
	abstract = {We present a novel end-to-end plane detection and description network named SuperPlane to detect and match planes in two RGB images. SuperPlane takes a single image as input and extracts 3D planes and generates corresponding descriptors simultaneously. A mask-attention module and an instance-triplet loss are proposed to improve the distinctiveness of the plane descriptor. For image matching, we also propose an area-aware Kullback-Leibler (KL) divergence retrieval method. Extensive experiments show that the proposed method outperforms state-of-the-art methods and retains good generalization capacity. The applications in image-based localization and augmented reality also demonstrate the effectiveness of SuperPlane.},
	author = {Ye, Weicai and Li, Hai and Zhang, Tianxiang and Zhou, Xiaowei and Bao, Hujun and Zhang, Guofeng},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00042},
	issn = {2642-5254},
	keywords = {Location awareness;Three-dimensional displays;Image matching;User interfaces;Augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality;Computing methodologies-Artificial intelligence-Computer vision-Computer vision problems},
	month = {March},
	pages = {207-215},
	title = {SuperPlane: 3D Plane Detection and Description from a Single Image},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00042}}

@inproceedings{9417784,
	abstract = {Virtual Reality (VR) technologies are widely employed to investigate human behavior in dangerous situations that cannot be safely reproduced in the real world, allowing researchers to study in an ecological way complex scenarios such as training for risky jobs, safety procedures, emergencies and, more recently, moral dilemmas in driving context. Understanding how people act when facing severe accidents involving unavoidable collisions has extremely important implications for the design and development of the ``decisional system'' of Autonomous Vehicles (AV s). However, previous studies have not focused on the differences between being the driver acting in a complex moral situation or being in a self-driving car that chooses for you. In the present paper, we described a case study that uses a first-person virtual reality simulation to investigate people's emotional reactions, perceived sense of responsibility, and acceptability of moral behavior in human and autonomous driving modalities. The main findings showed that participants experienced a high sense of presence in our simulation and react differently to the two driving conditions, showing a greater arousal, a more negative valence, and an increased sense of responsibility when faced moral dilemmas as drivers. Instead, in scenarios that did not involve killing someone (non-moral dilemmas), being in a fully autonomous vehicle was judged less pleasant than being the actual driver. These results suggest that people prefer to be in control only in common driving situations and not when their actions have deadly consequences on other people, suggesting the need to consider emotional factors in studying decision-making applied to autonomous vehicles, as a mean to reach a more complete understanding of people's reactions to this new technology, and to possibly gain insights for the design of autonomous driving systems and, more generally, AI-driven machines.},
	author = {Benvegn{\`u}, Giulia and Pluchino, Patrik and Garnberini, Luciano},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00054},
	issn = {2642-5254},
	keywords = {Training;Ethics;Solid modeling;Three-dimensional displays;Biological system modeling;Virtual reality;Safety;Virtual reality;Moral dilemma;Autonomous vehicles;Emotion;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI) -HCI design and evaluation methods-Laboratory experiments},
	month = {March},
	pages = {316-325},
	title = {Virtual Morality: Using Virtual Reality to Study Moral Behavior in Extreme Accident Situations},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00054}}

@inproceedings{9417785,
	abstract = {Many studies have explored how individual differences can affect users' susceptibility to cybersickness in a VR application. However, the lack of strategy to integrate the influence of each factor on cybersickness makes it difficult to utilize the results of existing research. Based on the fuzzy logic theory that can represent the effect of different factors as a single value containing integrated information, we developed two approaches including the knowledge-based Mamdani-type fuzzy inference system and the data-driven Adaptive neuro-fuzzy inference system (ANFIS) to involve three individual differences (Age, Gaming experience and Ethnicity). We correlated the corresponding outputs with the simulator sickness questionnaire (SSQ) scores in a simple navigation scenario. The correlation coefficients obtained through a 4- fold cross validation were found statistically significant with both fuzzy logic approaches, indicating their effectiveness to influence the occurrence and the level of cybersickness. Our work provides insights to establish customized experiences for VR navigation by involving individual differences.},
	author = {Wang, Yuyang and Chardonnet, Jean-R{\'e}my and Merienne, Fr{\'e}d{\'e}ric and Ovtcharova, Jivka},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00060},
	issn = {2642-5254},
	keywords = {Fuzzy logic;Correlation coefficient;Visualization;Solid modeling;Three-dimensional displays;Cybersickness;Navigation;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Computing methodologies-Artificial intelligence-Knowledge representation and reasoning-Vagueness and fuzzy logic;Information systems-Information retrieval-Users and interactive retrieval-Personalization},
	month = {March},
	pages = {373-381},
	title = {Using Fuzzy Logic to Involve Individual Differences for Predicting Cybersickness during VR Navigation},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00060}}

@inproceedings{9417786,
	abstract = {Information visualization techniques play an important role in Virtual Reality (VR) because they improve task performance, support cognitive processes, and eventually increase the feeling of immersion. Deaf and Hard-of-Hearing (DHH) persons have special needs for information presentation because they feel and perceive VR environments differently. Therefore, it is necessary to pay attention to requirements about presenting information in VR for this group of users. Previous research showed that adding special features and using haptic methods helps DHH persons to do VR tasks better. In this paper, we propose a novel Omni-directional particle visualization method and also evaluate multi-modal presentation methods in VR for DHH persons, such as audio, visual, haptic, and a combination of them (AVH). Additionally, we compare the results with the results of persons without hearing problems. The methods for information presentation in our study focus on spatial object localization in VR. Our user studies show that both DHH persons and persons without hearing problems were able to do VR tasks significantly faster using AVH. Also, we found out that DHH persons can do visual-related VR tasks faster than persons without hearing problems by using our new proposed visualization method. Our results suggest that the benefits of using audio among persons without hearing problems and the benefits of using vision among DHH persons cause an interesting balance in the results of AVH between both groups. Finally, our qualitative and quantitative evaluation indicates that both groups of participants preferred and enjoyed AVH modality more than other modalities.},
	author = {Mirzaei, Mohammadreza and K{\'a}n, Peter and Kaufmann, Hannes},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00084},
	issn = {2642-5254},
	keywords = {Location awareness;Visualization;Three-dimensional displays;Cognitive processes;Auditory system;Virtual reality;User interfaces;Virtual Reality-Information Presentation-Visualization Techniques-Spatial Object Localization-Deaf and Hard-of- Hearing},
	month = {March},
	pages = {588-596},
	title = {Multi-modal Spatial Object Localization in Virtual Reality for Deaf and Hard-of-Hearing People},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00084}}

@inproceedings{9417787,
	abstract = {Researchers and developers have suggested that the use of diegetic interfaces can enhance users' sense of presence and immersion in virtual reality (VR) applications. While concepts of diegetic interfaces in VR are analogous to those seen on 2D displays, little work has considered how they might integrate with the movement-based controllers commonly used in consumer VR systems, to create higher fidelity interactions. In this paper we present a study (N = 58) in which we compare participants' experiences of diegetic and non-diegetic tool management interfaces, in a prototype VR crime scene investigation (CSI) training application. Contrary to expectations, we do not find evidence that participants' sense of presence is elevated when using the diegetic interface; however, we suggest that this may be due to reported higher levels of perceived workload, which can act to degrade user experience and engagement. We conclude by discussing the relationship between diegetic interface design and interaction fidelity, and highlighting trade-offs between fidelity, engagement, and learning outcomes in VR training applications.},
	author = {Dickinson, Patrick and Cardwell, Andrew and Parke, Adrian and Gerling, Kathrin and Murray, John},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00034},
	issn = {2642-5254},
	keywords = {Training;Solid modeling;Three-dimensional displays;Two dimensional displays;Prototypes;Virtual reality;Tools;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Interaction design-Interaction design process and methods-User interface design;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
	month = {March},
	pages = {131-139},
	title = {Diegetic Tool Management in a Virtual Reality Training Simulation},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00034}}

@inproceedings{9417788,
	abstract = {This paper presents a novel method for the visualization of 3D spatial sounds in Virtual Reality (VR) for Deaf and Hard-of-Hearing (DHH) people. Our method enhances traditional VR devices with additional haptic and visual feedback, which aids spatial sound localization. The proposed system automatically analyses 3D sound from VR application, and it indicates the direction of sound sources to a user by two Vibro-motors and two Light-Emitting Diodes (LEDs). The benefit of automatic sound analysis is that our method can be used in any VR application without modifying the application itself. We evaluated the proposed method for 3D spatial sound visualization in a user study. Additionally, the conducted user study investigated which condition (corresponding to different senses) leads to faster performance in 3D sound localization task. For this purpose, we compared three conditions: haptic feedback only, LED feedback only, combined haptic and LED feedback. Our study results suggest that DHH participants could complete sound-related VR tasks significantly faster using LED and haptic+LED conditions in comparison to only haptic feedback. The presented method for spatial sound visualization can be directly used to enhance VR applications for use by DHH persons, and the results of our user study can serve as guidelines for the future design of accessible VR systems.},
	author = {Mirzaei, Mohammadreza and K{\'a}n, Peter and Kaufmann, Hannes},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00083},
	issn = {2642-5254},
	keywords = {Location awareness;Visualization;Three-dimensional displays;Virtual reality;Auditory system;User interfaces;Light emitting diodes;Virtual Reality-Haptic-Vision-Sound Localization-Deaf;Hard-of-Hearing},
	month = {March},
	pages = {582-587},
	title = {Head Up Visualization of Spatial Sound Sources in Virtual Reality for Deaf and Hard-of-Hearing People},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00083}}

@inproceedings{9417789,
	abstract = {Body-centric locomotion allows users to navigate virtual environments with body parts (e.g. head tilt, arm swing or torso lean). Transfer functions are an important determinant of the locus of such a locomotion method. However, there is little known about the effects of transfer functions on virtual locomotion with different body parts. In this work, we selected four typical transfer functions (linear function: L, power function: P, a piecewise function with constant and linear functions: CL, and a piecewise function with constant and power functions: CP) and four body parts (head, arm, torso, and knee) from existing works, and conducted an experiment to evaluate their effects on virtual locomotion under three distances (5, 10, and 15 m) in Virtual Reality (VR). Results show that (1) CP function generally led to the longest task time with a low rate of failed trials, while CL function had the shortest task time with a high rate of failed trials; (2) body parts significantly affected the rate of failed trials, but not task time and final position offset. Head and torso resulted in the lowest and highest rate of failed trials respectively; (3) body parts did not differ in User Experience Questionnaire-Short (UEQ-S), UEQ-S Pragmatic and UEQ-S Hedonic. L was rated as the highest score for UEQ-S, UEQ-S Pragmatic and UEQ-S Hedonic, but CP had the lowest score. According to the results, we provide implications of designing body-centric locomotion with different transfer functions in VR.},
	author = {Gao, BoYu and Mai, Zijun and Tu, Huawei and Duh, Henry Been-Lirn},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00073},
	issn = {2642-5254},
	keywords = {Torso;Three-dimensional displays;Navigation;Transfer functions;Virtual environments;User interfaces;Market research;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
	month = {March},
	pages = {493-500},
	title = {Evaluation of Body-centric Locomotion with Different Transfer Functions in Virtual Reality},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00073}}

@inproceedings{9417790,
	abstract = {A new form of telexistence is achieved by recording videos with a camera on an Uncrewed aerial vehicle (UAV) and playing the videos to a user via a head-mounted display (HMD). One key problem here is how to let the user freely and naturally control the UAV and thus the viewpoint. In this paper, we develop an HMD-based telexistence technique that achieves full 6- DOF control of the viewpoint. The core of our technique is an improved rate-based control technique with our adaptive origin update (AOU), in which the origin of the coordinate system of the user changes adaptively. This makes the user naturally perceive the origin and thus easily perform the control motion to get his/her desired viewpoint changing. As a consequence, without the aid of any auxiliary equipment, the AOU scheme handles the well known self-centering problem in the rate-based control methods. A real prototype is also built to evaluate this feature of our technique. To explore the advantage of our telexistence technique, we further use it as an interactive tool to perform the task of 3D scene reconstruction. User studies demonstrate that comparing with other telexistence solutions and the widely used joystick-based solutions, our solution largely reduces the workload and saves time and moving distance for the user.},
	author = {Zhang, Di and Pun, Chi-Man and Yang, Yang and Gao, Hao and Xu, Feng},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00108},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Prototypes;Virtual reality;User interfaces;Tools;Telexistence},
	month = {March},
	pages = {807-816},
	title = {A Rate-based Drone Control with Adaptive Origin Update in Telexistence},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00108}}

@inproceedings{9417791,
	abstract = {We present Impossible Staircase, a real-walking virtual reality system that allows users to climb an infinite virtual tower. Our set-up consists of an one-level scaffold and a lifter. A user climbs up the scaffold by real walking on a stairway while wearing a head-mounted display, and gets reset to the ground level by a lifter imperceptibly. By repeating this process, the user perceives an illusion of climbing an infinite number of levels. Our system achieves the illusion by (1) controlling the movement of the lifter to generate reverse and imperceptible motion, (2) guiding the user through the scaffold with delay mechanisms to reset the lifter in time, and (3) procedural generating overlapping structures to enlarge perceived height of each level. We built a working system and demonstrated it with a 15-min experience. With the working system, we conducted user studies to gain deeper insights into vertical motion simulation and vertical real walking in virtual reality.},
	author = {Cheng, Jen-Hao and Chen, Yi and Chang, Ting-Yi and Lin, Hsu-En and Wang, Po-Yao Cosmos and Cheng, Lung-Pan},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00025},
	issn = {2642-5254},
	keywords = {Legged locomotion;Solid modeling;Three-dimensional displays;Head-mounted displays;Multimedia systems;Poles and towers;Virtual reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;Augmented;Virtual Realities},
	month = {March},
	pages = {50-56},
	title = {Impossible Staircase: Vertically Real Walking in an Infinite Virtual Tower},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00025}}

@inproceedings{9417792,
	abstract = {In real life, people communicate using both speech and non-verbal signals such as gestures, face expression or body pose. Non-verbal signals impact the meaning of the spoken utterance in an abundance of ways. An absence of non-verbal signals impoverishes the process of communication. Yet, when users are represented as avatars, it is difficult to translate non-verbal signals along with the speech into the virtual world without specialized motion-capture hardware. In this paper, we propose a novel, data-driven technique for generating gestures directly from speech. Our approach is based on the application of Generative Adversarial Neural Networks (GANs) to model the correlation rather than causation between speech and gestures. This approach approximates neuroscience findings on how non-verbal communication and speech are correlated. We create a large dataset which consists of speech and corresponding gestures in a 3D human pose format from which our model learns the speaker-specific correlation. We evaluate the proposed technique in a user study that is inspired by the Turing test. For the study, we animate the generated gestures on a virtual character. We find that users are not able to distinguish between the generated and the recorded gestures. Moreover, users are able to identify our synthesized gestures as related or not related to a given utterance.},
	author = {Rebol, Manuel and G{\"u}tl, Christian and Pietroszek, Krzysztof},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00082},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Correlation;Neuroscience;Pose estimation;Pipelines;Predictive models;User interfaces;Animation;Data models;Gesture Animation;GAN;3D Human Pose Estimation;Human Body Language;VR},
	month = {March},
	pages = {573-581},
	title = {Passing a Non-verbal Turing Test: Evaluating Gesture Animations Generated from Speech},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00082}}

@inproceedings{9417793,
	abstract = {Current Virtual Reality systems typically use cameras to capture user input from controllers or free-hand mid-air interaction. In this paper, we argue that this is a key impediment to productivity scenarios in VR, which require continued interaction over prolonged periods of time-a requirement that controller or free-hand input in mid-air does not satisfy. To address this challenge, we bring rapid touch interaction on surfaces to Virtual Reality-the input modality that users have grown used to on phones and tablets for continued use. We present TapID, a wrist-based inertial sensing system that complements headset-tracked hand poses to trigger input in VR. TapID embeds a pair of inertial sensors in a flexible strap, one at either side of the wrist; from the combination of registered signals, TapID reliably detects surface touch events and, more importantly, identifies the finger used for touch. We evaluated TapID in a series of user studies on event-detection accuracy (F1 = 0.997) and hand-agnostic finger-identification accuracy (within-user: F1 = 0.93; across users: F1 = 0.91 after 10 refinement taps and F1 = 0.87 without refinement) in a seated table scenario. We conclude with a series of applications that complement hand tracking with touch input and that are uniquely enabled by TapID, including UI control, rapid keyboard typing and piano playing, as well as surface gestures.},
	author = {Meier, Manuel and Streli, Paul and Fender, Andreas and Holz, Christian},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00076},
	issn = {2642-5254},
	keywords = {Wrist;Productivity;Three-dimensional displays;Wearable computers;Keyboards;Virtual reality;User interfaces;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction techniques-Gestural input},
	month = {March},
	pages = {519-528},
	title = {TapID: Rapid Touch Interaction in Virtual Reality using Wearable Sensing},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00076}}

@inproceedings{9417794,
	abstract = {In this paper, we provide a bidirectional shadow rendering method to render shadows between real and virtual objects in the 360$\,^{\circ}$ videos in real time. We construct a 3D scene approximation from the current output viewpoint to approximate the real scene geometry nearby in the video. Then, we propose a ray casting based algorithm to determine the shadow regions on the virtual objects cast by the real objects. After that, we introduce an object-aware shadow mapping method to cast shadows from virtual objects to real objects. Finally, we use a shadow intensity estimation algorithm to determine the shadow intensity of virtual objects and real objects to obtain shadows consistent with the input 360$\,^{\circ}$ video. The experiment results prove the effectiveness of our bidirectional shadow rendering method for mixed 360$\,^{\circ}$ videos. Our method can generate visually realistic shadows for virtual objects and real objects in 360$\,^{\circ}$ video in realtime, and make virtual objects more natural to integrate with real scenes in 360$\,^{\circ}$ videos of the mixed reality applications.},
	author = {Wang, Lili and Wang, Hao and Dai, Danqing and Leng, Jiaye and Han, Xiaoguang},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00038},
	issn = {2642-5254},
	keywords = {Geometry;Three-dimensional displays;Mixed reality;Estimation;Virtual reality;User interfaces;Rendering (computer graphics);Mixed reality-360$\,^{\circ}$;videos-Shadow rendering},
	month = {March},
	pages = {170-178},
	title = {Bidirectional Shadow Rendering for Interactive Mixed 360$\,^{\circ}$ Videos},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00038}}

@inproceedings{9417795,
	abstract = {Contextual illusions, such as the Ebbinghaus Illusion, can be potentially used to improve or hinder reach-to-grasp interaction in a virtual environment by affecting the perception of object size and the action. However, the illusion effect has only been evaluated using 2D objects like discs or annuluses, and limited research has been conducted in a virtual environment. Moreover, it remains unknown how the sudden, or dynamic, change of surrounding features will impact the perception and then the action towards the object. In this paper, we conducted a series of experiments to evaluate the effects of 3D Ebbinghaus illusion with dynamic surrounding features on the task of reaching to grasp a 3D object in an immersive virtual environment. An innovative 3D perceptual judgment task revealed that the static 3D illusion affected the perceived size of the 3D object. Then, we experimentally manipulated the visual gain and loss of the 3D contextual inducers, the participant&#x0027;s virtual hand, and the entire 3D contextual object. Results revealed that the depth error (error in depth of reaching action) was influenced by a dynamic change in the size of the inducers, the fine adjustment of grasp was dependent on the visual presence of the virtual hand and vision of 3D contextual object was required for the reaching and grasping movements. These results will benefit the understanding of reach-to-grasp interactions in immersive virtual environments and can improve interaction design.},
	author = {Todd, Russell and Zhu, Qin and Bani&#x0107;, Amy},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00109},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Virtual environments;Grasping;Kinematics;User interfaces;Apertures;3d perceptual judgement task;reach-to-grasp;Ebbing-haus;illusion;perception;3D Contextual Objects;temporal availability;dynamic inducers;static inducers;tracking;virtual reality},
	month = {March},
	pages = {817-825},
	title = {Temporal Availability of Ebbinghaus Illusions on Perceiving and Interacting with 3D Objects in a Contextual Virtual Environment},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00109}}

@inproceedings{9417796,
	abstract = {Like real humans, virtual characters also need to dress up according to different application scenarios so that the virtual character appears professionally, harmoniously, and naturally. However, manual selection is tedious, and the appearances of virtual characters usually lack variety. In this paper, we propose a new problem of synthesizing appropriate dress for a virtual character based on the scenario analysis where he/she shows up. We come up with a pipeline to tackle the scenario-aware dress synthesis problem. Firstly, given a scene, our approach predicts a dress code from the extracted high-level information in the scene, consisting of season, occasion, and scene category. Then our approach tunes the dress details to fit the aesthetic criteria and the virtual character's attributes. An optimization of a cost function implements the tuning process. We carried out experiments to validate the efficacy of the proposed approach. The perceptual study results show the good performance of our approach.},
	author = {Hou, Sifan and Wang, Yujia and Ning, Bing and Liang, Wei},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00026},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Semantics;Clothing;Color;Virtual reality;Lakes;Digital Fashion;Visualization Design and Evaluation Methods;Fashion Outfit Generation},
	month = {March},
	pages = {57-64},
	title = {Climaxing VR Character with Scene-Aware Aesthetic Dress Synthesis},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00026}}

@inproceedings{9417797,
	abstract = {We explore BouncyScreen, an actuated 1D display system that enriches indirect interaction with a virtual object by pseudo-haptic feedback mechanics enhanced through the screen's physical movements. We configured a proof-of-concept prototype of BouncyScreen with a flat-screen mounted on a mobile robot. When the user manipulates a virtual object using virtual reality (VR) controllers, the screen moves in accordance with the virtual object. We conducted psychophysical studies to examine how BouncyScreen's physical movements would affect users' pseudo-haptic perceptions and interaction experiences. Our preliminary study using a weight discrimination task for object pushing interaction showed that BouncyScreen offers identical pseudo-force feedback to the vision-based pseudo-haptic technique. We conducted a follow-up psychophysical study using a weight magnitude estimation task for object pushing and bumping interactions. The results reveal that a users' perceived weight magnitude is enhanced by the screen's physical motion under different characteristics depending on interaction styles (i.e., pushing and bumping). Our study also confirmed that the screen's synchronous physical motions significantly enhance the reality of the interaction and the sense of presence. We close this paper with some applications and use suggestions for BouncyScreen in future HMD-free flat-screen 3D user interface systems.},
	author = {Onishi, Yuki and Takashima, Kazuki and Fujita, Kazuyuki and Kitamura, Yoshifumi},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00059},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Force feedback;Display systems;Force;Prototypes;Estimation;Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Displays and imagers;Human-centered computing-Human computer interaction (HCI)-Interaction devices-Haptic devices},
	month = {March},
	pages = {363-372},
	title = {BouncyScreen: Physical Enhancement of Pseudo-Force Feedback},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00059}}

@inproceedings{9417799,
	abstract = {2D sketch-based tree modeling cannot guarantee to generate plausible depth values and full 3D tree shapes. With the advent of virtual reality (VR) technologies, 3D sketching enables a new form for 3D tree modeling. However, it is labor-intensive and difficult to create realistically-looking 3D trees with complicated geometry and lots of detailed twigs with a reasonable amount of effort. In this paper, we explore the use of mid-air finger 3D sketching in VR for tree modeling. We present a hybrid approach that integrates freehand 3D sketches with an automatic population of branch geometries. The user only needs to draw a few 3D strokes in mid-air to define the envelope of the foliage (denoted as lobes) and main branches. Our algorithm then automatically generates a full 3D tree model based on these stroke inputs. Additionally, the shape of the 3D tree model can be modified by freely dragging, squeezing, or moving lobes in mid-air. We demonstrate the ease-of-use, efficiency, and flexibility in tree modeling and overall shape control. We perform user studies and show a variety of realistic tree models generated instantaneously from 3D finger sketching.},
	author = {Zhang, Fanxing and Liu, Zhihao and Cheng, Zhanglin and Deussen, Oliver and Chen, Baoquan and Wang, Yunhai},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00110},
	issn = {2642-5254},
	keywords = {Geometry;Solid modeling;Three-dimensional displays;Shape;Computational modeling;Fingers;Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Computing methodologies-Computer graphics-Shape modeling},
	month = {March},
	pages = {826-834},
	title = {Mid-Air Finger Sketching for Tree Modeling},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00110}}

@inproceedings{9417801,
	abstract = {In this paper, we aim at guiding people to accomplish a personalized task, work surface organizing, in mixed reality environment, which can also be applied to intelligent robots. Through the cameras mounted in a MR device, e.g., Hololens, we firstly capture a person's daily activities in real scene when he uses the work surface. From such activities, we model the individual behavior habits and apply them to optimize the arrangement of the work surface. A cost function is defined for the optimization, considering general arrangement rules and human habitual behavior. The optimized arrangement is suggested to the user by augmenting the virtual arrangement on the real scene. To evaluate the effectiveness of our approach, we conducted experiments on a variety of scenes.},
	author = {Liu, Jingjing and Liane, Wei and Ning, Bing and Mao, Ting},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00049},
	issn = {2642-5254},
	keywords = {Solid modeling;Three-dimensional displays;Robot vision systems;Mixed reality;Virtual reality;User interfaces;Cost function;Human-centered Design;Mixed Reality;Work Surface Design;Remodeling},
	month = {March},
	pages = {270-278},
	title = {Work Surface Arrangement Optimization Driven by Human Activity},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00049}}

@inproceedings{9417802,
	abstract = {With more attention being paid to large scale virtual environments, the demand for more users to collaborate in the same physical space is growing rapidly. Due to the complex collision problem caused by the limitation of physical space, the multi-user redirected walking methods are devoted to improving the ability of multi-user navigation in large-scale virtual environments by reducing the disturbance of resets. Because the existing multi-user redirected walking methods do not consider the density of users in the physical space, there are more boundary conflicts in the real walking for the multi-user virtual environment. In order to decrease the boundary conflicts, this paper presents a novel method of dynamic density-based redirected walking towards multi-user virtual environments. This method dynamically adjusts the user distribution to a state with high center density and low boundary density through the density force, which is generated by the density difference between standard density and actual density. In our method, the users in high-density areas are guided by a repulsive force away from the central area while the users in low-density areas are guided by the gravitational forces towards the central area. Our method can select a double-density optimal gravitational point as the turning target, so all users can move to the area of minimum density to make better use of the whole physical space. Our method also adopts the artificial potential field (APF) forces to prevent user collisions caused by usergathering. The users are guided to move in the direction of the resultant force vector of density force, gravitational force and APF force. In addition, this paper introduces a matching resetting method to further adjust the density distribution while dealing with user conflicts. The results of experiments show that our method successfully reduces the potential conflicts about 30% on average compared with the existing reactive multi-user redirection algorithms. Especially as the number of users increases, our method can avoid more boundary conflicts by using the adjustment of density.},
	author = {Dong, Tianyang and Shen, Yue and Gao, Tieqi and Fan, Jing},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00088},
	issn = {2642-5254},
	keywords = {Legged locomotion;Three-dimensional displays;Navigation;Heuristic algorithms;Force;Virtual environments;User interfaces;Virtual Reality (VR);Redirected Walking (RDW);Multiple users;Artificial Potential Field;Collision Avoidance},
	month = {March},
	pages = {626-634},
	title = {Dynamic Density-based Redirected Walking Towards Multi-user Virtual Environments},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00088}}

@inproceedings{9417803,
	abstract = {Recent research indicates that many post-secondary students feel overwhelming anxiety, negatively impacting academic performance and overall well-being. In this paper, based on multidisciplinary literature analysis and innovative ideas in cognitive science, learning models, and emerging technologies, we introduce a theoretical framework that shows how and when priming activities can be introduced into the experiential learning cycle to reduce anxiety and increase cognitive bandwidth. This framework proposes a Virtual Reality based priming approach that uses games and meditative interventions. Our results show this approach's potential compared to no-priming scenarios for reducing anxiety and significance for VR gaming in improving cognitive bandwidth.},
	author = {Hawes, Dan and Arya, Ali},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00046},
	issn = {2642-5254},
	keywords = {Solid modeling;Analytical models;Epidemics;Three-dimensional displays;Buildings;Bandwidth;Virtual reality;Anxiety;scarcity;priming;virtual reality;games},
	month = {March},
	pages = {245-254},
	title = {VR-based Student Priming to Reduce Anxiety and Increase Cognitive Bandwidth},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00046}}

@inproceedings{9417804,
	abstract = {Redirected jumping (RDJ) is a locomotion technique that allows users to explore a virtual space that is larger than the available physical space by imperceptibly manipulating users' virtual viewpoints according to different gains. In previous redirected jumping work, different types of gains were imposed separately, without considering the possible interaction effects of horizontal and vertical gains on the jumping distance perception. To figure out how humans perceive distance manipulation when more than one gain is used, in this paper, we explored joint horizontal and vertical gains that manipulate horizontal and vertical distances at the same time during two-legged takeoff jumping in the virtual space. We estimated and analyzed horizontal and vertical detection thresholds by conducting a user study, fitting the data to two-dimensional psychometric functions, and visualizing the fitted 3D plots. We provided quantitative insights into the effects of joint gains on detection thresholds, where the imperceptible range for one gain can be affected by the variation of the other gain. Finally, we designed redirected jumping-based games as applications with joint horizontal and vertical gains and demonstrated the effectiveness of the redirected jumping technique.},
	author = {Li, Yi-Jun and Jin, De-Rong and Wang, Miao and Chen, Jun-Long and Steinicke, Frank and Hu, Shi-Min and Zhao, Qinping},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00030},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Fitting;Estimation;Data visualization;Games;Virtual reality;Space exploration;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality},
	month = {March},
	pages = {95-102},
	title = {Detection Thresholds with Joint Horizontal and Vertical Gains in Redirected Jumping},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00030}}

@inproceedings{9417806,
	abstract = {This virtual reality study was conducted to assess the impact of the appearance of virtual characters on the avoidance movement behavior of participants. Five experimental conditions were examined. Under each condition, one of the five different virtual characters (classified as mannequin, human, cartoon, robot, and zombie) was studied. Each participant had to experience only one condition and was asked to perform the collision avoidance tasks two times. During the walking task, the motion of participants was recorded. After finishing the collision avoidance segment of the study, a questionnaire that examined different concepts (emotional reactivity, emotional contagion, attentional allocation, behavioral independence, perceived skill, presence, immersion, virtual character realism, and virtual character unpleasantness) was distributed to the participants. Based on the collected measurements (avoidance movement behavior and self-reported ratings), we tried to understand the effects of the appearance of a virtual character on the avoidance movement behavior, and its possible correlation to subjective ratings. The results obtained from this study indicated that the appearance of the virtual characters did affect the avoidance movement behavior and also some of the examined concepts. Additionally, participant avoidance movement behavior correlates with some subjective ratings.},
	author = {Mousas, Christos and Koilias, Alexandros and Rekabdar, Banafsheh and Kao, Dominic and Anastaslou, Dimitris},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00024},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Motion segmentation;Virtual reality;User interfaces;Particle measurements;Resource management;Motion measurement;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	month = {March},
	pages = {40-49},
	title = {Toward Understanding the Effects of Virtual Character Appearance on Avoidance Movement Behavior},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00024}}

@inproceedings{9417808,
	abstract = {Virtual Reality (VR) offers significant potential for public speaking training. Virtual Reality Speech Training (VR-ST) helps trainees develop presentation skills and practice their application in the real world. Additionally, participants with public speaking anxiety can improve their presentation skills in a safe virtual environment without fear of judgment. Another benefit is direct feedback based on gamification principles, which provides users with information about their performance during training and allows for the adjustment of behavior in real-time. However, it is not yet clear if direct feedback based on visualization through icons is accepted by participants, such that it may support learning transfer in VR training applications. As a result, we set out to investigate how direct feedback in a VR -ST affects the participants' technology acceptance based on the Technology Acceptance Model (TAM). We conducted a between-subjects experimental study in order to compare a VR-ST with direct feedback (n = 100) with a simulation-based VR-ST (n = 100). The resulting MANOVAs demonstrated a preference for the direct feedback version for all TAM determinations, showing that direct feedback offers benefits to trainees by improving technology acceptance, independent of location and without supervision by trainers. Further results show that VR-ST is generally more accepted by participants without public speaking anxiety. Our findings indicate that developers of VR public speaking applications should focus on the inclusion of meaningful direct feedback and consider individual differences between users in order to optimally implement training measures.},
	author = {Palmas, Fabrizio and Reinelt, Ramona and Cichor, Jakub E. and Plecher, David A. and Klinker, Gudrun},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00070},
	issn = {2642-5254},
	keywords = {Training;Visualization;Three-dimensional displays;Technology acceptance model;Atmospheric measurements;Virtual environments;Tools;Virtual Reality;Public Speaking;Gamification;Virtual Training;Direct Feedback;Experimental Study},
	month = {March},
	pages = {463-472},
	title = {Virtual Reality Public Speaking Training: Experimental Evaluation of Direct Feedback Technology Acceptance},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00070}}

@inproceedings{9417809,
	abstract = {The operation of telerobotic systems can be a challenging task, requiring intuitive and efficient interfaces to enable inexperienced users to attain a high level of proficiency. Body-Machine Interfaces (BoMI) represent a promising alternative to standard control devices, such as joysticks, because they leverage intuitive body motion and gestures. It has been shown that the use of Virtual Reality (VR) and first-person view perspectives can increase the user's sense of presence in avatars. However, it is unclear if these beneficial effects occur also in the teleoperation of non-anthropomorphic robots that display motion patterns different from those of humans. Here we describe experimental results on teleoperation of a non-anthropomorphic drone showing that VR correlates with a higher sense of spatial presence, whereas viewpoints moving coherently with the robot are associated with a higher sense of embodiment. Furthermore, the experimental results show that spontaneous body motion patterns are affected by VR and viewpoint conditions in terms of variability, amplitude, and robot correlates, suggesting that the design of BoMIs for drone teleoperation must take into account the use of Virtual Reality and the choice of the viewpoint.},
	author = {Macchini, Matteo and Lortkipanidze, Manana and Schiano, Fabrizio and Floreano, Dario},
	booktitle = {2021 IEEE Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:27 -0400},
	date-modified = {2024-03-18 02:30:27 -0400},
	doi = {10.1109/VR50410.2021.00075},
	issn = {2642-5254},
	keywords = {Robot motion;Three-dimensional displays;Avatars;User interfaces;Robot sensing systems;Task analysis;Telerobotics;Virtual Reality;Presence;Human-Robot Interfaces;Human Body Motion},
	month = {March},
	pages = {511-518},
	title = {The Impact of Virtual Reality and Viewpoints in Body Motion Based Drone Teleoperation},
	year = {2021},
	bdsk-url-1 = {https://doi.org/10.1109/VR50410.2021.00075}}

@inproceedings{9089433,
	abstract = {Augmented reality head-worn displays (AR HWDs) have the potential to assist personal computing and the acquisition of everyday information. In this research, we propose Glanceable AR, an interaction paradigm for accessing information in AR HWDs. In Glanceable AR, secondary information resides at the periphery of vision to stay unobtrusive and can be accessed by a quick glance whenever needed. We propose two novel hands-free interfaces: "head-glance", in which virtual contents are fixed to the user's body and can be accessed by head rotation, and "gaze-summon" in which contents can be "summoned" into central vision by eye-tracked gazing at the periphery. We compared these techniques with a baseline heads-up display (HUD), which we call "eye-glance" interface in two dual-task scenarios. We found that the head-glance and eye-glance interfaces are more preferred and more efficient than the gaze-summon interface for discretionary information access. For a continuous monitoring task, the eye-glance interface was preferred. We discuss the implications of our findings for designing Glanceable AR interfaces in AR HWDs.},
	author = {Lu, Feiyu and Davari, Shakiba and Lisle, Lee and Li, Yuan and Bowman, Doug A.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00113},
	issn = {2642-5254},
	keywords = {Augmented reality;User interfaces;Human computer interaction;Head-mounted displays;Human-centered computing;Mixed / augmented reality;Human-centered computing;User interface design},
	month = {March},
	pages = {930-939},
	title = {Glanceable AR: Evaluating Information Access Methods for Head-Worn Augmented Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00113}}

@inproceedings{9089437,
	abstract = {Virtual reality sickness typically results from visual-vestibular conflict. Because self-motion from optical flow is driven most strongly by motion at the periphery of the retina, reducing the user's field-of-view (FOV) during locomotion has proven to be an effective strategy to minimize visual vestibular conflict and VR sickness. Current FOV restrictor implementations reduce the user's FOV by rendering a restrictor whose center is fixed at the center of the head mounted display (HMD), which is effective when the user's eye gaze is aligned with head gaze. However, during eccentric eye gaze, users may look at the FOV restrictor itself, exposing them to peripheral optical flow which could lead to increased VR sickness. To address these limitations, we develop a foveated FOV restrictor and we explore the effect of dynamically moving the center of the FOV restrictor according to the user's eye gaze position. We conducted a user study (n=22) where each participant uses a foveated FOV restrictor and a head-fixed FOV restrictor while navigating a virtual environment. We found no statistically significant difference in VR sickness measures or noticeability between both restrictors. However, there was a significant difference in eye gaze behavior, as measured by eye gaze dispersion, with the foveated FOV restrictor allowing participants to have a wider visual scan area compared to the head-fixed FOV restrictor, which confined their eye gaze to the center of the FOV.},
	author = {Adhanom, Isayas Berhe and Navarro Griffin, Nathan and MacNeilage, Paul and Folmer, Eelke},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00087},
	issn = {2642-5254},
	keywords = {Optical sensors;Visualization;Virtual reality;Integrated optics;Navigation;Retina;Resists;Virtual Reality;VR Sickness;Field-of-view Manipulation;Eye Tracking},
	month = {March},
	pages = {645-652},
	title = {The Effect of a Foveated Field-of-view Restrictor on VR Sickness},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00087}}

@inproceedings{9089439,
	abstract = {Virtual Reality (VR) has grown since the first devices for personal use became available on the market. However, the production of cinematographic content in this new medium is still in an early exploratory phase. The main reason is that cinematographic language in VR is still under development, and we still need to learn how to tell stories effectively. A key element in traditional film editing is the use of different cutting techniques, in order to transition seamlessly from one sequence to another. A fundamental aspect of these techniques is the placement and control over the camera. However, VR content creators do not have full control of the camera. Instead, users in VR can freely explore the 360$\,^{\circ}$ of the scene around them, which potentially leads to very different experiences. While this is desirable in certain applications such as VR games, it may hinder the experience in narrative VR. In this work, we perform a systematic analysis of users' viewing behavior across cut boundaries while watching professionally edited, narrative 360$\,^{\circ}$ videos. We extend previous metrics for quantifying user behavior in order to support more complex and realistic footage, and we introduce two new metrics that allow us to measure users' exploration in a variety of different complex scenarios. From this analysis, (i) we confirm that previous insights derived for simple content hold for professionally edited content, and (ii) we derive new insights that could potentially influence VR content creation, informing creators about the impact of different cuts in the audience's behavior.},
	author = {Mara{\~n}es, Carlos and Gutierrez, Diego and Serrano, Ana},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00025},
	issn = {2642-5254},
	keywords = {Human computer interaction;Videos;Motion pictures;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	month = {March},
	pages = {73-82},
	title = {Exploring the impact of 360$\,^{\circ}$ movie cuts in users' attention},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00025}}

@inproceedings{9089441,
	abstract = {Spatial computing experiences are physically constrained by the geometry and semantics of the local user environment. This limitation is elevated in remote multi-user interaction scenarios, where finding a common virtual ground physically accessible for all participants becomes challenging. Locating a common accessible virtual ground is difficult for the users themselves, particularly if they are not aware of the spatial properties of other participants. In this paper, we introduce a framework to generate an optimal mutual virtual space for a multi-user interaction setting where remote users' room spaces can have different layout and sizes. The framework further recommends movement of surrounding furniture objects that expand the size of the mutual space with minimal physical effort. Finally, we demonstrate the performance of our solution on real-world datasets and also a real HoloLens application. Results show the proposed algorithm can effectively discover optimal shareable space for multi-user virtual interaction and hence facilitate remote spatial computing communication in various collaborative workflows.},
	author = {Keshavarzi, Mohammad and Yang, Allen Y. and Ko, Woojin and Caldas, Luisa},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00055},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Optimization;Semantics;Layout;Avatars;Collaboration;Computing methodologies---Computer graphics---Graphics systems and interfaces---Mixed / augmented reality;Human-centered computing---Human computer interaction ---Interaction paradigms---Collaborative interaction;Applied computing---Decision analysis---Multi-criterion optimization and decision-making;Theory of computation---Mathematical optimization---Optimization with randomized search heuristics---Evolutionary algorithms},
	month = {March},
	pages = {353-362},
	title = {Optimization and Manipulation of Contextual Mutual Spaces for Multi-User Virtual and Augmented Reality Interaction},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00055}}

@inproceedings{9089442,
	abstract = {Many studies have been conducted in the past few years that focus on interaction and embodiment in the field of virtual reality. However, despite the recent widespread use and continuing rise of controller-based head-mounted display (HMD) hardware for VR, there is little research on the use of handheld controllers in this context. We explore the effects of different virtual hand representations on interaction and the user's sense of embodiment, extending the work of Argelaguet et al. in 2016, in this case using controllers. We designed an experiment where users perform the task of selecting and moving a cube from and to specific positions on a table inside an immersive virtual environment, interacting with three representations: the abstract shape of a Sphere, the 3D model of the Controller, and a realistic human-looking Hand. For each representation, users were asked to perform the same task with and without obstacles (Brick Wall, Barbed Wire, Electric Current). Statistical analysis of the results show that although no significant differences were identified in the sense of agency, the users' performance with the Sphere was significantly worse compared to the other two, and in the case of the positioning task the Controller outperformed the others. Additionally, the Hand generated the strongest sense of ownership, and it was the favorite representation.},
	author = {Lougiakis, Christos and Katifori, Akrivi and Roussou, Maria and Ioannidis, Ioannis-Panagiotis},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00072},
	issn = {2642-5254},
	keywords = {Virtual environments;Virtual reality;Human computer interaction;Grasping;Human-centered computing;Virtual reality;Human-centered computing;User studies;Human-centered computing;Empirical studies in interaction design;Computing methodologies;Perception},
	month = {March},
	pages = {510-518},
	title = {Effects of Virtual Hand Representation on Interaction and Embodiment in HMD-based Virtual Environments Using Controllers},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00072}}

@inproceedings{9089444,
	abstract = {Several redirected walking techniques have been introduced and analyzed in recent years, while the main focus was on manipulations in horizontal directions, in particular, by means of curvature, rotation, and translation gains. However, less research has been conducted on the manipulation of vertical movements and its possible use as a redirection technique. Actually, vertical movements are fundamentally important, e.g., for remotely steering a drone using a virtual reality headset.In this paper, we explored vertical gains, a novel redirection technique, which enables us to purposefully manipulate the mapping of the user's physical vertical movements to movements in the virtual space and the remote space. This approach allows natural and more active physical control of a real drone. To demonstrate the usability of vertical gains, we implemented a telepresence drone and vertical redirection techniques for stretching and crouching actions using common VR devices. We conducted two user studies to investigate the effective manipulation ranges and its usability: one study using a virtual environment (VE), and one using a camera stream from a telepresence drone. The results revealed that our technique could manipulate a users vertical movement without her/his noticing.},
	author = {Matsumoto, Keigo and Langbehn, Eike and Narumi, Takuji and Steinicke, Frank},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00028},
	issn = {2642-5254},
	keywords = {Drones;Telepresence;Legged locomotion;Resists;Virtual reality;Aerospace electronics;Three-dimensional displays;Drone;Vertical movement;Redirection;Telepresence},
	month = {March},
	pages = {101-107},
	title = {Detection Thresholds for Vertical Gains in VR and Drone-based Telepresence Systems},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00028}}

@inproceedings{9089445,
	abstract = {Immersive Virtual Reality (VR) laparoscopy simulation is emerging to enhance the attractiveness and realism of surgical procedural training. This study analyses the usability and presence of a Virtual Operating Room (VOR) setup via user evaluation and sets out the key elements for an immersive environment during a laparoscopic procedural training.In the VOR setup, a VR headset displayed a 360-degree computer-generated Operating Room (OR) around a VR laparoscopic simulator during laparoscopy procedures. Thirty-seven surgeons and surgical trainees performed the complete cholecystectomy task in the VOR. Questionnaires (i.e., Localized Postural Discomfort scale, Questionnaire for Intuitive Use, NASA-Task Load Index, and Presence Questionnaire) followed by a semi-structured interview were used to collect the data.The participants could intuitively adapt to the VOR and were satisfied when performing their tasks (M=3.90, IQR=0.70). The participants, particularly surgical trainees, were highly engaged to accomplish the task. Despite the higher mental workload on four subscales (p <; 0.05), the surgical trainees had a lower effort of learning (4 vs 3.33, p <; 0.05) compared to surgeons. The participants experienced very slight discomfort in seven body segments (0.59-1.16). In addition, they expected improvements for team interaction and personalized experience within the setup.The VOR showed potential to become a useful tool in providing immersive training during laparoscopy procedure simulation based on the usability and presence noted in the study. Future developments of user interfaces, VOR environment, team interaction and personalization should result in improvements of the system.},
	author = {Li, Meng and Ganni, Sandeep and Ponten, Jeroen and Albayrak, Armagan and Rutkowski, Anne-F and Jakimowicz, Jack},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00078},
	issn = {2642-5254},
	keywords = {Surgery;Laparoscopes;Task analysis;Training;Usability;Virtual reality;Instruments;Laparoscopy simulation;Virtual reality operating room;Surgical training;Presence;Usability;User evaluation;Human-centered computing [Virtual Reality];Human computer interaction;User evaluation;Human-centered computing;[Applied Computing];Life and medical science},
	month = {March},
	pages = {566-572},
	title = {Analysing usability and presence of a virtual reality operating room (VOR) simulator during laparoscopic surgery training},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00078}}

@inproceedings{9089446,
	abstract = {Immersive Analytics (IA) uses immersive virtual and augmented reality displays for data visualization and visual analytics. Designers rely on studies of how accurately people interpret data in different visualizations to make effective visualization choices. However, these studies focus on data analysis in traditional desktop environments. We lack empirical grounding for how to best visualize data in immersive environments. This study explores how people interpret data visualizations across different display types by measuring how quickly and accurately people conduct three analysis tasks over five visual channels: color, size, height, orientation, and depth. We identify key quantitative differences in performance and user behavior, indicating that stereo viewing resolves some of the challenges of visualizations in 3D space. We also find that while AR displays encourage increased navigation, they decrease performance with color-based visualizations. Our results provide guidelines on how to tailor visualizations to different displays in order to better leverage the affordances of IA modalities.},
	author = {Whitlock, Matt and Smart, Stephen and Szafir, Danielle Albers},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00084},
	issn = {2642-5254},
	keywords = {Data visualization;Task analysis;Image color analysis;Visualization;Navigation;Three-dimensional displays;Two dimensional displays;Human-centered computing---Visualization---Empirical studies in visualization;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Mixed / augmented reality;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality},
	month = {March},
	pages = {616-625},
	title = {Graphical Perception for Immersive Analytics},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00084}}

@inproceedings{9089450,
	abstract = {Daily operation and management of complex systems typically include multiple working sessions during which a team presents a set of information and discusses issues relevant to their decision making. A complex set of operational technology (OT) networks installed onboard a Navy ship is an example of such a system. A crew's ability to effectively communicate OT networks status to the ship commander, visualize, and discuss the options available in a given situation, has a significant impact on mission success. While the complexity of contemporary OT networks has dramatically increased, visualization tools have witnessed little improvement over several decades---they include sets of two-dimensional blueprints that are inherently hard to understand and conceptualize as three-dimensional (3D) information. To address this problem, we designed and implemented an augmented reality (AR) system that allowed a small team to visualize a 3D model of the ship with details of its computer networks. We recruited 30 individuals familiar with network management tasks central to our study and examined the usability of the tool on a set of real-world scenarios focused on network management. Analysis of objective and subjective data suggested that there was a general agreement among the participants that AR portrayal of the network was very supportive of their understanding of the physical-to-logical relationship within the network and that it fostered constructive collaboration among the team members. The reported levels of discomfort associated with oculomotor symptoms made the highest contribution to the total Simulator Sickness Questionnaire score; we believe that those symptoms should be given more attention in future studies with AR setups. The results provided in this empirical study offer early insights into the benefits and challenges of AR approaches applied to the decision making of small teams in high stakes scenarios and real-world situations.},
	author = {Timmerman, Matthew and Sadagic, Amela and Irvine, Cynthia},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00093},
	issn = {2642-5254},
	keywords = {Task analysis;Three-dimensional displays;Collaboration;Marine vehicles;Two dimensional displays;Tools;Computer networks;augmented reality;collaborative environment;usability;network visualization;small team collaboration;decision making;complex domains},
	month = {March},
	pages = {704-712},
	title = {Peering Under the Hull: Enhanced Decision Making via an Augmented Environment},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00093}}

@inproceedings{9089451,
	abstract = {Field trips are a key component of learning in STEM disciplines such as geoscience to develop skills, integrate knowledge, and prepare students for lifelong learning. Given the reported success of technology-based learning and the prevalence of new forms of technology, especially with immersive virtual reality (iVR) entering the mainstream, virtual field trips (VFTs) are increasingly being considered as an effective form of teaching to either supplement or replace actual field trips (AFTs). However, little research has investigated the implications of VFTs in place-based STEM education, and empirical evidence is still limited about differences between students' learning experiences and outcomes in VFTs experienced on desktop displays and field trips experienced in iVR. We report on a study that divided an introductory geoscience laboratory course into three groups with the first two groups experiencing a VFT either on desktop (dVFT) or in iVR (iVFT), while the third group went on an AFT. We compared subjective experiences (assessed via questionnaires) and objective learning outcomes for these groups. Our results suggest that, although students reported higher motivation and being more present in the iVFT group, they did not learn more compared to those in the dVFT group; both VFT groups yielded higher scores for learning experience and perceived learning outcomes than the actual field site visit. These findings demonstrate positive learning effects of VFTs relative to AFTs and provide evidence that geology VFTs need not be limited to iVR setups, which lead to considerable equipment costs and increased implementation complexity. Discussing the results, we reflect on the implications of our findings and point out future research directions.},
	author = {Zhao, Jiayan and LaFemina, Peter and Carr, Julia and Sajjadi, Pejman and Wallgr{\"u}n, Jan Oliver and Klippel, Alexander},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00012},
	issn = {2642-5254},
	keywords = {Interactive systems;Education;Virtual environments;STEM;Virtual reality;Virtual field trips;immersion;place-based education},
	month = {March},
	pages = {893-902},
	title = {Learning in the Field: Comparison of Desktop, Immersive Virtual Reality, and Actual Field Trips for Place-Based STEM Education},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00012}}

@inproceedings{9089452,
	abstract = {This paper examines a common problem found in a number of virtual reality setups---mismatches between real and virtual environments. Specifically, this paper investigates whether the mismatching between a real and a virtual environment in terms of appearance and physical constraints can affect the arousal (electrodermal activity) and movement behavior in the participants. For this study, one baseline condition and four mismatch conditions that examine different mismatching types were developed and tested in a between-group study design. The participants were immersed in a virtual environment and were asked to walk in a direction given to them along a provided path. During that time, electrodermal activity and the walking motion of participants were captured to assess potential alterations in their arousal and movement behavior respectively. Results obtained from this study indicate significant differences in the electrodermal activity and movement behavior of participants, especially when walking in a virtual environment that is mismatched both in appearance and physical constraints. Even though to a lesser degree, evidence was also found that correlates electrodermal activity with movement behavior. Limitations and future research directions are discussed.},
	author = {Mousas, Christos and Kao, Dominic and Koilias, Alexandros and Rekabdar, Banafsheh},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00085},
	issn = {2642-5254},
	keywords = {Virtual environments;Virtual reality;Human computer interaction;Head-mounted displays;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies},
	month = {March},
	pages = {626-635},
	title = {Real and Virtual Environment Mismatching Induces Arousal and Alters Movement Behavior},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00085}}

@inproceedings{9089453,
	abstract = {In this paper, we explore how a familiarly shaped object can serve as a physical proxy to manipulate virtual objects in Augmented Reality (AR) environments. Using the example of a tangible, handheld sphere, we demonstrate how irregularly shaped virtual objects can be selected, transformed, and released. After a brief description of the implementation of the tangible proxy, we present a buttonless interaction technique suited to the characteristics of the sphere. In a user study (N = 30), we compare our approach with three different controller-based methods that increasingly rely on physical buttons. As a use case, we focused on an alignment task that had to be completed in mid-air as well as on a flat surface. Results show that our concept has advantages over two of the controller-based methods regarding task completion time and user ratings. Our findings inform research on integrating tangible interaction into AR experiences.},
	author = {Englmeier, David and D{\"o}rner, Julia and Butz, Andreas and H{\"o}llerer, Tobias},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00041},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Human computer interaction;Shape;Manipulators;Task analysis;Augmented reality;Haptic interfaces;Human-centered computing---Human computer interaction (HCI)---Interaction devices---Haptic devices;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms--- Mixed / augmented reality},
	month = {March},
	pages = {221-229},
	title = {A Tangible Spherical Proxy for Object Manipulation in Augmented Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00041}}

@inproceedings{9089455,
	abstract = {Physical practice with one hand results in performance gains of the other (un-practiced) hand in a unilateral motor task. Yet how it induces performance gains of interlimb coordination in the bimanual movements between trained limb and the opposite, untrained limb is unclear. The present study designed a game-like interactive system for physical practice, in which an avatar's hands could be controlled itself or by the subject during a bimanual movement task in an immersive virtual reality environment. Participants practiced with the bimanual task by simultaneously drawing non-symmetric three-sided squares (e.g., U and C) to learn limb coordination with the following training strategies: (1) performing and seeing a bimanual task (BH-BH); (2) performing a unimanual task with right hand and seeing a bimanual action (RH-BH); (3) not performing a task but seeing a bimanual action (noH-BH); (4) performing and seeing a unimanual task (RH-RH). We found that the learning performance was better after BH-BH and RH-BH compared with other training strategies. In addition, we examined the effects of virtual hand representations on the learning performance after RH-BH. We found that the performance after training was increased with the realism level of virtual hands. These findings suggest that the proposed approach of RH-BH with realistic virtual hand would result in transfer of coordination skill to the unpracticed hand, which puts forward a new approach for learning and rehabilitation of coordination skill in patients with unilateral motor deficit in immersive environments.},
	author = {Xiao, Shan and Ye, Xupeng and Guo, Yaqiu and Gao, Boyu and Long, Jinyi},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00045},
	issn = {2642-5254},
	keywords = {Training;Task analysis;Visualization;Thumb;Shape;Virtual reality;Avatar hands;bimanual movement;coordination skill;virtual reality},
	month = {March},
	pages = {258-265},
	title = {Transfer of Coordination Skill to the Unpracticed Hand in Immersive Environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00045}}

@inproceedings{9089460,
	abstract = {Stereoscopic rendering is a prominent feature of virtual reality applications to generate depth cues and to provide depth perception in the virtual world. However, straight-forward stereo rendering methods usually are expensive since they render the scene from two eye-points which in general doubles the frame times. This is particularly problematic since virtual reality sets high requirements for real-time capabilities and image resolution. Hence, this paper presents a hybrid rendering system that combines classic rasterization and real-time ray-tracing to accelerate stereoscopic rendering. The system reprojects the pre-rendered left half of the stereo image pair into the right perspective using a forward grid warping technique and identifies resulting reprojection errors, which are then efficiently resolved by adaptive real-time ray-tracing. A final analysis shows that the system achieves a significant performance gain, has a negligible quality impact, and is suitable even for higher rendering resolutions.},
	author = {Wi{\ss}mann, Niko and Mi{\v s}iak, Martin and Fuhrmann, Arnulph and Latoschik, Marc Erich},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00107},
	issn = {2642-5254},
	keywords = {Rendering (computer graphics);Ray tracing;Real-time systems;Three-dimensional displays;Acceleration;Stereo image processing;Hardware;Computing methodologies;Computer Graphics;Rendering;Ray tracing;Rasterization;Graphics systems and interfaces;Virtual reality},
	month = {March},
	pages = {828-835},
	title = {Accelerated Stereo Rendering with Hybrid Reprojection-Based Rasterization and Adaptive Ray-Tracing},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00107}}

@inproceedings{9089461,
	abstract = {While text tends to lead a rather static life on paper and screens, virtual reality (VR) allows readers to interact with it in novel ways: the reading surface is no longer confined to a 2D plane. We conducted two user studies, in which we assessed text rendered on different surface shapes in VR and their effects on legibility and the reading experience. Comparing differently curved surfaces, these studies disclose the impact of warp angles and view box widths on reading comfort, speed, and distraction. Our results suggest that text should be warped around the horizontal rather than the vertical axis, and we provide recommendations for the extent of warp and view box width. In a proof-of-concept application, we used everyday 3D objects as text canvases and studied them through an information-seeking task. The studies' implications inform VR interfaces and, more generally, the rendering of text on 3D objects.},
	author = {Wei, Chunxue and Yu, Difeng and Dingler, Tilman},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00095},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Rendering (computer graphics);Virtual environments;Shape;Two dimensional displays;Task analysis;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality;Interaction design;Interaction design process and methods;User interface design},
	month = {March},
	pages = {721-728},
	title = {Reading on 3D Surfaces in Virtual Environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00095}}

@inproceedings{9089462,
	abstract = {Virtual reality (VR) is limited in many ways and often is incomparable to real-world experience. Walkable smooth uneven surfaces are inherent to reality but extremely lacking in VR. At the same time, VR offers a lot of possibilities for manipulations. In this paper, we focus on human height and slant perception of the uneven surfaces with multi-sensory stimulation in VR. By employing viewport manipulations, haptic, and vibrotactile stimuli, we explore the possibility to simulate uneven surfaces different from the physical props used.Our results suggest that the use of a rounded prop helps to create a more convincing illusion of an uneven surface that is significantly higher than the physical one. The multi-sensory stimulation brings both height and slant estimations closer to the values suggested by the visual cues if there is no conflict with the haptic sensations. The use of a flat prop is less realistic and leads to massive height and slant underestimations as opposed to those suggested by visual cues. However, if the curved prop cannot be used, a flat surface might still be used to simulate small dents and bumps.},
	author = {Vasylevska, Khrystyna and Kov{\'a}cs, B{\'a}lint Istv{\'a}n and Kaufmann, Hannes},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00059},
	issn = {2642-5254},
	keywords = {Bridges;Visualization;Cameras;Legged locomotion;Haptic interfaces;Vibrations;Virtual reality;Human-centered computing;User studies, Human-centered computing;Virtual reality, Computing methodologies;Perception},
	month = {March},
	pages = {388-397},
	title = {VR Bridges: Simulating Smooth Uneven Surfaces in VR},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00059}}

@inproceedings{9089463,
	abstract = {Leaning (the upper body) has several times been shown to be a suitable virtual travel technique when being seated; in both, flying as well as ground-based scenarios. The direction of the steering method most commonly used is gaze/head-directed. However, this does not allow to inspect the environment independently from the direction of movement. The change to torso-directed steering allows for the latter and additionally does not take anything from the natural character of the leaning metaphor. We empirically investigated the impact of this freedom in a ground-based scenario and complemented the conditions with a virtual body-directed method and then crossed all with device-based control conditions. In the conducted study (n = 25), we found the torso-directed methods objectively performed the best (traveled distance, completion time & number of collisions), and found torso-directed leaning subjectively rated the most usable one.},
	author = {Zielasko, Daniel and Law, Yuen C. and Weyers, Benjamin},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00060},
	issn = {2642-5254},
	keywords = {Torso;Task analysis;Velocity control;Visualization;Virtual reality;Three-dimensional displays;Wireless communication;Human-centered computing;Virtual reality, Human-centered computing;Empirical studies in interaction design},
	month = {March},
	pages = {398-406},
	title = {Take a Look Around -- The Impact of Decoupling Gaze and Travel-direction in Seated and Ground-based Virtual Reality Utilizing Torso-directed Steering},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00060}}

@inproceedings{9089472,
	abstract = {In this paper, we present ARCHIE, a framework for testing augmented reality applications in the wild. ARCHIE collects user feedback and system state data in situ to help developers identify and debug issues important to testers. It also supports testing of multiple application versions (called "profiles") in a single evaluation session, prioritizing those versions which the tester finds more appealing. To evaluate ARCHIE, we implemented four distinct test case applications and used these applications to examine the performance overhead and context switching cost of incorporating our framework into a pre-existing code base. With these, we demonstrate that ARCHIE provides no significant overhead for AR applications, and introduces at most 2% processing overhead when switching among large groups of testable profiles.},
	author = {Lehman, Sarah M. and Ling, Haibin and Tan, Chiu C.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00013},
	issn = {2642-5254},
	keywords = {Testing;Interviews;Smart phones;Augmented reality;Google;User interfaces;Debugging;Software and its engineering---Software testing and debugging;Human-centered computing---User interface toolkits;Human-centered computing---Mixed / augmented reality},
	month = {March},
	pages = {903-912},
	title = {ARCHIE: A User-Focused Framework for Testing Augmented Reality Applications in the Wild},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00013}}

@inproceedings{9089474,
	abstract = {Using one's hands can be a natural and intuitive method for interacting with 3D objects in a mixed reality environment. This study explores three hand-interaction techniques, including the gaze and pinch, touch and grab, and worlds-in-miniature interaction for selecting and moving virtual furniture in the 3D scene. Overall, a comparative analysis reveals that the worlds-in-miniature provided the best usability and task performance than other studied techniques. We also conducted in-depth interviews and analyzed participants' hand gestures in order to identify desired attributes for 3D hand interaction design. Findings from interviews suggest that, when it comes to enjoyment and discoverability, users prefer directly manipulating the virtual furniture to interacting with objects remotely or using in-direct interactions such as gaze. Another insight this study provides is the critical roles of the virtual object's visual appearance in designing natural hand interaction. Gesture analysis reveals that shapes of furniture, as well as its perceived features such as weight, largely determined the participant's instinctive form of hand interaction (i.e., lift, grab, push). Based on these findings, we present design suggestions that can aid 3D interaction designers to develop a natural and intuitive hand interaction for mixed reality.},
	author = {Kang, Hyo Jeong and Shin, Jung-hye and Ponto, Kevin},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00047},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Task analysis;Virtual reality;Tracking;Usability;Meters;Head;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
	month = {March},
	pages = {275-284},
	title = {A Comparative Analysis of 3D User Interaction: How to Move Virtual Objects in Mixed Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00047}}

@inproceedings{9089475,
	abstract = {Creating stereoscopic 3D media content has wide applications in virtual reality. In this paper, we are interested in a challenging application, casual stereoscopic photography, that allows ordinary users to create a stereoscopic photo using two images captured by a hand-held monocular camera. To handle the geometric constraints and disparity adjustment for casually captured left and right images, we present a coarse-to-fine framework. In the coarse stage, we propose a unified reinforcement learning-based method, in which the produced stereo image is iteratively adjusted and evaluated in the term of visual comfort. In addition, to further enhance the visual comfort of the stereoscopic image produced in the coarse stage, we introduce another independent recurrent network to fine-tune its disparity range. Lastly, we perform comprehensive experiments to evaluate our method and demonstrate the applicability of our model for real images.},
	author = {Niu, Yuzhen and Zheng, Qingyang and Liu, Wenxi and Guo, Wenzhong},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00061},
	issn = {2642-5254},
	keywords = {Stereo image processing;Visualization;Cameras;Photography;Learning systems;Rendering (computer graphics);Virtual reality;Human-centered computing;Computer vision;Image and video acquisition;3D imaging},
	month = {March},
	pages = {407-415},
	title = {Recurrent Enhancement of Visual Comfort for Casual Stereoscopic Photography},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00061}}

@inproceedings{9089476,
	abstract = {Enlightening Patients with Augmented Reality (EPAR) enhances patient education with new possibilities offered by Augmented Reality. Medical procedures are becoming increasingly complex and printed information sheets are often hard to understand for patients. EPAR developed an augmented reality prototype that helps patients with strabismus to better understand the processes of examinations and eye surgeries. By means of interactive storytelling, three identified target groups based on user personas were able to adjust the level of information transfer based on their interests. We performed a 2-phase evaluation with a total of 24 test subjects, resulting in a final system usability score of 80.0. For interaction prompts concerning virtual 3D content, visual highlights were considered to be sufficient. Overall, participants thought that an AR system as a complementary tool could lead to a better understanding of medical procedures.},
	author = {Jakl, Andreas and Lienhart, Anna-Maria and Baumann, Clemens and Jalaeefar, Arian and Schlager, Alexander and Sch{\"o}ffer, Lucas and Bruckner, Franziska},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00038},
	issn = {2642-5254},
	keywords = {Education;Three-dimensional displays;Augmented reality;Surgery;Human computer interaction;Usability;Prototypes;Human-centered computing;Mixed / augmented reality Human-centered computing;Interface design prototyping Human-centered computing;Interaction design theory;concepts and paradigms Human-centered computing;Usability testing},
	month = {March},
	pages = {195-203},
	title = {Enlightening Patients with Augmented Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00038}}

@inproceedings{9089479,
	abstract = {Filmmakers of panoramic videos frequently struggle to guide attention to Regions of Interest (ROIs) due to consumers' freedom to explore. Some researchers hypothesize that peripheral cues attract reflexive/involuntary attention whereas cues within central vision engage and direct voluntary attention. This mixed-methods study evaluated the effectiveness of using central arrows and peripheral flickers to guide and focus attention in panoramic videos. Twenty-five adults wore a head-mounted display with an eye tracker and were guided to 14 ROIs in two panoramic videos. No significant differences emerged in regard to the number of followed cues, the time taken to reach and observe ROIs, ROI-related memory and user engagement. However, participants' gaze travelled a significantly greater distance toward ROIs within the first 500 ms after flicker-onsets compared to arrow-onsets. Nevertheless, most users preferred the arrow and perceived it as significantly more rewarding than the flicker. The findings imply that traditional attention paradigms are not entirely applicable to panoramic videos, as peripheral cues appear to engage both involuntary and voluntary attention. Theoretical and practical implications as well as limitations are discussed.},
	author = {Schmitz, Anastasia and MacQuarrie, Andrew and Julier, Simon and Binetti, Nicola and Steed, Anthony},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00024},
	issn = {2642-5254},
	keywords = {Videos;Virtual reality;Visualization;Focusing;Head-mounted displays;Modulation;Color;Cinematic Virtual Reality;360$\,^{\circ}$ video;head-mounted display;guiding attention;memory;eye-tracking},
	month = {March},
	pages = {63-72},
	title = {Directing versus Attracting Attention: Exploring the Effectiveness of Central and Peripheral Cues in Panoramic Videos},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00024}}

@inproceedings{9089480,
	abstract = {In virtual reality (VR), natural physical hand interaction allows users to interact with virtual content using physical gestures. While the most straightforward use of tracked hand motion maintains a one-to-one mapping between the physical and virtual world, some cases might benefit from changing this mapping through scaled or redirected interactions that modify the mapping between user's physical movements and the magnitude of corresponding virtual movements. However, large deviations in interaction fidelity may potentially provide distractions or a loss of perceived realism. Therefore, it is important to know the extent to which remapping techniques can be applied to scaled interactions in VR without users detecting the difference. In this paper, we extend prior research on redirected hand techniques by investigating user perception of scaled hand movements and estimating detection thresholds for different types of hand motion in VR. We conducted two experiments with a two-alternative forced-choice (2AFC) design to estimate the detection thresholds of remapped interaction. The first experiment tested the perception of motion scaling for simple hand movements, and the second experiment involved more complex reaching motions in a cognitively demanding game scenario. We present estimated detection thresholds for scale values that can be applied to virtual hand movements without users noticing the difference. Our findings show that detection thresholds differ significantly based on the type of hand movement (horizontal, vertical, and depth).},
	author = {Esmaeili, Shaghayegh and Benda, Brett and Ragan, Eric D.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00066},
	issn = {2642-5254},
	keywords = {Haptic interfaces;Task analysis;Virtual environments;Tracking;Legged locomotion;Visualization;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Empirical studies in HCI;Information interfaces and presentation--- Multimedia Information Systems---Artificial;augmented;and virtual realities},
	month = {March},
	pages = {453-462},
	title = {Detection of Scaled Hand Interactions in Virtual Reality: The Effects of Motion Direction and Task Complexity},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00066}}

@inproceedings{9089482,
	abstract = {Immersive Virtual Reality (VR) is increasingly being explored as an alternative medium for gambling games to attract players. Typically, gambling games try to impair a player's decision making, usually for the disadvantage of the players' financial outcome. An impaired decision making results in the inability to differentiate between advantageous and disadvantageous options. We investigated if and how immersion impacts decision making using a VR-based realization of the Iowa Gambling Task (IGT) to pinpoint potential risks and effects of gambling in VR. During the IGT, subjects are challenged to draw cards from four different decks of which two are advantageous. The selections made serve as a measure of a participant's decision making during the task. In a novel user study, we compared the effects of immersion on decision making between a low-immersive desktop-3D-based IGT realization and a high immersive VR version. Our results revealed significantly more disadvantageous decisions when playing the immersive VR version. This indicates an impairing effect of immersion on simulated real life decision making and provides empirical evidence for a high risk potential of gambling games targeting immersive VR.},
	author = {Oberd{\"o}rfer, Sebastian and Heidrich, David and Latoschik, Marc Erich},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00069},
	issn = {2642-5254},
	keywords = {Decision making;Games;Task analysis;Virtual reality;Atmospheric measurements;Particle measurements;Human computer interaction;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Human-centered computing;Interaction paradigms;Virtual Reality},
	month = {March},
	pages = {483-492},
	title = {Think Twice: The Influence of Immersion on Decision Making during Gambling in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00069}}

@inproceedings{9089483,
	abstract = {A critical decision when designing glanceable information displays is where to place the content. Since blocking the center of the field of view with virtual information is not desirable, designers often opt for placement in the visual periphery. Another option is to only show virtual content when needed. However, no study has been made to systematically evaluate world-locked content position, considering both cognitive and physiological constraints. With this goal in mind, we designed a scenario that mimics context switching between a real world-task and an information display. We then conducted a within-subjects study to evaluate the effect of position, parameterized by horizontal angle, vertical angle, and distance from the user. Our results show that context switching time increases as the information is displayed far from the task position. The same happens with discomfort: content placed at eye level, or below, was faster and more comfortable than in other positions. We also found participants preferred content at medium distances, although they were also faster with content at far distances.},
	author = {Imamov, Samat and Monzel, Daniel and Lages, Wallace S.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00110},
	issn = {2642-5254},
	keywords = {Task analysis;Switches;Virtual reality;Three-dimensional displays;Visualization;TV;Layout;Human-centered computing;Mixed and Augmented Reality Human-centered computing;Information Interfaces and Presentation;Miscellaneous},
	month = {March},
	pages = {851-858},
	title = {Where to display? How Interface Position Affects Comfort and Task Switching Time on Glanceable Interfaces},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00110}}

@inproceedings{9089485,
	abstract = {Ray-casting, i.e., a ray cast from a hand-held controller to select targets, is widely used in 3D environments. Inspired by the bubble cursor [12] which dynamically resizes its selection range on 2D surfaces, we investigate a bubble mechanism for ray-casting in virtual reality. Bubble mechanism identifies the target nearest to the ray, with which users do not have to accurately shoot through the target. We first design the criterion of selection and the visual feedback of the bubble. We then conduct two experiments to evaluate ray-casting techniques with bubble mechanism in both simple and complicated 3D target acquisition tasks. Results show the bubble mechanism significantly improves ray-casting on both performance and preference, and our Bubble Ray technique with angular distance definition is competitive compared with other target acquisition techniques. We also discuss potential improvements to show more practical implementations of ray-casting with bubble mechanism.},
	author = {Lu, Yiqin and Yu, Chun and Shi, Yuanchun},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00021},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Virtual reality;Two dimensional displays;Visualization;Task analysis;Euclidean distance;Conferences;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Interaction design;Interaction design process and methods;User interface design},
	month = {March},
	pages = {35-43},
	title = {Investigating Bubble Mechanism for Ray-Casting to Improve 3D Target Acquisition in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00021}}

@inproceedings{9089486,
	abstract = {Live virtual reality (VR) streaming has become a popular and trending video application in the consumer market providing users with 360-degree, immersive viewing experiences. To provide premium quality of experience, VR streaming faces unique challenges due to the significantly increased bandwidth consumption. To address the bandwidth challenge, VR video viewport prediction has been proposed as a viable solution, which predicts and streams only the user's viewport of interest with high quality to the VR device. However, most of the existing viewport prediction approaches target only the video-on-demand (VOD) use cases, requiring offline processing of the historical video and/or user data that are not available in the live streaming scenario. In this work, we develop a novel viewport prediction approach for live VR streaming, which only requires video content and user data in the current viewing session. To address the challenges of insufficient training data and real-time processing, we propose a live VR-specific deep learning mechanism, namely LiveDeep, to create the online viewport prediction model and conduct real-time inference. LiveDeep employs a hybrid approach to address the unique challenges in live VR streaming, involving (1) an alternate online data collection, labeling, training, and inference schedule with controlled feedback loop to accommodate for the sparse training data; and (2) a mixture of hybrid neural network models to accommodate for the inaccuracy caused by a single model. We evaluate LiveDeep using 48 users and 14 VR videos of various types obtained from a public VR user head movement dataset. The results indicate around 90% prediction accuracy, around 40% bandwidth savings, and premium processing time, which meets the bandwidth and real-time requirements of live VR streaming.},
	author = {Feng, Xianglong and Liu, Yao and Wei, Sheng},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00104},
	issn = {2642-5254},
	keywords = {Streaming media;Predictive models;Bandwidth;Real-time systems;Data models;Machine learning;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	month = {March},
	pages = {800-808},
	title = {LiveDeep: Online Viewport Prediction for Live Virtual Reality Streaming Using Lifelong Deep Learning},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00104}}

@inproceedings{9089490,
	abstract = {When implementing an Augmented Reality (AR) interface, it is essential to track camera motion in order to precisely register the virtual overlay in the view of the user. However, unlike most indoor AR scenarios, in many outdoor scenarios the user maintains a static position performing mostly rotational movements. Simultaneous Localization and Mapping (SLAM) methods typically used to solve the tracking problem require significant translational camera motion to perform reliably. The magnitude of the required translation is proportional to the size of the scene, exacerbating this problem in large environments such as open places or stadiums. In this paper, we present an alternative SLAM method, which combines spherical Structure-from-Motion and a robust 3D tracking method. We compare our method to ORB SLAM2 in synthetic and real tests, and show that our method can track more reliably in large spaces, with simpler calculation due to the spherical motion constraint. We discuss this issue in the context of implementing an AR interface for live sport events in stadiums or other open environments, but possible application scenarios for our technique go beyond and can be applied to handheld AR in many outdoor environments.},
	author = {Baker, Lewis and Ventura, Jonathan and Zollmann, Stefanie and Mills, Steven and Langlotz, Tobias},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00105},
	issn = {2642-5254},
	keywords = {Tracking;Simultaneous localization and mapping;Cameras;Three-dimensional displays;Augmented reality;Mobile handsets;Computing methodologies---Tracking---;Human-centered computing---Mixed / augmented reality},
	month = {March},
	pages = {809-817},
	title = {SPLAT: Spherical Localization and Tracking in Large Spaces},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00105}}

@inproceedings{9089491,
	abstract = {This paper identifies a new phenomenon: when users interact with simulated objects in a virtual environment where the user is much smaller than usual, there is a mismatch between the object physics that they expect and the object physics that would be correct at that scale. We report the findings of our study investigating the relationship between perceived realism and a physically accurate approximation of reality in a virtual reality experience in which the user has been scaled down by a factor of ten. We conducted a within-subjects experiment in which 44 subjects performed a simple interaction task with objects under two different physics simulation conditions. In one condition, the objects, when dropped and thrown, behaved accurately according to the physics that would be correct at that reduced scale in the real world, our true physics condition. In the other condition, the movie physics condition, the objects behaved in a similar manner as they would if no scaling of the user had occurred. We found that a significant majority of the users considered the latter condition to be the more realistic one. We argue that our findings have implications for many virtual reality and telepresence applications involving operation with simulated or physical objects in small scales.},
	author = {Pouke, Matti and Mimnaugh, Katherine J. and Ojala, Timo and LaValle, Steven M.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00014},
	issn = {2642-5254},
	keywords = {Physics;Motion pictures;Virtual environments;Ubiquitous computing;Cameras;Visualization;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems;Artificial, augmented and virtual realities},
	month = {March},
	pages = {913-921},
	title = {The Plausibility Paradox For Scaled-Down Users In Virtual Environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00014}}

@inproceedings{9089492,
	abstract = {We present ReViVD, a tool for exploring and filtering large trajectory-based datasets using virtual reality. ReViVD's novelty lies in using simple 3D shapes---such as cuboids, spheres and cylinders---as queries for users to select and filter groups of trajectories. Building on this simple paradigm, more complex queries can be created by combining previously made selection groups through a system of user-created Boolean operations. We demonstrate the use of ReViVD in different application domains, from GPS position tracking to simulated data (e. g., turbulent particle flows and traffic simulation). Our results show the ease of use and expressiveness of the 3D geometric shapes in a broad range of exploratory tasks. Re- ViVD was found to be particularly useful for progressively refining selections to isolate outlying behaviors. It also acts as a powerful communication tool for conveying the structure of normally abstract datasets to an audience.},
	author = {Homps, Fran{\c c}ois and Beugin, Yohan and Vuillemot, Romain},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00096},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Shape;Trajectory;Data visualization;Tools;Virtual reality;Two dimensional displays;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual Reality;Human- centered computing;Visualization;Visualization systems and tools;Visualization toolkits},
	month = {March},
	pages = {729-737},
	title = {ReViVD: Exploration and Filtering of Trajectories in an Immersive Environment using 3D Shapes},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00096}}

@inproceedings{9089499,
	abstract = {This paper introduces a physically-based approach of grasping and manipulation regarding virtual objects that would enable fine and stable grasping without haptic force feedback. The main contribution is to enhance an existing method which couples a virtual kinematic hand with a visual hand tracking system. The mismatches between the tracked and virtual hands often yield unstable grasps, especially for small objects. This is overcome by the implementation of grasping assistance based on virtual springs between the tracked and virtual hands. The assistance is triggered based on an analysis of usual grasping criteria, to determine whether a grasp is feasible or not. The proposed method has been validated in a supervised experiment which showed that our assistance improves speed and accuracy for a "pick and place" task involving an exhaustive object set, sized for precision grasp. Moreover, users' feedback shows a clear preference for the present approach in terms of naturalness and efficiency.},
	author = {DELRIEU, Thibauld and Weistroffer, Vincent and Gazeau, Jean Pierre},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00046},
	issn = {2642-5254},
	keywords = {Grasping;Couplings;Visualization;Kinematics;Task analysis;Virtual reality;Robustness;[Human-centered computing]: Virtual Reality--- Virtual grasping;[Human-centered computing]: User interface design---Dexterous interaction Precision grasp;[Human-centered computing]: User studies},
	month = {March},
	pages = {266-274},
	title = {Precise and realistic grasping and manipulation in Virtual Reality without force feedback},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00046}}

@inproceedings{9089500,
	abstract = {In a one-to-many mixed reality collaboration environment, where multiple local users wearing AR headsets are supervised by a remote expert wearing a VR HMD, we evaluated three view-sharing techniques: 2D video, 360 video, and 3D model augmented with 2D video. Through a pilot test, the weaknesses of the techniques were identified, and additional features were integrated into them. Then, their performances were compared in two different collaboration scenarios based on search and assembling. In the first scenario, a local user performed both search and assembling. In the second scenario, two local users had dedicated roles, one for search and the other for assembling. The experiment results showed that the 3D model augmented with 2D video was time-efficient, usable, less demanding and most preferred in one-to-many mixed reality collaborations.},
	author = {Lee, Geonsun and Kang, HyeongYeop and Lee, JongMin and Han, JungHyun},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00054},
	issn = {2642-5254},
	keywords = {Collaboration;Three-dimensional displays;Cameras;Virtual reality;Two dimensional displays;Resists;Solid modeling;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Collaborative and social computing;Collaborative and social computing theory;concepts and paradigms;Computer supported cooperative work},
	month = {March},
	pages = {343-352},
	title = {A User Study on View-sharing Techniques for One-to-Many Mixed Reality Collaborations},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00054}}

@inproceedings{9089501,
	abstract = {Navigation and wayfinding are often accomplished collectively, with groups of people. Yet most studies of navigation and wayfinding behavior, and of the acquisition of spatial knowledge more generally, focus on the individual. In this paper we extend the investigation of these topics to dyads. In particular, we focus on how well straight-line distances and directions between objects (survey knowledge) were learned by individuals and by the same individuals when comprising a dyad. Our experiment was carried out in a shared virtual environment and we report on the technical issues in conducting such a collaborative experiment in a shared virtual environment, such as the choice of locomotion mode and the provision of full-body self-avatars. Our findings indicate that dyads outperform individuals in their acquisition of survey knowledge.},
	author = {Buck, Lauren E. and McNamara, Timothy P. and Bodenheimer, Bobby},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00080},
	issn = {2642-5254},
	keywords = {Virtual environments;Navigation;Collaboration;Task analysis;Avatars;Tracking;Human-centered computing;Human com puter interaction (HCI);Interaction paradigms;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Collaborative interaction},
	month = {March},
	pages = {579-587},
	title = {Dyadic Acquisition of Survey Knowledge in a Shared Virtual Environment},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00080}}

@inproceedings{9089504,
	abstract = {Previous research on eye-hand coordination training systems has investigated user performance on a wall, 2D touchscreens, and in Virtual Reality (VR). In this paper, we designed an eye-hand coordination reaction test to investigate and compare user performance in three different virtual environments (VEs) -- VR, Augmented Reality (AR), and a 2D touchscreen. VR and AR conditions also included two feedback conditions -- mid-air and passive haptics. Results showed that compared to AR, participants were significantly faster and made fewer errors both in 2D and VR. However, compared to VR and AR, throughput performance of the participants was significantly higher in the 2D touchscreen condition. No significant differences were found between the two feedback conditions. The results show the importance of assessing precision and accuracy in eye-hand coordination training and suggest that it is currently not advisable to use AR headsets in such systems.},
	author = {Batmaz, Anil Ufuk and Mutasim, Aunnoy K and Malekmakan, Morteza and Sadr, Elham and Stuerzlinger, Wolfgang},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00037},
	issn = {2642-5254},
	keywords = {Training;Haptic interfaces;Task analysis;Two dimensional displays;Throughput;Mathematical model;Human-centered computing;Human Computer Interaction (HCI);Human-centered computing;Virtual Reality;Human-centered computing;Pointing;Human-centered computing;Touch screens},
	month = {March},
	pages = {184-193},
	title = {Touch the Wall: Comparison of Virtual and Augmented Reality with Conventional 2D Screen Eye-Hand Coordination Training Systems},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00037}}

@inproceedings{9089505,
	abstract = {We present ThermAirGlove (TAGlove), a pneumatic glove which provides thermal feedback for users, to support the haptic experience of grabbing objects of different temperatures and materials in virtual reality (VR). The system consists of a glove with five inflatable airbags on the fingers and the palm, two temperature chambers (one hot and one cold), and the closed-loop pneumatic thermal control system. Our technical experiments showed that the highest temperature-changing speed of TAGlove system was 2.75$\,^{\circ}$C/s for cooling, and the pneumatic-control mechanism could generate the thermal cues of different materials (e.g., foam, glass, copper, etc.). The user-perception experiments showed that the TAGlove system could provide five distinct levels of thermal sensation (ranging from very cool to very warm). The user-perception experiments also showed that the TAGlove could support users' material identification among foam, glass, and copper with the average accuracy of 87.2%, with no significant difference compared to perceiving the real physical objects. The user studies on VR experience showed that using TAGlove in immersive VR could significantly improve users' experience of presence compared to the situations without any temperature or material simulation.},
	author = {Cai, Shaoyu and Ke, Pingchuan and Narumi, Takuji and Zhu, Kening},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00044},
	issn = {2642-5254},
	keywords = {Temperature control;Temperature sensors;Copper;Haptic interfaces;Skin;Glass;Human-centered computing---Virtual reality;Human-centered computing---Haptic devices},
	month = {March},
	pages = {248-257},
	title = {ThermAirGlove: A Pneumatic Glove for Thermal Perception and Material Identification in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00044}}

@inproceedings{9089506,
	abstract = {With recent advancements in head-mounted displaying technologies, virtual reality pedestrian simulators have become a common tool for traffic safety research. In contrast to field studies, test track studies and traffic observations, simulators enable researchers to analyze pedestrian behavior in a safe and controlled environment. However, creating the necessary virtual environments is time-consuming, especially in terms of meeting today's expectations regarding graphical level of detail and realism. Furthermore, VR experiments often lack a body representation or require additional sensors to create an avatar. Due to the laboratory setting, VR simulators might fail to convey the feeling of standing on an actual street. In addition, simulators on the one hand and real-world testing on the other hand leave a methodological gap on the reality-virtuality continuum. This paper presents a novel approach for an augmented reality pedestrian simulator. With this simulator, the participant experiences virtual vehicles, augmented on a real scenario, allowing for safe and controlled testing in a realistic setting. In a between-subject design, 13 participants experienced a gap acceptance scenario with virtual vehicles, while 30 participants experienced the same scenario with real vehicles in the same environment. These participants were instructed to initiate a street crossing if they considered that the gap between the two experimental vehicles was safe to cross the street. Results indicate similar, but also offset behavior for both conditions. Lower acceptance rates and later crossing initiation times could be observed in the augmented reality condition. Still, it was shown that augmented reality renders a promising tool for pedestrian research but also features limitations depending on the use case.},
	author = {Maruhn, Philipp and Dietrich, Andr{\'e} and Prasch, Lorenz and Schneider, Sonja},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00051},
	issn = {2642-5254},
	keywords = {Roads;Virtual environments;Cameras;Streaming media;Augmented reality;Safety;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Human-centered computing;Human computer interaction (HCI);Empirical studies in HCI;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers},
	month = {March},
	pages = {313-321},
	title = {Analyzing Pedestrian Behavior in Augmented Reality --- Proof of Concept},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00051}}

@inproceedings{9089510,
	abstract = {When embodying a virtual avatar in immersive VR applications where body tracking is enabled, users typically are and feel in control the avatar movements. However, there are situations in which the technology could be tweaked to flip this relationship so that an embodied avatar could affect the user's motor behavior without users noticing it. This has been shown in action retargeting applications and motor contagion experiments. Here we discuss a different way in which an embodied avatar could implicitly drive users movements: the self-avatar follower effect. We review previous evidences and present new experimental results showing how, whenever the virtual body does not overlay with their physical body, users tend to unconsciously follow their avatar, filling the gap if the system allows for it. We discuss this effect in the context of the relevant neuroscientific literature, and propose a theoretical account of the follower effect at the intersection of motor control and inference theories.},
	author = {Gonzalez-Franco, Mar and Cohn, Brian and Ofek, Eyal and Burin, Dalila and Maselli, Antonella},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00019},
	issn = {2642-5254},
	keywords = {Avatars;Motor drives;Visualization;Computational modeling;Predictive models;Tracking;Human-centered computing;Virtual reality;Embodiment;Perception;Motor control},
	month = {March},
	pages = {18-25},
	title = {The Self-Avatar Follower Effect in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00019}}

@inproceedings{9089513,
	abstract = {The commercialization and lowering costs of consumer grade Virtual Reality (VR) devices has made the technology increasingly accessible to users around the world. The usage of VR technology is often accompanied by an undesirable side effect called cybersickness. Cyber-sickness is the feeling of discomfort that occurs during VR experiences, producing symptoms similar to those of motion sickness. It continues to remain one of the biggest hurdles to the widespread adoption of VR, making it increasingly important to explore and understand the factors that influence its onset. In this work, we investigated the influence of the presence/absence of motion control on the onset and severity of cybersickness in an HMD based VR driving simulation employing steering as a travel metaphor. Towards this end, we conducted a between subjects study manipulating the presence of control between three experimental conditions, two of which (Driving condition and Yoked Pair condition) formed a yoked control design where every pair of drivers and their yoked pairs were exposed to identical vehicular motion stimuli created by participants in the driving condition. In the other condition (Autonomous Car condition), participants experienced a program driven autonomous vehicle simulation. Results indicated that participants in the Driving condition experienced higher levels of cybersickness than participants in the Yoked Pair condition. While these results don't conform to findings from previous research which suggests that having control over motion reduces cybersickness, it seems to point towards the importance of the fidelity of the control metaphor's feedback response in alleviating cybersickness. Simply allowing one control their motion may not readily alleviate cybersickness but could instead increase it in such HMD based VR driving simulations. It may hence be important to consider how well the control metaphor and its feedback matches users' expectations if we want to successfully mitigate cybersickness.},
	author = {Venkatakrishnan, Roshan and Venkatakrishnan, Rohith and Bhargava, Ayush and Lucaites, Kathryn and Solini, Hannah and Volonte, Matias and Robb, Andrew and Babu, Sabarish V and Lin, Wen-Chieh and Lin, Yun-Xuan},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00090},
	issn = {2642-5254},
	keywords = {Virtual environments;Virtual reality;Feedback;Motion control;Simulation;Cybersickness;Human-centered computing;Empirical studies in HCI;Human-centered computing;Virtual reality},
	month = {March},
	pages = {672-681},
	title = {Comparative Evaluation of the Effects of Motion Control on Cybersickness in Immersive Virtual Environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00090}}

@inproceedings{9089517,
	abstract = {Mid-air hand interaction has long been proposed as a `natural' input method for Augmented Reality (AR) systems. Current AR Head-Mounted Displays (HMDs) have a limited area for hand-based interactions. Because of this, users may easily move their hand(s) outside this tracked area during interaction, especially in dynamic tasks (e.g., when translating an object). Compared to common midair interaction issues, such as gesture recognition, arm/hand fatigue, and unnatural ways of interacting with virtual objects (e.g., selecting a distant object), boundary awareness issues in AR devices have received little attention. In this research, we explore visual techniques for boundary awareness in AR HMDs, focusing on object translation tasks. Through a systematic formative study, we first identify the challenges that users might face when interacting with AR HMDs without any boundary awareness information (i.e., how current systems work). Based on the findings, we then propose four methods (i.e., static surfaces, dynamic surface(s), static coordinated lines, and dynamic coordinate line(s)) and evaluate them against the benchmark (i.e., baseline condition without boundary awareness) to make users aware of the tracked interaction area. Our results show that visual methods for boundary awareness can help with dynamic mid-air hand interactions in AR HMDs, but their effectiveness and application are user-dependent.},
	author = {Xu, Wenge and Liang, Hai-Ning and Chen, Yuzheng and Li, Xiang and Yu, Kangyou},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00039},
	issn = {2642-5254},
	keywords = {Tracking;Visualization;Cameras;Task analysis;Gesture recognition;Sensors;Collaboration;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed/augmented reality;Human-centered computing;Visualization;Visualization techniques},
	month = {March},
	pages = {204-211},
	title = {Exploring Visual Techniques for Boundary Awareness During Interaction in Augmented Reality Head-Mounted Displays},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00039}}

@inproceedings{9089518,
	abstract = {We present a novel approach for generating plausible verbal interactions between virtual human-like agents and user avatars in shared virtual environments. Sense-Plan-Ask, or SPA, extends prior work in propositional planning and natural language processing to enable agents to plan with uncertain information, and leverage question and answer dialogue with other agents and avatars to obtain the needed information and complete their goals. The agents are additionally able to respond to questions from the avatars and other agents using natural-language enabling real-time multi-agent multi-avatar communication environments.Our algorithm can simulate tens of virtual agents at interactive rates interacting, moving, communicating, planning, and replanning. We find that our algorithm creates a small runtime cost and enables agents to complete their goals more effectively than agents without the ability to leverage natural-language communication. We demonstrate quantitative results on a set of simulated benchmarks and detail the results of a preliminary user-study conducted to evaluate the plausibility of the virtual interactions generated by SPA. Overall, we find that participants prefer SPA to prior techniques in 84% of responses including significant benefits in terms of the plausibility of natural-language interactions and the positive impact of those interactions.},
	author = {Best, Andrew and Narang, Sahil and Manocha, Dinesh},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00030},
	issn = {2642-5254},
	keywords = {Avatars;Planning;Natural languages;Uncertainty;Computational modeling;Robot sensing systems;Virtual environments;Computing methodologies;Artificial intelligence;Distributed artificial intelligence;Multi-agent systems},
	month = {March},
	pages = {117-126},
	title = {SPA: Verbal Interactions between Agents and Avatars in Shared Virtual Environments using Propositional Planning},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00030}}

@inproceedings{9089519,
	abstract = {With the development of the virtual reality applications, predicting human visual attention on 360$\,^{\circ}$ images is valuable to content creators and encoding algorithms, and becomes essential to understand user behaviour. In this paper, we propose a local-global bifurcated deep network for saliency prediction on 360$\,^{\circ}$ images, which is named as SalBiNet360. In the global deep sub-network, multiple multi-scale contextual modules and a multilevel decoder are utilized to integrate the features from the middle and deep layers of the network. In the local deep sub-network, only one multi-scale contextual module and a single-level decoder are utilized to reduce the redundancy of local saliency maps. Finally, fused saliency maps are generated by linear combination of the global and local saliency maps. Experiments on two publicly available datasets illustrate that the proposed SalBiNet360 outperforms the tested state-of-the-art methods.},
	author = {Chen, Dongwen and Qing, Chunmei and Xu, Xiangmin and Zhu, Huansheng},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00027},
	issn = {2642-5254},
	keywords = {Feature extraction;Observers;Predictive models;Two dimensional displays;Solid modeling;Decoding;Visualization;360$\,^{\circ}$ images;SalBiNet360;virtual reality (VR)},
	month = {March},
	pages = {92-100},
	title = {SalBiNet360: Saliency Prediction on 360$\,^{\circ}$ Images with Local-Global Bifurcated Deep Network},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00027}}

@inproceedings{9089521,
	abstract = {Design discussion is a very important course in architecture education. In this paper, we developed a VR based architecture design discussion system that allows members to visualize and discuss the architectural models and to modify the models during discussion. Since the system is designed to work on top of Rhinoceros and Grasshopper, the object database is updated right after the object modification. Members communicate via voice, object manipulations, and mid-air sketching as well as on-surface sketching in the virtual environment. Several tools have been designed to enhance the sense of presence and to make the discussion more effective. We also developed a rollback mechanism to help users intuitively and quickly revert to a previous state of discussion to make some changes or to start a new direction of discussion. To evaluate the system, we conducted an initial user study with 14 participants to assess the user experience, user impression and effectiveness of the system. The feedback from participants were positive and suggested that the system could be effective and useful for supporting architecture design discussion.},
	author = {Hsu, Ting-Wei and Tsai, Ming-Han and Babu, Sabarish V. and Hsu, Pei-Hsien and Chang, Hsuan-Ming and Lin, Wen-Chieh and Chuang, Jung-Hong},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00056},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Virtual environments;Collaboration;Education;Architecture;Human-centered computing;Virtual reality;Human-centered computing;Computer supported cooperative work;Human-centered computing;User interface design;Applied computing;E-learning},
	month = {March},
	pages = {363-371},
	title = {Design and Initial Evaluation of a VR based Immersive and Interactive Architectural Design Discussion System},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00056}}

@inproceedings{9089526,
	abstract = {Hand-held capture of stereo panoramas involves spinning the camera in a roughly circular path to acquire a dense set of views of the scene. However, most existing structure-from-motion pipelines fail when trying to reconstruct such trajectories, due to the small baseline between frames. In this work, we evaluate the use of spherical structure-from-motion for reconstructing handheld stereo panorama captures. The spherical motion constraint introduces a strong regularization on the structure-from-motion process which mitigates the small-baseline problem, making it well-suited to the use case of stereo panorama capture with a handheld camera. We demonstrate the effectiveness of spherical structure-from-motion for casual capture of high-resolution stereo panoramas and validate our results with a user study.},
	author = {Baker, Lewis and Mills, Steven and Zollmann, Stefanie and Ventura, Jonathan},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00102},
	issn = {2642-5254},
	keywords = {Image stitching;Image reconstruction;Stereo image processing;Three-dimensional displays;Human computer interaction;Virtual reality;Human-centered computing---Interaction paradigms---Virtual reality---;Computer vision---Image and video acquisition---3D imaging},
	month = {March},
	pages = {782-790},
	title = {CasualStereo: Casual Capture of Stereo Panoramas with Spherical Structure-from-Motion},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00102}}

@inproceedings{9089531,
	abstract = {This paper explores the use of Virtual Reality (VR) to study humanrobot interactions during navigation tasks by both immersing a user and a robot in a shared virtual spaces. VR combines the advantages of being safe (as robots and humans interacting by the means of VR but can physically be in remote places) and ecological (realistic environments are perceived by the robot and the human, and natural behaviors can be observed). Nevertheless, VR can introduce perceptual biases in the interaction and affect in some ways the observed behaviors, which can be problematic when used to acquire experimental data. In our case, not only human perception is concerned, but also the one of the robot which requires to be simulated to perceive the VR world. Thus, the contribution of this paper is twofold. It first provides a technical solution to perform human robot interactions in navigation tasks through VR: we describe how we combine motion tracking, VR devices, as well as robot sensors simulation algorithms to immerse together a human and a robot in a shared virtual space. We then assess a simple interaction task that we replicate in real and in virtual conditions to perform a first estimation of the importance of the biases introduced by the use of VR on both a Human and a robot. Our conclusions are in favor of using VR to study human-robot interactions, and we are developing directions for future work.},
	author = {Grzeskowiak, Fabien and Babel, Marie and Bruneau, Julien and Pettre, Julien},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00100},
	issn = {2642-5254},
	keywords = {Robot sensing systems;Collision avoidance;Human-robot interaction;Solid modeling;Task analysis;Virtual reality;Human Robot Interaction;Motion Tracking;Virtual Reality;Robots simulation},
	month = {March},
	pages = {766-774},
	title = {Toward Virtual Reality-based Evaluation of Robot Navigation among People},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00100}}

@inproceedings{9089532,
	abstract = {Redirected Walking (RDW) enables users to walk in both virtual and physical tracking spaces simultaneously, which is an effective method to increase presence in Virtual Reality (VR). Recently, RDW technologies have been developed in a multi-user environment where multiple users share the same physical tracking space and simultaneously explore the same virtual space. Meanwhile, in the Steer-To-Optimal-Target (S2OT) method, user actions are planned in RDW by introducing machine learning models such as reinforcement learning. In this paper, we propose a new predictive RDW algorithm "Multiuser-Steer-to-Optimal-Target (MS2OT)" that extends the S2OT method into an environment with multiple users and various types of tracking space. In addition to the steering actions used in S2OT, MS2OT considers pre-reset actions and uses more steering targets and an improved reward function. The locations of multiple users and tracking space information are treated as visual information to be the state of the reinforcement learning model in MS2OT. Hence, the artificial neural network of a multilayer three-dimensional convolutional neural network with a dueling double deep network architecture is learned through Q-Learning. MS2OT significantly reduces the total number of resets compared to the conventional RDW algorithms such as S2C and APF-RDW in a multi-user environment and improves the total distance and average distance between resets during the same period. Experimental results show that MS2OT can process up to 32 users in real-time.},
	author = {Lee, Dong-Yong and Cho, Yong-Hun and Min, Dae-Hong and Lee, In-Kwon},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00034},
	issn = {2642-5254},
	keywords = {Prediction algorithms;Space vehicles;Learning (artificial intelligence);Legged locomotion;Target tracking;Shape;Virtual reality;Redirected walking;resetting;virtual environments;multi-user;collision avoidance;reinforcement learning},
	month = {March},
	pages = {155-163},
	title = {Optimal Planning for Redirected Walking Based on Reinforcement Learning in Multi-user Environment with Irregularly Shaped Physical Space},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00034}}

@inproceedings{9089533,
	abstract = {Text entry in virtual reality (VR) is currently a common activity and a challenging problem. In this paper, we introduce HiPad, leveraging a circular touchpad with a circular virtual keyboard, to support the one-hand text entry in mobile head-mounted displays (HMDs). The design of HiPad's layout is based on a circle and a square with rounded corners, where the outer circle is subdivided into six keys' regions containing letters. This technique input text by a common hand-held controller with a circular touchpad for HMDs and disambiguates the word based on the sequence of keys pressed by the user. In our first study, three potential layouts are considered and evaluated, leading to the design containing six keys. By analyzing the touch behavior of users, we optimize the 6-keys layout and conduct the second study, showing that the optimized layout has better performance. Then the third study is conducted to evaluate the performance of 6-keys HiPad with VE-layout and TP-layout and to study the learning curves. The results show that novices can achieve 13.57 Words per Minute (WPM) with VE-layout and 11.60 WPM with TP-layout and the speeds increase by 74.42% for VE-layout users and by 81.53% for TP-layout users through a short 60-phrase training.},
	author = {Jiang, Haiyan and Weng, Dongdong},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00092},
	issn = {2642-5254},
	keywords = {Layout;Keyboards;Training;Virtual reality;Visualization;Sensors;Conferences;Human-centered computing;Human computer interaction;Interaction paradigms;Virtual reality;Human-centered computing;Interaction techniques;Text input},
	month = {March},
	pages = {692-703},
	title = {HiPad: Text entry for Head-Mounted Displays Using Circular Touchpad},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00092}}

@inproceedings{9089534,
	abstract = {When persons interact with the environment and experience or witness an error (e.g. an unexpected event), a specific brain pattern, known as error-related potential (ErrP) can be observed in the electroencephalographic signals (EEG). Virtual Reality (VR) technology enables users to interact with computer-generated simulated environments and to provide multi-modal sensory feedback. Using VR systems can, however, be error-prone. In this paper, we investigate the presence of ErrPs when Virtual Reality users face 3 types of visualization errors: (Te) tracking errors when manipulating virtual objects, (Fe) feedback errors, and (Be) background anomalies. We conducted an experiment in which 15 participants were exposed to the 3 types of errors while performing a center-out pick and place task in virtual reality. The results showed that tracking errors generate error-related potentials, the other types of errors did not generate such discernible patterns. In addition, we show that it is possible to detect the ErrPs generated by tracking losses in single trial, with an accuracy of 85%. This constitutes a first step towards the automatic detection of error-related potentials in VR applications, paving the way to the design of adaptive and self-corrective VR/AR applications by exploiting information directly from the user's brain.},
	author = {Si-Mohammed, Hakim and Lopes-Dias, Catarina and Duarte, Maria and Argelaguet, Ferran and Jeunet, Camille and Casiez, G{\'e}ry and M{\"u}ller-Putz, Gernot R and L{\'e}cuyer, Anatole and Scherer, Reinhold},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00088},
	issn = {2642-5254},
	keywords = {Virtual reality;Electroencephalography;Task analysis;Human computer interaction;Brain;Three-dimensional displays;Visualization;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design and evaluation methods},
	month = {March},
	pages = {653-661},
	title = {Detecting System Errors in Virtual Reality Using EEG Through Error-Related Potentials},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00088}}

@inproceedings{9089537,
	abstract = {Selection studies are prevalent and indispensable for VR research. However, due to the tedious and repetitive nature of many such experiments, participants can become disengaged during the study, which is likely to impact the results and conclusions. In this work, we investigate participant disengagement in VR selection experiments and how this issue affects the outcomes. Moreover, we evaluate the usefulness of four engagement strategies to keep participants engaged during VR selection studies and investigate how they impact user performance when compared to a baseline condition with no engagement strategy. Based on our findings, we distill several design recommendations that can be useful for future VR selection studies or user tests in other domains that employ similar repetitive features.},
	author = {Yu, Difeng and Zhou, Qiushi and Tag, Benjamin and Dingler, Tilman and Velloso, Eduardo and Goncalves, Jorge},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00071},
	issn = {2642-5254},
	keywords = {Task analysis;Human computer interaction;Human factors;Virtual reality;Games;Human-centered computing;HCI design and evaluation methods;User studies;Human-centered computing;Virtual reality;Human-centered computing;Pointing},
	month = {March},
	pages = {500-509},
	title = {Engaging Participants during Selection Studies in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00071}}

@inproceedings{9089539,
	abstract = {Virtual Reality (VR) and Augmented Reality (AR) applications have seen a drastic increase in commercial popularity. Different representations have been used to create 3D reconstructions for AR and VR. Point clouds are one such representation characterized by their simplicity and versatility, making them suitable for real time applications, such as reconstructing humans for social virtual reality. In this study, we evaluate how the visual quality of digital humans, represented using point clouds, is affected by compression distortions. We compare the performance of the upcoming point cloud compression standard against an octree-based anchor codec. Two different VR viewing conditions enabling 3- and 6 degrees of freedom are tested, to understand how interacting in the virtual space affects the perception of quality. To the best of our knowledge, this is the first work performing user quality evaluation of dynamic point clouds in VR; in addition, contributions of the paper include quantitative data and empirical findings. Results highlight how perceived visual quality is affected by the tested content, and how current data sets might not be sufficient to comprehensively evaluate compression solutions. Moreover, shortcomings in how point cloud encoding solutions handle visually-lossless compression are discussed.},
	author = {Subramanyam, Shishir and Li, Jie and Viola, Irene and Cesar, Pablo},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00031},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Transform coding;Image color analysis;Measurement;Codecs;Geometry;Image coding;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Interaction paradigms;Virtual reality},
	month = {March},
	pages = {127-136},
	title = {Comparing the Quality of Highly Realistic Digital Humans in 3DoF and 6DoF: A Volumetric Video Case Study},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00031}}

@inproceedings{9089540,
	abstract = {Users are increasingly able to move around and perform tasks in virtual environments (VEs). Such movements and tasks are typically represented in a VE using either a first-person perspective (1PP) or a third-person perspective (3PP). In Virtual Reality (VR), 1PP is almost universally used. 3PP can be represented as either egocentric or allocentric. However, there is little empirical evidence about which view may be better suited to dynamic tasks in particular. This paper compares the use of 1PP, egocentric 3PP and allocentric 3PP for dynamic tasks in VR. Our results indicate that 1PP provides the best spatial perception and performance across several dynamic tasks. This advantage is less pronounced as the task becomes more dynamic.},
	author = {Bhandari, Naval and O'Neill, Eamonn},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00114},
	issn = {2642-5254},
	keywords = {Task analysis;Cameras;Avatars;Navigation;Virtual environments;Dynamic task;virtual reality;first-person perspective;third-person perspective;egocentric;allocentric;virtual environment;dynamic view;immersion;presence;spatial perception;task performance.},
	month = {March},
	pages = {939-948},
	title = {Influence of Perspective on Dynamic Tasks in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00114}}

@inproceedings{9089541,
	abstract = {With the advent of consumer grade depth sensors, low-cost volumetric capture systems are easier to deploy. Their wider adoption though depends on their usability and by extension on the practicality of spatially aligning multiple sensors. Most existing alignment approaches employ visual patterns, e.g. checkerboards, or markers and require high user involvement and technical knowledge. More user-friendly and easier-to-use approaches rely on markerless methods that exploit geometric patterns of a physical structure. However, current SoA approaches are bounded by restrictions in the placement and the number of sensors. In this work, we improve markerless data-driven correspondence estimation to achieve more robust and flexible multi-sensor spatial alignment. In particular, we incorporate geometric constraints in an end-to-end manner into a typical segmentation based model and bridge the intermediate dense classification task with the targeted pose estimation one. This is accomplished by a soft, differentiable procrustes analysis that regularizes the segmentation and achieves higher extrinsic calibration performance in expanded sensor placement configurations, while being unrestricted by the number of sensors of the volumetric capture system. Our model is experimentally shown to achieve similar results with marker-based methods and outperform the mark-erless ones, while also being robust to the pose variations of the calibration structure. Code and pretrained models are available at https://vcl3d.github.io/StructureNet/.},
	author = {Sterzentsenko, Vladimiros and Doumanoglou, Alexandros and Thermos, Spyridon and Zioulis, Nikolaos and Zarpalas, Dimitrios and Daras, Petros},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00106},
	issn = {2642-5254},
	keywords = {Cameras;Three-dimensional displays;Calibration;Pose estimation;Solid modeling;Semantics;Computing methodologies;Artificial intelligence;Computer vision;Image segmentation;Computing methodologies;Artificial intelligence;Computer vision;Camera calibration;Computing methodologies;Artificial intelligence;Computer vision;3D imaging},
	month = {March},
	pages = {818-827},
	title = {Deep Soft Procrustes for Markerless Volumetric Sensor Alignment},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00106}}

@inproceedings{9089542,
	abstract = {Neonatal endotracheal intubation (ETI) is a complex procedure. Low intubation success rates for pediatric residents indicate the current training regimen is inadequate for achieving positive patient outcomes. Computer-based training systems in this field have been limited due to the complex nature of simulating in real-time, the anatomical structures, soft tissue deformations and frequent tool interactions with large forces which occur during actual patient intubation. This paper addresses the issues of neonatal ETI training in an attempt to bridge the gap left by traditional training methods. We propose a fully interactive physics-based virtual reality (VR) simulation framework for neonatal ETI that converts the training of this medical procedure to a completely immersive virtual environment where both visual and physical realism were achieved. Our system embeds independent dynamics models and interaction devices in separate modules while allowing them to interact with each other within the same environment, which offers a flexible solution for multi-modal medical simulation scenarios. The virtual model was extracted from CT scans of a neonatal patient, which provides realistic anatomical structures and was parameterized to allow variations in a range of features that affect the level of difficulty. Moreover, with this manikin-free VR system, we can capture and visualize an even larger set of performance parameters in relation to the internal geometric change of the virtual model for real-time guidance and post-trial assessment. Lastly, validation study results from a group of neonatologists are presented demonstrating that VR is a promising platform to train medical professionals effectively for this procedure.},
	author = {Xiao, Xiao and Zhao, Shang and Meng, Yan and Soghier, Lamia and Zhang, Xiaoke and Hahn, James},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00077},
	issn = {2642-5254},
	keywords = {Pediatrics;Training;Computational modeling;Haptic interfaces;Atmospheric modeling;Three-dimensional displays;Deformable models;Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality;Computing methodologies;Animation;Physical simulation;Computing methodologies;Modeling and simulation;Simulation types and techniques;Real-time simulation;Human-centered computing;Human computer interaction (HCI);Interaction devices;Haptic devices},
	month = {March},
	pages = {557-565},
	title = {A Physics-based Virtual Reality Simulation Framework for Neonatal Endotracheal Intubation},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00077}}

@inproceedings{9089543,
	abstract = {Manipulating the appearance of a Virtual Environment to enable natural walking has so far focused on modifications that are intended to be unnoticed by users. In our research, we took a radically different approach by embracing the overt nature of the change. To explore this method, we designed the Space Bender, a natural walking technique for room-scale VR. It builds on the idea of overtly manipulating the Virtual Environment by "bending" the geometry whenever the user comes in proximity of a physical boundary. Our aim was to evaluate the feasibility of this approach in terms of performance and subjective feedback. We compared the Space Bender to two other similarly situated techniques: Stop and Reset and Teleportation, in a task requiring participants to traverse a 100 m path. Results show that the Space Bender was significantly faster than Stop and Reset, and preferred to the Teleportation technique, highlighting the potential of overt manipulation to facilitate natural walking.},
	author = {Simeone, Adalberto L. and Christian Nilsson, Niels and Zenner, Andr{\'e} and Speicher, Marco and Daiber, Florian},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00082},
	issn = {2642-5254},
	keywords = {Legged locomotion;Teleportation;Virtual environments;Three-dimensional displays;Geometry;Engines;Human-centered computing;Interaction Paradigms;Virtual Reality},
	month = {March},
	pages = {598-606},
	title = {The Space Bender: Supporting Natural Walking via Overt Manipulation of the Virtual Environment},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00082}}

@inproceedings{9089546,
	abstract = {We explore the adaptation of 2D small-multiples visualisation on flat screens to 3D immersive spaces. We use a "shelves" metaphor for layout of small multiples and consider a design space across a number of layout and interaction dimensions. We demonstrate the applicability of a prototype system informed by this design space to data sets from different domains. We perform two user studies comparing the effect of the shelf curvature dimension from our design space on users' ability to perform comparison and trend analysis tasks. Our results suggest that, with fewer multiples, a flat layout is more performant despite the need for participants to walk further. With an increase in the number of multiples, this performance difference disappears due to the time participants had to spend walking. In the latter case, users prefer a semi-circular layout over either a fully surrounding or a flat arrangement.},
	author = {Liu, Jiazhou and Prouzeau, Arnaud and Ens, Barrett and Dwyer, Tim},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00081},
	issn = {2642-5254},
	keywords = {Layout;Three-dimensional displays;Data visualization;Buildings;Two dimensional displays;Atmospheric modeling;Visualization;H.5.1 [Multimedia Information Systems]: Artificial, augmented and virtual realities;H.5.2 [User Interfaces]: Evaluation/methodology},
	month = {March},
	pages = {588-597},
	title = {Design and Evaluation of Interactive Small Multiples Data Visualisation in Immersive Spaces},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00081}}

@inproceedings{9089548,
	abstract = {Supporting perceptual-cognitive tasks is an important part of our daily lives. We use rich, multi-sensory feedback through sight, sound, touch, smell, and taste to support better perceptual-cognitive things we do, such as sports, cooking, and searching for a location, and to increase our confidence in performing those tasks in daily life. Same with real life, the demand for perceptual-cognitive tasks exists in serious VR simulations such as surgical or safety training systems. However, in contrast to real life, VR simulations are typically limited to visual and auditory cues, while sometimes adding simple tactile feedback. This could make it difficult to make confident decisions in VR.In this paper, we investigate the effects of multi-sensory stimuli, namely visuals, audio, two types of tactile (floor vibration and wind), and smell in terms of the confidence levels on a location-matching task which requires a combination of perceptual and cognitive work inside a virtual environment. We also measured the level of presence when participants visited virtual places with different combinations of sensory feedback. Our results show that our multi-sensory VR system was superior to a typical VR system (vision and audio) in terms of the sense of presence and user preference. However, the subjective confidence levels were higher in the typical VR system.},
	author = {Jung, Sungchul and Wood, Andrew L. and Hoermann, Simon and Abhayawardhana, Pramuditha L. and Lindeman, Robert W.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00067},
	issn = {2642-5254},
	keywords = {Vibrations;Multisensory integration;Cognition;Virtual reality;User interfaces;Wind;Multisensory VR;Perception;Confidence;Cognition;Floor vibration;Wind;Smell},
	month = {March},
	pages = {463-472},
	title = {The Impact of Multi-sensory Stimuli on Confidence Levels for Perceptual-cognitive Tasks in VR},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00067}}

@inproceedings{9089551,
	abstract = {The commercialization of Virtual Reality (VR) devices is making the technology increasingly accessible to users around the world. Despite the success that VR is starting to see with its growing popularity, it has yet to become widely adopted and achieve its ultimate goal- convincingly simulate real life like experiences. The inability to generate adequate levels of presence and to prevent the manifestation of cybersickness are the two prominent barriers that have hindered VR from achieving its ultimate goal. While traditional research has examined factors that influence (correlate with) the onset and severity cybersickness, there is still a gap in our knowledge about the consequences of having motion control on cybersickness in immersive virtual environments (IVE's) achieved using tracked Head Mounted Displays (HMD's). Furthermore, outside of a correlational capacity, it is still unclear as to what causes cybersickness to affect presence in immersive virtual environments. The success of immersive virtual reality as a technology will hence largely come down to our ability to understand the interrelationship between these variables and then address the challenges they pose. Towards this end, we investigated how the affordance of motion control affects cybersickness and presence in an HMD based VR driving simulation by conducting a between subjects study where we manipulated the affordance of control between three experimental conditions. We leverage structural equation modeling in an attempt to build a framework that explains the relationship between virtual motion control, workload, cybersickness, time spent in the simulation, perceived time and presence. Our structural model helps explain why motion control could be an important factor to consider in addressing VR's challenges and realizing its ultimate aim to simulate reality.},
	author = {Venkatakrishnan, Rohith and Venkatakrishnan, Roshan and Anaraky, Reza Ghaiumy and Volonte, Matias and Knijnenburg, Bart and Babu, Sabarish V},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00091},
	issn = {2642-5254},
	keywords = {Virtual environments;Cybersickness;Virtual reality;Motion control;Human computer interaction;Human-centered computing;Empirical studies in HCI;Human-centered computing;Virtual reality},
	month = {March},
	pages = {682-691},
	title = {A Structural Equation Modeling Approach to Understand the Relationship between Control, Cybersickness and Presence in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00091}}

@inproceedings{9089552,
	abstract = {The effectiveness of Augmented Reality (AR) for spatial applications is likely influenced by the extent to which users perceive that they can act on virtual objects as if they are real. While there are some promising initial results that action capabilities in AR are perceived similarly to the real world, AR differs from completely real-world viewing both in the visual information available within a restricted field of view (FOV) and in the ways that perceptual-motor feedback can be provided for the outcome of actions. We addressed both of these differences in a study of judgments of passing through an AR aperture defined by two moveable virtual walls presented via the Microsoft HoloLens. First, we investigated the possible effects of viewing distance on affordance judgments, predicting greater error in judgments at a close distance because of the inability to see the entirety of the aperture within the FOV. Second, we explored the effect of feedback on affordance judgments, predicting that verbal feedback about whether a judgment was correct would be sufficient to see improvements in judgments over time. In contrast to our predictions for distance, we found that judgments for passing through were closer to actual shoulder width when viewed at a distance near the aperture (0.85 m) compared to a farther viewing point (3.20 m). Verbal feedback reduced error over trials, but only at the farther distance, possibly because the judgments at the near distance were close to the ceiling of performance. These findings have implications for tasks that require accurate perception of action capabilities in AR, such as training procedures.},
	author = {Gagnon, Holly C. and Na, Dun and Heiner, Keith and Stefanucci, Jeanine and Creem-Regehr, Sarah and Bodenheimer, Bobby},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00112},
	issn = {2642-5254},
	keywords = {Apertures;Virtual environments;Psychology;Augmented reality;Augmented reality;affordances;perception},
	month = {March},
	pages = {922-929},
	title = {The Role of Viewing Distance and Feedback on Affordance Judgments in Augmented Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00112}}

@inproceedings{9089553,
	abstract = {In outdoor scenes, the inhomogeneity of the atmosphere and the ground effect have a great impact on sound propagation, but these two effects are usually ignored in previous methods. We propose an adaptive FDTD-PE method to simulate sound propagation in 3D scenes taking into account atmospheric inhomogeneity and the ground effect to produce more realistic sound propagation results. In the simulation, the ground is considered as a porous medium with a certain thickness. The scene is categorized into a number of two-dimensional vertical ground planes in the three-dimensional cylindrical coordinate system. These planes are decomposed into the near-source complex regions and the far-source regions, which are solved by the FDTD solver and the parabolic equation (PE) solver, respectively. Furthermore, a novel encoding method was designed to process sound pressure data. In the far-source regions, the one-way sound propagation is only affected by the ground and atmosphere inhomogeneity, so we encode sound pressure data through function fitting. Finally, an efficient sound rendering method with this encoding representation is developed to perform auralization in the frequency-domain. We validated our method in various outdoor scenes, and the results indicate that our method can realistically simulate outdoor sound propagation, with quite higher speed and lower storage.},
	author = {Liu, Shiguang and Liu, Jin},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00111},
	issn = {2642-5254},
	keywords = {Atmospheric modeling;Acoustics;Finite difference methods;Nonhomogeneous media;Time-domain analysis;Solid modeling;Adaptation models;Applied computing;Arts and humanities;Sound and music computing;Computing methodologies;Computer graphics;Animation},
	month = {March},
	pages = {859-867},
	title = {Outdoor Sound Propagation Based on Adaptive FDTD-PE},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00111}}

@inproceedings{9089554,
	abstract = {Various studies have been conducted to realize realistic direct interaction in the virtual environment. In this study, we focus on a situation wherein two users using the same physical space explore the same virtual environment using redirected walking (RDW) technology. For two users to meet each other in a virtual environment to realize realistic direct interaction, they must simultaneously meet each other in physical space. However, if the RDW algorithm is applied to each user independently, the relative positions and orientations of the two users can be significantly different in the virtual and physical spaces. We present a recovery algorithm that adjusts the relative position and orientation such that they become the same in the two spaces. Our recovery algorithm uses either modified subtle RDW techniques or overt recovery techniques in three cases depending on the relative position and orientation of the two users. Once the recovered state is reached, the two users can go forward to meet each other and directly interact in the virtual and physical spaces simultaneously. Based on the experiment results, we can confirm that the application of our recovery technology to the system increases the user's satisfaction in usability and the presence of coexistence in the virtual environment with other users.},
	author = {Min, Dae-Hong and Lee, Dong-Yong and Cho, Yong-Hun and Lee, In-Kwon},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00035},
	issn = {2642-5254},
	keywords = {Virtual environments;Legged locomotion;Haptic interfaces;Redirected walking;Multi-user redirected walking;Direct interaction;Recovery in redirected walking},
	month = {March},
	pages = {164-173},
	title = {Shaking Hands in Virtual Space: Recovery in Redirected Walking for Direct Interaction between Two Users},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00035}}

@inproceedings{9089559,
	abstract = {The present study examined the potential health and safety effects of short-term virtual reality (VR) use by children in an educational use case scenario (that is, relatively brief episodes of use across a limited number of sequential days), such as how VR may be used in the classroom or at a museum. Ophthalmological, vestibular functioning, balance, hand-eye coordination, 3D spatial representation, and subjective comfort effects were assessed using a variety of optometric, psychophysical, and self-report measures. Thirty child participants (ages 10 to 12 years) were immersed in VR for 30 minutes daily across five consecutive days of use. Measurements were taken prior to the onset of VR use (baseline), at the end of the fifth day of VR use (to assess potential acute effects), and 24 hours after the fifth day of VR use (to assess potential longer-lasting effects).There were no statistically significant adverse effects found, with the exception of slightly elevated scores on a self-reported measure of subjective comfort, which, however, were below the range of scores reported in past research as being indicative of subject discomfort. In other words, the current study found no empirical evidence that short-term use of VR in an educational use setting by children ages 10 through 12 years is associated with any adverse visual, spatial representational, or balance aftereffects, or that it causes undue nausea, oculomotor discomfort, or disorientation. The present study does not address longer-term use or potential psychological effects of different VR content.},
	author = {Rauschenberger, Robert and Barakat, Brandon},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00010},
	issn = {2642-5254},
	keywords = {Atmospheric measurements;Particle measurements;Virtual reality;Health and safety;Pediatrics;Area measurement;Three-dimensional displays;Virtual reality;children;education;health and safety;SSQ;optometry;balance;hand-eye coordination;spatial representation;[Human-centered computing---Interaction Paradigms]:Virtual Reality;[User Characteristics]:Age---Children},
	month = {March},
	pages = {878-884},
	title = {Health and Safety of VR Use by Children in an Educational Use Case},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00010}}

@inproceedings{9089561,
	abstract = {Locomotion is one of the most important problems in virtual reality. Real walking experience is the key to immersively explore the virtual world. Several strategies have been proposed to solve the problem, but most are not suitable to solve the locomotion problem in Room-Scale VR. The omnidirectional treadmill is an effective way to provide a natural walking experience within the Room-Scale VR. This paper proposes a novel omnidirectional treadmill named HEX-CORE-PROTOTYPE (HCP). The principle of synthesis and decomposition of velocity is applied to form an omnidirectional velocity field. Our system could provide a full degree of freedom and real walking experience in place. Compared to the current best system, the height of HCP is only 40% of it. The application shows the effectiveness of our system to solve the locomotion problem in Room-Scale VR.},
	author = {Wang, Ziyao and Wei, Haikun and Zhang, KanJian and Xie, Liping},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00058},
	issn = {2642-5254},
	keywords = {Legged locomotion;Gears;Aerospace electronics;Servomotors;Belts;Virtual reality;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality, Computing methodologies;Computer graphics;Graphics systems and interfaces;Virtual reality},
	month = {March},
	pages = {382-387},
	title = {Real Walking in Place: HEX-CORE-PROTOTYPE Omnidirectional Treadmill},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00058}}

@inproceedings{9089562,
	abstract = {To investigate the effects of visual discomfort caused by long-term immersing in virtual environments (VEs), we conducted a comparative study to evaluate users' visual discomfort in an eight-hour working rhythm and compared the differences between the VEs and the physical environments. Twenty-seven participants performed four different visual tasks with a head-mounted display (HMD) for the VE condition and with a monitor for the physical condition. Their subjective visual discomfort and objective oculomotor indicators were measured to evaluate their visual performances. The results show that the subjective visual fatigue symptoms, the objective pupil size, and the relative accommodation response vary across time for the two conditions, in which VEs affects visual fatigue the most compared to the physical environments. The results also show that pupil size is negatively related to subjective visual fatigue, and the long-term work based on displays only influences the maximum accommodation response of participants. This work is a supplement to the necessary but insufficient-researched field of visual fatigue in long-term immersing in VEs, which should be valuable to researchers involved in the evaluation of visual fatigue using HMDs.},
	author = {Guo, Jie and Weng, Dongdong and Fang, Hui and Zhang, Zhenliang and Ping, Jiamin and Liu, Yue and Wang, Yongtian},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00065},
	issn = {2642-5254},
	keywords = {Visualization;Fatigue;Two dimensional displays;Resists;Three-dimensional displays;Virtual environments;Optical imaging;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies;Human-centered computing;Interaction paradigms;Virtual reality},
	month = {March},
	pages = {443-452},
	title = {Exploring the Differences of Visual Discomfort Caused by Long-term Immersion between Virtual Environments and Physical Environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00065}}

@inproceedings{9089568,
	abstract = {Virtual Reality (VR) produces a highly realistic simulation environment to engage users with Immersive Virtual Environments (IVEs). To interact effectively with users, VR builds intensive media through the multi-modal sense functions in the lower level, such as visual, auditory, tactile, and olfactory senses. However, the higher-level perceptions, e.g., the temporal duration, the sense of presence, and the cognitive load are less explored. These higher-level perceptions are part of the critical evaluation criteria for VR design. In this paper, we divide the external zeitgebers into visual and auditory zeitgebers. We then combine these zeitgebers with the attention-oriented cognitive load to investigate their effects on temporal estimation and presence, particularly in IVEs. We propose a data-driven method to build a multi-modal predictive equation for time estimation and presence, in an effort to figure out the essential elements of users' spatial and temporal perception in VR. We also design a complicated application and validate the predictive equation. Our feature-based model is able to guide the VR application design in terms of the subjective time length judgment and presence of users as well as achieve a better VR user experience.},
	author = {Liao, Haodong and Xie, Ning and Li, Huiyuan and Li, Yuhang and Su, Jianping and Jiang, Feng and Huang, Weipeng and Shen, Heng Tao},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00068},
	issn = {2642-5254},
	keywords = {Estimation;Task analysis;Visualization;Mathematical model;Circadian rhythm;Lighting;Human-centered computing;Virtual Reality;Human-centered computing;Emprical studies in HCI},
	month = {March},
	pages = {473-482},
	title = {Data-Driven Spatio-Temporal Analysis via Multi-Modal Zeitgebers and Cognitive Load in VR},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00068}}

@inproceedings{9089569,
	abstract = {Immersive Virtual Reality (VR) systems that combine Head Mounted Displays (HMDs) and a position tracking system support the multiple users or participants to collaborate in the same physical space for a large-scale virtual environment. Because the multiple users sharing the same physical space are in a dynamic state, the key technique of multi-user VR system is how to solve the problem of potential collisions among the users who are moving both virtually and physically. In order to better solve the collision problem caused by such dynamic changes, this work presents a new strategy of multi-user redirected walking using dynamic artificial potential fields, which generates repulsion to `push' users away from obstacles and other users, and uses gravity to `attract' users to an open or unobstructed space. In this method, the users not only get repulsive forces from walls, but also from other users and their future states that are called avatars. At the same time, the users will get gravitational force from the steering target. The target selection considers the size of open space, the distance between the steering target and the boundary of physical space, and the distance between the steering target and the center of the physical space. Therefore, the system can steer users to an open area in the physical space to further reduce collisions. To verify the validity of our method, we developed a software to statistically analyze the influence of different factors, such as the physical space size and the number of users. Data from experiments shows that our method reduces the potential user resets by about 20%.},
	author = {Dong, Tianyang and Chen, Xianwei and Song, Yifan and Ying, Wenyuan and Fan, Jing},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00033},
	issn = {2642-5254},
	keywords = {Legged locomotion;Space vehicles;Virtual environments;Force;Heuristic algorithms;Prediction algorithms;Redirected Walking;Virtual Reality;Head-Mounted Display;Multiple users;Virtual Roaming},
	month = {March},
	pages = {146-154},
	title = {Dynamic Artificial Potential Fields for Multi-User Redirected Walking},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00033}}

@inproceedings{9089572,
	abstract = {In recent years, virtual reality (VR) based training has greatly changed surgeons learning mode. It can simulate the surgery from the visual, auditory, and tactile aspects. VR medical simulator can greatly reduce the risk of the real patient and the cost of hospitals. Laparoscopic cholecystectomy is one of the typical representatives in minimal invasive surgery (MIS). Due to the large incidence of cholecystectomy, the application of its VR-based simulation is vital and necessary for the residents' surgical training. In this paper, we present a VR simulation framework based on position-based dynamics (PBD) for cholecystectomy. To further accelerate the deformation of organs, PBD constraints are solved in parallel by a graph coloring algorithm. We introduce a bio-thermal conduction model to improve the realism of the fat tissue electrocautery. Finally, we design a hybrid multi-model connection method to handle the interaction and simulation of the liver-gallbladder separation. This simulation system has been applied to laparoscopic cholecystectomy training in several hospitals. From the experimental results, users can operate in real-time with high stability and fidelity. The simulator is also evaluated by a number of digestive surgeons through preliminary studies. They believed that the system can offer great help to the improvement of surgical skills.},
	author = {Pan, Junjun and Zhang, Leiyu and Yu, Peng and Shen, Yang and Wang, Haipeng and Hao, Haimin and Qin, Hong},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00076},
	issn = {2642-5254},
	keywords = {Surgery;Biological system modeling;Strain;Biological tissues;Image color analysis;Deformable models;Solid modeling;Human-centered computing---Human computer interaction---Interactive systems and tools---User interface programming;Computer systems organization---Real-time systems---Real- time system architecture},
	month = {March},
	pages = {548-556},
	title = {Real-time VR Simulation of Laparoscopic Cholecystectomy based on Parallel Position-based Dynamics in GPU},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00076}}

@inproceedings{9089573,
	abstract = {In this contribution we examined the effects on users during interaction with a virtual human crowd in an immersive virtual reality environment. We developed an agent-based crowd model with rich properties including eye gaze, facial expression, body motion, and verbal and non-verbal behaviors. The scenario was a virtual market in which the users needed to gather specific items. In a betweensubjects design, users interacted with a virtual human crowd that showed opposite valenced emotional expressions. There are four conditions in the between-subjects design. These includes different affective virtual crowds namely, Positive, Negative, Neutral, and a Mix condition. The Mix group is defined by a combination of Positive, Negative and Neutral emotional expressive characters. Depending on the specific condition, the virtual humans showed specific verbal and non-verbal behaviors. During the experiment we collected objective measures such as skin electrodermal activity, total time in the simulation, the number of interactions with the agents and performance measures. The subjective measures were the differential emotional survey (DES), and a user experience survey. We reported our findings with an in-depth analysis.},
	author = {Volonte, Matias and Hsu, Yu-Chun and Liu, Kuan-Yu and Mazer, Joe P. and Wong, Sai-Keung and Babu, Sabarish V.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00049},
	issn = {2642-5254},
	keywords = {Avatars;Virtual reality;Legged locomotion;Animation;Computer graphics;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems---Animations, Evaluation/methodology;I.3.3 [Computer Graphics]: Three-Dimensional Graphics and Realism---Virtual reality},
	month = {March},
	pages = {293-302},
	title = {Effects of Interacting with a Crowd of Emotional Virtual Humans on Users' Affective and Non-Verbal Behaviors},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00049}}

@inproceedings{9089578,
	abstract = {Virtual Reality (VR) headsets with embedded eye trackers are appearing as consumer devices (e.g. HTC Vive Eye, FOVE). These devices could be used in VR-based education (e.g., a virtual lab, a virtual field trip) in which a live teacher guides a group of students. The eye tracking could enable better insights into students' activities and behavior patterns. For real-time insight, a teacher's VR environment can display student eye gaze. These visualizations would help identify students who are confused/distracted, and the teacher could better guide them to focus on important objects. We present six gaze visualization techniques for a VR-embedded teacher's view, and we present a user study to compare these techniques. The results suggest that a short particle trail representing eye trajectory is promising. In contrast, 3D heatmaps (an adaptation of traditional 2D heatmaps) for visualizing gaze over a short time span are problematic.},
	author = {Rahman, Yitoshee and Asish, Sarker M. and Fisher, Nicholas P. and Bruce, Ethan C. and Kulshreshth, Arun K. and Borst, Christoph W.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00009},
	issn = {2642-5254},
	keywords = {Data visualization;Visualization;Gaze tracking;Heating systems;Image color analysis;Three-dimensional displays;Monitoring;Human-centered computing;Visualization techniques;Human-centered computing;Visualization design and evaluation methods;Human-centered computing;Usability Testing},
	month = {March},
	pages = {868-877},
	title = {Exploring Eye Gaze Visualization Techniques for Identifying Distracted Students in Educational VR},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00009}}

@inproceedings{9089579,
	abstract = {Path redirection for virtual reality (VR) navigation allows the user to explore a large virtual environment (VE) while the VR application is hosted in a limited physical space. Static mapping redirection methods deform the virtual scene to fit the physical space. The challenge is to deform the virtual scene in a reasonable way, making the distortions friendly to the user's visual perception. In this paper we propose a feature-guided path redirection method that finds and takes into account the visual features of 3D virtual scenes. In a first offline step, a collection of view-independent and view-dependent visual features of the VE are extracted and stored in a visual feature map. Then, in a second offline step, the navigation path is deformed to fit in the confines of the available physical space through a mass-spring system optimization, according to distortion sensitive factors derived from the visual feature map. Finally, a novel detail preserving rendering algorithm is employed to preserve the original visual detail as the user navigates the VE on the redirected path. We tested our method on several scenes, where our method showed a reduced VE 3D mesh distortion, when compared to the path redirection methods without feature guidance.},
	author = {Cao, Antong and Wang, Lili and Liu, Yi and Popescu, Voicu},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00032},
	issn = {2642-5254},
	keywords = {Visualization;Distortion;Feature extraction;Geometry;Navigation;Three-dimensional displays;Legged locomotion;Virtual reality;Navigation;Path redirection},
	month = {March},
	pages = {137-145},
	title = {Feature Guided Path Redirection for VR Navigation},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00032}}

@inproceedings{9089581,
	abstract = {In this paper, we consider the effect of different types of virtual annotations on performance during a navigation task in virtual reality. Two major types of annotations were shown to users: screen-fixed annotations that remained fixed in the user's field of view, and world- fixed annotations that are linked to specific locations in the world. We also considered three different levels of navigation information, including destination markers, maps visualizing the layout of the space being navigated, and path markers showing the optimal route to the destination. We ran a within-subjects study where participants completed three trials with each of the six combinations of annotation type and information level, for a total of 18 trials in a virtual environment. Average speed, distance traveled, and the time taken to reach the destination were recorded during each trial. Participants were also asked to point back to where they started the trial upon reaching the destination, as a measure of spatial memory. Finally, participants were tasked with completing a secondary activity while navigating, so as to assess what effect annotation types had on multitasking performance. Participants navigated significantly more quickly when using world-fixed annotations; however an interaction effect was observed between the type of annotation and the level of information, which suggests that world-fixed annotations are not inherently better than screen-fixed annotations; instead, it is important to consider both the type of annotation and what information it displays.},
	author = {Dominic, James and Robb, Andrew},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00083},
	issn = {2642-5254},
	keywords = {Task analysis;Three-dimensional displays;Space exploration;Aircraft navigation;Virtual reality;Layout;Human factors and ergonomics;locomotion and navigation},
	month = {March},
	pages = {607-615},
	title = {Exploring Effects of Screen-Fixed and World-Fixed Annotation on Navigation in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00083}}

@inproceedings{9089584,
	abstract = {Measuring Visual Latency in VR and AR devices has become increasingly complicated as many of the components will influence others in multiple loops and ultimately affect the human cognitive and sensory perception. In this paper we present a new method based on the idea that the performance of humans on a rapid motor task will remain constant, and that any added delay will correspond to the system latency. We ask users to perform a task inside different video see-through devices and also in front of a computer. We also calculate the latency of the systems using a hardware instrumentation-based measurement technique for bench-marking. Results show that this new form of latency measurement through human cognitive performance can be reliable and comparable to hardware instrumentation-based measurement. Our method is adaptable to many forms of user interaction. It is particularly suitable for systems, such as AR and VR, where externalizing signals is difficult, or where it is important to measure latency while the system is in use by a user.},
	author = {Gruen, Robert and Ofek, Eyal and Steed, Anthony and Gal, Ran and Sinclair, Mike and Gonzalez-Franco, Mar},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00103},
	issn = {2642-5254},
	keywords = {Task analysis;Cameras;Tracking;Visualization;Instruments;Virtual reality;Hardware;Human-centered computing---Virtual reality---;------ Computing methodologies---Perception---},
	month = {March},
	pages = {791-799},
	title = {Measuring System Visual Latency through Cognitive Latency on Video See-Through AR devices},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00103}}

@inproceedings{9089587,
	abstract = {In this work, we propose a framework to simplify the creation of Augmented Reality (AR) extensions for web applications, without modifying the original web applications. We implemented the framework in an open source package called Alpaca. AR extensions developed using Alpaca appear as a web-browser extension, and automatically bridge the Document Object Model (DOM) of the web with the SceneGraph model of AR. To transform the web application into a multi-device, mixed-space web application, we designed a restrictive and minimized interface for cross-device event handling. We demonstrate our approach to develop mixed-space applications using three examples. These applications are, respectively, for exploring Google Books, exploring biodiversity distribution hosted by the National Park Service of the United States, and exploring YouTube's recommendation engine. The first two cases show how a 3rd-party developer can create AR extensions without making any modifications to the original web applications. The last case serves as an example of how to create AR extensions when a developer creates a web application from scratch. Alpaca works on the iPhone X, the Google Pixel, and the Microsoft HoloLens.},
	author = {Hobson, Tanner and Duncan, Jeremiah and Raji, Mohammad and Lu, Aidong and Huang, Jian},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00036},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Graphics;Browsers;Runtime;Servers;Two dimensional displays;Synchronization;Computing methodologies;Computer graphics;Graphics systems and interfaces;Mixed / augmented reality;Computer systems organization;Architectures;Distributed architectures;Client-server architectures},
	month = {March},
	pages = {174-183},
	title = {Alpaca: AR Graphics Extensions for Web Applications},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00036}}

@inproceedings{9089588,
	abstract = {Archiving Performative Objects aimed at applying and conserving puppetry as creative practice in VR. It included 3D scanning and interaction design to capture puppets and their varying control schemes from the archives of the Center for Puppetry Arts. This paper reports on their design and implementation in a VR puppetry set up. Its focuses on the evaluation study (n=18) comparing the interaction of non-experts vs expert puppeteers. The data initially show little differences but a more detailed discussion indicates differing qualitative assessments of puppetry that support its value for VR. Results suggests successful creative activation especially among experts.},
	author = {Nitsche, Michael and McBride, Pierce},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00018},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Human computer interaction;Cultural differences;Art;Media;Virtual reality;Solid modeling;Puppetry;Virtual Reality;interaction design},
	month = {March},
	pages = {10-17},
	title = {Manipulating Puppets in VR},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00018}}

@inproceedings{9089591,
	abstract = {Mechanisms for guiding a user's visual attention to a particular point of interest play a crucial role in areas such as collaborative VR and AR, cinematic VR, and automated or live guided tour experiences in xR-based education. The attention guiding mechanism serves as a communication tool that helps users find entities currently not visible in their view, referenced for instance by another user or in some accompanying audio commentary. We report on a user study in which we compared three different visual guiding mechanisms (arrow, butterfly guide, and radar) in the context of 360$\,^{\circ}$ image-based educational VR tour applications of real-world sites. A fourth condition with no guidance tool available was added as a baseline. We investigate the question: How do the different approaches compare in terms of target finding performance and participants' assessments of the experiences. While all three mechanisms were perceived as improvements over the no-guidance condition and resulted in significantly improved target finding times, the arrow mechanism stands out as the most generally accepted and favored approach, whereas the other two (butterfly guide and radar) received a more polarized assessment due to their specific strengths and drawbacks.},
	author = {Wallgr{\"u}n, Jan Oliver and Bagher, Mahda M. and Sajjadi, Pejman and Klippel, Alexander},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00026},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Virtual reality;Radar;Human computer interaction;Two dimensional displays;Head;Human-centered computing---User studies;Human-centered computing---Virtual reality;Human-centered computing--- Empirical studies in HCI;Human-centered computing---Empirical studies in visualization},
	month = {March},
	pages = {83-91},
	title = {A Comparison of Visual Attention Guiding Approaches for 360$\,^{\circ}$ Image-Based VR Tours},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00026}}

@inproceedings{9089592,
	abstract = {When building virtual reality applications teams must choose between different configurations of the hardware and/or software aspects, and other factors, of the experience. In this paper we extend a framework for assessing how these factors contribute to quality of experience in an example evaluation. We consider how four factors related to avatar expressiveness affect quality of experience: Eye Gaze, Eye Blinking, Mouth Animation, and Microexpressions. 55 participants experienced an avatar delivering a presentation in virtual reality. At fixed times participants had the opportunity to spend a virtual budget to modify the factors to incrementally improve their quality of experience. They could stop making transitions when they felt further changes would make no further difference. From these transitions a Markov matrix was built, along with probabilities of a factor being present at a given level on participants' final configurations. Most participants did not spend the full budget, suggesting that there was a point of equilibrium which did not require maximizing all factor levels. We discuss that point of equilibrium and present this work as an extended contribution to the evaluation of people's responses to immersive virtual environments.},
	author = {Murcia-L{\'o}pez, Mar{\'\i}a and Collingwoode-Williams, Tara and Steptoe, William and Schwartz, Raz and Loving, Timothy J. and Slater, Mel},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00098},
	issn = {2642-5254},
	keywords = {Avatars;Computer graphics;Virtual reality;I.3.7;Computer Graphics;Three-Dimensional Graphics and Realism;Virtual Reality},
	month = {March},
	pages = {747-755},
	title = {Evaluating Virtual Reality Experiences Through Participant Choices},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00098}}

@inproceedings{9089593,
	abstract = {Encountered-Type Haptic Displays (ETHDs) represent a field of haptic displays with the premise of not using any type of actuator directly in contact with the user skin, thus providing an alternative integration of haptic displays in virtual environments. In this paper, we present novel interaction techniques (ITs) dedicated to ETHDs. The techniques aim at addressing the issues commonly presented for these devices such as limited contact areas, lags and unexpected collisions with the user. First, our paper proposes a design framework based on several parameters defining the interactive process between user and ETHD (input, movement control, displacement and contact). Five techniques based on different ramifications of the design space framework were conceived, respectively named: Swipe, Drag, Clutch, Bubble and Follow. Then, a use-case scenario was designed to depict the usage of these techniques on the task of touching and coloring a wide, flat surface. Finally, a user study based on the coloring task was conducted to assess the performance and user experience for each IT. Results were in favor of Drag and Clutch techniques which are based on manual surface displacement, absolute position selection and intermittent contact interaction. Taken together our results and design methodology pave the way to the design of future ITs for ETHDs in virtual environments.},
	author = {Mercado, Victor and Marchai, Maud and L{\'e}cuyer, Anatole},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00042},
	issn = {2642-5254},
	keywords = {Haptic interfaces;Task analysis;Shape;Aerospace electronics;Rendering (computer graphics);Three-dimensional displays;Virtual environments;Encountered-Type Haptic Displays;Interaction Techniques;Haptic Rendering;Human-Machine Interaction},
	month = {March},
	pages = {230-238},
	title = {Design and Evaluation of Interaction Techniques Dedicated to Integrate Encountered-Type Haptic Displays in Virtual Environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00042}}

@inproceedings{9089596,
	abstract = {Collaboration in a group has the potential to achieve more effective solutions for challenging problems, but collaboration per se is not an easy task, rather a stressful burden if the collaboration partners do not communicate well with each other. While Intelligent Virtual Assistants (IVAs), such as Amazon Alexa, are becoming part of our daily lives, there are increasing occurrences in which we collaborate with such IVAs for our daily tasks. Although IVAs can provide important support to users, the limited verbal interface in the current state of IVAs lacks the ability to provide effective non-verbal social cues, which is critical for improving collaborative performance and reducing task load.In this paper, we investigate the effects of IVA embodiment on collaborative decision making. In a within-subjects study, participants performed a desert survival task in three conditions: (1) performing the task alone, (2) working with a disembodied voice assistant, and (3) working with an embodied assistant. Our results show that both assistant conditions led to higher performance over when performing the task alone, but interestingly the reported task load with the embodied assistant was significantly lower than with the disembodied voice assistant. We discuss the findings with implications for effective and efficient collaborations with IVAs while also emphasizing the increased social presence and richness of the embodied assistant.},
	author = {Kim, Kangsoo and de Melo, Celso M. and Norouzi, Nahal and Bruder, Gerd and Welch, Gregory F.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00074},
	issn = {2642-5254},
	keywords = {Human computer interaction;Augmented reality;Decision making;Ubiquitous computing;Virtual assistants;Collaboration;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Mixed / augmented reality;Human-centered computing;Ubiquitous and mobile computing;Ubiquitous and mobile devices;Personal digital assistants;Human-centered computing;Human computer interaction (HCI);HCI design and evaluation methods;User studies},
	month = {March},
	pages = {529-538},
	title = {Reducing Task Load with an Embodied Intelligent Virtual Assistant for Improved Performance in Collaborative Decision Making},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00074}}

@inproceedings{9089598,
	abstract = {Presentation of more haptic information to improve the users interactive capability is an important and challenging task in the virtual environment. As an emerging haptic feedback technology, electrovibration is an underlying solution to enhance the systems interactivity and improve user experience. However, such a solution is rarely studied in a virtual environment. In this work, we explore a new VR interaction method based on electrovibration technology with a touch screen and conduct evaluations about it. The key idea is to incorporate a set of manipulation gestures and three types of electrovibration in the VR interaction to help users acquire different kinds of tactile perception in the virtual manipulation. We present the evaluation in which we compare user performance measured first in a Fitts law task to evaluate different electrovibration types and then in a virtual office application to assess the interactive user interface. Our results show that the precision of interactions is significantly improved with the electrovibration haptic feedback. To the best of our knowledge, this is the first work to introduce electrovibration haptic feedback into the VR human-computer interaction and our work enlightens the potential of the electrovibration touchscreen-based interaction in virtual environments.},
	author = {Zhao, Lu and Liu, Yue and Ye, Dejiang and Ma, Zhuoluo and Song, Weitao},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00043},
	issn = {2642-5254},
	keywords = {Haptic interfaces;Three-dimensional displays;Virtual environments;Task analysis;Thumb;Haptic feedback;electrovibration;interactive mode;virtual reality},
	month = {March},
	pages = {239-247},
	title = {Implementation and Evaluation of Touch-based Interaction Using Electrovibration Haptic Feedback in Virtual Environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00043}}

@inproceedings{9089599,
	abstract = {We propose a mid-air image system for telepresence. Virtual reality (VR) social networks enable users to interact with each other through CG avatars and choose their appearances freely. However, this is only possible in VR space. We propose a system that takes the avatar from VR space to real space with the help of mid-air imaging technology. In this system, the micro-mirror array plates (MMAPs) display the mid-air image and optically transfer the camera viewpoint to capture users from the mid-air image position. Luminance measurement and modulation transfer function (MTF) measurement were performed to evaluate the image capturing performance of this system. As a result, we found that the MMAPs ccause a decrease in brightness and an increase in blur. In addition, the stray light generated by the MMAPs was in the captured video. We also confirmed that face detection works correctly on the captured video by adjusting the ISO sensitivity of the camera. Furthermore, we designed an application for telepresence called Levitar, which uses a dual camera to output the captured video to the HMD and controls the camera gaze direction.},
	author = {Tsuchiya, Kei and Koizumi, Naoya},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00029},
	issn = {2642-5254},
	keywords = {Cameras;Avatars;Optical imaging;High-speed optical techniques;Telepresence;Optical sensors;Optical refraction;Human-centered computing;Human computer interaction (HCI);Interaction devices;Displays and imagers},
	month = {March},
	pages = {108-116},
	title = {An Optical Design for Avatar-User Co-axial Viewpoint Telepresence},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00029}}

@inproceedings{9089604,
	abstract = {Past work in augmented reality has shown that temperature-associated AR stimuli can induce warming and cooling sensations in the user, and prior work in psychology suggests that a person's body temperature can influence that person's sense of subjective perception of duration. In this paper, we present a user study to evaluate the relationship between temperature-associated virtual stimuli presented on an AR-HMD and the user's sense of subjective perception of duration and temperature. In particular, we investigate two independent variables: the apparent temperature of the virtual stimuli presented to the participant, which could be hot or cold, and the location of the stimuli, which could be in direct contact with the user, in indirect contact with the user, or both in direct and indirect contact simultaneously. We investigate how these variables affect the users' perception of duration and perception of body and environment temperature by having participants make prospective time estimations while observing the virtual stimulus and answering subjective questions regarding their body and environment temperatures. Our work confirms that temperature-associated virtual stimuli are capable of having significant effects on the users' perception of temperature, and highlights a possible limitation in the current augmented reality technology in that no secondary effects on the users' perception of duration were observed.},
	author = {Erickson, Austin and Bruder, Gerd and Wisniewski, Pamela J. and Welch, Gregory F.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00070},
	issn = {2642-5254},
	keywords = {Psychology;Augmented reality;Head-mounted displays;Virtual reality;Computer graphics;Computer graphics;Graphics systems and interfaces;Virtual reality;Human-centered computing;Interaction paradigms;Virtual reality},
	month = {March},
	pages = {493-499},
	title = {Examining Whether Secondary Effects of Temperature-Associated Virtual Stimuli Influence Subjective Perception of Duration},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00070}}

@inproceedings{9089606,
	abstract = {Air traffic control officers (ATCOs) are specialized workers responsible to monitor and guide airplanes in their assigned airspace. Such a task is highly visual and mainly supported by 2D visualizations. In this paper, we designed and assessed an application for visualizing air traffic in both orthographic (2D) and perspective (3D) views. A user study was then performed to compare these two types of representations in terms of situation awareness, workload, performance, and user acceptance. Results show that the 3D view yielded both higher situation awareness and less workload than the 2D view condition. However, such a performance does not match the opinion of the ATCOs about the 3D representation.},
	author = {Rottermanner, Gernot and de Jesus Oliveira, Victor Adriel and Lechner, Patrik and Graf, Philipp and Kreiger, Mylene and Wagner, Markus and Iber, Michael and Rokitansky, Carl-Herbert and Eschbacher, Kurt and Grantz, Volker and Settgast, Volker and Judmaier, Peter},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00011},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Airplanes;Prototypes;Two dimensional displays;Visualization;Task analysis;User interfaces;Human-centered computing;User studies;Humancentered computing;Graphical user interfaces;User interface design;User centered design},
	month = {March},
	pages = {885-892},
	title = {Design and Evaluation of a Tool to Support Air Traffic Control with 2D and 3D Visualizations},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00011}}

@inproceedings{9089608,
	abstract = {We compare two methods for characterizing the angular dependence of the spatial resolution in virtual reality head-mounted displays (HMDs) by measuring the line spread response (LSR) across the field of view (FOV) of the device. While slanted-edge is the standard method for determining the resolution of cameras, the standard approach for display devices is to used a line or edge aligned to the display pixel array. However, applying the LSR to head-mounted displays (HMDs) presents additional challenges due to the neareye optics. The LSRs of the HTC Vive and HTC Vive Pro were measured using a line of single white pixels by setting the red, green, and blue subpixels at maximum driving level. The white line was swept along a single direction over a 30$\,^{\circ}$ range in the FOV and the spatial resolution was measured using two approaches: wide-field and angle-scanning. In the wide-field method, the 30$\,^{\circ}$ FOV is imaged onto a stationary camera. In the second method, the camera is rotated across the FOV such that the white line remains static on the camera with the rotation axis located behind the lens to mimic the human visual system. The results show that the wide-field method overestimates the spatial resolution of the HMD by approximately 40% for angles larger than 10$\,^{\circ}$. Consistent results obtained for the Vive and the Vive Pro indicate that the cause of the resolution limitation depends on the location in the FOV. The limitation in the center of the FOV is the pixel density, whereas, the off-axis spatial resolution is limited by optical components. Achieving high resolution VR HMDs requires system-wide design and technology improvement.},
	author = {Beams, Ryan and Collins, Brendan and Kim, Andrea S. and Badano, Aldo},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00108},
	issn = {2642-5254},
	keywords = {Cameras;Spatial resolution;Resists;Apertures;Distortion measurement;Optical imaging;Lenses;Human-centered computing;Virtual Reality;Human-centered computing;Visualization;Visualization design and evaluation methods},
	month = {March},
	pages = {836-841},
	title = {Angular Dependence of the Spatial Resolution in Virtual Reality Displays},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00108}}

@inproceedings{9089609,
	abstract = {We propose "TEllipsoid", an ellipsoidal display for video conference systems that can provide not only accurate eye-gaze transmission but also practicality in conferences, namely the convenience to use and the preservation of the identity of the displayed face.The display comprises an ellipsoidal screen, a small projector, and a convex mirror, where the bottom-installed projector projects the facial image of a remote participant onto the screen via the convex mirror. The facial image is made from photos shot from 360 degrees around the participant. Moreover, the image is modified to improve identity. The gaze representation is implemented by projecting the 3D model of eyeballs onto a virtual ellipsoidal screen.We evaluated the gaze transmissibility of the display in conference situations. As a result of experiments, we concluded that accurate gaze transmission is available in conferences when the angular distance of the adjacent participants is more than 38.5 degrees.},
	author = {Ichii, Taro and Mitake, Hironori and Hasegawa, Shoichi},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00101},
	issn = {2642-5254},
	keywords = {Face;Prototypes;Three-dimensional displays;Mirrors;Shape;Conferences;Human-centered computing---Human computer interaction (HCI)---Interaction devices---Displays and imagers;Hardware---Communication hardware, interfaces and storage--- Displays and imagers},
	month = {March},
	pages = {775-781},
	title = {TEllipsoid: Ellipsoidal Display for Videoconference System Transmitting Accurate Gaze Direction},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00101}}

@inproceedings{9089611,
	abstract = {Virtual Reality enables the exploration of large information spaces. In physically constrained spaces such as airplanes or buses, controller-based or mid-air interaction in mobile Virtual Reality can be challenging. Instead, the input space on and above touchscreen enabled devices such as smartphones or tablets could be employed for Virtual Reality interaction in those spaces.In this context, we compared an above surface interaction technique with traditional 2D on-surface input for navigating large planar information spaces such as maps in a controlled user study (n = 20). We find that our proposed above surface interaction technique results in significantly better performance and user preference compared to pinch-to-zoom and drag-to-pan when navigating planar information spaces.},
	author = {Menzner, Tim and Gesslein, Travis and Otte, Alexander and Grubert, Jens},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00057},
	issn = {2642-5254},
	keywords = {Navigation;Two dimensional displays;Three-dimensional displays;Sensors;Aerospace electronics;Virtual reality;User interfaces;Human-centered computing;Visualization;Visualization techniques;Treemaps;Human-centered computing;Visualization;Visualization design;evaluation methods},
	month = {March},
	pages = {372-381},
	title = {Above Surface Interaction for Multiscale Navigation in Mobile Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00057}}

@inproceedings{9089612,
	abstract = {Recent advances in 3D reconstruction and tracking technologies have made it possible to volumetrically capture human body and performance at real time. In the field of human-computer interaction, however, no works have been reported on the user study made with such volumetric capture avatars. This paper investigates how the volumetric capture avatar affects users' sense of social presence in immersive virtual environments. In our experiments, the volumetric capture avatar of an actor is compared with the actor captured in 2D video and another 3D avatar obtained by pre-scanning the actor. The experiment results show that users have the highest sense of social presence with the volumetric capture avatar when performing dynamic tasks whereas they have higher sense of social presence with the volumetric capture avatar and 2D video than with the pre-scanned avatar when performing static tasks. These imply that the emerging volumetric capture techniques can be an attractive tool for mixed reality, telepresence, and many other 3D applications.},
	author = {Cho, SungIk and Kim, Seung-wook and Lee, JongMin and Ahn, JeongHyeon and Han, JungHyun},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00020},
	issn = {2642-5254},
	keywords = {Avatars;Three-dimensional displays;Cameras;Two dimensional displays;Image reconstruction;Real-time systems;Resists;Human-centered computing;Visualization;Visualization techniques;Human-centered computing;Visualization;Visualization application domains},
	month = {March},
	pages = {26-34},
	title = {Effects of volumetric capture avatars on social presence in immersive virtual environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00020}}

@inproceedings{9089617,
	abstract = {It is important to include an olfactory cue to enhance the reality in the virtual environment. We have developed the virtual olfactory environment where a user searches for an odor source. The virtual olfactory environment was prepared using computational fluid dynamics calculation. It enables us to have the dynamic odor concentration distribution even if we have complicated obstacles in the virtual environment. Moreover, we developed the wearable olfactory display made up of multiple micro dispensers and SAW (Surface Acoustic Wave) device so that the rapid switching of the smells could be achieved. The wearable olfactory display was attached beneath a head mount display to present a smell quickly. We made the virtual environment of the two-story building where four rooms were located at each floor. A user searched for the source of smoke smell located at one room among four ones at the second floor since we simulated the fire at the early stage. A half of the users could reach the correct source locations in the experiment.},
	author = {Nakamoto, Takamichi and Hirasawa, Tatsuya and Hanyu, Yukiko},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00094},
	issn = {2642-5254},
	keywords = {Olfactory;Surface acoustic wave devices;Liquids;Surface acoustic waves;Virtual environments;Floors;Solid modeling;Wearable olfactory display;CFD;disaster simulator;micro dispenser;SAW atomizer},
	month = {March},
	pages = {713-720},
	title = {Virtual environment with smell using wearable olfactory display and computational fluid dynamics simulation},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00094}}

@inproceedings{9089620,
	abstract = {Virtual reality (VR) technologies provide a shared platform for collaboration among users in a spatial context. To enhance the quality of social signals during interaction between users, researchers and practitioners started augmenting users' interpersonal space with different types of virtual embodied social cues. A prominent example is commonly referred to as the "Big Head" technique, in which the head scales of virtual interlocutors are slightly increased to leverage more of the display's visual space to convey facial social cues. While beneficial in improving interpersonal social communication, the benefits and thresholds of human perception of facial cues and comfort in such Big Head environments are not well understood, limiting their usefulness and subjective experience.In this paper, we present a human-subject study that we conducted to understand the impact of an increased or decreased head scale in social VR on participants' ability to perceive facial expressions as well as their sense of comfort and feeling of "uncanniness." We explored two head scaling methods and compared them with respect to perceptual thresholds and user preferences. We further show that the distance to interlocutors has an important effect on the results. We discuss implications and guidelines for practical applications that aim to leverage VR-enhanced social cues.},
	author = {Choudhary, Zubin and Kim, Kangsoo and Schubert, Ryan and Bruder, Gerd and Welch, Gregory F.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00063},
	issn = {2642-5254},
	keywords = {Avatars;Games;Virtual reality;Computer graphics;Human computer interaction;Virtual environments;Computing methodologies---Computer graphics---Graphics systems and interfaces---Virtual reality;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies},
	month = {March},
	pages = {425-433},
	title = {Virtual Big Heads: Analysis of Human Perception and Comfort of Head Scales in Social Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00063}}

@inproceedings{9089623,
	abstract = {The Ebbinghaus illusion, also known as Titchner Circles, is a well- known perceptual illusion affecting the perceived size of a disc enclosed by an annulus of either larger or smaller discs. Though many have found highly consistent results with regard to the effect of the illusion on size perception, there have been mixed results when studying its effect on action-based tasks. In this paper, we present a study utilizing a head-worn virtual environment to examine the effect of the Ebbinghaus illusion on depth judgments as measured using a blind-reaching task. We found that participants' size judgments were symmetrically affected by the classic "large annulus" and "small annulus" configurations, but their distance judgments were asymmetrically affected. Large annulus configurations had no significant effect on distance judgments while small annulus configurations resulted in significant underestimation of target distances. Despite this asymmetry, both configurations resulted in response times of similar magnitude that were significantly longer than those of the non-illusory control condition.},
	author = {Finney, Hunter and Jones, J. Adam},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00079},
	issn = {2642-5254},
	keywords = {Task analysis;Virtual environments;Atmospheric measurements;Particle measurements;Visualization;Time factors;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism---Virtual Reality;I.4.8 [Scene Analysis]: Depth Cues;H.5.1 [Information Systems]: Multimedia Information Systems---Artificial, Augmented, and Virtual Realities;H.1.2 [Information Systems]: User/Machine Systems---Human Factors},
	month = {March},
	pages = {573-578},
	title = {Asymmetric Effects of the Ebbinghaus Illusion on Depth Judgments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00079}}

@inproceedings{9089629,
	abstract = {Especially in the field of mechanical engineering, the market pressure for small and medium-sized enterprises increases because of faster developments and more complex designs. Nevertheless, standards and ergonomic safety regulations must be observed. In recent years, various applications were presented to help users understand and comply with inconvenient requirements. However, the time-consuming tools are primarily for ergonomics experts and often overwhelm engineers from small and medium-sized enterprises. We present an immersive concept that allows inexperienced users to quickly assess the ergonomic parameters of the visual field, represented by easy-to-understand visualizations. The solution is compared with standard market and scientific approaches.},
	author = {G{\"u}nther, Tobias and Hilgers, Inga-Lisa and Groh, Rainer and Schmauder, Martin},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00062},
	issn = {2642-5254},
	keywords = {Ergonomics;Tools;Solid modeling;Visualization;Prototypes;Three-dimensional displays;Virtual environments;Human-centered computing;Ergonomics, Human-centered computing;Virtual reality, Human-centered computing;User centered design, Human-centered computing;Assistance},
	month = {March},
	pages = {416-424},
	title = {Visualization and evaluation of ergonomic visual field parameters in first person virtual environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00062}}

@inproceedings{9089632,
	abstract = {With the advancement of Artificial Intelligence technology to make smart devices, understanding how humans develop trust in virtual agents is emerging as a critical research field. Through our research, we report on a novel methodology to investigate user's trust in auditory assistance in a Virtual Reality (VR) based search task, under both high and low cognitive load and under varying levels of agent accuracy. We collected physiological sensor data such as electroencephalography (EEG), galvanic skin response (GSR), and heart-rate variability (HRV), subjective data through questionnaire such as System Trust Scale (STS), Subjective Mental Effort Questionnaire (SMEQ) and NASA-TLX. We also collected a behavioral measure of trust (congruency of users' head motion in response to valid/ invalid verbal advice from the agent). Our results indicate that our custom VR environment enables researchers to measure and understand human trust in virtual agents using the matrices, and both cognitive load and agent accuracy play an important role in trust formation. We discuss the implications of the research and directions for future work.},
	author = {Gupta, Kunal and Hajika, Ryo and Pai, Yun Suen and Duenser, Andreas and Lochner, Martin and Billinghurst, Mark},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00099},
	issn = {2642-5254},
	keywords = {Task analysis;Electroencephalography;Physiology;Heart rate variability;Games;Virtual reality;Shape;H.5.2 [User Interfaces]: Evaluation/methodology;H.1.2 [User/Machine Systems]: Human factors;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems--- Artificial;augmented;and virtual realities},
	month = {March},
	pages = {756-765},
	title = {Measuring Human Trust in a Virtual Assistant using Physiological Sensing in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00099}}

@inproceedings{9089634,
	abstract = {Training is one of the major use cases of Virtual Reality (VR) due to the flexibility and reproducibility of VR simulations. However, the use of the user's cognitive state, and in particular mental workload (MWL), remains largely unexplored in the design of training scenarios. In this paper, we propose to consider MWL for the design of complex training scenarios involving multiple parallel tasks in VR. The proposed approach is based on the assessment of the MWL elicited by each potential task configuration in the training application. Following the assessment, the resulting model is then used to create training scenarios able to modulate the user's MWL over time. This approach is illustrated by a VR flight training simulator based on the Multi-Attribute Task Battery II, which solicits different cognitive resources, able to generate 12 different tasks configurations. A first user study (N = 38) was conducted to assess the MWL for each task configuration using self-reports and performance measurements. This assessment was then used to generate three training scenarios in order to induce different levels of MWL over time. A second user study (N = 14) confirmed that the proposed approach was able to induce the expected mental workload over time for each training scenario. These results pave the way to further studies exploring how MWL modulation can be used to improve VR training applications.},
	author = {Luong, Tiffany and Argelaguet, Ferran and Martin, Nicolas and Lecuyer, Anatole},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00089},
	issn = {2642-5254},
	keywords = {Task analysis;Training;Physiology;Virtual reality;Brain modeling;Electroencephalography;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	month = {March},
	pages = {662-671},
	title = {Introducing Mental Workload Assessment for the Design of Virtual Reality Training Scenarios},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00089}}

@inproceedings{9089635,
	abstract = {Augmented Reality (AR) benefits telementoring by enhancing the communication between the mentee and the remote mentor with mentor authored graphical annotations that are directly integrated into the mentee's view of the workspace. An important problem is conveying the workspace to the mentor effectively, such that they can provide adequate guidance. AR headsets now incorporate a frontfacing video camera, which can be used to acquire the workspace. However, simply providing to the mentor this video acquired from the mentee's first-person view is inadequate. As the mentee moves their head, the mentor's visualization of the workspace changes frequently, unexpectedly, and substantially. This paper presents a method for robust high-level stabilization of a mentee first-person video to provide effective workspace visualization to a remote mentor. The visualization is stable, complete, up to date, continuous, distortion free, and rendered from the mentee's typical viewpoint, as needed to best inform the mentor of the current state of the workspace. In one study, the stabilized visualization had significant advantages over unstabilized visualization, in the context of three number matching tasks. In a second study, stabilization showed good results, in the context of surgical telementoring, specifically for cricothyroidotomy training in austere settings.},
	author = {Lin, Chengyuan and Rojas-Mu{\~n}oz, Edgar and Cabrera, Maria Eugenia and Sanchez-Tamayo, Natalia and Andersen, Daniel and Popescu, Voicu and Barragan Noguera, Juan Antonio and Zarzaur, Ben and Murphy, Pat and Anderson, Kathryn and Douglas, Thomas and Griffis, Clare and Wachs, Juan},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00040},
	issn = {2642-5254},
	keywords = {Cameras;Visualization;Geometry;Task analysis;Feeds;Resists;Three-dimensional displays;Human-centered computing;Visualization;Graphics systems and interfaces;Mixed/augmented reality},
	month = {March},
	pages = {212-220},
	title = {How About the Mentor? Effective Workspace Visualization in AR Telementoring},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00040}}

@inproceedings{9089636,
	abstract = {Trapped air in liquid is an important factor which affect the realism of fluid simulation. However, due to the complex physical properties, simulating the interaction and transformation between air and Liquid is extremely challenging and time-consuming. In this paper, we propose a multi-scale simulation method under particle-based framework to achieve the realistic and efficient simulation of air-liquid fluid. A unified generation rule is proposed according to the kinetic energy and the velocity difference between fluid particles. Two velocity-based dynamic models are then established for different size of air materials respectively. The Brownian motion of small scale air materials is achieved by Schilk random function. The interaction and air transfer between large scale air materials is achieved by inverse diffusion equation and a new high-order kernel function. Experimental results show that the proposed method can improve the fidelity and richness of the fluid simulation. The post-processing scheme makes it able to be integrated with existing particle method easily.},
	author = {Liu, Sinuo and Wang, Ben and Ban, Xiaojuan},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00109},
	issn = {2642-5254},
	keywords = {Atmospheric modeling;Solid modeling;Mathematical model;Liquids;Computational modeling;Visual effects;Couplings;Computing methodologies;Computer graphics;Animation;Physical simulation},
	month = {March},
	pages = {842-850},
	title = {Multiple-scale Simulation Method for Liquid with Trapped Air under Particle-based Framework},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00109}}

@inproceedings{9089637,
	abstract = {When we are walking in crowds, we mainly use visual information to avoid collisions with other pedestrians. Thus, gaze activity should be considered to better understand interactions between people in a crowd. In this work, we use Virtual Reality (VR) to facilitate motion and gaze tracking, as well as to accurately control experimental conditions, in order to study the effect of crowd density on eye-gaze behavior. Our motivation is to better understand how interaction neighborhood (i.e., the subset of people actually influencing one's locomotion trajectory) changes with density. To this end, we designed two experiments. The first one evaluates the biases introduced by the use of VR on the visual activity when walking among people, by comparing eye-gaze activity while walking in a real and virtual street. We then designed a second experiment where participants walked in a virtual street with different levels of pedestrian density. We demonstrate that gaze fixations are performed at the same frequency despite increases in pedestrian density, while the eyes scan a narrower portion of the street. These results suggest that in such situations walkers focus more on people in front and closer to them. These results provide valuable insights regarding eye-gaze activity during interactions between people in a crowd, and suggest new recommendations in designing more realistic crowd simulations.},
	author = {Berton, Florian and Hoyet, Ludovic and Olivier, Anne-H{\'e}l{\`e}ne and Bruneau, Julien and Le Meur, Olivier and Pettre, Julien},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00052},
	issn = {2642-5254},
	keywords = {Legged locomotion;Virtual reality;Visualization;Collision avoidance;Trajectory;Navigation;Solid modeling;Gaze Activity;Locomotion;Crowd;Virtual Reality;Eye-tracking;Collision Avoidance;Human-centered computing;Visualization;Visualization techniques;Treemaps;Visualization design;evaluation methods},
	month = {March},
	pages = {322-331},
	title = {Eye-Gaze Activity in Crowds: Impact of Virtual Reality and Density},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00052}}

@inproceedings{9089640,
	abstract = {Current virtual reality (VR) head-mounted displays (HMDs) are characterized by a low angular resolution that makes it difficult to make out details, leading to reduced legibility of text and increased visual fatigue. Light-on-dark graphics modes, so-called "dark mode" graphics, are becoming more and more popular over a wide range of display technologies, and have been correlated with increased visual comfort and acuity, specifically when working in low-light environments, which suggests that they might provide significant advantages for VR HMDs.In this paper, we present a human-subject study investigating the correlations between the color mode and the ambient lighting with respect to visual acuity and fatigue on VR HMDs. We compare two color schemes, characterized by light letters on a dark background (dark mode), or dark letters on a light background (light mode), and show that the dark background in dark mode provides a significant advantage in terms of reduced visual fatigue and increased visual acuity in dim virtual environments on current HMDs. Based on our results, we discuss guidelines for user interfaces and applications.},
	author = {Erickson, Austin and Kim, Kangsoo and Bruder, Gerd and Welch, Gregory F.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00064},
	issn = {2642-5254},
	keywords = {Visualization;Fatigue;Color;Image color analysis;Virtual reality;Lighting;Resists;Computer graphics---Graphics systems and interfaces---Virtual reality;Human-centered computing--- Interaction paradigms---Virtual reality},
	month = {March},
	pages = {434-442},
	title = {Effects of Dark Mode Graphics on Visual Acuity and Fatigue with Virtual Reality Head-Mounted Displays},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00064}}

@inproceedings{9089644,
	abstract = {Manipulating virtual objects using bare hands has been an attractive interaction paradigm in virtual and augmented reality due to its intuitive nature. However, one limitation of freehand input lies in the ambiguous resulting effect of the interaction. The same gesture performed on a virtual object could invoke different operations on the object depending on the context, object properties, and user intention. We present an experimental analysis of a set of disambiguation techniques in a virtual reality environment, comparing three input modalities (head gaze, speech, and foot tap) paired with three different timings in which options appear to resolve ambiguity (before, during, and after an interaction). The results indicate that using head gaze for disambiguation during an interaction with the object achieved the best performance.},
	author = {Chen, Di Laura and Balakrishnan, Ravin and Grossman, Tovi},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00048},
	issn = {2642-5254},
	keywords = {Task analysis;Three-dimensional displays;Timing;Virtual reality;Human computer interaction;Feedforward systems;Freehand gestures;VR;uncertainty;ambiguity;Human-centered computing;Human computer interaction (HCI);Interaction techniques;Gestural input},
	month = {March},
	pages = {285-292},
	title = {Disambiguation Techniques for Freehand Object Manipulations in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00048}}

@inproceedings{9089645,
	abstract = {As Virtual Reality (VR) devices become more accessible, a multitude of VR applications engage users in highly immersive virtual environments that feature realistic graphics, real-life scenarios, and self-avatars. Many of these simulations require users to make spontaneous affordance judgments such as stepping over obstacles, passing through gaps, etc. which are shown to be affected by the nature of our self-representation in the virtual world. As the technology for creating self-avatars becomes more widely available, it is important to explore how various affordance judgments are affected by the presence of self-avatars. In this work, we investigate the effects of body-scaled self-avatars on the affordance of passability in a natural setting. We implemented a gender-matched body-scaled self-avatar using HTC Vive trackers and evaluated how passability judgments for a sliding doorway in VR, with and without an avatar, compared to the real world judgments. The results suggest that passability judgments are more conservative in VR as compared to the real world. However, the presence of a self-avatar does not significantly affect passability judgments made in VR. This does not align with previous findings which show that having a self-avatar improves judgments and estimates.},
	author = {Bhargava, Ayush and Solini, Hannah and Lucaites, Kathryn and Bertrand, Jeffrey W. and Robb, Andrew and Pagano, Christopher C. and Babu, Sabarish V.},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00073},
	issn = {2642-5254},
	keywords = {Affordances;Virtual environments;Avatars;Virtual reality;Real-time systems;Interactive systems;Self-Avatars;Affordance;Passability;Virtual Reality;Human-centered computing;Empirical studies in HCI;Human-centered computing;Interaction design},
	month = {March},
	pages = {519-528},
	title = {Comparative Evaluation of Viewing and Self-Representation on Passability Affordances to a Realistic Sliding Doorway in Real and Immersive Virtual Environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00073}}

@inproceedings{9089646,
	abstract = {Encouraged by technological advancements, more and more companies consider virtual reality (VR) for training of their workforce in particular for situations that occur rarely, are dangerous, expensive, or very difficult to recreate in the real world. Thereby the need arises for understanding the potentials and limitations of VR training and establishing best practices. In pursuit of this, we have developed a VR Training simulation for a use case at Grundfos, in which apprentices learn a sequential maintenance task. We evaluated this simulation in a user study with 36 participants, comparing it to two traditional forms of training (Pairwise Training and Video Training). This paper describes the developed virtual training scenario and discusses design considerations for such VR simulations. Further, it presents the results of our evaluation, which support that VR Training is effective in teaching the procedure of a maintenance task. However, according to our evidence, traditional approaches with hands-on experience still lead to a significantly better outcome.},
	author = {Winther, Frederik and Ravindran, Linoj and Svendsen, Kasper Paab{\o}l and Feuchtner, Tiare},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00097},
	issn = {2642-5254},
	keywords = {Training;Task analysis;Maintenance engineering;Solid modeling;Virtual reality;Computational modeling;Manuals;Human-centered computing---Virtual reality;Human-centered computing---User studies},
	month = {March},
	pages = {738-746},
	title = {Design and Evaluation of a VR Training Simulation for Pump Maintenance Based on a Use Case at Grundfos},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00097}}

@inproceedings{9089649,
	abstract = {Currently, immersive virtual reality is experienced through head-mounted displays while the user is physically present into a real, cluttered environment. This causes the problem of avoiding dangerous collisions with obstacles in the real environment that are invisible to the user, and also hampers the interaction with real objects. Following the augmented virtuality paradigm, these obstacles should be embedded into the virtual environment. Thus, there is the need of knowing the 3D structure of the real environment to align it with the virtual one. In this paper, we present a method to create a virtual scenario composed of virtual objects having the same spatial occupancy of the corresponding real ones. The real scene is scanned to detect the position and bounding box of objects and obstacles, then virtual elements having similar spatial properties are added to the virtual scene. Two different real environment structure detection and clustering techniques are described and compared, both quantitatively and by considering users' sense of presence with respect to the standard Chaperone technique. Our results show that the method is a good solution to maintain the real environment awareness while keeping an high level of immersivity and sense of presence.},
	author = {Valentini, Ivan and Ballestin, Giorgio and Bassano, Chiara and Solari, Fabio and Chessa, Manuela},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00022},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Virtual environments;Solid modeling;Computational modeling;Transmission line matrix methods;Resists;Human-centered computing;Interaction paradigms;Virtual Reality;Human-centered computing;Interaction paradigms;Mixed / augmented reality},
	month = {March},
	pages = {44-52},
	title = {Improving Obstacle Awareness to Enhance Interaction in Virtual Reality},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00022}}

@inproceedings{9089654,
	abstract = {Telepresence avatars enable users in different environments to interact with each other. In order to increase the effectiveness of these interactions, however, the movements of avatars must be adjusted accordingly to account for differences between user environments. For instance, if a user moves from one point to another in one environment, the avatar's locomotion speed must be adjusted to move to the corresponding target point in another environment at the same time. Several locomotion styles can be used to achieve this speed change. This paper investigates how different avatar locomotion styles (speed, stride, and glide), body visibility levels (full body and head-to-knee), and views (front views and side views) influence human perceptions of the naturalness of motion, similarity to the user's locomotion, and the degree of preserving the user's intention. Our results indicate that 1) speed and stride styles are perceived as more natural than the glide style, while the glide style is more intention-preserving than the others, 2) a greater locomotion speed of the avatar is perceived as more natural, similar, and intention-preserving than slower motion, 3) the perception of naturalness has the greatest impact on people's preferences for locomotion styles, and that 4) head-to-knee body visibility may enhance the perception of naturalness for the glide style.},
	author = {Choi, Youjin and Lee, Jeongmi and Lee, Sung-Hee},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00017},
	issn = {2642-5254},
	keywords = {Avatars;Legged locomotion;Telepresence;Torso;Three-dimensional displays;Conferences;Telepresence;Motion Retargeting;Perception;Virtual Avatar},
	month = {March},
	pages = {1-9},
	title = {Effects of Locomotion Style and Body Visibility of a Telepresence Avatar},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00017}}

@inproceedings{9089655,
	abstract = {Prior studies have explored the possibility of inducing social anxiety (SA) in virtual reality (VR). Among various existing protocols for this purpose, the Trier Social Stress Test (TSST) has been proven to be robust in evoking SA in the majority of participants in both in vivo as well as VR conditions. The TSST consists of giving a speech and performing mental arithmetic calculations each for five minutes in front of three persons. In this paper, we present an adaptation of TSST to investigate the effects of different numbers of virtual humans (VHs) (i.e., three, six, or fifteen) on perceived SA. In addition, we compare the results with an in vivo TSST with three real persons in the audience. Twenty four participants took part in this experiment. As a result, physiological arousal could be observed with VR inducing SA yet less than in vivo TSST. Furthermore, some of the subjective measures showed a high state of anxiety experienced during the experiment. An effect of the virtual audience size could be observed only in heart rate (HR) as a virtual audience size of three VHs induced the highest HR responses which was significantly different from an audience of size six and fifteen.},
	author = {Mostajeran, Fariba and Balci, Melik Berk and Steinicke, Frank and K{\"u}hn, Simone and Gallinat, J{\"u}rgen},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00050},
	issn = {2642-5254},
	keywords = {Stress;In vivo;Protocols;Public speaking;Heart rate;Virtual reality;Psychology;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality},
	month = {March},
	pages = {303-312},
	title = {The Effects of Virtual Audience Size on Social Anxiety during Public Speaking},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00050}}

@inproceedings{9089656,
	abstract = {In this paper, we propose an optimization-based approach for automatically generating virtual scenarios for wheelchair training in virtual reality. To generate a virtual training scenario, our approach automatically generates a realistic furniture layout for a scene as well as a training path that the user needs to go through by controlling a simulated wheelchair. The training properties of the path, namely, its desired length, the extent of rotation, and narrowness, are optimized so as to deliver the desired training effects. We conducted an evaluation to validate the efficacy of the proposed virtual reality training approach. Users showed improvement in wheelchair control skills in terms of proficiency and precision after receiving the proposed virtual reality training.},
	author = {Li, Wanwan and Talavera, Javier and Samayoa, Amilcar Gomez and Lien, Jyh-Ming and Yu, Lap-Fai},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00075},
	issn = {2642-5254},
	keywords = {Training;Wheelchairs;Virtual reality;Solid modeling;Layout;Optimization;Task analysis;Virtual Reality;Modeling and Simulation;Wheelchair Training Simulator},
	month = {March},
	pages = {539-547},
	title = {Automatic Synthesis of Virtual Wheelchair Training Scenarios},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00075}}

@inproceedings{9089660,
	abstract = {In this paper we examine the extent of functional reaching space, or peripersonal space, in immersive three-dimensional virtual reality. In the real world a person's peripersonal space boundaries can be altered by factors in the environment and by social context. We completed two studies with visual and tactile stimuli to determine peripersonal space boundaries. These studies investigated whether peripersonal space boundaries in an immersive virtual environment are consistent with those in the real world, and could be altered by object and virtual agent interactions. We found that while peripersonal space boundaries were consistent with those in the real world, they were responsive to object and agent interactions. Moreover, while people's reactions to the objects and agents varied, the peripersonal space boundaries remained consistent. These findings have potential implications for the design of virtual environments.},
	author = {Buck, Lauren E. and Park, Sohee and Bodenheimer, Bobby},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00053},
	issn = {2642-5254},
	keywords = {Virtual environments;Visualization;Task analysis;Tools;Three-dimensional displays;Avatars;Human-centered computing;Human computer interaction (HCI);Interaction paradigms;Virtual reality;Interaction design;Empirical studies in interaction design},
	month = {March},
	pages = {332-342},
	title = {Determining Peripersonal Space Boundaries and Their Plasticity in Relation to Object and Agent Characteristics in an Immersive Virtual Environment},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00053}}

@inproceedings{9089661,
	abstract = {Latency in Virtual Reality (VR) applications can have numerous detrimental effects, e.g., a hampered user experience, a reduced user performance, or the occurrence of cybersickness. In VR environments, latency usually is measured as Motion-to-Photon (MTP) latency and reported as a mean value. This mean is taken during some specific intervals of sample runs with the target system, often detached in significant aspects from the final target scenario, to provide the necessary boundary conditions for the measurements. Additionally, the reported mean value is agnostic to dynamic and spiking latency behavior. This paper introduces an apparatus that is capable of determining per-frame MTP latency to capture dynamic MTP latency and latency jitter in addition to the commonly reported mean values of latency. The approach is evaluated by measuring MTP latency of a VR simulation based on the Unreal engine and the HTC Vive as a typical consumer-grade Head-Mounted Display (HMD). In contrast to previous approaches, the system does not rely on the HMD to be fixed to an external apparatus, can be used to assess any simulation setup, and can be extended to continuously measure latency during run-time. We evaluate the accuracy of our apparatus by injecting a controlled artificial latency in a VR simulation. We show that latency jitter artifacts already occur without system load, potentially caused by the tracking of the specific HMD, and how mean latency and jitter increase under system load, leading to dropped frames and an overall degraded system performance. The presented system can be used to monitor latency and latency jitter as critical simulation characteristics necessary to report and control to avoid unwanted effects and detrimental system performance.},
	author = {Stauffert, Jan-Philipp and Niebling, Florian and Latoschik, Marc Erich},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00086},
	issn = {2642-5254},
	keywords = {Resists;Photodiodes;Cameras;Jitter;Tracking;Load modeling;Time measurement;D.4.8 [Operating Systems]: Performance---Measurements;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems---Artificial, augmented, and virtual realities},
	month = {March},
	pages = {636-644},
	title = {Simultaneous Run-Time Measurement of Motion-to-Photon Latency and Latency Jitter},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00086}}

@inproceedings{9089663,
	abstract = {3D selection in dense VR environments (e.g., point clouds) is extremely challenging due to occlusion and imprecise mid-air input modalities (e.g., 3D controllers and hand gestures). In this paper, we propose "Slicing-Volume", a hybrid selection technique that enables simultaneous 3D interaction in mid-air, and a 2D pen-and-tablet metaphor in VR. Inspired by well-known slicing plane techniques in data visualization, our technique consists of a 3D volume that encloses target objects in mid-air, which are then projected to a 2D tablet view for precise selection on a tangible physical surface. While slicing techniques and tablets-in-VR have been previously explored, in this paper, we evaluated the potential of this hybrid approach to improve accuracy in highly occluded selection tasks, comparing different multimodal interactions (e.g., Mid-air, Virtual Tablet and Real Tablet). Our results showed that our hybrid technique significantly improved overall accuracy of selection compared to Mid-air selection only, thanks to the added haptic feedback given by the physical tablet surface, rather than the added visualization given by the tablet view.},
	author = {Montano-Murillo, Roberto A. and Nguyen, Cuong and Kazi, Rubaiat Habib and Subramanian, Sriram and DiVerdi, Stephen and Martinez-Plasencia, Diego},
	booktitle = {2020 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:20 -0400},
	date-modified = {2024-03-18 02:30:20 -0400},
	doi = {10.1109/VR46266.2020.00023},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Task analysis;Two dimensional displays;Stability analysis;Navigation;Virtual reality;Data visualization;3D selection;hybrid systems;slicing plane;virtual reality;bimanual interaction;tablet},
	month = {March},
	pages = {53-62},
	title = {Slicing-Volume: Hybrid 3D/2D Multi-target Selection Technique for Dense Virtual Environments},
	year = {2020},
	bdsk-url-1 = {https://doi.org/10.1109/VR46266.2020.00023}}

@inproceedings{8797705,
	abstract = {Prehospital emergency healthcare personnel are responsible for finding, rescuing, and taking prehospital care of emergency patients. They are regularly exposed to stressful and traumatic lifesaving situations. The stress involved can impact their performance and can cause mental disorders in the long term. Stress inoculation training (SIT) inoculates individuals to potential stressors by letting them practice stress-coping skills in a controlled environment. Our work explores a story-driven stressful virtual environment design that can potentially be used for SIT in the new context of emergency healthcare personnel. Users role-play a first-time emergency worker on a rescue mission. The interactive storytelling is designed to engage users and elicit strong emotional responses, and follows the three-act structure commonly found in films and video games. To understand the stress-inducing and sense of presence qualities of our approach including the previously untested impact of an emotional connection factor, we conduct a between-subjects experiment involving 60 subjects. Results show that the approach successfully induces stress by increasing heart rate, galvanic skin response, and subjective stress rating. Questionnaire results indicate positive presence. One subject group engages in an initial friendly conversation with a virtual co-worker to establish an emotional connection. Another group includes no such conversation. The group with the emotional connection shows higher physiological stress levels and more occurrences of subject behaviors reflecting presence. Medical experts review our approach and suggest several applications that can benefit from its stress inducing ability.},
	author = {Prachyabrued, Mores and Wattanadhirach, Disathon and Dudrow, Richard B. and Krairojananan, Nat and Fuengfoo, Pusit},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797705},
	issn = {2642-5254},
	keywords = {Stress;Training;Personnel;Medical services;Virtual environments;Emotional responses;Biomedical monitoring;Stress;training;emergency healthcare personnel;virtual reality;storytelling;emotional connection;presence;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism---Virtual Reality;J.3 [Life And Medical Sciences]: Health},
	month = {March},
	pages = {671-679},
	title = {Toward Virtual Stress Inoculation Training of Prehospital Healthcare Personnel: A Stress-Inducing Environment Design and Investigation of an Emotional Connection Factor},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797705}}

@inproceedings{8797708,
	abstract = {Online learning platforms such as MOOCs have been prevalent sources of self-paced learning to people nowadays. However, the lack of peer accompaniment and social interaction may increase learners' sense of isolation and loneliness. Prior studies have shown the positive effects on visualizing peer students' appearances with virtual avatars or virtualized online learners in VR learning environments. In this work, we propose to build virtual classmates, which were constructed by synthesizing previous learners' messages (time-anchored comments). Configurations of virtual classmates, such as the number of classmates participating in a VR class and the behavioral features of the classmates, can also be adjusted. To build the characteristics of virtual classmates, we propose a technique called comment mapping to aggregate prior online learners' comments to shape virtual classmates' behaviors. We conduct a study with 100 participants to evaluate the effects of the virtual classmates built with and without the comment mapping and the amount of virtual classmates rendered in VR. The findings of our study suggest design implications for developing virtual classmates in VR environments.},
	author = {Liao, Meng-Yun and Sung, Ching-Ying and Wang, Hao-Chuan and Lin, Wen-Chieh},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797708},
	issn = {2642-5254},
	keywords = {Avatars;Streaming media;Task analysis;Discussion forums;Virtual environments;Computer science;Games},
	month = {March},
	pages = {163-171},
	title = {Virtual Classmates: Embodying Historical Learners' Messages as Learning Companions in a VR Classroom through Comment Mapping},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797708}}

@inproceedings{8797716,
	abstract = {Performing motor tasks in virtual environments is best achieved with motion capture and animation of a 3D character that participants control in real time and perceive as being their avatar in the virtual environment. A strong Sense of Embodiment (SoE) for the virtual body not only relies on the feeling that the virtual body is their own (body ownership), but also that the virtual body moves in the world according to their will and replicates precisely their body movement (sense of agency). Within that frame of mind our specific aim is to demonstrate that the avatar can even be programmed to be better at executing a given task or to perform a movement that is normally difficult or impossible to execute precisely by the user. More specifically, our experimental task consists in asking subjects to follow with the hand a target that is animated using non-biological motion; the unnatural nature of the movement leads to systematic errors by the subjects. The challenge here is to introduce a subtle distortion between the position of the real hand and the position of the virtual hand, so that the virtual hand succeeds in performing the task while still letting subjects believe they are fully in control. Results of two experiments (N=16) show that our implementation of a distortion function, that we name the attraction well, successfully led participants to report being in control of the movement (agency) and being embodied in the avatar (body ownership) even when the distortion was above a threshold that they can detect. Furthermore, a progressive introduction of the distortion (starting without help, and introducing distortion on the go) could even further increase its acceptance.},
	author = {Porssut, Thibault and Herbelin, Bruno and Boulic, Ronan},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797716},
	issn = {2642-5254},
	keywords = {Distortion;Avatars;Task analysis;Haptic interfaces;Three-dimensional displays;Training;Virtual environments;Virtual Reality;Avatar;Embodiment;Agency;Body Ownership;Movement Distortion;K.6.1 [Management of Computing and Information Systems]: Project and People Management---Life Cycle;K.7.m [The Computing Profession]: Miscellaneous---Ethics},
	month = {March},
	pages = {529-537},
	title = {Reconciling Being in-Control vs. Being Helped for the Execution of Complex Movements in VR},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797716}}

@inproceedings{8797719,
	abstract = {This paper investigates the effect of avatar appearance on Social Presence and users' perception in an Augmented Reality (AR) telepresence system. Despite the development of various commercial 3D telepresence systems, there has been little evaluation and discussions about the appearance of the collaborator's avatars. We conducted two user studies comparing the effect of avatar appearances with three levels of body part visibility (head & hands, upper body, and whole body) and two different character styles (realistic and cartoon-like) on Social Presence while performing two different remote collaboration tasks. We found that a realistic whole body avatar was perceived as being the best for remote collaboration, but an upper body or cartoon style could be considered as a substitute depending on the collaboration context. We discuss these results and suggest guidelines for designing future avatar-mediated AR remote collaboration systems.},
	author = {Yoon, Boram and Kim, Hyung-il and Lee, Gun A. and Billinghurst, Mark and Woo, Woontack},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797719},
	issn = {2642-5254},
	keywords = {Avatars;Collaboration;Three-dimensional displays;Telepresence;Augmented reality;Task analysis;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Mixed/augmented reality;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies},
	month = {March},
	pages = {547-556},
	title = {The Effect of Avatar Appearance on Social Presence in an Augmented Reality Remote Collaboration},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797719}}

@inproceedings{8797721,
	abstract = {This paper investigates whether the body anticipation synergies in real environments (REs) are preserved during navigation in virtual environments (VEs). Experimental studies related to the control of human locomotion in REs during curved trajectories report a top-down reorientation strategy with the reorientation of the gaze anticipating the reorientation of head, the shoulders and finally the global body motion. This anticipation behavior provides a stable reference frame to the walker to control and reorient his/her body according to the future walking direction. To assess body anticipation during navigation in VEs, we conducted an experiment where participants, wearing a head-mounted display, performed a lemniscate trajectory in a virtual environment (VE) using five different navigation techniques, including walking, virtual steering (head, hand or torso steering) and passive navigation. For the purpose of this experiment, we designed a new control law based on the power-law relation between speed and curvature during human walking. Taken together our results showed a similar ordered top-down sequence of reorientation of the gaze, head and shoulders during curved trajectories between walking in REs and in VEs (for all the evaluated techniques). However, the anticipation mechanism was significantly higher for the walking condition compared to the others. The results presented in this paper pave the way to the better understanding of the underlying mechanisms of human navigation in VEs and to the design of navigation techniques more adapted to humans.},
	author = {Brument, Hugo and Podkosova, Iana and Kaufmann, Hannes and Olivier, Anne H{\'e}l{\`e}ne and Argelaguet, Ferran},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797721},
	issn = {2642-5254},
	keywords = {Trajectory;Indexes;Navigation;Conferences;Virtual reality;Three-dimensional displays;User interfaces;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality},
	month = {March},
	pages = {680-689},
	title = {Virtual vs. Physical Navigation in VR: Study of Gaze and Body Segments Temporal Reorientation Behaviour},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797721}}

@inproceedings{8797725,
	abstract = {Current head-mounted displays (HMDs) have a limited field-of-view (FOV). A limited FOV further decreases the already restricted human visual range and amplifies the problem of objects receding from view (e.g., opponents in computer games). However, there is no previous work that investigates how to best perceive moving out-of-view objects on head-mounted displays. In this paper, we compare two visualization approaches: (1) Overview+detail, with 3D Radar, and (2) Focus+context, with EyeSee360, in a user study to evaluate their performances for visualizing moving out-of-view objects. We found that using 3D Radar resulted in a significantly lower movement estimation error and higher usability, measured by the system usability scale. 3D Radar was also preferred by 13 out of 15 participants for visualization of moving out-of-view objects.},
	author = {Gruenefeld, Uwe and Koethe, Ilja and Lange, Daniel and Wei{\ss}, Sebastian and Heuten, Wilko},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797725},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Radar;Games;Estimation error;Usability;Meters;Human-centered computing---Visualization---Visualization techniques;Human-centered computing---Human computer interaction (HCI)---Empirical studies in HCI},
	month = {March},
	pages = {742-746},
	title = {Comparing Techniques for Visualizing Moving Out-of-View Objects in Head-mounted Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797725}}

@inproceedings{8797733,
	abstract = {Immersive environments have gradually become standard for visualizing and analyzing large or complex datasets that would otherwise be cumbersome, if not impossible, to explore through smaller scale computing devices. However, this type of workspace often proves to possess limitations in terms of interaction, flexibility, cost and scalability. In this paper we introduce a novel immersive environment called Dataspace, which features a new combination of heterogeneous technologies and methods of interaction towards creating a better team workspace. Dataspace provides 15 high-resolution displays that can be dynamically reconfigured in space through robotic arms, a central table where information can be projected, and a unique integration with augmented reality (AR) and virtual reality (VR) headsets and other mobile devices. In particular, we contribute novel interaction methodologies to couple the physical environment with AR and VR technologies, enabling visualization of complex types of data and mitigating the scalability issues of existing immersive environments. We demonstrate through four use cases how this environment can be effectively used across different domains and reconfigured based on user requirements. Finally, we compare Dataspace with existing technologies, summarizing the trade-offs that should be considered when attempting to build better collaborative workspaces for the future.},
	author = {Cavallo, Marco and Dholakia, Mishal and Havlena, Matous and Ocheltree, Kenneth and Podlaseck, Mark},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797733},
	issn = {2642-5254},
	keywords = {Data visualization;Three-dimensional displays;Collaboration;Headphones;Two dimensional displays;Augmented reality;Human-centered computing---Visualization---Visualization Systems and Tools;Human-centered computing---Human computer interaction (HCI)---Interactive systems and tools},
	month = {March},
	pages = {145-153},
	title = {Dataspace: A Reconfigurable Hybrid Reality Environment for Collaborative Information Analysis},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797733}}

@inproceedings{8797744,
	abstract = {We present an empirical evaluation on how stereoscopic viewing and haptic feedback deferentially affects fine motor perception-action coordination in a pick-and-place task in Virtual Reality (VR). The factors considered were stereoscopic viewing, haptic feedback, sensory-motor congruence and mismatch, and calibration on perception-action coordination in near field fine motor task performance in VR. Quantitative measures of placement error, distance, collision, and time to complete trials were recorded and analyzed. Overall, we found that participants' manual dexterous task performance was enhanced in the presence of both stereoscopic viewing and haptic feedback. However, we found that time to complete task was greatly enhanced by the presence of haptic feedback, and economy and efficiency of movement of the end effector as well as the manipulated object was enhanced by the presence of both haptic feedback and stereoscopic viewing. Whereas, number of collisions and placement accuracy were greatly enhanced by the presence of stereoscopic viewing in near-field fine motor perception-action coordination. Our research additionally shows that mismatch in sensory-motor stimuli can detrimentally affect the number of collisions, and efficiency of end effector and object movements in near-field fine motor activities, and can be further negatively affected by the absence of haptic feedback and stereoscopic viewing. In spite of reduced cue situations in VR, and the absence or presence of stereoscopic viewing and haptic feedback, we found that participants tend to calibrate or adapt their perception-action coordination rapidly with a set of at least 5 trials.},
	author = {Brickler, David and Volonte, Matias and Bertrand, Jeffrey W. and Duchowski, Andrew T. and Babu, Sabarish V.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797744},
	issn = {2642-5254},
	keywords = {Haptic interfaces;Stereo image processing;Task analysis;Surgery;Three-dimensional displays;Training;Solid modeling;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {28-37},
	title = {Effects of Stereoscopic Viewing and Haptic Feedback, Sensory-Motor Congruence and Calibration on Near-Field Fine Motor Perception-Action Coordination in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797744}}

@inproceedings{8797751,
	abstract = {Walking in place is a standard method for moving through large virtual environments when physical space or positional tracking is limited. This technique has become increasingly prominent with the advent of mobile virtual reality in which external tracking may not be present. In this paper, we revisit walking in place algorithms to address some of their technical challenges. Namely, our solutions attend to improving starting, stopping, and speed control for individual users. From a hand-tuned threshold based algorithm, we provide a new, fast method for individualizing the walking in place algorithm based on biomechanic measures of step rate. In addition, we introduce a new walking in place model based on a convolutional neural network trained to differentiate walking and standing. Over two experiments we assess these methods against a traditional threshold based algorithm on two mobile virtual reality platforms. The assessments are based on controllability, scale, and presence. Our results suggest that an adequately trained convolutional neural network can be an effective way of implementing walking in place.},
	author = {Hanson, Sara and Paris, Richard A. and Adams, Haley A. and Bodenheimer, Bobby},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797751},
	issn = {2642-5254},
	keywords = {Legged locomotion;Virtual environments;Tracking;Acceleration;Gears;Neural networks;Magnetic heads;Virtual environments;locomotion;walking in place;convolutional neural network;perception;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism---Virtual Reality;J.4 [Computer Applications]: Social and Behavioral Sciences---Psychology},
	month = {March},
	pages = {367-376},
	title = {Improving Walking in Place Methods with Individualization and Deep Networks},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797751}}

@inproceedings{8797752,
	abstract = {Visual information plays an important part in the perception of the world around us. Recently, head-mounted displays (HMD) came to the consumer market and became a part of everyday life of thousands of people. Like with the desktop screens or hand-held devices before, the public is concerned with the possible health consequences of the prolonged usage and question the adequacy of the default settings. It has been shown that the brightness and contrast of a display should be adjusted to match the external light to decrease eye strain and other symptoms. Currently, there is a noticeable mismatch in brightness between the screen and dark background of an HMD that might cause eye strain, insomnia, and other unpleasant symptoms. In this paper, we explore the possibility to significantly lower the screen brightness in the HMD and successfully compensate for the loss of the visual information on a dimmed screen. We designed a user study to explore the connection between the screen brightness in HMD and task performance, cybersickness, users' comfort, and preferences. We have tested three levels of brightness: the default Full Brightness, the optional Night Mode and a significantly lower brightness with original content and compensated content. Our results suggest that although users still prefer the brighter setting, the HMDs can be successfully used with significantly lower screen brightness, especially if the low screen brightness is compensated.},
	author = {Vasylevska, Khrystyna and Yoo, Hyunjin and Akhavan, Tara and Kaufmann, Hannes},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797752},
	issn = {2642-5254},
	keywords = {Brightness;Resists;Image color analysis;Visualization;Strain;Color;Standards;Human-centered computing---User studies;Human-centered computing---Virtual reality;Computing methodologies---Perception},
	month = {March},
	pages = {566-574},
	title = {Towards Eye-Friendly VR: How Bright Should It Be?},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797752}}

@inproceedings{8797777,
	abstract = {Navigation is a major challenge in exploring data within immersive environments, especially of large omnidirectional spherical images. We propose a method of auto-scaling to allow users to navigate using teleportation within the safe boundary of their physical environment with different levels of focus. Our method combines physical navigation with virtual teleportation. We also propose a ``peek then warp'' behavior when using a zoom lens and evaluate our system in conjunction with different teleportation transitions, including a proposed transition for exploration of omnidirectional and 360-degree panoramic imagery, termed Envelop, wherein the destination view expands out from the zoom lens to completely envelop the user. In this work, we focus on visualizing and navigating large omnidirectional or panoramic images with application to GIS visualization as an inside-out omnidirectional image of the earth. We conducted two user studies to evaluate our techniques over a search and comparison task. Our results illustrate the advantages of our techniques for navigation and exploration of omnidirectional images in an immersive environment.},
	author = {Mirhosseini, Seyedkoosha and Ghahremani, Parmida and Ojal, Sushant and Marino, Joseph and Kaufman, Arie},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797777},
	issn = {2642-5254},
	keywords = {Navigation;Teleportation;Cameras;Data visualization;Lenses;Earth;Virtual environments;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Human-centered computing---Visualization---Visualization application domains---Geographic visualization},
	month = {March},
	pages = {413-422},
	title = {Exploration of Large Omnidirectional Images in Immersive Environments},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797777}}

@inproceedings{8797787,
	abstract = {Most commercial virtual reality applications with self avatars provide users with a ``one-size fits all'' avatar. While the height of this body may be scaled to the user's height, other body proportions, such as limb length and hand size, are rarely customized to fit an individual user. Prior research has shown that mismatches between users' avatars and their actual bodies can affect size perception and feelings of body ownership. In this paper, we consider how concepts related to the virtual hand illusion, user experience, and task efficiency are influenced by variations between the size of a user's actual hand and their avatar's hand. We also consider how using a tracked controller or tracked gestures affect these concepts. We conducted a 23 within-subjects study (n=20), with two levels of input modality: using tracked finger motion vs. a hand-held controller (Glove vs. Controller), and three levels of hand scaling (Small, Fit, and Large). Participants completed 2 block-assembly trials for each condition (for a total of 12 trials). Time, mistakes, and a user experience survey were recorded for each trial. Participants experienced stronger feelings of ownership and realism in the Glove condition. Efficiency was higher in the Controller condition and supported by play data of more time spent, blocks grabbed, and blocks dropped in the Glove condition. We did not find enough evidence for a change in agency and the intensity of the virtual hand illusion depending on hand size. Over half of the participants indicated preferring the Glove condition over the Controller condition, mentioning fun and efficiency as factors in their choices. Preferences on hand scaling were mixed but often attributed to efficiency. Participants liked the appearance of their virtual hand more while using the Fit instead of Large hands. Several interaction effects were observed between input modality and hand scaling, for example, for smaller hands, tracked hands evoked stronger feelings of ownership compared to using a controller. Our results show that the virtual hand illusion is stronger when participants are able to control a hand directly rather than with a hand-held device, and that the virtual reality task must first be considered to determine which modality and hand size are the most applicable.},
	author = {Lin, Lorraine and Normoyle, Aline and Adkins, Alexandra and Sun, Yu and Robb, Andrew and Ye, Yuting and Di Luca, Massimiliano and J{\"o}rg, Sophie},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797787},
	issn = {2642-5254},
	keywords = {Avatars;Tracking;Rubber;Task analysis;Games;Virtual environments;Human-centered computing---Virtual reality Human-centered computing---Gestural input;Human-centered computing---Interaction design;Computing methodologies---Perception},
	month = {March},
	pages = {510-518},
	title = {The Effect of Hand Size and Interaction Modality on the Virtual Hand Illusion},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797787}}

@inproceedings{8797807,
	abstract = {The collaborative exploration of virtual environments benefits from joint group navigation capabilities. In this paper, we focus on the design and evaluation of a short-range teleportation technique (jumping) for a group of collocated users wearing head-mounted displays. In a pilot study with expert users, we tested three na{\"\i}ve group jumping approaches and derived the requirements for comprehensible group jumping. We propose a novel Multi-Ray Jumping technique to meet these requirements and report results of two formal user studies, one exploring the effects of passive jumping on simulator sickness symptoms (N=20) and a second one investigating the advantages of our novel technique compared to na{\"\i}ve group jumping (N=22). The results indicate that Multi-Ray Jumping decreases spatial confusion for passengers, increases planning accuracy for navigators, and reduces cognitive load for both.},
	author = {Weissker, Tim and Kulik, Alexander and Froehlich, Bernd},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797807},
	issn = {2642-5254},
	keywords = {Navigation;Virtual environments;Head-mounted displays;Collaboration;Avatars;Visualization;Virtual reality;head-mounted displays;multi-user interaction;group navigation;collocation;teleportation;jumping;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism---Virtual reality;I.3.6 [Computer Graphics]: Methodology and Techniques---Interaction techniques;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces---Collaborative computing},
	month = {March},
	pages = {136-144},
	title = {Multi-Ray Jumping: Comprehensible Group Navigation for Collocated Users in Immersive Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797807}}

@inproceedings{8797812,
	abstract = {This paper introduces GeoGate, an Augmented Reality tabletop system that extends the Space-Time Cube and utilizes a ring-shaped tangible user interface to explore correlations between entities in multiple location datasets. We demonstrate GeoGate in the context of the maritime domain, where operators seek to find geo-temporal associations between trajectories recorded from a global positioning system, and light data extracted from night time satellite images. GeoGate utilizes a tabletop system displaying a traditional 2D map in conjunction with a Microsoft Hololens to present a single view of the data with a novel Augmented Reality extension of the Space-Time Cube. To validate GeoGate, we present the results of a user study comparing GeoGate with the existing 2D approach used in a normal desktop environment. The outcomes of the user study show that GeoGate's approach reduces mistakes in the interpretation of the correlations between various datasets, while the qualitative results show that such a system is preferable for the majority of geo-temporal maritime tasks compared.},
	author = {Ssin, Seung Youb and Walsh, James A. and Smith, Ross T. and Cunningham, Andrew and Thomas, Bruce H.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797812},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Data visualization;Two dimensional displays;Trajectory;Complexity theory;Augmented reality;Uncertainty;Augmented Reality;Space Time Cube;Multivariate Network Visualization;Multiple Data-sets;Maritime Visualization;Geo-visualization;Tangible User Interface;Tabletop;H.1.2 [Information Systems]: User/Machine Systems---Human factors;I.3.6 [Computer Graphics]: Methodology and Techniques---Interaction techniques},
	month = {March},
	pages = {210-219},
	title = {GeoGate: Correlating Geo-Temporal Datasets Using an Augmented Reality Space-Time Cube and Tangible Interactions},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797812}}

@inproceedings{8797816,
	abstract = {Eye-tracking enables researchers to conduct complex analysis on human behavior. With the recent introduction of eye-tracking into consumer-grade virtual reality headsets, the barrier of entry to visual attention analysis in virtual environments has been lowered significantly. Whether for arranging artwork in a virtual museum, posting banners for virtual events or placing advertisements in virtual worlds, analyzing visual attention patterns provides a powerful means for guiding visual element placement. In this work, we propose a novel data-driven optimization approach for automatically analyzing visual attention and placing visual elements in 3D virtual environments. Using an eye-tracking virtual reality headset, we collect eye-tracking data which we use to train a regression model for predicting gaze duration. We then use the predicted gaze duration output of our regressors to optimize the placement of visual elements with respect to certain visual attention and design goals. Through experiments in several virtual environments, we demonstrate the effectiveness of our optimization approach for predicting gaze duration and for placing visual elements in different practical scenarios. Our approach is implemented as a useful plug-in that level designers can use to automatically populate visual elements in 3D virtual environments.},
	author = {Alghofaili, Rawan and Solah, Michael S and Huang, Haikun and Sawahata, Yasuhito and Pomplun, Marc and Yu, Lap-Fai},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797816},
	issn = {2642-5254},
	keywords = {Visualization;Three-dimensional displays;Virtual environments;Headphones;Solid modeling;Public transportation;Computing methodologies---Computer graphics---Graphics systems and interfaces---Virtual Reality},
	month = {March},
	pages = {464-473},
	title = {Optimizing Visual Element Placement via Visual Attention Analysis},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797816}}

@inproceedings{8797818,
	abstract = {Immersive Virtual Environment systems that utilize Head Mounted Displays and a large tracking area have the advantage of being able to use natural walking as a locomotion interface. In such systems, difficulties arise when the virtual world is larger than the tracking area and users approach area boundaries. Redirected walking (RDW) is a technique that distorts the correspondence between physical and virtual world motion to steer users away from boundaries and obstacles, including other co-immersed users. Recently, a RDW algorithm was proposed based on the use of artificial potential fields (APF), in which walls and obstacles repel the user. APF-RDW effectively supports multiple simultaneous users and, unlike other RDW algorithms, can easily account for tracking area dimensions and room shape when generating steering instructions. This work investigates the performance of a refined APF-RDW algorithm in different sized tracking areas and in irregularly shaped rooms, as compared to a Steer-to-Center (STC) algorithm and an un-steered control condition. Data was generated in simulation using logged paths of prior live users, and is presented for both single-user and multi-user scenarios. Results show the ability of APF-RDW to steer effectively in irregular concave shaped tracking areas such as L-shaped rooms or crosses, along with scalable multi-user support, and better performance than STC algorithms in almost all conditions.},
	author = {Messinger, Justin and Hodgson, Eric and Bachmann, Eric R.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797818},
	issn = {2642-5254},
	keywords = {Legged locomotion;Force;Shape;Task analysis;Target tracking;Navigation;Redirected walking;virtual environments;navigation;simulation},
	month = {March},
	pages = {72-80},
	title = {Effects of Tracking Area Shape and Size on Artificial Potential Field Redirected Walking},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797818}}

@inproceedings{8797824,
	abstract = {We propose an approach to facilitate adjustable grip for object interaction in virtual reality. It enables the user to handle objects with loose and firm grip using conventional controllers. Pivotal design properties were identified and evaluated in a qualitative pilot study. Two revised interaction designs with variable grip were compared to the status quo of invariable grip in a quantitative study. The users performed placing actions with all interaction modes. Performance, clutching, task load, and usability were measured. While the handling time increased slightly using variable grip, the usability score was significantly higher. No substantial differences were measured in positioning accuracy. The results lead to the conclusion that variable grip can be useful and improve realism depending on tasks, goals, and user preference.},
	author = {Bonfert, Michael and Porzel, Robert and Malaka, Rainer},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797824},
	issn = {2642-5254},
	keywords = {Grasping;Task analysis;Gravity;Glass;Virtual reality;Usability;Control systems;Virtual reality;grip;grasping;object manipulation;Human-centered computing---Virtual reality;Human-centered computing---Empirical studies in interaction design},
	month = {March},
	pages = {604-612},
	title = {Get a Grip! Introducing Variable Grip for Controller-Based VR Systems},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797824}}

@inproceedings{8797826,
	abstract = {The topic of distance perception has been widely investigated in Virtual Reality (VR). However, the vast majority of previous work mainly focused on distance perception of objects placed in front of the observer. Then, what happens when the observer looks on the side? In this paper, we study differences in distance estimation when comparing objects placed in front of the observer with objects placed on his side. Through a series of four experiments (n=85), we assessed participants' distance estimation and ruled out potential biases. In particular, we considered the placement of visual stimuli in the field of view, users' exploration behavior as well as the presence of depth cues. For all experiments a two-alternative forced choice (2AFC) standardized psychophysical protocol was employed, in which the main task was to determine the stimuli that seemed to be the farthest one. In summary, our results showed that the orientation of virtual stimuli with respect to the user introduces a distance perception bias: objects placed on the sides are systematically perceived farther away than objects in front. In addition, we could observe that this bias increases along with the angle, and appears to be independent of both the position of the object in the field of view as well as the quality of the virtual scene. This work sheds a new light on one of the specificities of VR environments regarding the wider subject of visual space theory. Our study paves the way for future experiments evaluating the anisotropy of distance perception in real and virtual environments.},
	author = {Peillard, Etienne and Thebaud, Thomas and Normand, Jean-Marie and Argelaguet, Ferran and Moreau, Guillaume and L{\'e}cuyer, Anatole},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797826},
	issn = {2642-5254},
	keywords = {Visualization;Resists;Observers;Calibration;Anisotropic magnetoresistance;Virtual reality;Protocols;Perception;Distance;Virtual Reality;User Experiment;Psychophysical Study;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies},
	month = {March},
	pages = {227-236},
	title = {Virtual Objects Look Farther on the Sides: The Anisotropy of Distance Perception in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797826}}

@inproceedings{8797828,
	abstract = {Virtual Reality (VR) technology offers promising opportunities to improve traditional treadmill-based rehabilitation programs. We present an immersive VR rehabilitation system that includes a head-mounted display and motion sensors. The application is designed to promote the experience of relatedness, autonomy, and competence. The application uses procedural content generation to generate diverse landscapes. We evaluated the effect of the immersive rehabilitation system on motivation and affect. We conducted a repeated measures study with 36 healthy participants to compare the immersive program to a traditional rehabilitation program. Participants reported significant greater enjoyment, felt more competent and experienced higher decision freedom and meaningfulness in the immersive VR gait training compared to the traditional training. They experienced significantly lower physical demand, simulator sickness, and state anxiety, and felt less pressured while still perceiving a higher personal performance. We derive three design implications for future applications in gait rehabilitation: Immersive VR provides a promising augmentation for gait rehabilitation. Gamification features provide a design guideline for content creation in gait rehabilitation. Relatedness and autonomy provide critical content features in gait rehabilitation.},
	author = {Kern, Florian and Winter, Carla and Gall, Dominik and K{\"a}thner, Ivo and Pauli, Paul and Latoschik, Marc Erich},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797828},
	issn = {2642-5254},
	keywords = {Games;Task analysis;Training;Psychology;Virtual reality;Multiple sclerosis;Resists;Human-centered computing---Human computer interaction (HCI)---Empirical studies in HCI;Applied computing---Health informatics},
	month = {March},
	pages = {500-509},
	title = {Immersive Virtual Reality and Gamification Within Procedurally Generated Environments to Increase Motivation During Gait Rehabilitation},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797828}}

@inproceedings{8797840,
	abstract = {Virtual Reality (VR) is effective in various training scenarios across multiple domains, such as education, health and defense. However, most of those applications are not adaptive to the real-time cognitive or subjectively experienced load placed on the trainee. In this paper, we explore a cognitively adaptive training system based on real-time measurement of task related alpha activity in the brain. This measurement was made by a 32-channel mobile Electroencephalography (EEG) system, and was used to adapt the task difficulty to an ideal level which challenged our participants, and thus theoretically induces the best level of performance gains as a result of training. Our system required participants to select target objects in VR and the complexity of the task adapted to the alpha activity in the brain. A total of 14 participants undertook our training and completed 20 levels of increasing complexity. Our study identified significant differences in brain activity in response to increasing levels of task complexity, but response time did not alter as a function of task difficulty. Collectively, we interpret this to indicate the brain's ability to compensate for higher task load without affecting behaviourally measured visuomotor performance.},
	author = {Dey, Arindam and Chatburn, Alex and Billinghurst, Mark},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797840},
	issn = {2642-5254},
	keywords = {Training;Electroencephalography;Task analysis;Adaptive systems;Real-time systems;Frequency measurement;Virtual reality;H.1.2 [Models and Principles]: User/Machine Systems---Human Factors;H.5.1 [Multimedia Information Systems]: Artificial---Augmented and Virtual Realities;Virtual Reality;Cognitively Adaptive Training;Electroencephalography;Alpha Activity},
	month = {March},
	pages = {220-226},
	title = {Exploration of an EEG-Based Cognitively Adaptive Training System in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797840}}

@inproceedings{8797843,
	abstract = {360$\,^{\circ}$ videos can be viewed in an immersive manner with a head-mounted display (HMD). However, it is unclear how the viewing experience is affected by basic properties of 360$\,^{\circ}$ videos, such as how high they are recorded from, and whether there are people close to the camera. We conducted a 24-participant user study where we explored whether the viewing experience is affected by A) camera height, B) the proximity and actions of people appearing in the videos, and C) viewer position (standing/sitting). The results, surprisingly, suggest that the viewer's own height has little to no effect on the preferred camera height and the experience. The most optimal camera height situates at around 150 centimeters, which hits the comfortable height range for both sitting and standing viewers. Moreover, in some cases, people being close to the camera, or the camera being very low, has a negative effect on the experience. Our work contributes to understanding and designing immersive 360$\,^{\circ}$ experiences.},
	author = {Keskinen, Tuuli and M{\"a}kel{\"a}, Ville and Kallioniemi, Pekka and Hakulinen, Jaakko and Karhu, Jussi and Ronkainen, Kimmo and M{\"a}kel{\"a}, John and Turunen, Markku},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797843},
	issn = {2642-5254},
	keywords = {360$\,^{\circ}$ videos;camera height;omnidirectional videos;head-mounted displays;virtual environments;virtual reality;user experience;viewer height;viewer position;Human-centered computing---Virtual reality},
	month = {March},
	pages = {423-430},
	title = {The Effect of Camera Height, Actor Behavior, and Viewer Position on the User Experience of 360$\,^{\circ}$ Videos},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797843}}

@inproceedings{8797852,
	abstract = {Volumetric capture allows the creation of near-video-quality content that can be explored with six degrees of freedom. Due to limitations in these experiences, such as the content being fixed at the point of filming, an understanding of eye-gaze awareness is critical. A repeated measures experiment was conducted that explored users' ability to evaluate where a volumetrically captured avatar (VCA) was looking. Wearing one of two head-mounted displays (HMDs), 36 participants rotated a VCA to look at a target. The HMD resolution, target position, and VCA's eye-gaze direction were varied. Results did not show a difference in accuracy between HMD resolutions, while the task became significantly harder for target locations further away from the user. In contrast to real-world studies, participants consistently misjudged eye-gaze direction based on target location, but not based on the avatar's head turn direction. Implications are discussed, as results for VCAs viewed in HMDs appear to differ from face-to-face scenarios.},
	author = {MacQuarrie, Andrew and Steed, Anthony},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797852},
	issn = {2642-5254},
	keywords = {Avatars;Receivers;Three-dimensional displays;Task analysis;Cameras;Solid modeling;Resists;User study;Virtual reality;Gaze perception;1.1.1 [Human-centered computing]---Empirical studies in HCI;1.1.3.2 [Human-centered computing]: Interaction devices---Displays and imagers;1.1.4 [Human-centered computing]---HCI theory, concepts and models},
	month = {March},
	pages = {645-654},
	title = {Perception of Volumetric Characters' Eye-Gaze Direction in Head-Mounted Displays},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797852}}

@inproceedings{8797854,
	abstract = {French Army infantrymen's are equipped today with a combat system called FELIN, which includes an infrared sighting device: the IR sight. One of the first manipulations learned by the soldier is the IR sight calibration. Currently, calibration training is a two-step process. The first step consists of practicing on a 2D WIMP software until making no mistakes. Then, the soldiers can apply his knowledge in the real situation on the shooting range. In this paper, we present an ad-hoc study of a learning method including a prototype in Virtual Reality for training on the FELIN IR sight calibration procedure. It has been experimented on real infantrymen learners in an infantry school. Results showed an attractive added value of Virtual Reality in this specific use case. It improved the learners' intrinsic motivation to repeat the training task as well as the learning efficiency. It also helped the training team to identify specific mistake types not detected by the traditional learning software.},
	author = {Taupiac, Jean-Daniel and Rodriguez, Nancy and Strauss, Olivier and Rabier, Martin},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797854},
	issn = {2642-5254},
	keywords = {Training;Virtual reality;Software;Calibration;Two dimensional displays;Prototypes;Task analysis},
	month = {March},
	pages = {190-199},
	title = {Ad-hoc Study on Soldiers Calibration Procedure in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797854}}

@inproceedings{8797862,
	abstract = {Immersive Virtual Reality (IVR) is a growing 3D environment, where social and commercial applications will require user authentication. Similarly, smart homes in the real world (RW), offer an opportunity to authenticate in the third dimension. For both environments, there is a gap in understanding which elements of the third dimension can be leveraged to improve usability and security of authentication. In particular, investigating transferability of findings between these environments would help towards understanding how rapid prototyping of authentication concepts can be achieved in this context. We identify key elements from prior research that are promising for authentication in the third dimension. Based on these, we propose a concept in which users' authenticate by selecting a series of 3D objects in a room using a pointer. We created a virtual 3D replica of a real world room, which we leverage to evaluate and compare the factors that impact the usability and security of authentication in IVR and RW. In particular, we investigate the influence of randomized user and object positions, in a series of user studies (N=48). We also evaluate shoulder surfing by real world bystanders for IVR (N=75). Our results show that 3D passwords within our concept are resistant against shoulder surfing attacks. Interactions are faster in RW compared to IVR, yet workload is comparable.},
	author = {George, Ceenu and Khamis, Mohamed and Buschek, Daniel and Hussmann, Heinrich},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797862},
	issn = {2642-5254},
	keywords = {Authentication;Three-dimensional displays;Password;Virtual reality;Resists;Usability;Human-centered computing---User studies;Human-centered computing---Virtual reality},
	month = {March},
	pages = {277-285},
	title = {Investigating the Third Dimension for Authentication in Immersive Virtual Reality and in the Real World},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797862}}

@inproceedings{8797867,
	abstract = {Immersive technologies have the potential to overcome physical limitations and virtually deliver field site experiences, for example, into the classroom. Yet, little is known about the features of immersive technologies that contribute to successful place-based learning. Immersive technologies afford embodied experiences by mimicking natural embodied interactions through a user's egocentric perspective. Additionally, they allow for beyond reality experiences integrating contextual information that cannot be provided at actual field sites. The current study singles out one aspect of place-based learning: Scale. In an empirical evaluation, scale was manipulated as part of two immersive virtual field trip (iVFT) experiences in order to disentangle its effect on place-based learning. Students either attended an actual field trip (AFT) or experienced one of two iVFTs using a head-mounted display. The iVFTs either mimicked the actual field trip or provided beyond reality experiences offering access to the field site from an elevated perspective using pseudo-aerial 360$\,^{\circ}$ imagery. Results show that students with access to the elevated perspective had significantly better scores, for example, on their spatial situation model (SSM). Our findings provide first results on how an increased (geographic) scale, which is accessible through an elevated perspective, boosts the development of SSMs. The reported study is part of a larger immersive education effort. Inspired by the positive results, we discuss our plan for a more rigorous assessment of scale effects on both self- and objectively assessed performance measures of spatial learning.},
	author = {Zhao, Jiayan and Klippel, Alexander},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797867},
	issn = {2642-5254},
	keywords = {Immersive learning;virtual field trips;scale;place-based education;Applied computing---Education---Interactive learning environments;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {155-162},
	title = {Scale - Unexplored Opportunities for Immersive Technologies in Place-based Learning},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797867}}

@inproceedings{8797871,
	abstract = {Virtual reality (VR) environments are typically designed so users feel present in a single virtual world at a time, but this creates a problem for applications that require visual comparisons (e.g., forest scientists comparing multiple data-driven virtual forests). To address this, we present Worlds-in-Wedges, a 3D user interface and visualization technique that supports comparative immersive visualization by dividing the virtual space surrounding the user into volumetric wedges. There are three visual/interactive levels. The first, worlds-in-context, visualizes high-level relationships between the worlds (e.g., a map for worlds that are related in space). The second level, worlds-in-miniature, is a multi-instance implementation of the World-in-Miniature technique extended to support mutlivari-ate glyph visualization. The third level, worlds-in-wedges, displays multiple large-scale worlds in wedges that act as volumetric portals. The interface supports navigation, selection, and view manipulation. Since the techniques were inspired directly by problems facing forest scientists, the interface was evaluated by building a complete multivariate data visualization of the US Forest Service Forest Inventory and Analysis public dataset. Scientist user feedback and lessons from iterative design are reported.},
	author = {Nam, Jung Who and McCullough, Krista and Tveite, Joshua and Espinosa, Maria Molina and Perry, Charles H. and Wilson, Barry T. and Keefe, Daniel F.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797871},
	issn = {2642-5254},
	keywords = {Data visualization;Forestry;Visualization;Three-dimensional displays;Portals;Computer science;Task analysis;worlds-in-miniature;3D user interface;presence;comparative visualization;Human-centered computing---Virtual reality;Human-centered computing---Interaction techniques;Human-centered computing---Scientific visualization;Human-centered computing---Geographic visualization},
	month = {March},
	pages = {747-755},
	title = {Worlds-in-Wedges: Combining Worlds-in-Miniature and Portals to Support Comparative Immersive Visualization of Forestry Data},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797871}}

@inproceedings{8797876,
	abstract = {To capture a remote 3D image, conventional stereo cameras attached to a robot head have been commonly used. However, when the head and cameras rotate, the captured image in buffers is degraded by latency and motion blur, which may cause VR sickness. In the present study, we propose a method named TwinCam in which we use two 360$\,^{\circ}$ cameras spaced at the standard interpupillary distance and keep the direction of the lens constant in the world coordinate even when the camera bodies are rotated to reflect the orientation of the observer's head and the position of the eyes. We consider that this method can suppress the image buffer size to send to the observer because each camera captures the omnidirectional image without lens rotation. This paper introduces the mechanical design of our camera system and its potential for visual telepresence through three experiments. Experiment 1 confirmed the requirement of a stereoscopic rather than monoscopic camera for highly accurate depth perception, and Experiments 2 and 3 proved that our mechanical camera setup can reduce motion blur and VR sickness.},
	author = {Ikei, Yasushi and Yem, Vibol and Tashiro, Kento and Fujie, Toi and Amemiya, Tomohiro and Kitazaki, Michiteru},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797876},
	issn = {2642-5254},
	keywords = {Cameras;Streaming media;Head;Lenses;Robot vision systems;Resists;Stereo image processing;Stereoscopic 360$\,^{\circ}$ image;motion blur;latency;VR sickness;telepresence;Human-centered computing---Displays and imagers;Virtual reality;Computing methodologies---3D imaging},
	month = {March},
	pages = {431-439},
	title = {Live Stereoscopic 3D Image with Constant Capture Direction of 360$\,^{\circ}$ Cameras for High-Quality Visual Telepresence},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797876}}

@inproceedings{8797889,
	abstract = {The decisions made by an Air Attack Supervisor (AAS) helicopter co-pilots in aerial firefighting have critical and immediate impacts. It is difficult to always make fast, high quality decisions due to the mental and physical stress being experienced. Real world training exercises have limitations such as safety, cost, time and difficulty in reproducing events, making frequent training infeasible. Virtual Reality (VR) offers new training opportunities, but it is challenging to create a virtual environment with the analogous level of stress experienced in the real-world. In this paper, we investigate the use of a multi-user, collaborative, multi-sensory (vision, audio, tactile) VR system to produce a realistic training environment for practising aerial firefighting training scenarios. We focus on a comparison between our VR training system, an equivalent real-world field training and an existing radio-only exercise currently in use, where we compare Heart-Rate Variability (HRV) and self reported stress using the Short Stress State Questionnaire (SSSQ). We conducted the study with real trainee AAS firefighters to determine the effectiveness of the system. Our results show that there were no significant differences between the VR training exercise and the real-world exercise in terms of the level of stress, measured by HRV, and no significant difference between VR and radio-only exercises, as reported by the SSSQ.},
	author = {Clifford, Rory M.S. and Jung, Sungchul and Hoermann, Simon and Billinghurst, Mark and Lindeman, Robert W.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797889},
	issn = {2642-5254},
	keywords = {Training;Stress;Stress measurement;Decision making;Helicopters;Task analysis;Heart rate;Virtual Reality;Training;Assessment;User Centered Design;Situated Learning;Aerial Firefighting;Human-centered computing---Visualization---Visualization techniques---Treemaps;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {181-189},
	title = {Creating a Stressful Decision Making Environment for Aerial Firefighter Training in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797889}}

@inproceedings{8797906,
	abstract = {This paper tackles a challenging problem for interactive rigid-fluid interaction sound synthesis. One core issue of the rigid-fluid interaction in multisensory VR system is how to balance the algorithm efficiency, result authenticity and result synchronization. Since the sampling rate of audio is far greater than visual and haptic modalities, sound synthesis for a multisensory VR system is more difficult than visual simulation and haptic rendering, which still remains an open challenge until now. Therefore, this paper focuses on developing an efficient sound synthesis method tailored for a multisensory system. To improve the result authenticity while ensuring real time performance and result synchronization, we propose a novel haptic force guided granular sound synthesis method tailored for sounding in multisensory VR systems. To the best of our knowledge, this is the first step that exploits haptic force feedback from the tactile channel for guiding sound synthesis in a multisensory VR system. Specifically, we propose a modified spectral granular sound synthesis method, which can ensure real time simulation and improve the result authenticity as well. Then, to balance the algorithm efficiency and result synchronization, we design a multi-force (MF) granulation algorithm which avoids repeated analysis of fluid particle motion and thereby improves the synchronization performance. Various results show that the proposed sound synthesis method effectively overcomes the limitations of existing methods in terms of audio modality, which has great potential to provide powerful technological support for building a more immersive multisensory VR system.},
	author = {Cheng, Haonan and Liu, Shiguang},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797906},
	issn = {2642-5254},
	keywords = {Haptic interfaces;Rendering (computer graphics);Synchronization;Force;Computational modeling;Real-time systems;Solid modeling;Human-centered computing---Human computer interaction---Interaction devices---Haptic devices;Applied computing---Arts and humanities---Sound and music computing},
	month = {March},
	pages = {111-119},
	title = {Haptic Force Guided Sound Synthesis in Multisensory Virtual Reality (VR) Simulation for Rigid-Fluid Interaction},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797906}}

@inproceedings{8797911,
	abstract = {Viewers of virtual reality appear to have an incorrect sense of space when performing blind directed-action tasks, such as blind walking or blind throwing. It has been shown that various manipulations can influence this incorrect sense of space, and that the degree of misperception varies by person. It follows that one could measure the degree of misperception an individual experiences and generate some manipulation to correct for it, though it is not clear that correct behavior in a specific blind directed action task leads to correct behavior in all tasks in general. In this work, we evaluate the effectiveness of correcting perceived distance in virtual reality by first measuring individual perceived distance through blind throwing, then manipulating sense of space using a vertex shader to make things appear more or less distant, to a degree personalized to the individual's perceived distance. Two variants of the manipulation are explored. The effects of these personalized manipulations are first evaluated when performing the same blind throwing task used to calibrate the manipulation. Then, in order to observe the effects of the manipulation on dissimilar tasks, participants perform two perceptual matching tasks which allow full visual feedback as objects, or the participants themselves, move through space.},
	author = {Peer, Alex and Ponto, Kevin},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797911},
	issn = {2642-5254},
	keywords = {Task analysis;Virtual environments;Measurement uncertainty;Atmospheric measurements;Particle measurements;Rendering (computer graphics);I.3.7 [Computing Methodologies]: Graphics Utilities---Virtual reality},
	month = {March},
	pages = {244-250},
	title = {Mitigating Incorrect Perception of Distance in Virtual Reality through Personalized Rendering Manipulation},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797911}}

@inproceedings{8797914,
	abstract = {We introduce a novel, perceptually derived metric (P - Reverb) that relates the just-noticeable difference (JND) of the early sound field (also called early reflections) to the late sound field (known as late reflections or reverberation). Early and late reflections are crucial components of the sound field and provide multiple perceptual cues for auditory displays. We conduct two extensive user evaluations that relate the JNDs of early reflections and late reverberation in terms of the mean-free path of the environment and present a novel P - Reverb metric. Our metric is used to estimate dynamic reverberation characteristics efficiently in terms of important parameters like reverberation time (RT60). We show the numerical accuracy of our P - Reverb metric in estimating RT60. Finally, we use our metric to design an interactive sound propagation algorithm and demonstrate its effectiveness on various benchmarks.},
	author = {Rungta, Atul and Rewkowski, Nicholas and Klatzky, Roberta and Manocha, Dinesh},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797914},
	issn = {2642-5254},
	keywords = {Reverberation;Measurement;Erbium;Rendering (computer graphics);Ray tracing;Auditory displays},
	month = {March},
	pages = {455-463},
	title = {P-Reverb: Perceptual Characterization of Early and Late Reflections for Auditory Displays},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797914}}

@inproceedings{8797921,
	abstract = {We introduce TacTiles, light (1.8g), low-power (130 mW), and small form-factor (1 cm3) electromagnetic actuators that can form a flexible haptic array to provide localized tactile feedback. Our novel hardware design uses a custom 8-layer PCB, dampening materials, and asymmetric latching, enabling two distinct modes of actuation: contact and pulse mode. We leverage these modes in Virtual Reality (VR) to render continuous contact with objects and the exploration of object surfaces and volumes with spatial haptic patterns. Results from a series of experiments show that users are able to localize feedback, discriminate between modes with high accuracy, and differentiate objects from haptic surfaces and volumes even without looking at them.},
	author = {Vechev, Velko and Zarate, Juan and Lindlbauer, David and Hinchet, Ronan and Shea, Herbert and Hilliges, Otmar},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797921},
	issn = {2642-5254},
	keywords = {Actuators;Haptic interfaces;Rendering (computer graphics);Skin;Electromagnetics;Force;Tactile sensors;Human-centered computing---Human computer interaction (HCI)---Interaction devices---Haptic devices;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality},
	month = {March},
	pages = {312-320},
	title = {TacTiles: Dual-Mode Low-Power Electromagnetic Actuators for Rendering Continuous Contact and Spatial Haptic Patterns in VR},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797921}}

@inproceedings{8797923,
	abstract = {Car interior design, such as dashboard, broadly consists of two parts. One is shape design, where the processes of 2D drawing, 3D modeling and evaluation with full-scale mockups are iterated, which takes a massive amount of time and cost. The other is material design such as surface property tuning, where designers compare material samples. This way, however, has a limitation on the number of material samples to compare and does not allow applying of the samples of interest to the whole mockups in early phases. In this paper, we apply projection mapping technique to boost the design process by altering the appearance of the surface of projected objects and enabling various shape and material evaluations in early phases. Our proposed system uses multiple projectors, one of which is 4K projector to reproduce fine leather surface. Utilizing physiological and psychological depth cues, the system allows the user to perceive the projected mockup as deformed. Psychological experiments confirm that users perceive deformation and have controlled impression on leather reproduced with certain parameters. In addition, we discuss the usability of the proposed system as a support system of car interior design.},
	author = {Takezawa, Takuro and Iwai, Daisuke and Sato, Kosuke and Hara, Toshihiro and Takeda, Yusaku and Murase, Kenji},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797923},
	issn = {2642-5254},
	keywords = {Shape;Automobiles;Strain;Three-dimensional displays;Usability;Resists;Stereo image processing;Human-centered computing---Interaction design---Interaction design process and methods---Scenario-based design;Computing methodologies---Computer graphics---graphics systems and interfaces---Perception},
	month = {March},
	pages = {251-258},
	title = {Material Surface Reproduction and Perceptual Deformation with Projection Mapping for Car Interior Design},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797923}}

@inproceedings{8797925,
	abstract = {Wearable VR/AR devices provide users with fully immersive experience in a virtual environment, enabling possibilities to reshape the forms of entertainment and telepresence. While the body language is a crucial element in effective communication, wearing a head-mounted display (HMD) could severely hinder the eye contact and block facial expressions. We present a novel headset removal technique that enables high-quality occlusion-free communication in virtual environment. In particular, our solution synthesizes photoreal faces in the occluded region with faithful reconstruction of facial expressions and eye movements. Towards this goal, we develop a novel capture setup that consists of two near-infrared (NIR) cameras inside the HMD for eye capturing and one external RGB camera for recording visible face regions. To enable realistic face synthesis with consistent illuminations, we propose a data-driven approach to fuse the narrow-field-of-view NIR images with the RGB image captured from the external camera. In addition, to generate pho-torealistic eyes, a dedicated algorithm is proposed to colorize the NIR eye images and further rectify the color distortion caused by the non-linear mapping of IR light sensitivity. Experimental results demonstrate that our framework is capable to synthesize high-fidelity unoccluded facial images with accurate tracking of head motion, facial expression and eye movement.},
	author = {Zhao, Yajie and Xu, Qingguo and Chen, Weikai and Du, Chao and Xing, Jun and Huang, Xinyu and Yang, Ruigang},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797925},
	issn = {2642-5254},
	keywords = {Face;Cameras;Headphones;Three-dimensional displays;Image reconstruction;Resists;Tracking;AR/VR;headset removal;face inpainting;eye synthesis},
	month = {March},
	pages = {267-276},
	title = {Mask-off: Synthesizing Face Images in the Presence of Head-mounted Displays},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797925}}

@inproceedings{8797974,
	abstract = {This paper proposes a novel interface for virtual reality in which physical interface components are mapped to multiple virtual counterparts using haptic retargeting illusions. This gives virtual reality interfaces the ability to have correct haptic sensations for many virtual buttons although in the physical space there is only one. This is a generic system that can be applied to areas including design, interaction tasks, product prototype development and interactive games in virtual reality. The system presented extends existing retargeting algorithms to support asymmetric bimanual interactions. A new warp technique, called interface warp, was developed to support remapped virtual reality user interfaces. Through an experimental user study, we explore the effects of bimanual retargeting and the interface warp technique on task response time, errors, presence, perceived manipulation compared to unimanual (single handed) retargeting and other existing warp techniques. The results demonstrated faster task response time and less errors for the interface warp technique and shows no significant effect of bimanual interactions.},
	author = {Matthews, Brandon J. and Thomas, Bruce H. and Von Itzstein, Stewart and Smith, Ross T.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797974},
	issn = {2642-5254},
	keywords = {Haptic interfaces;Virtual reality;Task analysis;User interfaces;Mathematical model;Visualization;Shape;H.5.2 [Information Interfaces and Presentation]: User Interfaces---Haptic I/O;H.1.2 [Models and Principles]: User/Machine Systems---Human Factors},
	month = {March},
	pages = {19-27},
	title = {Remapped Physical-Virtual Interfaces with Bimanual Haptic Retargeting},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797974}}

@inproceedings{8797975,
	abstract = {Most AR and VR headsets use stereoscopic displays to show virtual objects in 3D. However, the limitations of current stereo display systems affect depth perception through conflicting depth cues, which then also affect virtual hand interaction in peri-personal space, i.e., within arm's reach. We performed a Fitts' law experiment to better understand the impact of stereo display deficiencies of AR and VR headsets on pointing at close-by targets arranged laterally or along the line of sight. According to our results, the movement direction and the corresponding change in target depth affect pointing time and throughput; subjects' movements towards/away from their head were slower and less accurate than their lateral movements (left/right). However, even though subjects moved faster in AR, we did not observe a significant difference for pointing performance between AR and VR headsets, which means that previously identified differences in depth perception between these platforms seem to have no strong effect on interaction. Our results also help 3D user interface designers understand how changes in target depth affect users' performance in different movement directions in AR and VR.},
	author = {Batmaz, Anil Ufuk and Machuca, Mayra Donaji Barrera and Pham, Duc Minh and Stuerzlinger, Wolfgang},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797975},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Headphones;Task analysis;Visualization;Stereo image processing;TV;Display systems;3D pointing;virtual hand;selection;Fitts Law;AR;VR;Human-centered computing---Human-computer interaction (HCI)---Interaction paradigms---Virtual reality;Human-centered computing---Human-computer interaction (HCI)---Interaction paradigms---Mixed/Augmented Reality;Human-centered computing---Human-computer interaction (HCI)---Interaction techniques---Pointing;Human-centered computing---Interaction design},
	month = {March},
	pages = {585-592},
	title = {Do Head-Mounted Display Stereo Deficiencies Affect 3D Pointing Tasks in AR and VR?},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797975}}

@inproceedings{8797977,
	abstract = {Virtual Reality can impose cognitive demands on users and influence their task performance. These cognitive demands, however, have been difficult to measure precisely without inducing breaks of presence. Based on findings in psychological science on how motion trajectories reflect underlying cognitive processes, we investigated entropy (i.e. the degree of movement irregularity) as an unobtrusive measure of mental workload. Entropy values were obtained from a time-series history of controller movement data. Mental workload is considered high over a given time interval, when the measured entropy is high as well. By manipulating the difficulty of a simple rhythm game we could show that the results are comparable to the results of the NASA-TLX questionnaire, which is currently used as the gold standard in VR for measuring mental workload. Thus, our results pave the way for further investigating the entropy of controller movements as a precise measurement of mental workload in VR.},
	author = {Reinhardt, Daniel and Haesler, Steffen and Hurtienne, J{\"o}rn and Wienrich, Carolin},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797977},
	issn = {2642-5254},
	keywords = {Task analysis;Entropy;Physiology;Games;Current measurement;Atmospheric measurements;Particle measurements;Sample Entropy;entropy of controller movements;virtual reality;non-intrusive measure;evaluation method;mental workload;H.5.2 [Information Interfaces and Presentation (e.g., HCI)]: User Interfaces---Evaluation\methodology},
	month = {March},
	pages = {802-808},
	title = {Entropy of Controller Movements Reflects Mental Workload in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797977}}

@inproceedings{8797978,
	abstract = {We introduce IATK, the Immersive Analytics Toolkit, a software package for Unity that allows interactive authoring and exploration of data visualisation in immersive environments. The design of IATK was informed by interdisciplinary expert-collaborations as well as visual analytics applications and iterative refinement over several years. IATK allows for easy assembly of visualisations through a grammar of graphics that a user can configure in a GUI-in addition to a dedicated visualisation API that supports the creation of novel immersive visualisation designs and interactions. IATK is designed with scalability in mind, allowing visualisation and fluid responsive interactions in the order of several million points at a usable frame rate. This paper outlines our design requirements, IATK's framework design and technical features, its user interface, as well as application examples.},
	author = {Cordeil, Maxime and Cunningham, Andrew and Bach, Benjamin and Hurter, Christophe and Thomas, Bruce H. and Marriott, Kim and Dwyer, Tim},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797978},
	issn = {2642-5254},
	keywords = {Data visualization;Grammar;Visualization;Three-dimensional displays;Geometry;Two dimensional displays;Human-centered computing---Visualization---Visualization techniques---;Human-centered computing---Visualization---visualisation design and evaluation methods},
	month = {March},
	pages = {200-209},
	title = {IATK: An Immersive Analytics Toolkit},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797978}}

@inproceedings{8797983,
	abstract = {Redirected walking enables users to locomote naturally within a virtual environment that is larger than the available physical space. These systems depend on steering algorithms that continuously redirect users within limited real world boundaries. While a majority of the most recent research has focused on predictive algorithms, it is often necessary to utilize reactive approaches when the user's path is unconstrained. Unfortunately, previously proposed reactive algorithms assume a completely empty space with convex boundaries and perform poorly in complex real world spaces containing obstacles. To overcome this limitation, we present Push/Pull Reactive (P2R), a novel algorithm that uses an artificial potential function to steer users away from potential collisions. We also introduce three new reset strategies and conducted an experiment to evaluate which one performs best when used with P2R. Simulation results demonstrate that the proposed approach outperforms the previous state-of-the-art reactive algorithm in non-convex spaces with and without interior obstacles.},
	author = {Thomas, Jerald and Rosenberg, Evan Suma},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797983},
	issn = {2642-5254},
	keywords = {Prediction algorithms;Legged locomotion;Force;Virtual environments;Heuristic algorithms;Layout;Redirected Walking;Virtual Reality;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality},
	month = {March},
	pages = {56-62},
	title = {A General Reactive Algorithm for Redirected Walking Using Artificial Potential Functions},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797983}}

@inproceedings{8797988,
	abstract = {In 2D interfaces, actions are often represented by fixed tools arranged in menus, palettes, or dedicated parts of a screen, whereas 3D interfaces afford their arrangement at different depths relative to the user and the user can move them relative to each other. In this paper, we introduce EyeSeeThrough as a novel interaction technique that utilizes eye-tracking in VR. The user can apply an action to an intended object by visually aligning the object with the tool at the line-of-sight, and then issue a confirmation command. The underlying idea is to merge the two-step process of 1) selection of a mode in a menu and 2) applying it to a target, into one unified interaction. We present a user study where we compare the method to the baseline two-step selection. The results of our user study showed that our technique outperforms the two step selection in terms of speed and comfort. We further developed a prototype of a virtual living room to demonstrate the practicality of the proposed technique.},
	author = {Mardanbegi, Diako and Mayer, Benedikt and Pfeuffer, Ken and Jalaliniya, Shahram and Gellersen, Hans and Perzl, Alexander},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797988},
	issn = {2642-5254},
	keywords = {Tools;Visualization;Three-dimensional displays;Task analysis;User interfaces;Space exploration;Virtual environments;Human-centered computing---Human-centered-computing---Gestural input},
	month = {March},
	pages = {474-483},
	title = {EyeSeeThrough: Unifying Tool Selection and Application in Virtual Environments},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797988}}

@inproceedings{8797989,
	abstract = {Jumping is a fundamental movement in our daily lives that is often used in many video games. However, little research has been done on jumping and its possible use as a redirection technique in virtual reality (VR). In this study we explore Redirected Jumping, a novel redirection technique which enables us to purposefully manipulate the mapping of the user's physical jumping movements (e.g., distance and direction) to movement in the virtual space, allowing richer and more active physical VR experiences within a limited tracking area. To demonstrate the possibilities afforded by Redirected Jumping, we implemented a jump detection algorithm and jumping redirection methods for three basic jumping actions (i.e., horizontal, vertical, and rotational jumps) using common VR devices. We conducted three user studies to investigate the effective manipulation ranges, and the results revealed that our methods can manipulate a user's jumping movements without his/her noticing, similar to walking.},
	author = {Hayashi, Daigo and Fujita, Kazuyuki and Takashima, Kazuki and Lindeman, Robert W. and Kitamura, Yoshifumi},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797989},
	issn = {2642-5254},
	keywords = {Legged locomotion;Virtual reality;Meters;Tracking;Games;Sports;Foot;Virtual reality;virtual locomotion;redirected walking;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems---Artificial, augmented, and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism---Virtual reality},
	month = {March},
	pages = {386-394},
	title = {Redirected Jumping: Imperceptibly Manipulating Jump Motions in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797989}}

@inproceedings{8797990,
	abstract = {Line drawing is an important and concise method to depict the shape of an object. Stereo line drawing, a combination of line drawing and stereo rendering, not only efficiently conveys shape but also provides users with a visual experience of a stereoscopic 3D world. Contours are the most important lines to draw. However, contours must be rendered consistently for two eyes because of their view-dependent nature; otherwise, they cause binocular rivalry and viewing discomfort. This paper proposes a novel solution to draw stereo-consistent contours in real time. First, we extend the concept of epipolar-slidability and derive a new criterion to check epipolar-slidability by the monotonicity of the trajectory of the viewpoints of contour points. Then, we design an algorithm to test the epipolar-slidability of contours by conducting an image space search rather than sampling multiple viewpoints. Results show that the proposed method has a much lower cost than that of previous works, therefore enables the real-time rendering and editing of stereo-consistent contours for users, such as changing camera viewpoints, editing object geometry, tweaking parameters to show contours with different details, etc.},
	author = {He, Dejing and Wang, Rui and Bao, Hujun},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797990},
	issn = {2642-5254},
	keywords = {Rendering (computer graphics);Real-time systems;Trajectory;Three-dimensional displays;Shape;Geometry;Solid modeling;Stereo contour rendering;binocular rivalry;stereo consistency;epipolar-slidability;real-time rendering;Computing methodologies---Computer graphics---Rendering},
	month = {March},
	pages = {81-87},
	title = {Real-Time Rendering of Stereo-Consistent Contours},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797990}}

@inproceedings{8797992,
	abstract = {We investigate how reading text in augmented reality (AR) glasses and the simultaneous execution of three real-world tasks interfere with each other. The three tasks are a visual stimulus-response task (VSRT), a simple walking task and a walking obstacle course. Also, we investigate the effects of different AR text positions on primary task and reading performance as well as subjective preference. We propose a novel out of sight body-locked text placement for AR text presentation to be used in dual-task situations and compare it to head-locked text placement, each in two heights. AR reading affected performance in all tasks and reading speed was affected in all dual-task conditions. Participants subjectively preferred the body-locked text presentation, while objective measures do not reflect that preference. Differences between the tasks and several interaction effects between task and AR text placement demonstrate the necessity to carefully consider the context of use when designing AR reading UIs. The presented study with 12 participants provides insights into the effects of AR glasses usage in dual-task situations and several design recommendations are derived from the results.},
	author = {Klose, Elisa Maria and Mack, Nils Adrian and Hegenberg, Jens and Schmidt, Ludger},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797992},
	issn = {2642-5254},
	keywords = {Task analysis;Glass;Legged locomotion;Augmented reality;Visualization;Smart phones;body-locked;head-locked;dual task;user study;text presentation;Augmented Reality;Human-Computer Interaction;Human Factors},
	month = {March},
	pages = {636-644},
	title = {Text Presentation for Augmented Reality Applications in Dual-Task Situations},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797992}}

@inproceedings{8797994,
	abstract = {Immersive movies take advantage of virtual reality (VR) to bring new opportunities for storytelling that allow users to naturally turn their heads and bodies to view a 3D virtual world and follow the story in a surrounding space. However, while many designers often assume scenarios where viewers stand and are free to physically turn without constraints, this excludes many commonly desired usage settings where the user may wish to remain seated, such as the use of VR while relaxing on the couch or passing the time during a flight. For such situations, large amounts of physical turning may be uncomfortable due to neck strain or awkward twisting. Our research investigates a technique that automatically rotates the virtual scene to help redirect the viewer's physical rotation while viewing immersive narrative experiences. By slowly rotating the virtual content, viewers are encouraged to gradually turn physically to align their head positions to a more comfortable straight-ahead viewing direction in seated situations where physical turning is not ideal. We present our study of technique design and an evaluation of how the redirection approach affects user comfort, sickness, the amount of physical rotation, and likelihood of viewers noticing the rotational adjustments. Evaluation results show the rotation technique was effective at significantly reducing the amount of physical turning while watching immersive videos, and only 39% of participants noticed the automated rotation when the technique rotated at a speed of 3 degrees per second.},
	author = {Stebbins, Travis and Ragan, Eric D.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797994},
	issn = {2642-5254},
	keywords = {Videos;Motion pictures;Turning;Head;Legged locomotion;Virtual reality;Three-dimensional displays;Human-centered computing---Visualization---Visualization techniques---Virtual reality},
	month = {March},
	pages = {377-385},
	title = {Redirecting View Rotation in Immersive Movies with Washout Filters},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797994}}

@inproceedings{8797997,
	abstract = {This paper discusses a virtual reality (VR) therapeutic video game for treatment of the neurological eye disorder, Amblyopia. Amblyopia is often referred to as lazy eye, and it entails weaker vision in one eye due to a poor connection between the eye and the brain. Until recently it was thought to be untreatable in adults, but new research has proven that with consistent therapy even adults can improve their Amblyopia, especially through perceptual learning and video games. Even so, therapy compliance remains low due to the fact that conventional therapies are perceived as either invasive, dull and/or boring. Our game aims to make Amblyopia therapy more immersive, enjoyable and playful. The game was perceived by our users to be a fun and accessible alternative, as it involves adhering a Bangerter foil (an opaque sticker) on a VR headset to blur vision in an Amblyopic person's dominant eye while having them playa VR video game. To perform well in the video game, their brain must adapt to rely on seeing with their weaker eye, thereby reforging that neurological connection. While testing our game, we also studied users behavior to investigate what visual and kinetic components were more effective therapeutically. Our findings generally show positive results, showing that visual acuity in adults increases with 45 minutes of therapy. Amblyopia has many negative symptoms including poor depth perception (nec-essary for daily activities such as driving), so this therapy could be life changing for adults with Amblyopia.},
	author = {Hurd, Ocean and Kurniawan, Sri and Teodorescu, Mircea},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8797997},
	issn = {2642-5254},
	keywords = {Games;Vision defects;Medical treatment;Visualization;Virtual reality;Headphones;Testing;Visual acuity---Visual stereopsis---LogMAR---Crowding},
	month = {March},
	pages = {492-499},
	title = {Virtual Reality Video Game Paired with Physical Monocular Blurring as Accessible Therapy for Amblyopia},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8797997}}

@inproceedings{8798016,
	abstract = {In this paper we present an approach for 6 DoF panoramic videos from omni-directional stereo (ODS) images using convolutional neural networks (CNNs). More specifically, we use CNNs to generate panoramic depth maps from ODS images in real-time. These depth maps would then allow for re-projection of panoramic images thus providing 6 DoF to a viewer in virtual reality (VR). As the boundaries of a panoramic image must touch in order to envelope a viewer, we introduce a border weighted loss function as well as new error metrics specifically tailored for panoramic images. We show experimentally that training with our border weighted loss function improves performance by benchmarking a baseline skip-connected encoder-decoder style network as well as other state-of-the-art methods in depth map estimation from mono and stereo images. Finally, a practical application for VR using real world data is also demonstrated.},
	author = {Lai, Po Kong and Xie, Shuang and Lang, Jochen and Lagani{\`e}re, Robert},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798016},
	issn = {2642-5254},
	keywords = {Cameras;Videos;Estimation;Real-time systems;Virtual reality;Training;Measurement;Computing methodologies---Virtual reality;Computer systems organization---Neural networks;Computing methodologies---Reconstruction},
	month = {March},
	pages = {405-412},
	title = {Real-Time Panoramic Depth Maps from Omni-directional Stereo Images for 6 DoF Videos in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798016}}

@inproceedings{8798018,
	abstract = {When a user interacts with a virtual agent via a mixed reality device, such as a Hololens or a Magic Leap headset, it is important to consider the semantics of the real-world scene in positioning the virtual agent, so that it interacts with the user and the objects in the real world naturally. Mixed reality aims to blend the virtual world with the real world seamlessly. In line with this goal, in this paper, we propose a novel approach to use scene semantics to guide the positioning of a virtual agent. Such considerations can avoid unnatural interaction experiences, e.g., interacting with a virtual human floating in the air. To obtain the semantics of a scene, we first reconstruct the 3D model of the scene by using the RGB-D cameras mounted on the mixed reality device (e.g., a Hololens). Then, we employ the Mask R-CNN object detector to detect objects relevant to the interactions within the scene context. To evaluate the positions and orientations for placing a virtual agent in the scene, we define a cost function based on the scene semantics, which comprises a visibility term and a spatial term. We then apply a Markov chain Monte Carlo optimization technique to search for an optimized solution for placing the virtual agent. We carried out user study experiments to evaluate the results generated by our approach. The results show that our approach achieved a higher user evaluation score than that of the alternative approaches.},
	author = {Lang, Yining and Liang, Wei and Yu, Lap-Fai},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798018},
	issn = {2642-5254},
	keywords = {Virtual reality;Semantics;Three-dimensional displays;Solid modeling;Optimization;Geometry;Cameras;Mixed Reality---Scene Understanding---Virtual Agent Positioning},
	month = {March},
	pages = {767-775},
	title = {Virtual Agent Positioning Driven by Scene Semantics in Mixed Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798018}}

@inproceedings{8798019,
	abstract = {In this paper, we introduce LargeSpace, the world's largest immersive display, and discuss the principles of its design. To clarify the design of large-scale projection-based immersive displays, we address the optimum screen shape, projection approach, and arrangement of projectors and tracking cameras. In addition, a novel distortion correction method for panoramic stereo rendering is described. The method can be applied to any projection-based immersive display with any screen shape, and can generate real-time panoramic-stereoscopic views from the viewpoints of tracked participants. To validate the design principles and the rendering algorithm, we implement the LargeSpace and confirm that the method can generate the correct perspective from any position inside the screen viewing area. We implement several applications and show that large-scale immersive displays can be used in the fields of art and experimental psychology.},
	author = {Takatori, Hikaru and Hiraiwa, Masashi and Yano, Hiroaki and Iwata, Hiroo},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798019},
	issn = {2642-5254},
	keywords = {Shape;Position measurement;Optical variables measurement;Mirrors;Virtual environments;Buildings;Cameras;Human-centered computing---Human computer interaction (HCI) ---Interaction devices---Displays and imagers;Human-centered computing---Visualization---Visualization systems and tools;Applied computing---Arts and humanities---Media arts},
	month = {March},
	pages = {557-565},
	title = {Large-Scale Projection-Based Immersive Display: The Design and Implementation of LargeSpace},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798019}}

@inproceedings{8798021,
	abstract = {Slot machines are one of the most played games by pathological gamblers. New technologies, e.g. immersive Virtual Reality (VR), offer more possibilities to exploit erroneous beliefs in the context of gambling. However, the risk potential of VR-based gambling has not been researched, yet. A higher immersion might increase harmful aspects, thus making VR realizations more dangerous. Measuring harm-inducing factors reveals the risk potential of virtual gambling. In a user study, we analyze a slot machine realized as a desktop 3D and as an immersive VR version. Both versions are compared in respect to effects on dissociation, urge to gamble, dark flow, and illusion of control. Our study shows significantly higher values of dissociation, dark flow, and urge to gamble in the VR version. Presence significantly correlates with all measured harm-inducing factors. We demonstrate that VR-based gambling has a higher risk potential. This creates the importance of regulating VR-based gambling.},
	author = {Heidrich, David and Oberd{\"o}rfer, Sebastian and Latoschik, Marc Erich},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798021},
	issn = {2642-5254},
	keywords = {Games;Pathology;Virtual reality;Three-dimensional displays;Indexes;Visualization;Current measurement;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods;Human-centered computing---Human computer interaction (HCI)---Empirical studies in HCI;Human-centered computing---Interaction paradigms---Virtual Reality},
	month = {March},
	pages = {793-801},
	title = {The Effects of Immersion on Harm-inducing Factors in Virtual Slot Machines},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798021}}

@inproceedings{8798025,
	abstract = {VR applications rely on the user's ability to explore the virtual scene efficiently. In complex scenes, occlusions limit what the user can see from a given location, and the user has to navigate the viewpoint around occluders to gain line of sight to the hidden parts of the scene. When the disoccluded regions prove to be of no interest, the user has to retrace their path, making scene exploration inefficient. Furthermore, the user might not be able to assume a viewpoint that would reveal the occluded regions due to physical limitations, such as obstacles in the real world hosting the VR application, viewpoints beyond the tracked area, or viewpoints above the user's head that cannot be reached by walking. Several occlusion management methods have been proposed in visualization research, such as top view, X-ray, and multiperspective visualization, which help the user see more from the current position, having the potential to improve the exploration efficiency of complex scenes. This paper reports on a study that investigates the potential of these three occlusion management methods in the context of VR applications, compared to conventional navigation. Participants were required to explore two virtual scenes to purchase five items in a virtual Supermarket, and to find three people in a virtual parking garage. The task performance metrics were task completion time, total distance traveled, and total head rotation. The study also measured user spatial awareness, depth perception, and simulator sickness. The results indicate that users benefit from top view visualization which helps them learn the scene layout and helps them understand their position within the scene, but the top view does not let the user find targets easily due to occlusions in the vertical direction, and due to the small image footprint of the targets. The X-ray visualization method worked better in the garage scene, a scene with a few big occluders and a low occlusion depth complexity' and less well in the Supermarket scene, a scene with many small occluders that create high occlusion depth complexity. The multi-perspective visualization method achieves better performance than the top view method and the X-ray method, in both scenes. There are no significant differences between the three methods and the conventional method in terms of spatial awareness, depth perception, and simulator sickness.},
	author = {Wang, Lili and Zhao, Han and Wang, Zesheng and Wu, Jian and Li, Bingqiang and He, Zhiming and Popescu, Voicu},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798025},
	issn = {2642-5254},
	keywords = {Visualization;Cameras;Task analysis;X-ray imaging;Navigation;Legged locomotion;Resists;Scene Exploration;Occlusion Management;Virtual Reality;Top View;X-ray;Multiperspective Visualization;Human-centered computing---Human-centered interaction---Virtual reality;Computer graphics---Ocllusion management---Visualization},
	month = {March},
	pages = {708-716},
	title = {Occlusion Management in VR: A Comparative Study},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798025}}

@inproceedings{8798029,
	abstract = {Is there an effect of Virtual Reality (VR) Head-Mounted Display (HMD) on the user's mental effort? In this paper, we compare the mental effort in VR versus in real environments. An experiment (N=27) was conducted to assess the effect of being immersed in a virtual environment (VE) using an HMD on the user's mental effort while performing a standardized cognitive task (the well-known N-back task, with three levels of difficulty, N  {1,2,3}). In addition to test the effect of the environment (i.e., virtual versus real), we also explored the impact of performing a dual task (i.e., sitting versus walking) in both environments on mental effort. The mental effort was assessed through self-report, task performance, behavioural and physiological measures. In a nutshell, the analysis of all measurements revealed no significant effect of being immersed in the VE on the users' mental effort. In contrast, natural walking significantly increased the users' mental effort. Taken together, our results support the fact there is no specific additional mental effort related to the immersion in a VE using a VR HMD.},
	author = {Luong, Tiffany and Martin, Nicolas and Argelaguet, Ferran and L{\'e}cuyer, Anatole},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798029},
	issn = {2642-5254},
	keywords = {Task analysis;Resists;Training;Legged locomotion;Physiology;Virtual environments;Visualization;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality},
	month = {March},
	pages = {809-816},
	title = {Studying the Mental Effort in Virtual Versus Real Environments},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798029}}

@inproceedings{8798036,
	abstract = {In immersive virtual environments (IVE), users' visual and auditory perception is replaced by computer-generated stimuli. Thus, knowing the positions of real objects is crucial for physical safety. While some solutions exist, e. g., using virtual replicas or visible cues indicating the interaction space boundaries, these are limiting the IVE design or depend on the hardware setup. Moreover, most solutions cannot handle lost tracking, erroneous tracker calibration, or moving obstacles. However, these are common scenarios especially for the increasingly popular home virtual reality settings. In this paper, we present a stand-alone hardware device designed to alert IVE users for potential collisions with real-world objects. It uses distance sensors mounted on a head-mounted display (HMD) and vibro-tactile actuators inserted into the HMD's face cushion. We implemented different types of sensor-actuator mappings with the goal to find a mapping function that is minimally obtrusive in normal use, but efficiently alerting in risk situations.},
	author = {Valkov, Dimitar and Linsen, Lars},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798036},
	issn = {2642-5254},
	keywords = {Sensors;Vibrations;Actuators;Haptic interfaces;Hardware;Resists;Virtual environments;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Human-centered computing---Human computer interaction (HCI)---Interaction devices---Haptic devices},
	month = {March},
	pages = {340-349},
	title = {Vibro-tactile Feedback for Real-world Awareness in Immersive Virtual Environments},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798036}}

@inproceedings{8798040,
	abstract = {How does the representation of an embodied avatar influence the way in which one perceives the scale of a virtual environment? In virtual reality, it is common to embody avatars of various appearances, from abstract to realistic. It is known that changes in the realism of virtual hands affect the self-body perception, including body ownership. However, the influence of self-avatar realism on the perception of non-body objects has not been investigated. Considering the theory that the scale of the external environment is perceived relative to the size of one's body (body-based scaling), it can be hypothesized that the realism of an avatar affects not only body ownership but also the fidelity of the avatar with respect to our own body as a metric. Therefore, this study examines how avatar realism affects perceived object sizes as the size of the virtual hand changes. In the experiment, we manipulate the level of realism (realistic, iconic, and abstract) and size (veridical and enlarged) of the virtual hand and measure the perceived size of a cube. The results show that the size of the cube is perceived to be smaller when the virtual hand is enlarged compared to when it is veridical, indicating that the participants perceive the sizes of objects based on the size of the avatar, only in the case of a highly realistic hand. Our findings indicate that the more realistic the avatar, the stronger is the sense of embodiment including body ownership, which fosters scaling the size of objects using the size of the body as a fundamental metric. This provides evidence that self-avatar appearances affect how we perceive not only virtual bodies but also virtual spaces.},
	author = {Ogawa, Nami and Narumi, Takuji and Hirose, Michitaka},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798040},
	issn = {2642-5254},
	keywords = {Avatars;Estimation;Visualization;Measurement;Three-dimensional displays;Haptic interfaces;Human-centered computing---Virtual reality},
	month = {March},
	pages = {519-528},
	title = {Virtual Hand Realism Affects Object Size Perception in Body-Based Scaling},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798040}}

@inproceedings{8798043,
	abstract = {The present study investigates users' movement behavior in a virtual environment when they attempted to avoid a virtual character. At each iteration of the experiment, four conditions (Self-Avatar LookAt, No Self-Avatar LookAt, Self-Avatar No LookAt, and No Self-Avatar No LookAt) were applied to examine users' movement behavior based on kinematic measures. During the experiment, 52 participants were asked to walk from a starting position to a target position. A virtual character was placed at the midpoint. Participants were asked to wear a head-mounted display throughout the task, and their locomotion was captured using a motion capture suit. We analyzed the captured trajectories of the participants' routes on four kinematic measures to explore whether the four experimental conditions influenced the paths they took. The results indicated that the Self-Avatar LookAt condition affected the path the participants chose more significantly than the other three conditions in terms of length, duration, and deviation, but not in terms of speed. Overall, the length and duration of the task, as well as the deviation of the trajectory from the straight line, were greater when a self-avatar represented participants. An additional effect on kinematic measures was found in the LookAt (Gaze) conditions. Implications for future research are discussed.},
	author = {Mousas, Christos and Koilias, Alexandros and Anastasiou, Dimitris and Rekabdar, Banafsheh and Anagnostopoulos, Christos-Nikolaos},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798043},
	issn = {2642-5254},
	keywords = {Legged locomotion;Virtual reality;Collision avoidance;Trajectory;Task analysis;Resists;Kinematics;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality},
	month = {March},
	pages = {726-734},
	title = {Effects of Self-Avatar and Gaze on Avoidance Movement Behavior},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798043}}

@inproceedings{8798065,
	abstract = {Augmented Reality (AR) provides real-time information by superimposing virtual information onto users' view of the real world. Our work is the first to explore how peripheral vision, instead of central vision, can be used to read text on AR and smart glasses. We present Peritext, a multiword reading interface using rapid serial visual presentation (RSVP). This enables users to observe the real world using central vision, while using peripheral vision to read virtual information. We first conducted a lab-based study to determine the effect of different text transformation by comparing reading efficiency among 3 capitalization schemes, 2 font faces, 2 text animation methods, and 3 different numbers of words for RSVP paradigm. We found that title case capitalization, sans-serif font and word-wise typewriter animation with multiword RSVP display resulted in better reading efficiency, which together formed our Peritext design. Another lab-based study followed, investigating the performance of the Peritext against control text, and the results showed significant better performance. Finally, we conducted a field study to collect user feedback while using Peritext in real-world walking scenarios, and all users reported a preference of 5$\,^{\circ}$ eccentricity over 8$\,^{\circ}$.},
	author = {Ku, Pin-Sung and Lin, Yu-Chih and Peng, Yi-Hao and Chen, Mike Y.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798065},
	issn = {2642-5254},
	keywords = {Visualization;Animation;Legged locomotion;Augmented reality;Smart glasses;Monitoring;reading interface;augmented reality;peripheral vision;multiword;mobile;rapid serial visual presentation;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies;Human-centered computing---Visualization---Visualization design and evaluation methods Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Mixed/ augmented reality},
	month = {March},
	pages = {630-635},
	title = {PeriText: Utilizing Peripheral Vision for Reading Text on Augmented Reality Smart Glasses},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798065}}

@inproceedings{8798070,
	abstract = {When human operators locomote actively in virtual environments (VE), the movement range often has to be adapted to the limited dimensions of the physical space. This however might lead to a conflict between sensory information originating from user movements and sensory feedback provided through the virtual locomotion. To investigate whether different locomotion strategies that adapt virtual movement to the limited physical space impact cognitive processes, two experiments were conducted. The first experiment used walking in place, the second study scale of locomotion to investigate the impact of locomotion adaptation on the acquisition of spatial knowledge and user experience. We systematically analyzed body-based sensorial conflicts for the different adaptation strategies and reveal that neither walking in place nor scale of locomotion impacts spatial knowledge acquisition or user experience. We can conclude that visual cues indicating locomotion combined with body-based rotational cues seem to be sufficient for the acquisition of spatial knowledge and that locomotion with controllers seems efficient and preferable for users. The results link system-driven typologies with human-centered factors to guide systematic tests of locomotion techniques in virtual environments for future studies.},
	author = {Wienrich, Carolin and D{\"o}llinger, Nina and Kock, Simon and Gramann, Klaus},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798070},
	issn = {2642-5254},
	keywords = {Legged locomotion;Floors;Visualization;Cognitive processes;User experience;Task analysis;Locomotion;Body-Based Sensorial Cues;Spatial Cognition;User Experience;H.1.2 User/Machine Systems: Human information processing;H.5.2 User Interfaces: Evaluation/methodology, User-centered design},
	month = {March},
	pages = {690-698},
	title = {User-Centered Extension of a Locomotion Typology: Movement-Related Sensory Feedback and Spatial Learning},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798070}}

@inproceedings{8798074,
	abstract = {Procedural generation in virtual reality (VR) has been used to adapt the virtual world to various indoor environments, fitting different geometries and interiors with virtual environments. However, such applications require that the physical environment be known or pre-scanned prior to use to then generate the corresponding virtual scene, thus restricting the virtual experience to a controlled space. In this paper, we present VRoamer, which enables users to walk unseen physical spaces for which VRoamer procedurally generates a virtual scene on-the-fly. Scaling to the size of office buildings, VRoamer extracts walkable areas and detects physical obstacles in real time, instantiates pre-authored virtual rooms if their sizes fit physically walkable areas or otherwise generates virtual corridors and doors that lead to undiscovered physical areas. The use of these virtual structures allows VRoamer to (1) temporarily block users' passage, thus slowing them down while increasing VRoamer's insight into newly discovered physical areas, (2) prevent users from seeing changes beyond the current virtual scene, and (3) obfuscate the appearance of physical environments. VRoamer animates virtual objects to reflect dynamically discovered changes of the physical environment, such as people walking by or obstacles that become apparent. In our proof-of-concept study, participants were able to walk long distances through a procedurally generated dungeon experience and reported high levels of immersion.},
	author = {Cheng, Lung-Pan and Ofek, Eyal and Holz, Christian and Wilson, Andrew D.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798074},
	issn = {2642-5254},
	keywords = {Legged locomotion;Geometry;Virtual environments;Aerospace electronics;Real-time systems;Cameras;Virtual reality;procedural generation;real walking;locomotion techniques;redirected walking;H.5.1 [Information Interfaces and Presentation]: Multimedia, Information Systems-Virtual Realities},
	month = {March},
	pages = {359-366},
	title = {VRoamer: Generating On-The-Fly VR Experiences While Walking inside Large, Unknown Real-World Building Environments},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798074}}

@inproceedings{8798080,
	abstract = {We present an assessment of asymmetric interactions in Collaborative Virtual Environments (CVEs). In our asymmetric setup, two co-located users interact with virtual 3D objects, one in immersive Virtual Reality (VR) and the other in mobile Augmented Reality (AR). We conducted a study with 36 participants to evaluate performance and collaboration aspects of pair work, and compare it with two symmetric scenarios, either with both users in immersive VR or mobile AR. To perform this experiment, we adopt a collaborative AR manipulation technique from literature and develop and evaluate a VR manipulation technique of our own. Our results indicate that pairs in asymmetric VR-AR achieved significantly better performance than the AR symmetric condition, and similar performance to VR symmetric. Regardless of the condition, pairs had similar work participation indicating a high cooperation level even when there is a visualization and interaction asymmetry between the participants.},
	author = {Grandi, Jer{\^o}nimo Gustavo and Debarba, Henrique Galvan and Maciel, Anderson},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798080},
	issn = {2642-5254},
	keywords = {Collaboration;Three-dimensional displays;Visualization;Task analysis;Augmented reality;User interfaces;Human-centered computing---Human computer interaction (HCI)---Interaction techniques;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Mixed/augmented reality Human-centered computing---Collaborative and social computing},
	month = {March},
	pages = {127-135},
	title = {Characterizing Asymmetric Collaborative Interactions in Virtual and Augmented Realities},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798080}}

@inproceedings{8798089,
	abstract = {Objective: In recent years, the training of novice medical professionals with simulated environments such as virtual reality (VR) and augmented reality (AR) has increased dramatically. However, the usability of these technologies is limited due to the complexity involved in creating the clinical content. To be comparable to a clinical environment, the simulation platform should include real-world learning parameters such as patient physiology, emotions, and clinical team behaviors. Incorporating such nondeterministic parameters has historically required faculty to possess advanced programming skills. Lack of effective software for instructors to easily develop VR curriculum content is a hurdle in developing VR based curriculum. Method: We address this challenge through a software platform that simplifies the creation of Interactive Mixed Reality (IMR) scenarios. Three educational components we were able to embed into an IMR scenario includes 1) integrated 360-degree video recording of a clinical encounter to provide a first-person perspective, 2) rich annotated knowledge content, and 3) assessment questionnaire. We developed a sepsis prevention education scenario using the IMR software to demonstrate the potential of enhancing simulated medical training by accelerating clinical exposure for novice students. Result: An IRB approved study was conducted with a group of 28 novice students to evaluate the efficacy of the IMR technology. The participants provided feedback by answering demographics, NASA-TLX and system usability scale questionnaires. Significance: Our software is a step towards improving VR based education content development process. Conclusion: The studies conducted here provide preliminary evidence that the IMR software is a usable technology based on the NASA-TLX and system usability studies conducted. Future work will compare our new educational strategy for medical training with live simulation scenarios inside a hospital room and a simple video-based curriculum.},
	author = {Sankaran, Naveen Kumar and Nisar, Harris J. and Zhang, Ji and Formella, Kyle and Amos, Jennifer and Barker, Lisa T. and Vozenilek, John A. and LaValle, Steven M. and Kesavadas, Thenkurussi},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798089},
	issn = {2642-5254},
	keywords = {Training;Task analysis;Virtual reality;Software;Solid modeling;Biomedical imaging;Mixed reality---Medical training---Medical simulation---Sepsis training---Emergency medicine},
	month = {March},
	pages = {664-670},
	title = {Efficacy Study on Interactive Mixed Reality (IMR) Software with Sepsis Prevention Medical Education},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798089}}

@inproceedings{8798095,
	abstract = {Augmented reality (AR) technologies have the potential to provide individuals with unique training and visualizations, but the effectiveness of these applications may be influenced by users' perceptions of the distance to AR objects. Perceived distances to AR objects may be biased if these objects do not appear to make contact with the ground plane. The current work compared distance judgments of AR targets presented on the ground versus off the ground when no additional AR depth cues, such as shadows, were available to denote ground contact. We predicted that without additional information for height off the ground, observers would perceive the off-ground objects as placed on the ground, but at farther distances. Furthermore, this bias should be exaggerated when targets were viewed with one eye rather than two. In our experiment, participants judged the absolute egocentric distance to various cubes presented on or off the ground with an action-based measure, blind walking. We found that observers walked farther for off-ground AR objects and that this effect was exaggerated when participants viewed off-ground objects with monocular vision compared to binocular vision. However, we also found that the restriction of binocular cues influenced participants' distance judgments for on-ground AR objects. Our results suggest that distances to off-ground AR objects are perceived differently than on-ground AR objects and that the elimination of binocular cues further influences how users perceive these distances.},
	author = {Rosales, Carlos Salas and Pointon, Grant and Adams, Haley and Stefanucci, Jeanine and Creem-Regehr, Sarah and Thompson, William B. and Bodenheimer, Bobby},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798095},
	issn = {2642-5254},
	keywords = {Augmented reality;Visualization;Legged locomotion;Observers;Virtual environments;Meters;Augmented reality;Virtual environments;distance perception;depth cues;I.3.7 [Computer Graphics]: Three---Dimensional Graphics and Realism---Virtual Reality;J.4 [Computer Applications]: Social and Behavioral Sciences---Psychology},
	month = {March},
	pages = {237-243},
	title = {Distance Judgments to On- and Off-Ground Objects in Augmented Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798095}}

@inproceedings{8798108,
	abstract = {There is evidence that adding motion-tracked avatars to virtual environments increases users' sense of presence. High quality motion capture systems are cost sensitive for the average user and low cost resource-constrained systems introduce various forms of error to the tracking. Much research has looked at the impact of particular kinds of error, primarily latency, on factors such as body ownership, but it is still not known what level of tracking error is permissible in these systems to afford compelling social interaction. This paper presents a series of experiments employing a sizable subject pool (n=96) that study the impact of motion tracking errors on user experience for activities including social interaction and virtual object manipulation. Diverse forms of error that arise in tracking are examined, including latency, popping (jumps in position), stuttering (positions held in time) and constant noise. The focus is on error on a person's own avatar, but some conditions also include error on an interlocutor, which appears underexplored. The picture that emerges is complex. Certain forms of error impact performance, a person's sense of embodiment' enjoyment and perceived usability, while others do not. Notably, evidence was not found that tracking errors impact social presence, even when those errors are severe.},
	author = {Toothman, Nicholas and Neff, Michael},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798108},
	issn = {2642-5254},
	keywords = {Task analysis;Tracking;Avatars;Delays;Jitter;Headphones;Atmospheric measurements},
	month = {March},
	pages = {756-766},
	title = {The Impact of Avatar Tracking Errors on User Experience in VR},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798108}}

@inproceedings{8798111,
	abstract = {The advent of modern and affordable augmented reality head sets like Microsoft HoloLens has sparked new interest in using virtual and augmented reality technology in the analysis of molecular data. For all visualisation in immersive, mixed-reality scenarios, a sufficiently high rendering speed is an important factor, which leads to the issue of limited processing power available on fully untethered devices facing the situation of handling computationally expensive visualisations. Recent research shows that the space-filling model of even small data sets from the Protein Data Bank (PDB) cannot be rendered at desirable frame rates on the HoloLens. In this work, we report on how to improve the rendering speed of atom-based visualisation of proteins and how the rendering of more abstract representations of the molecules compares against it. We complement our findings with in-depth GPU and CPU performance numbers.},
	author = {M{\"u}ller, Christoph and Braun, Matthias and Ertl, Thomas},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798111},
	issn = {2642-5254},
	keywords = {Rendering (computer graphics);Atomic measurements;Data visualization;Sprites (computer);Graphics processing units;Image color analysis;Solids;Computing methodologies---Computer graphics---Graphics systems and interfaces---Mixed/augmented reality;Computing methodologies---Computer graphics---Graphics systems and interfaces---Graphics processors;Human-centered computing---Visualization---Scientific visualization},
	month = {March},
	pages = {97-102},
	title = {Optimised Molecular Graphics on the HoloLens},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798111}}

@inproceedings{8798112,
	abstract = {Assembly tasks are an essential component in complex operations done by a large number of humans on a regular basis; examples include system maintenance (preventive and corrective), industrial production lines, and teleoperation. Having access to superior and low-cost solutions that can be used to train personnel who need to conduct these tasks is essential. Virtual reality (VR) technology, with its immersive and non-immersive display solutions combined with hand controllers suitable for bimanual operations, is especially appealing for training purposes in this domain. We designed and executed a user study in which we tested the influence of stereopsis and immersion on execution of bimanual assembly task and examined the effects of tested system configurations on symptoms of cybersickness. Our user study, with its between-subjects format, collected comprehensive data sets in four distinct experimental conditions: immersive stereoscopic (IS), immersive non-stereoscopic (INS), non-immersive stereoscopic (NIS), and non-immersive non-stereoscopic (NINS). The results of this study suggest that IS platforms are the most promising contenders for an efficient system solution, and that NINS solutions that use larger screens (like a TV set, in our case) may also be considered. It is encouraging that no significant simulator sickness issues were recorded in any condition. The results of this study provide important input and guidance that people who work in the training domain need to have before making decisions about the acquisition of new solutions for assembly task training.},
	author = {de Moura, Douglas Yamashita and Sadagic, Amela},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798112},
	issn = {2642-5254},
	keywords = {Training;Task analysis;Maintenance engineering;Software;Virtual reality;Stereo image processing;TV;Bimanual assembly task;human performance;immersive VR;non-immersive VR;usability;cybersickness;H.5.1 [Information Interfaces & Presentations]: Multimedia Information Systems - Artificial, augmented, and virtual realities},
	month = {March},
	pages = {286-294},
	title = {The Effects of Stereopsis and Immersion on Bimanual Assembly Tasks in a Virtual Reality System},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798112}}

@inproceedings{8798119,
	abstract = {Mid-air interaction achieved through handtracking or controller in Virtual Reality (VR) is performed without an explicit spatial reference. One solution for creating such a reference is elastic feedback, which we realized via tension springs attached to a tracked VR-controller. Thereby, directional force is exerted and perceived by the user. A study was conducted to investigate the effects of elastic feedback on users in virtual travel tasks. Therefore, the elastic mode of the input device was compared to an isotonic mode, without any perceivable force. Also, two virtual travelling methods, a driving mode and a flight mode, were investigated in the study. The main goal of the study was to analyze, how elastic feedback influences user experience and presence perception. Statistical analysis with ANOVA of data from 24 subjects revealed significant main effects for the elastic mode on both, the User Experience Questionnaire (UEQ) and the Igroup Presence Questionnaire (IPQ). In addition, the subjects missed fewer target objects on average during the travel task. However, analysis of task completion times indicated no relevant differences. Although not all individual scales of UEQ and IPQ showed significant outcomes, the result indicates that immersive travel interfaces could benefit from the use of elastic feedback.},
	author = {G{\"u}nther, Tobias and Engeln, Lars and Busch, Sally J. and Groh, Rainer},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798119},
	issn = {2642-5254},
	keywords = {Task analysis;Input devices;Three-dimensional displays;Haptic interfaces;Muscles;Force;Cameras;Human-centered computing---Empirical studies in HCI;Human-centered computing---Virtual reality;Human-centered computing---User centered design;Human-centered computing---Haptic devices},
	month = {March},
	pages = {613-620},
	title = {The Effect of Elastic Feedback on the Perceived User Experience and Presence of Travel Methods in Immersive Environments},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798119}}

@inproceedings{8798121,
	abstract = {This work presents a novel control algorithm of redirected walking called steer-to-optimal-target (S2OT) for effective real-time planning in redirected walking. S2OT is a method of redirection estimating the optimal steering target that can avoid the collision on the future path based on the user's virtual and physical paths. We design and train the machine learning model for estimating optimal steering target through reinforcement learning, especially, using the technique called Deep Q-Learning. S2OT significantly reduces the number of resets caused by collisions between user and physical space boundaries compared to well-known algorithms such as steer-to-center (S2C) and Model Predictive Control Redirection (MPCred). The results are consistent for any combinations of room-scale and large-scale physical spaces and virtual maps with or without predefined paths. S2OT also has a fast computation time of 0.763 msec per redirection, which is sufficient for redirected walking in real-time environments.},
	author = {Lee, Dong-Yong and Cho, Yong-Hun and Lee, In-Kwon},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798121},
	issn = {2642-5254},
	keywords = {Planning;Legged locomotion;Heuristic algorithms;Real-time systems;Virtual environments;Reinforcement learning;Neural networks;Computer Graphics---Three-Dimensional Graphics and Realism---Virtual reality;Information Interfaces and Presentation---Multimedia Information Systems---Artificial, augmented, and virtual realities},
	month = {March},
	pages = {63-71},
	title = {Real-time Optimal Planning for Redirected Walking Using Deep Q-Learning},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798121}}

@inproceedings{8798135,
	abstract = {Many studies explored the effectiveness of augmented, virtual, and mixed reality for object placement tasks. Two main approaches for assisting users during object alignment exist: static visualization techniques and interactive guides. This paper presents a comparative evaluation of four static visualization techniques used to render virtual objects when precise alignment in 6 degrees of freedom (DoF) is required. The selection of these techniques is based on the amount of occlusion caused by the visual guides during the alignment task. To the best of our knowledge, no previous work exists that evaluates which visualization technique is most suitable to support users while precisely aligning objects in virtual environments. We designed a virtual reality scenario considering two conditions -with and without time constraints- in which users aligned pairs of objects. To evaluate the users performance, quantitative and qualitative scores were collected. Our results suggest that visualization techniques with low levels of occlusion can improve alignment performance and increase user acceptance.},
	author = {Martin-Gomez, Alejandro and Eck, Ulrich and Navab, Nassir},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798135},
	issn = {2642-5254},
	keywords = {Visualization;Task analysis;Solid modeling;Solids;Virtual reality;Three-dimensional displays;Rendering (computer graphics);Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies;Human-centered computing---Visualization---Visualization design and evaluation methods;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality},
	month = {March},
	pages = {735-741},
	title = {Visualization Techniques for Precise Alignment in VR: A Comparative Study},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798135}}

@inproceedings{8798143,
	abstract = {Virtual reality (VR) interaction techniques like haptic retargeting offset the user's rendered virtual hand from the real hand location to redirect the user's physical hand movement. This paper explores the order of magnitude of hand redirection that can be applied without the user noticing it. By deriving lower-bound estimates of detection thresholds, we quantify the range of unnoticeable redirection for the three basic redirection dimensions, horizontal, vertical and gain-based hand warping. In a two-alternative forced choice (2AFC) experiment, we individually explore these three hand warping dimensions each in three different scenarios: a very conservative scenario without any distraction and two conservative but more realistic scenarios that distract users from the redirection. Additionally, we combine the results of all scenarios to derive robust recommendations for each redirection technique. Our results indicate that within a certain range, desktop-scale VR hand redirection can go unnoticed by the user, but that this range is narrow. The findings show that the virtual hand can be unnoticeably displaced horizontally or vertically by up to 4.5$\,^{\circ}$ in either direction, respectively. This allows for a range of ca. 9$\,^{\circ}$, in which users cannot reliably detect applied redirection. For our gain-based hand redirection technique, we found that gain factors between g = 0.88 and g = 1.07 can go unnoticed, which corresponds to a user grasping up to 13.75% further or up to 6.18% less far than in virtual space. Our findings are of value for the development of VR applications that aim to redirect users in an undetectable manner, such as for haptic retargeting.},
	author = {Zenner, Andr{\'e} and Kr{\"u}ger, Antonio},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798143},
	issn = {2642-5254},
	keywords = {Haptic interfaces;Visualization;Three-dimensional displays;Virtual reality;Legged locomotion;Indexes;User interfaces;Virtual reality;hand redirection;detection thresholds;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems---Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces---Evaluation/methodology, Interaction styles},
	month = {March},
	pages = {47-55},
	title = {Estimating Detection Thresholds for Desktop-Scale Hand Redirection in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798143}}

@inproceedings{8798152,
	abstract = {In this work, we explore how advances in augmented reality technologies are creating a new design space for long-distance telepresence communication through virtual avatars. Studies have shown that the relative size of a speaker has a significant impact on many aspects of human communication including perceived dominance and persuasiveness. Our system synchronizes the body pose of a remote user with a realistic, virtual human avatar visible to a local user wearing an augmented reality head-mounted display. We conducted a two-by-two (relative system size: equivalent vs. small; leader vs. follower), between participants study (N = 40) to investigate the effect of avatar size on the interactions between remote and local user. We found the equal-sized avatars to be significantly more influential than the small-sized avatars and that the small avatars commanded significantly less attention than the equal-sized avatars. Additionally, we found the assigned leadership role to significantly impact participant subjective satisfaction of the task outcome.},
	author = {Walker, Michael E. and Szafir, Daniel and Rae, Irene},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798152},
	issn = {2642-5254},
	keywords = {Avatars;Telepresence;Robots;Augmented reality;Collaboration;Leadership;Avatars, augmented reality;mixed reality;avatar-mediated communication;human-avatar interaction;avatar telepresence systems;avatar size;scenario-based design;team role;Human-centered computing---HCI---Interaction paradigms---Mixed / augmented reality Human-centered computing---HCI---Interaction paradigms---Virtual reality Human-centered computing---HCI---Interaction paradigms---Web-based interaction Human-centered computing---HCI---Interaction paradigms---Collaborative interaction Human-centered computing---HCI---HCI design and evaluation methods---User studies Human-centered computing---HCI---HCI design and evaluation methods---Laboratory experiments Human-centered computing---Interaction design---Interaction design process and methods---Scenario-based design},
	month = {March},
	pages = {538-546},
	title = {The Influence of Size in Augmented Reality Telepresence Avatars},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798152}}

@inproceedings{8798154,
	abstract = {In this paper, we propose a new visual motion delay effect to enhance the presence of a user in a virtual underwater experience. To do this, we simulate the resistance in the underwater environment by delaying the hand and head movements of the user's avatar. The motion delay effect is implemented using two components: a drag force and a recovery force. The experimental results show that the combination of a drag force and a recovery force creates a realistic illusion of an underwater experience and enhances the user's presence, satisfaction, and immersion in the virtual underwater environment.},
	author = {Lee, Eun-Cheol and Cho, Yong-Hun and Lee, In-Kwon},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798154},
	issn = {2642-5254},
	keywords = {Force;Resistance;Drag;Visualization;Delay effects;Atmospheric modeling;Acceleration;Computer Graphics---Three-Dimensional Graphics and Realism---Virtual reality},
	month = {March},
	pages = {259-266},
	title = {Simulating Water Resistance in a Virtual Underwater Experience Using a Visual Motion Delay Effect},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798154}}

@inproceedings{8798158,
	abstract = {Virtual Reality (VR) sickness occurs when exposure to a virtual environment causes symptoms that are similar to motion sickness, and has been one of the major user experience barriers of VR. To reduce VR sickness, prior work has explored dynamic field-of-view modification and galvanic vestibular stimulation (GVS) that recou-ples the visual and vestibular systems. We propose a new approach to reduce VR sickness, called PhantomLegs, that applies alternating haptic cues that are synchronized to users' footsteps in VR. Our prototype consists of two servos with padded swing arms, one set on each side of the head, that lightly taps the head as users walk in VR. We conducted a three-session, multi-day user study with 30 participants to evaluate its effects as users navigate through a VR environment while physically being seated. Results show that our approach significantly reduces VR sickness during the initial exposure while remaining comfortable to users.},
	author = {Liu, Shi-Hong and Yu, Neng-Hao and Chan, Liwei and Peng, Yi-Hao and Sun, Wei-Zen and Chen, Mike Y.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798158},
	issn = {2642-5254},
	keywords = {Visualization;Legged locomotion;Haptic interfaces;Resists;Servomotors;Virtual reality;Vibrations;Human-centered computing---Virtual Reality---Interaction techniques---Locomotion;Human-centered computing---Virtual Reality---Interaction techniques---Haptics},
	month = {March},
	pages = {817-826},
	title = {PhantomLegs: Reducing Virtual Reality Sickness Using Head-Worn Haptic Devices},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798158}}

@inproceedings{8798177,
	abstract = {Auditory localization cues in the near-field are significantly different than in the far-field. The near-field region is within an arm's length of the listener allowing to integrate proprioceptive cues to determine the location of an object in space. This perceptual study compares three non-individualized methods to apply head-related transfer functions (HRTFs) in six-degrees-of-freedom near-field audio rendering, namely, far-field measured HRTFs, multi-distance measured HRTFs, and spherical-model-based HRTFs with near-field extrapolation. To set our findings in context, we provide a real-world hand-held audio source for comparison along with a distance-invariant condition. Two modes of interaction are compared in an audio-visual virtual reality: one allowing the participant to move the audio object dynamically and the other with a stationary audio object but a freely moving listener.},
	author = {Rummukainen, Olli S. and Schlecht, Sebastian J. and Robotham, Thomas and Plinge, Axel and Habets, Emanu{\"e}l A.P.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798177},
	issn = {2642-5254},
	keywords = {Rendering (computer graphics);Loudspeakers;Virtual reality;Visualization;Solid modeling;Engines;Acoustic measurements;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual Reality;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Mixed / augmented reality;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies},
	month = {March},
	pages = {448-454},
	title = {Perceptual Study of Near-Field Binaural Audio Rendering in Six-Degrees-of-Freedom Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798177}}

@inproceedings{8798179,
	abstract = {This paper presents the results of the Virtual Internship project that aims to help young job seekers get insights of different workplaces via immersive and interactive experiences. We designed a concept of `Immersive Job Taste' that provides a rich presentation of occupations with elements of workplace training, targeting a specific group of young job seekers, including high-school students and unemployed. We developed several scenarios and applied different virtual and augmented reality concepts to build prototypes for different types of devices. The intermediary and the final versions of the prototypes were evaluated by several groups of primary users and experts, including over 70 young job seekers and high school students and over 45 various professionals and experts. The data were collected using questionnaires and interviews. The results indicate a generally very positive attitude towards the concept of immersive job taste, although with significant differences between job seekers and experts. The prototype developed for room-scale virtual reality with controllers was generally evaluated better than those including cardboard with 360 videos or with animated 3D graphics and augmented reality glasses. In the paper, we discuss several aspects, such as the potential of immersive technologies for career guidance, fighting youth unemployment by better informing the young job seekers, and various practical and technology considerations.},
	author = {Prasolova-F{\o}rland, Ekaterina and Fominykh, Mikhail and Ekelund, Oscar Ihlen},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798179},
	issn = {2642-5254},
	keywords = {Task analysis;Employment;Interviews;Fish;Training;Three-dimensional displays;Videos;Virtual Reality;Career guidance;unemployment},
	month = {March},
	pages = {295-302},
	title = {Empowering Young Job Seekers with Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798179}}

@inproceedings{8798181,
	abstract = {Creatives in animation and film productions have forever been exploring the use of new means to prototype their visual sequences before realizing them, by relying on hand-drawn storyboards, physical mockups or more recently 3D modelling and animation tools. However these 3D tools are designed in mind for dedicated animators rather than creatives such as film directors or directors of photography and remain complex to control and master. In this paper we propose a VR authoring system which provides intuitive ways of crafting visual sequences, both for expert animators and expert creatives in the animation and film industry. The proposed system is designed to reflect the traditional process through (i) a storyboarding mode that enables rapid creation of annotated still images, (ii) a previsualisation mode that enables the animation of the characters, objects and cameras, and (iii) a technical mode that enables the placement and animation of complex camera rigs (such as cameras cranes) and light rigs. Our methodology strongly relies on the benefits of VR manipulations to re-think how content creation can be performed in this specific context, typically how to animate contents in space and time. As a result, the proposed system is complimentary to existing tools, and provides a seamless back-and-forth process between all stages of previsualisation. We evaluated the tool with professional users to gather experts' perspectives on the specific benefits of VR in 3D content creation.},
	author = {Galvane, Quentin and Lin, I-Sheng and Argelaguet, Fernando and Li, Tsai-Yen and Christie, Marc},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798181},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Cameras;Tools;Animation;Motion pictures;Layout;Visualization;Human-centered computing---Visualization---Visualization techniques---Treemaps;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {303-311},
	title = {VR as a Content Creation Tool for Movie Previsualisation},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798181}}

@inproceedings{8798204,
	abstract = {Simulating realistic interactions between virtual characters has been of interest to research communities for years, and is particularly important to automatically populate virtual environments. This problem requires to accurately understand and model how humans interact, which can be difficult to assess. In this context, Virtual Reality (VR) is a powerful tool to study human behaviour, especially as it allows assessing conditions which are both ecological and controlled. While VR was shown to allow realistic collision avoidance adaptations, in the frame of the ecological theory of perception and action, interactions between walkers can not solely be characterized through motion adaptations but also through the perception processes involved in such interactions. The objective of this paper is therefore to evaluate how different VR setups influence gaze behaviour during collision avoidance tasks between walkers. To this end, we designed an experiment involving a collision avoidance task between a participant and another walker (real confederate or virtual character). During this task, we compared both the partici-pant`s locomotion and gaze behaviour in a real environment and the same situation in different VR setups (including a CAVE, a screen and a Head-Mounted Display). Our results show that even if some quantitative differences exist, gaze behaviour is qualitatively similar between VR and real conditions. Especially, gaze behaviour in VR setups including a HMD is more in line with the real situation than the other setups. Furthermore, the outcome on motion adaptations confirms previous work, where collision avoidance behaviour is qualitatively similar in VR and real conditions. In conclusion, our results show that VR has potential for qualitative analysis of locomotion and gaze behaviour during collision avoidance. This opens perspectives in the design of new experiments to better understand human behaviour, in order to design more realistic virtual humans.},
	author = {Berton, Florian and Olivier, Anne-H{\'e}l{\`e}ne and Bruneau, Julien and Hoyet, Ludovic and Pettre, Julien},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798204},
	issn = {2642-5254},
	keywords = {Collision avoidance;Task analysis;Legged locomotion;Trajectory;Navigation;Kinematics;Virtual reality;Gaze Activity;Locomotion;Virtual Human;Virtual Reality;Eye-tracking;Collision Avoidance;Human-centered computing---Visualization---Visualization techniques---Treemaps;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {717-725},
	title = {Studying Gaze Behaviour during Collision Avoidance with a Virtual Walker: Influence of the Virtual Reality Setup},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798204}}

@inproceedings{8798205,
	abstract = {Tangible objects are a simple yet effective way for providing haptic sensations in Virtual Reality. For achieving a compelling illusion, there should be a good correspondence between what users see in the virtual environment and what they touch in the real world. The haptic features of the tangible object should indeed match those of the corresponding virtual one in terms of, e.g., size, local shape, mass, texture. A straightforward solution is to create perfect tangible replicas of all the virtual objects in the scene. However, this is often neither feasible nor desirable. This paper presents an innovative approach enabling the use of few tangible objects to render many virtual ones. The proposed algorithm analyzes the available tangible and virtual objects to find the best grasps in terms of matching haptic sensations. It starts by identifying several suitable pinching poses on the considered tangible and virtual objects. Then, for each pose, it evaluates a series of haptically-salient characteristics. Next, it identifies the two most similar pinching poses according to these metrics, one on the tangible and one on the virtual object. Finally, it highlights the chosen pinching pose, which provides the best matching sensation between what users see and touch. The effectiveness of our approach is evaluated through a user study. Results show that the algorithm is able to well combine several haptically-salient object features to find convincing pinches between the given tangible and virtual objects.},
	author = {de Tinguy, Xavier and Pacchierotti, Claudio and Marchal, Maud and L{\'e}cuyer, Anatole},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798205},
	issn = {2642-5254},
	keywords = {Shape;Grasping;Three-dimensional displays;Virtual environments;Object recognition;Human-centered computing---Human computer interaction---Interaction devices---Haptic devices},
	month = {March},
	pages = {321-330},
	title = {Toward Universal Tangible Objects: Optimizing Haptic Pinching Sensations in 3D Interaction},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798205}}

@inproceedings{8798208,
	abstract = {In this paper, we present a fully automatic pipeline for generating and stylizing high geometric and textural quality facial rigs. They are automatically rigged with facial blendshapes for animation, and can be used across platforms for applications including virtual reality, augmented reality, remote collaboration, gaming and more. From a set of input facial photos, our approach is to be able to create a photorealistic, fully rigged character in less than seven minutes. The facial mesh reconstruction is based on state-of-the art photogrammetry approaches. Automatic landmarking coupled with ICP registration with regularization provide direct correspondence and registration from a given generic mesh to the acquired facial mesh. Then, using deformation transfer, existing blendshapes are transferred from the generic to the reconstructed facial mesh. The reconstructed face is then fit to the full body generic mesh. Extra geometry such as jaws, teeth and nostrils are retargeted and transferred to the character. An automatic iris color extraction algorithm is performed to colorize a separate eye texture, animated with dynamic UVs. Finally, an extra step applies a style to the photorealis-tic face to enable blending of personalized facial features into any other character. The user's face can then be adapted to any human or non-human generic mesh. A pilot user study was performed to evaluate the utility of our approach. Up to 65% of the participants were successfully able to discern the presence of one's unique facial features when the style was not too far from a humanoid shape.},
	author = {Danieau, Fabien and Gubins, Ilja and Olivier, Nicolas and Dumas, Olivier and Denis, Bernard and Lopez, Thomas and Mollet, Nicolas and Frager, Brian and Avril, Quentin},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798208},
	issn = {2642-5254},
	keywords = {Face;Pipelines;Cameras;Three-dimensional displays;Geometry;Strain;Computational modeling;I.2.10 [artificial intelligence]: Vision and Scene Understanding---Intensity, color, photometry, and thresholding;I.3.7 [computer graphics]: Three-Dimensional Graphics and, Realism---Animation;character;animation;pipeline;virtual reality},
	month = {March},
	pages = {784-792},
	title = {Automatic Generation and Stylization of 3D Facial Rigs},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798208}}

@inproceedings{8798209,
	abstract = {The recent development of virtual reality (VR) devices such as head mounted displays (HMDs) increases opportunities for applications at the confluence of physical activity and gaming. Recently, the fields of sport and fitness have turned to VR, including for locomotor activities, to enhance motor and energetic resources, as well as motivation and adherence. For example, VR can provide visual feedbacks during treadmill running, thereby reducing monotony and increasing the feeling of movement and engagement with the activity. However, the relevance of using VR tools during locomotion depends on the ability of these systems to provide natural immersive feelings, specifically a coherent perception of speed. The objective of this study is to estimate the error between actual and perceived locomotor speed in VE using an enactive approach, i.e. allowing an active control of the environment. Sixteen healthy individuals participated in the experiment, which consisted in walking and running on a motorized treadmill at speeds ranging from 3 to 11 km/h with 0.5 km/h increments, in a randomized order while wearing a HMD device (HTC Vive) displaying a virtual racetrack. Participants were instructed to match VE speed with what they perceived was their ac-tuallocomotion speed (LS), using a handheld Vive controller. They were able to modify the optic flow speed (OFS) with a 0.02 km/h increment/decrement accuracy. An optic flow multiplier (OFM) was computed based on the error between OFS and LS. It represents the gain that exists between the visually perceived speed and the real locomotion speed experienced by participants for each trial. For all conditions, the average of OFM was 1.00$\pm$.25 to best match LS. This finding is at odds with previous works reporting an underestimation of speed perception in VR. It could be explained by the use of an enactive approach allowing an active and accurate matching of visually and proprioceptively perceived speeds by participants. But above all, our study showed that the perception of speed in VR is strongly individual, with some participants always overestimating and others constantly underestimating. Therefore, a general OFM should not be used to correct speed in VE to ensure congruence in speed perception, and we propose the use of individual models as recommendations for setting up locomotion-based VR applications.},
	author = {Perrin, Th{\'e}o and Kerherv{\'e}, Hugo A. and Faure, Charles and Sorel, Anthony and Bideau, Benoit and Kulpa, Richard},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798209},
	issn = {2642-5254},
	keywords = {Legged locomotion;Adaptive optics;Resists;Optical feedback;Virtual environments;Visualization;Virtual reality;speed perception assessment;running in virtual environment;enaction;physical activity in VR},
	month = {March},
	pages = {622-629},
	title = {Enactive Approach to Assess Perceived Speed Error during Walking and Running in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798209}}

@inproceedings{8798239,
	abstract = {Vision impairments, such as cataracts, affect the way many people interact with their environment, yet are rarely considered by architects and lighting designers because of a lack of design tools. To address this, we present a method to simulate vision impairments, in particular cataracts, graphically in virtual reality (VR), using eye tracking for gaze-dependent effects. We also conduct a VR user study to investigate the effects of lighting on visual perception for users with cataracts. In contrast to existing approaches, which mostly provide only simplified simulations and are primarily targeted at educational or demonstrative purposes, we account for the user's vision and the hardware constraints of the VR headset. This makes it possible to calibrate our cataract simulation to the same level of degraded vision for all participants. Our study results show that we are able to calibrate the vision of all our participants to a similar level of impairment, that maximum recognition distances for escape route signs with simulated cataracts are significantly smaller than without, and that luminaires visible in the field of view are perceived as especially disturbing due to the glare effects they create. In addition, the results show that our realistic simulation increases the understanding of how people with cataracts see and could therefore also be informative for health care personnel or relatives of cataract patients.},
	author = {Kr{\"o}sl, Katharina and Elvezio, Carmine and H{\"u}rbe, Matthias and Karst, Sonja and Wimmer, Michael and Feiner, Steven},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798239},
	issn = {2642-5254},
	keywords = {Cataracts;Lenses;Lighting;Diseases;Visualization;Solid modeling;Gaze tracking;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Computing methodologies---Computer Graphics---Graphics systems and interfaces---Perception;Applied computing---Life and medical sciences---Health informatics},
	month = {March},
	pages = {655-663},
	title = {ICthroughVR: Illuminating Cataracts through Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798239}}

@inproceedings{8798240,
	abstract = {We present a study of the usability and induced workload of a new head-mounted display (HMD) which allows the wearer to more easily switch between tasks in the virtual and real world using multiple optical channels dynamically at the press of a button. Most HMDs provide full immersion by occluding the real world from the view of the user, and replacing it with a computer-generated virtual environment. One trade-off, however, is that such HMDs make it difficult to interact with real-world objects and people, or to react to events while being immersed. Tasks that are simple in the unmediated real world, such as taking a drink or responding to a text, become nearly impossible while wearing an occlusive HMD, and normally require the user to take off the HMD. To address this limitation, we propose an HMD which provides optical channels to view the near-field real world in the periphery of the HMD while still wearing the HMD by pressing a button. We call our approach Multi-channel Dynamic Immersion (MDI). We embedded transparency controllable LCD panels around the periphery of an HMD and installed a fish-eye lens in front of its forward-facing camera. We then compared MDI to three other methods of switching from working in VR to real world activities: (a) a front camera video-see-through method, (b) using dynamic LCDs only, and (c) taking off the HMD. Participants had to interrupt their work in a VR office to carry out typical real world daily activities, such as picking up a mug, responding to a phone call, or replying to a text message. Our results show no significant differences in task execution time between the four conditions. However, we observed a tendency favouring MDI as the easiest to use among the HMDs. In addition, we found a significantly higher rated ease of use for the two HMDs with controllable LCDs compared to the two occlusive HMDs. Our data also underscores that a substantial amount of time is lost when switching between work contexts in occlusive HMDs and the real world.},
	author = {Tran, Kien T.P. and Jung, Sungchul and Hoermann, Simon and Lindeman, Robert W.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798240},
	issn = {2642-5254},
	keywords = {Resists;Optical switches;Task analysis;Visualization;Liquid crystal displays;Cameras;MDI VR HMD;Multi-channel dynamic immersion HMD;Comfort VR session;Virtual Reality and Real-world interaction;Real objects interaction from VR;H.5.2 [Information Interfaces and Presentation]: User Interfaces---Prototyping;H.5.2 [Information Interfaces and Presentation]: User Interfaces---Evaluation/methodology;H.5.2 [User Interfaces Interfaces and Presentation]: User Interfaces---User-centered design;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism---Virtual reality},
	month = {March},
	pages = {350-358},
	title = {MDI: A Multi-channel Dynamic Immersion Headset for Seamless Switching between Virtual and Real World Activities},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798240}}

@inproceedings{8798247,
	abstract = {Recent progresses in Virtual Reality (VR) and Augmented Reality (AR) allow us to experience various VR/AR applications in our daily life. In order to maximise the immersiveness of user in VR/AR environments, a plausible spatial audio reproduction synchronised with visual information is essential. In this paper, we propose a simple and efficient system to estimate room acoustic for plausible reproducton of spatial audio using 360$\,^{\circ}$ cameras for VR/AR applications. A pair of 360$\,^{\circ}$ images is used for room geometry and acoustic property estimation. A simplified 3D geometric model of the scene is estimated by depth estimation from captured images and semantic labelling using a convolutional neural network (CNN). The real environment acoustics are characterised by frequency-dependent acoustic predictions of the scene. Spatially synchronised audio is reproduced based on the estimated geometric and acoustic properties in the scene. The reconstructed scenes are rendered with synthesised spatial audio as VR/AR content. The results of estimated room geometry and simulated spatial audio are evaluated against the actual measurements and audio calculated from ground-truth Room Impulse Responses (RIRs) recorded in the rooms.},
	author = {Kim, Hansung and Remaggi, Luca and Jackson, Philip J.B. and Hilton, Adrian},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798247},
	issn = {2642-5254},
	keywords = {Acoustics;Cameras;Three-dimensional displays;Geometry;Image segmentation;Semantics;Visualization;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Computing methodologies---Artificial intelligence---Computer vision---Scene understanding},
	month = {March},
	pages = {120-126},
	title = {Immersive Spatial Audio Reproduction for VR/AR Using Room Acoustic Modelling from 360$\,^{\circ}$ Images},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798247}}

@inproceedings{8798251,
	abstract = {In a cable-driven suspension system developed to simulate the reduced gravity of lunar or Martian surfaces, we propose to manipu-late/reduce the physical cues of forward jumps so as to overcome the limited workspace problem. The physical cues should be manipulated in a way that the discrepancy from the visual cues provided through the HMD is not noticeable by users. We identified the extent to which forward jumps can be manipulated naturally. We combined it with visual gains, which can scale visual cues without being noticed by users. The test results obtained in a prototype application show that we can use both trajectory manipulation and visual gains to overcome the spatial limit. We also investigated the user experiences when making significantly high and far jumps. The results will be helpful in designing astronaut-training systems and various VR entertainment content.},
	author = {Kang, Hyeong Yeop and Lee, Geonsun and Kang, Dae Seok and Kwon, Ohung and Cho, Jun Yeup and Choi, Ho-Jung and Han, Jung Hyun},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798251},
	issn = {2642-5254},
	keywords = {Trajectory;Visualization;Moon;Wires;Acceleration;Gravity;Tracking;Human-centered computing---Human computer interaction (HCI)---Interaction paradigms---Virtual reality;Human-centered computing---Human computer interaction (HCI)---HCI design and evaluation methods---User studies},
	month = {March},
	pages = {699-707},
	title = {Jumping Further: Forward Jumps in a Gravity-reduced Immersive Virtual Environment},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798251}}

@inproceedings{8798255,
	abstract = {This work presents HapticSphere, a wearable spherical surface enabled by bridging a finger and the head-mounted display (HMD) with a passive string. Users perceive a physical support on a finger attached to a string, when extending their arm and reaching out to the string's maximum extension. This physical support assists users in precise touch interaction in the context of stationary and walking virtual or mixed-reality experiences. We propose three methods of attachment of the haptic string (directly on the head or on the body), and illustrate a novel single-step calibration algorithm that supports these configurations by estimating a grand haptic sphere, once a head-coordinated touch interaction is established. Two user studies were conducted to validate our approach and to compare the touch performance with physical support in sitting and walking conditions in the context of mobile mixed-reality scenarios. The results show that, in the walking condition, touch interaction with physical support significantly outperformed the visual-only condition.},
	author = {Wang, Chiu-Hsuan and Hsieh, Chen-Yuan and Yu, Neng-Hao and Bianchi, Andrea and Chan, Liwei},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798255},
	issn = {2642-5254},
	keywords = {Resists;Force;Virtual reality;Force feedback;Neck;Legged locomotion;Human-centered computing---Visualization---Visualization techniques---Treemaps;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {331-339},
	title = {HapticSphere: Physical Support To Enable Precision Touch Interaction in Mobile Mixed-Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798255}}

@inproceedings{8798257,
	abstract = {Phantom limb pain is a neuropathic condition in which a person feels pain in a limb that is not present. Cognitive treatments that visually recreate the limb in an attempt to create a cross modal interaction between vision, and touch/proprioception have shown to be effective at alleviating this pain. With improvements in technology, Virtual Mirror Therapy is starting to gain favor, however, there are currently no applications that utilize passive touch in the same way non-virtual reality applications do. This paper investigates whether a visual stimulus can relocate a tactile stimulus to a different location using principles from the rubber hand illusion and mirror therapy. We demonstrate that a displaced visual stimulus in virtual reality can disrupt accurate spatial perception of a physical vibrotactile sensation however the effects are small and require further investigation.},
	author = {Willis, Dion and Powell, Wendy and Powell, Vaughan and Stevens, Brett},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798257},
	issn = {2642-5254},
	keywords = {Mirrors;Medical treatment;Pain;Vibrations;Visualization;Phantoms;Virtual reality;Virtual mirror therapy;Rubber hand Illusion;Vibrotactile;Visuotactile interactions;cross modal interactions;Virtual reality;phantom limb pain;Human-centered computing~Virtual reality;Human-centered computing~Haptic devices;Human-centered computing~Empirical studies in HCI;Human-centered computing~Scenario-based design;Human-centered computing~Contextual design;Human-centered computing~Interface design prototyping},
	month = {March},
	pages = {484-491},
	title = {Visual Stimulus Disrupts the Spatial Localization of a Tactile Sensation in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798257}}

@inproceedings{8798261,
	abstract = {During the past five years, tons of economic 360 VR cameras (e.g., Ricoh Theta, Samsumg Gear360, LG 360, Insta 360) are sold in the market. While 360 VR videos become ubiquitous very soon, 360 VR video standardization is still under discussion in the digital industry, and more concrete efforts are desired to accelerate its standardization and applications. Though ERP has been widely used for projection and packing layout while encoding 360 VR videos, it has severe projection distortion near poles. In this paper, we introduce a new format for encoding and storing 360 VR videos using hybrid cylindrical projection after thoroughly analyzing the problems with ERP. We show that our new hybrid format can minimize stretching distortion and generate well balanced pixel distribution in the resulting projection.},
	author = {Tang, Jintao and Zhang, Xinyu},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798261},
	issn = {2642-5254},
	keywords = {Videos;Layout;Distortion;Encoding;Cameras;Standardization;Industries;Computing methodologies---Computer graphics---Graphics systems and interfaces---Virtual reality},
	month = {March},
	pages = {440-447},
	title = {Hybrid Projection For Encoding 360 VR Videos},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798261}}

@inproceedings{8798263,
	abstract = {It has recently been shown that inducing the ownership illusion and then manipulating the movements of one's self-avatar can lead to compensatory motor control strategies in gait rehabilitation. In order to maximize this effect, there is a need for a method that measures, and monitors embodiment levels of participants immersed in VR to induce and maintain a strong ownership illusion. The objective of this study was to propose a novel approach to measuring embodiment by presenting visual feedback that conflicts with motor control to embodied subjects. Twenty healthy participants were recruited. During experimentations, participants wore an EEG cap and motion capture markers, with an avatar displayed in a HMD from a first-person perspective. They were cued to either perform, watch or imagine a single step forward or to initiate walking on the treadmill. For some of the trials, the avatar took a step with the contralateral limb or stopped walking before the participant stopped (modified feedback). Results show that subjective levels of embodiment correlate strongly with the difference in  - ERS power over the motor and pre-motor cortex between the modified and non-modified feedback trials.},
	author = {Alchalabi, Bilal and Faubert, Jocelyn and Labbe, David R.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798263},
	issn = {2642-5254},
	keywords = {Avatars;Legged locomotion;Electroencephalography;Atmospheric measurements;Particle measurements;Foot;Virtual reality;rhythm EEG;event-related-potentials;gait rehabilitation;mirror neuron system},
	month = {March},
	pages = {776-783},
	title = {EEG Can Be Used to Measure Embodiment When Controlling a Walking Self-Avatar},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798263}}

@inproceedings{8798281,
	abstract = {We propose a novel method for estimating the 3D geometry of indoor scenes based on multiple spherical images. Our technique produces a dense depth map registered to a reference view so that depth-image-based-rendering (DIBR) techniques can be explored for providing three-degrees-of-freedom plus immersive experiences to virtual reality users. The core of our method is to explore large displacement optical flow algorithms to obtain point correspondences, and use cross-checking and geometric constraints to detect and remove bad matches. We show that selecting a subset of the best dense matches leads to better pose estimates than traditional approaches based on sparse feature matching, and explore a weighting scheme to obtain the depth maps. Finally, we adapt a fast image-guided filter to the spherical domain for enforcing local spatial consistency, improving the 3D estimates. Experimental results indicate that our method quantitatively outperforms competitive approaches on computer-generated images and synthetic data under noisy correspondences and camera poses. Also, we show that the estimated depth maps obtained from only a few real spherical captures of the scene are capable of producing coherent synthesized binocular stereoscopic views by using traditional DIBR methods.},
	author = {da Silveira, Thiago L. T. and Jung, Claudio R.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798281},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Cameras;Optical imaging;Feature extraction;Image reconstruction;Optical distortion;Geometry;Computing methodologies---Computer vision problems---Reconstruction;Computing methodologies---Computer graphics---Image manipulation---Image-based rendering;Computing methodologies---Computer graphics---Graphics systems and interfaces---Virtual reality},
	month = {March},
	pages = {9-18},
	title = {Dense 3D Scene Reconstruction from Multiple Spherical Images for 3-DoF+ VR Applications},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798281}}

@inproceedings{8798283,
	abstract = {Rendering for Head Mounted Displays (HMD) causes a doubled computational effort, since serving the human stereopsis requires the creation of one image for the left and one for the right eye. The difference in this image pair, called binocular disparity, is an important cue for depth perception and the spatial arrangement of surrounding objects. Findings in the context of the human visual system (HVS) have shown that especially in the near range of an observer, binocular disparities have a high significance. But as with rising distance the disparity converges to a simple geometric shift, also the importance as depth cue exponentially declines. In this paper, we exploit this knowledge about the human perception by rendering objects fully stereoscopic only up to a chosen distance and monoscopic, from there on. By doing so, we obtain three distinct images which are synthesized to a new hybrid stereoscopic image pair, which reasonably approximates a conventionally rendered stereoscopic image pair. The method has the potential to reduce the amount of rendered primitives easily to nearly 50 % and thus, significantly lower frame times. Besides of a detailed analysis of the introduced formal error and how to deal with occurring artifacts, we evaluated the perceived quality of the VR experience during a comprehensive user study with nearly 50 participants. The results show that the perceived difference in quality between the shown image pairs was generally small. An in-depth analysis is given on how the participants reached their decisions and how they subjectively rated their VR experience.},
	author = {Fink, Laura and Hensel, Nora and Markov-Vetter, Daniela and Weber, Christoph and Staadt, Oliver and Stamminger, Marc},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798283},
	issn = {2642-5254},
	keywords = {Rendering (computer graphics);Stereo image processing;Cameras;Virtual reality;Hardware;Visualization;Resists;Virtual Reality;Stereoscopic Rendering;Depth Perception;Virtual Reality---Stereoscopic Rendering---Hybrid Rendering---Depth Perception},
	month = {March},
	pages = {88-96},
	title = {Hybrid Mono-Stereo Rendering in Virtual Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798283}}

@inproceedings{8798284,
	abstract = {Real-time rendering of large point clouds requires acceleration structures that reduce the number of points drawn on screen. State-of-the art algorithms group and render points in hierarchically organized chunks with varying extent and density, which results in sudden changes of density from one level of detail to another, as well as noticeable popping artifacts when additional chunks are blended in or out. These popping artifacts are especially noticeable at lower levels of detail, and consequently in virtual reality, where high performance requirements impose a reduction in detail. We propose a continuous level-of-detail method that exhibits gradual rather than sudden changes in density. Our method continuously recreates a down-sampled vertex buffer from the full point cloud, based on camera orientation, position, and distance to the camera, in a point-wise rather than chunk-wise fashion and at speeds up to 17 million points per millisecond. As a result, additional details are blended in or out in a less noticeable and significantly less irritating manner as compared to the state of the art. The improved acceptance of our method was successfully evaluated in a user study.},
	author = {Sch{\"u}tz, Markus and Kr{\"o}sl, Katharina and Wimmer, Michael},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798284},
	issn = {2642-5254},
	keywords = {Rendering (computer graphics);Three-dimensional displays;Cameras;Distortion;Octrees;Real-time systems;Virtual reality;Computing methodologies---Computer graphics---Rendering;Computing methodologies---Computer graphics---Graphics systems and interfaces---Virtual reality},
	month = {March},
	pages = {103-110},
	title = {Real-Time Continuous Level of Detail Rendering of Point Clouds},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798284}}

@inproceedings{8798286,
	abstract = {Many virtual locomotion interfaces allowing users to move in virtual reality have been built and evaluated, such as redirected walking (RDW), walking-in-place (WIP), and joystick input. RDW has been shown to be among the most natural and immersive as it supports real walking, and many newer methods further adapt RDW to allow for customization and greater immersion. Most of these methods have been demonstrated to work with vision, in this paper we evaluate the ability for a general distractor-based RDW framework to be used with only auditory display. We conducted two studies evaluating the differences between RDW with auditory distractors and other distractor modalities using distraction ratio, virtual and physical path information, immersion, simulator sickness, and other measurements. Our results indicate that auditory RDW has the potential to be used with complex navigational tasks, such as crossing streets and avoiding obstacles. It can be used without designing the system specifically for audio-only users. Additionally, sense of presence and simulator sickness remain reasonable across all user groups.},
	author = {Rewkowski, Nicholas and Rungta, Atul and Whitton, Mary and Lin, Ming},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798286},
	issn = {2642-5254},
	keywords = {Legged locomotion;Distortion;Navigation;Task analysis;Visualization;Virtual environments;Dogs;virtual locomotion---redirected walking---distractors},
	month = {March},
	pages = {395-404},
	title = {Evaluating the Effectiveness of Redirected Walking with Auditory Distractors for Navigation in Virtual Environments},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798286}}

@inproceedings{8798300,
	abstract = {In this paper, we propose to reproduce drag forces in a virtual underwater environment. To this end, we first compute the drag forces to be exerted on human limbs in a physically correct way. Adopting a pseudo-haptic approach that generates visual discrepancies between the real and virtual limb motions, we compute the extent of drag forces that are applied to the virtual limbs and can be naturally perceived. Through two tests, our drag force simulation method is compared with others. The results show that our method is effective in reproducing the sense of being immersed in water. Our study can be utilized for various types of virtual underwater applications such as scuba diving training and aquatic therapy.},
	author = {Kang, HyeongYeop and Lee, Geonsun and Han, JungHyun},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798300},
	issn = {2642-5254},
	keywords = {Drag;Force;Visualization;Haptic interfaces;Avatars;Angular velocity;Shoulder;Human-centered computing---Visualization---Visualization techniques---Treemaps;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {38-46},
	title = {Visual Manipulation for Underwater Drag Force Perception in Immersive Virtual Environments},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798300}}

@inproceedings{8798312,
	abstract = {Many optical-see-through displays have a relatively narrow field of view. However, a limited field of view can constrain how information can be presented and searched through. To understand these constraints, we present a series of experiments that address the interrelationships between field of view, information density, and search performance. We do so by simulating various fields of view using two approaches: limiting the field of view presented on a Microsoft HoloLens optical-see-through head-worn display and dynamically changing the portion of a large tiled-display wall on which information is presented, for head-tracked users in both cases. Our results indicate a significant effect of information density and field of view on search performance, with potential search performance benefits of using a larger FOV between ca. 7-28%. Furthermore, while grids guided visual search, they did not significantly affect performance.},
	author = {Trepkowski, Christina and Eibich, David and Maiero, Jens and Marquardt, Alexander and Kruijff, Ernst and Feiner, Steven},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798312},
	issn = {2642-5254},
	keywords = {Visualization;Task analysis;Limiting;Three-dimensional displays;Semantics;Augmented reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems---Artificial, augmented, and virtual realities},
	month = {March},
	pages = {575-584},
	title = {The Effect of Narrow Field of View and Information Density on Visual Search Performance in Augmented Reality},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798312}}

@inproceedings{8798326,
	abstract = {Spherical VR cameras can capture high-quality immersive VR images with a 360$\,^{\circ}$ field of view. However, in practice, when the camera orientation is not straight, the acquired VR image appears tilted when displayed on a VR headset, which diminishes the quality of the VR experience. To overcome this problem, we present a deep learning-based approach that can automatically estimate the orientation of a VR image and return its upright version. In contrast to existing methods, our approach does not require the presence of lines or horizon in the image, and thus can be applied on a wide range of scenes. Extensive experiments and comparisons with state-of-the-art methods have successfully confirmed the validity of our approach.},
	author = {Jung, Raehyuk and Lee, Aiden Seung Joon and Ashtari, Amirsaman and Bazin, Jean-Charles},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798326},
	issn = {2642-5254},
	keywords = {Cameras;Headphones;Deep learning;Three-dimensional displays;Training;Standards;Estimation;Upright adjustment;deep learning;VR content;Computing methodologies---Computer graphics---Image manipulation---Image processing},
	month = {March},
	pages = {1-8},
	title = {Deep360Up: A Deep Learning-Based Approach for Automatic VR Image Upright Adjustment},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798326}}

@inproceedings{8798334,
	abstract = {Cybersickness is a symptom of dizziness that occurs while experiencing Virtual Reality (VR) technology and it is presumed to occur mainly by crosstalk between the sensory and cognitive systems. However, since the sensory and cognitive systems cannot be measured objectively, it is difficult to measure cybersickness. Therefore, methodologies for measuring cybersickness have been studied in various ways. Traditional studies have collected answers to questionnaires or analyzed EEG data using machine learning algorithms. However, the system relying on the questionnaires lacks objectivity, and it is difficult to obtain highly accurate measurements with the machine learning algorithms in previous studies. In this work, we apply and compare Deep Neural Network (DNN) and Convolutional Neural Network (CNN) deep learning algorithms for objective cy-bersickness measurement from EEG data. We also propose a data preprocessing for learning and signal quality weights allowing us to achieve high performance while learning EEG data with the deep learning algorithms. Besides, we analyze video characteristics where cybersickness occurs by examining the 360 video stream segments causing cybersickness in the experiments. Finally, we draw common patterns that cause cybersickness.},
	author = {Jeong, Daekyo and Yoo, Sangbong and Yun, Jang},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798334},
	issn = {2642-5254},
	keywords = {Electroencephalography;Deep learning;Motion measurement;Machine learning algorithms;Atmospheric measurements;Particle measurements;Emotion recognition;Human-centered computing---Virtual reality;Computing methodologies---Machine learning approaches},
	month = {March},
	pages = {827-835},
	title = {Cybersickness Analysis with EEG Using Deep Learning Algorithms},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798334}}

@inproceedings{8798338,
	abstract = {In this contribution, we design, implement and evaluate the pedagogical benefits of a novel interactive note taking interface (iVRNote) in VR for the purpose of learning and reflection lectures. In future VR learning environments, students would have challenges in taking notes when they wear a head mounted display (HMD). To solve this problem, we installed a digital tablet on the desk and provided several tools in VR to facilitate the learning experience. Specifically, we track the stylus' position and orientation in the physical world and then render a virtual stylus in VR. In other words, when students see a virtual stylus somewhere on the desk, they can reach out with their hand for the physical stylus. The information provided will also enable them to know where they will draw or write before the stylus touches the tablet. Since the presented iVRNote featuring our note taking system is a digital environment, we also enable students save efforts in taking extensive notes by providing several functions, such as post-editing and picture taking, so that they can pay more attention to lectures in VR. We also record the time of each stroke on the note to help students review a lecture. They can select a part of their note to revisit the corresponding segment in a virtual online lecture. Figures and the accompanying video demonstrate the feasibility of the presented iVRNote system. To evaluate the system, we conducted a user study with 20 participants to assess the preference and pedagogical benefits of the iVRNote interface. The feedback provided by the participants were overall positive and indicated that the iVRNote interface could be potentially effective in VR learning experiences.},
	author = {Chen, Yi-Ting and Hsu, Chi-Hsuan and Chung, Chih-Han and Wang, Yu-Shuen and Babu, Sabarish V.},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798338},
	issn = {2642-5254},
	keywords = {Three-dimensional displays;Writing;Tools;Tracking;Education;Portable computers;Task analysis;Virtual reality;classroom;on-line learning;Human-centered computing---Visualization---Visualization techniques---Education;Human-centered computing---Visualization---Visualization design and evaluation methods},
	month = {March},
	pages = {172-180},
	title = {iVRNote: Design, Creation and Evaluation of an Interactive Note-Taking Interface for Study and Reflection in VR Learning Environments},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798338}}

@inproceedings{8798340,
	abstract = {Freehand gesture interaction has long been proposed as a `natural' input method for Augmented Reality (AR) applications, yet has been little explored for intensive applications like multiscale navigation. In multiscale navigation, such as digital map navigation, pan and zoom are the predominant interactions. A position-based input mapping (e.g. grabbing metaphor) is intuitive for such interactions, but is prone to arm fatigue. This work focuses on improving digital map navigation in AR with mid-air hand gestures, using a horizontal intangible map display. First, we conducted a user study to explore the effects of handedness (unimanual and bimanual) and input mapping (position-based and rate-based). From these findings we designed DiveZoom and TerraceZoom, two novel hybrid techniques that smoothly transition between position- and rate-based mappings. A second user study evaluated these designs. Our results indicate that the introduced input-mapping transitions can reduce perceived arm fatigue with limited impact on performance.},
	author = {Satriadi, Kadek Ananta and Ens, Barrett and Cordeil, Maxime and Jenny, Bernhard and Czauderna, Tobias and Willett, Wesley},
	booktitle = {2019 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:14 -0400},
	date-modified = {2024-03-18 02:30:14 -0400},
	doi = {10.1109/VR.2019.8798340},
	issn = {2642-5254},
	keywords = {Navigation;Fatigue;Aerospace electronics;Three-dimensional displays;Task analysis;Augmented reality;Collaboration;Human-centered computing--- Mixed / augmented reality;Human-centered computing---Gestural input;Human-centered computing---Empirical studies in interaction design},
	month = {March},
	pages = {593-603},
	title = {Augmented Reality Map Navigation with Freehand Gestures},
	year = {2019},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2019.8798340}}

@inproceedings{8446049,
	abstract = {Optical tracking has become the most commonly used virtual reality (VR) tracking technology because of its high precision and non-contact characteristics. The optical tracking system represented by HTC VIVE has the problem that signals of base stations interfere with each other, and the number of base stations cannot be extended by cascades, thereby limiting the scope of its work. In this paper, an extensible optical tracking system is proposed, which can distinguish the signals from different base stations and support the simultaneous operation of multiple base stations. Furthermore, we designed an encoding scheme to generate independent code for up to 32 base stations and proposed a highspeed decoding method. Experiments demonstrate that the system has high tracking accuracy and low system latency. Users can adjust the number and layout of base stations according to the actual demand, which greatly improves the flexibility of the system, and benefits promoting the development of large scale optical tracking equipment with low cost and high precision.},
	author = {Li, Dong and Wang, Danli and Weng, Dongdong and Li, Yue and Xun, Hang and Bao, Yihua},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446049},
	keywords = {Conferences;Virtual reality;Three-dimensional displays;User interfaces;Optical tracking;virtual reality.: [Computer Vision]: Computer Vision Problems-Tracking;[Human Computer Interaction (HCI)]: Interaction Paradigms-Virtual Reality},
	month = {March},
	pages = {439-445},
	title = {Coded Light Based Extensible Optical Tracking System},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446049}}

@inproceedings{8446058,
	abstract = {For optical see-through head-mounted displays, the mismatch between a display's focal length and the real world scene inadvertently prevents users from simultaneously focusing on the presented virtual content and the scene. It has been shown that it is possible to ameliorate the out-of-focus blur for images with a known focus distance, by applying an algorithm called Sharp View. However, it remains unclear if Sharp View also improves the readability and clarity of text rendered on the display. In this study, we investigate whether users reported increased text clarity when Sharp View was applied to a text label, and how the focal demand of the display, the focal distance to real world content, and gaze condition affect the result. Our results indicate that, in non-fixated viewing, there is a significant user preference for Sharp View-enhanced text strings.},
	author = {Cook, Trey and Phillips, Nate and Massey, Kristen and Plopski, Alexander and Sandor, Christian and Edward Swan, J.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446058},
	keywords = {Observers;Lenses;Switches;Optical imaging;Electronic mail;Visualization;Augmented reality},
	month = {March},
	pages = {1-400},
	title = {User Preference for SharpView-Enhanced Virtual Text During Non-Fixated Viewing},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446058}}

@inproceedings{8446059,
	abstract = {We study the performance and user experience of two popular mainstream text entry devices, desktop keyboards and touchscreen keyboards, for use in Virtual Reality (VR) applications. We discuss the limitations arising from limited visual feedback, and examine the efficiency of different strategies of use. We analyze a total of 24 hours of typing data in VR from 24 participants and find that novice users are able to retain about 60% of their typing speed on a desktop keyboard and about 40-45% of their typing speed on a touchscreen keyboard. We also find no significant learning effects, indicating that users can transfer their typing skills fast into VR. Besides investigating baseline performances, we study the position in which keyboards and hands are rendered in space. We find that this does not adversely affect performance for desktop keyboard typing and results in a performance trade-off for touchscreen keyboard typing.},
	author = {Grubert, Jens and Witzani, Lukas and Ofek, Eyal and Pahud, Michel and Kranz, Matthias and Kristensson, Per Ola},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446059},
	keywords = {Keyboards;Visualization;Electronic mail;Virtual reality;Error analysis;Performance evaluation;User interfaces;H.5.2: [User Interfaces - Input devices and strategies.]},
	month = {March},
	pages = {159-166},
	title = {Text Entry in Immersive Head-Mounted Display-Based Virtual Reality Using Standard Keyboards},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446059}}

@inproceedings{8446068,
	abstract = {Immersive virtual reality (VR) systems offer flexible control of an interactive environment, along with precise position and orientation tracking of realistic movements. Immersive VR can also be used in conjunction with neurophysiological monitoring techniques, such as electroencephalography (EEG), to record neural activity as users perform complex tasks. As such, the fusion of VR, kinematic tracking, and EEG offers a powerful testbed for naturalistic neuroscience research. In this study, we combine these elements to investigate the cognitive and neural mechanisms that underlie motor skill learning during a multi-day simulated marksmanship training regimen conducted with 20 participants. On each of 3 days, participants performed 8 blocks of 60 trials in which a simulated clay pigeon was launched from behind a trap house. Participants attempted to shoot the moving target with a firearm game controller, receiving immediate positional feedback and running scores after each shot. Over the course of the 3 days that individuals practiced this protocol, shot accuracy and precision improved significantly while reaction times got significantly faster. Furthermore, results demonstrate that more negative EEG amplitudes produced over the visual cortices correlate with better shooting performance measured by accuracy, reaction times, and response times, indicating that early visual system plasticity underlies behavioral learning in this task. These findings point towards a naturalistic neuroscience approach that can be used to identify neural markers of marksmanship performance.},
	author = {Clements, Jillian M. and Kopper, Regis and Zielinski, David J. and Rao, Hrishikesh and Sommer, Marc A. and Kirsch, Elayna and Mainsah, Boyla O. and Collins, Leslie M. and Appelbaum, Lawrence G.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446068},
	keywords = {Task analysis;Electroencephalography;Visualization;Electrodes;Tracking;Trajectory;Electronic mail;EEG;virtual reality;neuroscience;mobile brain/body imaging;marksmanship: J.4 [Computer Applications]: Social and Behavioral Sciences=-Psychology},
	month = {March},
	pages = {451-458},
	title = {Neurophysiology of Visual-Motor Learning During a Simulated Marksmanship Task in Immersive Virtual Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446068}}

@inproceedings{8446130,
	abstract = {The confounding effect of player locomotion on the vestibulo-ocular reflex is one of the principal causes of motion sickness in immersive virtual reality. Continuous motion is particularly problematic for stationary user configurations, and teleportation has become the prevailing approach for providing accessible locomotion. Unfortunately, teleportation can also increase disorientation and reduce a player's sense of presence within a VR environment. This paper presents an alternative locomotion technique designed to preserve accessibility while maintaining feelings of presence. This is a node-based navigation system which allows the player to move between predefined node positions using a rapid, continuous, linear motion. An evaluation was undertaken to compare this locomotion technique with commonly used, teleportation-based and continuous walking approaches. Thirty-six participants took part in a study which examined motion sickness and presence for each technique, while navigating around a virtual house using PlayStation VR. Contrary to intuition, we show that rapid movement speeds reduce players' feelings of motion sickness as compared to continuous movement at normal walking speeds.},
	author = {Jacob Habgood, M. P. and Moore, David and Wilson, David and Alapont, Sergio},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446130},
	keywords = {Teleportation;Virtual reality;Games;Headphones;Steel;Navigation;Legged locomotion;PlayStation VR;virtual reality;locomotion;motion-sickness;cultural heritage;Edward Jenner;REVEAL.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
	month = {March},
	pages = {371-378},
	title = {Rapid, Continuous Movement Between Nodes as an Accessible Virtual Reality Locomotion Technique},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446130}}

@inproceedings{8446135,
	abstract = {Real-time interactive systems such as virtual environments have high performance requirements, and profiling is a key part of the optimisation process to meet them. Traditional techniques based on metadata and static analysis have difficulty following causality in asynchronous systems. In this paper we explore a new technique for such systems. Timestamped samples of the system state are recorded at instrumentation points at runtime. These are assembled into a graph, and edges between dependent samples recovered. This approach minimises the invasiveness of the instrumentation, while retaining high accuracy. We describe how our instrumentation can be implemented natively in common environments, how its output can be processed into a graph describing causality, and how heterogeneous data sources can be incorporated into this to maximise the scope of the profiling. Across three case studies, we demonstrate the efficacy of this approach, and how it supports a variety of metrics for comprehensively bench-marking distributed virtual environments.},
	author = {Friston, Sebastian and Griffith, Elias and Swapp, David and Marshall, Alan and Steed, Anthony},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446135},
	keywords = {Tools;Instruments;Synchronization;Virtual environments;Metadata;Haptic interfaces;profiling;benchmarking;tools;distributed;latency.: C.4 [Performance of Systems]-Measurement Techniques;D.2.8 [Software Engineering]: Metrics-Performance measures},
	month = {March},
	pages = {238-245},
	title = {Profiling Distributed Virtual Environments by Tracing Causality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446135}}

@inproceedings{8446152,
	abstract = {We present an interactive algorithm to generate plausible movements for human-like agents interacting with other agents or avatars in a virtual environment. Our approach takes into account high-dimensional human motion constraints and bio-mechanical constraints to compute collision-free trajectories for each agent. We present a novel full-body movement constrained-velocity computation algorithm that can easily be combined with many existing motion synthesis techniques. Compared to prior local navigation methods, our formulation reduces artefacts that arise in dense scenarios and close interactions, and results in smoother and plausible locomotive behaviors. We have evaluated the benefits of our new algorithm in single-agent and multi-agent environments. We investigated the perception of a single agent's movements in dense scenarios and observed that our algorithm has a strong positive effect on the perceived quality of the simulation. Our approach also allows the user to interact with the agents from a first-person perspective in immersive settings. We conducted a study to investigate the perception of such avatar-agent interactions, and found that interactions generated using our approach lead to an increase in the user's sense of co-presence.},
	author = {Narang, Sahil and Best, Andrew and Manocha, Dinesh},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446152},
	keywords = {Two dimensional displays;Avatars;Trajectory;Navigation;Computational modeling;Collision avoidance;Solid modeling;multi-agent simulation;virtual reality;avatars;human agents;interactive navigation: Human-centered computing-User studies;Human-centered computing-Virtual reality;Computing methodologies-Artificial intelligence;Computing methodologies-Motion path planning;Computing methodologies-Modeling and simulation;},
	month = {March},
	pages = {9-16},
	title = {Simulating Movement Interactions Between Avatars & Agents in Virtual Worlds Using Human Motion Constraints},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446152}}

@inproceedings{8446153,
	abstract = {This paper reports on the effect of visually-anchored prediction accuracy of haptic information on the perceived presence of virtual environments. We designed an experiment which explicitly prevented confounding factors potentially introduced by virtual body ownership and/or agency. The experimental design consisted of two main conditions defining congruent vs incongruent visual and haptic cues. Presence was measured during as well as after exposure. A distance estimation task solely based on motor action and the visually-anchored spatial model of the environment was executed to control for perceptual binding. 56 healthy volunteers were randomly assigned to one of two groups in a single-blind mixed-group design study. The study revealed increased presence for high prediction accuracy and decreased presence for low prediction accuracy, while perceptual binding still occurred. The observed effect sizes were in the medium range. The results indicate a significant correlation between prediction accuracy of haptic information and the perceived realness and presence of a virtual environment which gives rise to a discussion about models for dissociative symptom derealisation.},
	author = {Gall, Dominik and Latoschik, Marc Erich},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446153},
	keywords = {Haptic interfaces;Virtual environments;Visualization;Solid modeling;Head-mounted displays;Estimation;Task analysis;Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI},
	month = {March},
	pages = {73-80},
	title = {The Effect of Haptic Prediction Accuracy on Presence},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446153}}

@inproceedings{8446177,
	abstract = {Room-scale mapping facilitates natural locomotion in virtual reality (VR), but it creates a problem when encountering virtual walls. In traditional video games, player avatars can simply be prevented from moving through walls. This is not possible in VR with room-scale mapping due to the lack of physical boundaries. Game design is either limited by avoiding walls, or the players might ignore them, which endangers the immersion and the overall game experience. To prevent players from walking through walls, we propose a combination of auditory, visual, and vibrotactile feedback for wall collisions. This solution can be implemented with standard game engine features, does not require any additional hardware or sensors, and is independent of game concept and narrative. A between-group study with 46 participants showed that a large majority of players without the feedback did pass through virtual walls, while 87% of the participants with the feedback refrained from walking through walls. The study found no notable differences in game experience.},
	author = {Boldt, Mette and Bonfert, Michael and Lehne, Inga and Cahnbley, Melina and Korschinq, Kim and Bikas, Loannis and Finke, Stefan and Hanci, Martin and Kraft, Valentin and Liu, Boxuan and Nguyen, Tram and Panova, Alina and Singh, Ramneek and Steenbergen, Alexander and Malaka, Rainer and Smeddinck, Jan},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446177},
	keywords = {Games;Legged locomotion;Visualization;Hardware;Haptic interfaces;Resists;Vibrations;Virtual reality;virtual walls;tactile feedback;haptic feedback;visual feedback;auditory feedback;locomotion;game design;K.8.0 [Personal Computing]: General - games;H.5.2 [Information interfaces and presentation]: User Interfaces. - Interaction styles},
	month = {March},
	pages = {143-150},
	title = {You Shall Not Pass: Non-Intrusive Feedback for Virtual Walls in VR Environments with Room-Scale Mapping},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446177}}

@inproceedings{8446180,
	abstract = {This paper presents a study performed in virtual reality on the effect of gaze interception during collision avoidance between two walkers. In such a situation, mutual gaze can be considered as a form of nonverbal communication. Additionally, gaze is believed to detail future path intentions and to be part of the nonverbal negotiation to achieve avoidance collaboratively. We considered an avoidance task between a real subject and a virtual human character and studied the influence of the character's gaze direction on the avoidance behaviour of the participant. Virtual reality provided an accurate control of the situation: seventeen participants were immersed in a virtual environment, instructed to navigate across a virtual space using a joystick and to avoid a virtual character that would appear from either side. The character would either gaze or not towards the participant. Further, the character would either perform or not a reciprocal adaptation of its trajectory to avoid a potential collision with the participant. The findings of this paper were that during an orthogonal collision avoidance task, gaze behaviour did not influence the collision avoidance behaviour of the participants. Further, the addition of reciprocal collision avoidance with gaze did not modify the collision behaviour of participants. These results suggest that for the duration of interaction in such a task, body motion cues were sufficient for coordination and regulation. We discuss the possible exploitation of these results to improve the design of virtual characters for populated virtual environments and user interaction.},
	author = {Lynch, Sean D. and Pettr{\'e}, Julien and Bruneau, Julien and Kulpa, Richard and Cr{\'e}tual, Armel and Olivier, Anne-Helene},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446180},
	keywords = {Collision avoidance;Task analysis;Trajectory;Legged locomotion;Virtual environments;Navigation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;Augmented;and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality J.4 [Social and behavioural Sciences]: Psychology},
	month = {March},
	pages = {136-142},
	title = {Effect of Virtual Human Gaze Behaviour During an Orthogonal Collision Avoidance Walking Task},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446180}}

@inproceedings{8446194,
	abstract = {This study investigates and compares brain signals between persons with and without Multiple Sclerosis (MS) when exposed to cybersickness-provoking Virtual Reality (VR). Cybersickness is a set of discomforts and commonly triggered by VR exposure. It has symptoms similar to motion sickness, such as dizziness, nausea, and disorientation etc. Although cybersickness has been studied for decades, populations with neurological disabilities, such as MS, have remained minimally studied. Cybersickness could have negative impact on effectiveness of VR-based rehabilitation systems and limit the accessibility of VR for persons with disabilities. MS can disrupt communication between neurons (signal carrying nerve cells) from different areas of the brain. Cybersickness also can affect brain signals, for example, frequency powers may change due to cybersickness. This study investigates the combination of MS and cybersickness in terms of brain signals. To investigate the effect of cybersickness on participants' brain signals, electroencephalogram (EEG) data were recorded before, during and after exposure to a cybersickness-provoking VR driving simulation. The EEG data suggests that in response to cybersickness-provoking VR exposure, participants with MS have mostly shown similar changes in brain activity with different magnitudes than participants without MS. Also, for at least one scalp location we have found completely opposite brain signals in MS-Group when compared to Non-MS-Group. Difference in magnitude or completely different trend in brain signals can imply that cybersickness affects persons with MS differently than persons without MS and may be different cybersickness reduction techniques are required for different populations.},
	author = {Arafat, Imtiaz Muhammad and Shahnewaz Ferdous, Sharif Mohammad and Quarles, John},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446194},
	keywords = {Electroencephalography;Brain modeling;Neurons;Multiple sclerosis;Task analysis;Automobiles;Virtual reality;Cybersickness;Multiple Sclerosis (MS);VR;VE;Electroencephalogram (EEG);ERSP;ERP;User studies and evaluation;Ethical issues in VR.: Social and professional topics-User characteristics-People with disabilities;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies},
	month = {March},
	pages = {1-120},
	title = {Cybersickness-Provoking Virtual Reality Alters Brain Signals of Persons with Multiple Sclerosis},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446194}}

@inproceedings{8446195,
	abstract = {Low latency is a fundamental requirement for Virtual Reality (VR) systems to reduce the potential risks of cybersickness and to increase effectiveness, efficiency and user experience. In contrast to the effects of uniform latency degradation, the influence of latency jitter on user experience in VR is not well researched, although today's consumer VR systems are vulnerable in this respect. In this work we report on the impact of latency jitter on cybersickness in HMD-based VR environments. Test subjects are given a search task in Virtual Reality, provoking both head rotation and translation. One group experienced artificially added latency jitter in the tracking data of their head-mounted display. The introduced jitter pattern was a replication of a real-world latency behavior extracted and analyzed from an existing example VR-system. The effects of the introduced latency jitter were measured based on self-reports simulator sickness questionnaire (SSQ) and by taking physiological measurements. We found a significant increase in self-reported simulator sickness. We therefore argue that measure and control of latency based on average values taken at a few time intervals is not enough to assure a required timeliness behavior but that latency jitter needs to be considered when designing experiences for Virtual Reality.},
	author = {Stauffert, Jan-Philipp and Niebling, Florian and Latoschik, Marc Erich},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446195},
	keywords = {Jitter;Task analysis;Virtual reality;Delays;Probability distribution;Tracking;D.1.3 [Programming Techniques]: Concurrent Programming-Parallel programming;D.4.8 [Operating Systems]: Performance-Measurements;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
	month = {March},
	pages = {121-127},
	title = {Effects of Latency Jitter on Simulator Sickness in a Search Task},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446195}}

@inproceedings{8446210,
	abstract = {Visually-induced motion sickness (VIMS), also known as cyber-sickness, is a major challenge for wide-spread Virtual Reality (VR) adoption. VIMS can be reduced in different ways, for example by using high-quality tracking systems and reducing the user's field of view. However, there are no universal solutions for all situations, and a wide variety of techniques are needed in order for developers to choose the most appropriate options depending on their needs. One way to reduce VIMS is through the use of rest frames-portions of the virtual environment that remain fixed in relation to the real world and do not move as the user virtually moves. We report the results of two multi-day within-subjects studies with 44 subjects who used virtual travel to navigate the environment. In the first study, we investigated the influence of static rest frames with fixed opacity on user comfort. For the second study, we present an enhanced version of rest frames that we call dynamic rest frames, where the opacity of the rest frame changes in response to visually perceived motion as users virtually traversed the virtual environment. Results show that a virtual environment with a static or dynamic rest frame allowed users to travel through more waypoints before stopping due to discomfort compared to a virtual environment without a rest frame. Further, a virtual environment with a static rest frame was also found to result in more real-time reported comfort than when there was no rest frame.},
	author = {Cao, Zekun and Jerald, Jason and Kopper, Regis},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446210},
	keywords = {Radio frequency;Virtual environments;Visualization;Games;Nose;Metals;Navigation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
	month = {March},
	pages = {105-112},
	title = {Visually-Induced Motion Sickness Reduction via Static and Dynamic Rest Frames},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446210}}

@inproceedings{8446216,
	abstract = {Redirected walking allows users to explore immersive virtual environments by real walking even when the physical tracking space is limited. Redirected walking is usually implemented via translation gains, rotation gains, and curvature gains, while previous research was focused on identifying detection thresholds for such manipulations. To our knowledge, all previous experiments were conducted without a visual self-representation of the user in the virtual environment, in particular, without showing the user's feet. In this paper, we address the question if the virtual self-representation of the user's feet changes the detection thresholds for translation gains. Furthermore, we consider the influence of the holisticness of the visual stimulus, i. e., the type of virtual environment. Therefore, we conducted an experiment to identify detection thresholds for translation gains under three different conditions: (i) without visible virtual feet and (ii) with visible virtual feet both in a high fidelity visually rich virtual environment, and (iii) with visible virtual feet in a low cue virtual environment. The results revealed the range of detection thresholds for translations gains, which cannot be detected by the user when the feet are visible. Furthermore, the results show a significant difference between the two types of environment. Our findings suggest that the virtual environment is more important for manipulation detection than the visual self-representation of the user's feet.},
	author = {Kruse, Lucie and Langbehn, Eike and Steinicke, Frank},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446216},
	keywords = {Legged locomotion;Visualization;Foot;Virtual environments;Tracking;Cameras;Avatars;Locomotion;redirected walking;translation gains.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information SystemsArtificial;augmented;and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and RealismVirtual reality},
	month = {March},
	pages = {305-312},
	title = {I Can See on My Feet While Walking: Sensitivity to Translation Gains with Visible Feet},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446216}}

@inproceedings{8446217,
	abstract = {The effective design of virtual reality (VR) simulators requires a deeper understanding of VR mediated human actions such as hand movements, with specifically tailored experiments testing how different design parameters affect performance. The present experiment investigates the time and precision of hand (index finger) movements under varying conditions of structural complexity and image size in VR without tactile feed-back from object to hand/finger. 18 right-handed subjects followed a complex and a simple physiological structure of small, medium and large size in VR, with the index finger of one of their two hands, from right to left, and from left to right. The results show that subjects performed best with small-size-simple structures and large-size-complex structures in VR. Movement execution was generally faster and more precise on simple structures. Performance was less precise when the dominant hand was used to follow the complex structures and small object size in VR. It is concluded that both size and structural complexity critically influence task execution in VR when no tactile feed-back from object to finger is generated. Individual learning curves should be monitored from the beginning of the training as suggested by the individual speed-precision analyses.},
	author = {Batmaz, Anil Ufuk and de Mathelin, Michel and Dresp-Langley, Birgitta},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446217},
	keywords = {Indexes;Complexity theory;Three-dimensional displays;Head;Software;Image color analysis;Virtual reality;Computing methodologies-Computer Graphics-Graphic systems and interfaces-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction devices;Human-centered computing-Interaction design;Software and its engineering-Software organization and properties-Virtual worlds software-Virtual worlds training simulations},
	month = {March},
	pages = {167-174},
	title = {Effects of Image Size and Structural Complexity on Time and Precision of Hand Movements in Head Mounted Virtual Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446217}}

@inproceedings{8446229,
	abstract = {This article presents an experiment exploring the possible impact of avatar's body part visibility on players' experience and performance using current Virtual Reality (VR) gaming platform capacities. In an action-based VR game, a player sees an avatar in first person perspective which is replicating his/her hand, head and body motion. In contrast to the expected outcome from non-game VR contexts, our results did not reveal significant differences with an avatar presenting an increasing number of visible body parts. The body ownership, immersion, emotional and cognitive involvements as well as the perceived control and difficulty were not improved with a more coherent virtual body. This tends to confirm the strong performance aspect of action-based games, whereby control efficiency and enemy awareness is paramount, and could overcome the perceptual, behavioural or emotional effects of avatar embodiment. Digital games are indeed prone to create an intense flow state typically reducing self-awareness, and focusing on the game completion and high performance achievements. However, further experiments with full-body tracking and different game types are necessary to confirm this trend. This research outcome motivates further analysis of the mutual influence of bottom-up and top-down factors of avatar embodiment causing psychophysical effects. In addition, it provides useful indications for VR game developers and researchers on possible effects and evaluation methods.},
	author = {Lugrin, Jean-Luc and Ertl, Maximilian and Krop, Philipp and Kl{\"u}pfel, Richard and Stierstorfer, Sebastian and Weisz, Bianka and R{\"u}ck, Maximilian and Schmitt, Johann and Schmidt, Nina and Latoschik, Marc Erich},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446229},
	keywords = {Avatars;Games;Visualization;Virtual environments;Torso;Mirrors;Human-centered computing-Human computer interaction (HCl)-Interaction paradigms-Virtual reality},
	month = {March},
	pages = {17-24},
	title = {Any ``Body'' There? Avatar Visibility Effects in a Virtual Reality Game},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446229}}

@inproceedings{8446235,
	abstract = {The emergence of low cost virtual and augmented reality systems has encouraged the development of immersive training applications for medical, military, and many other fields. Many of the training scenarios for these various fields may require the presentation of realistic interactions with virtual humans. It is thus vital to determine the critical factors of fidelity required in those interactions to elicit naturalistic behavior on the part of trainees. Negative training may occur if trainees are inadvertently influenced to react in ways that are unexpected and unnatural, hindering proper learning and transfer of skills and knowledge back into real world contexts. In this research, we examined whether haptic priming (presenting an illusion of virtual human touch at the beginning of the virtual experience) and different locomotion techniques (either joystick or physical walking) might affect proxemic behavior in human users. The results of our study suggest that locomotion techniques can alter proxemic behavior in significant ways. Haptic priming did not appear to impact proxemic behavior, but did increase rapport and other subjective social measures. The results suggest that designers and developers of immersive training systems should carefully consider the impact of even simple design and fidelity choices on trainee reactions in social interactions.},
	author = {Krum, David M. and Kang, Sin-Hwa and Phan, Thai},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446235},
	keywords = {Haptic interfaces;Training;Legged locomotion;Virtual environments;Atmospheric measurements;Particle measurements;Virtual humans;virtual reality;immersive training;fidelity;proxemics;haptic priming;locomotion techniques.: Human-centered computing-Human Computer Interaction (HCI)-Interaction Paradigms-Virtual Reality;Human-centered computing-Interaction design-Interaction design process and methods-User interface design},
	month = {March},
	pages = {223-9},
	title = {Influences on the Elicitation of Interpersonal Space with Virtual Humans},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446235}}

@inproceedings{8446242,
	abstract = {Joint attention is critical to the education and development of a child. Deficits in joint attention are considered by many researchers to be an early predictor of children with Autism Spectrum Disorder (ASD). Training of joint attention have been a significant topic in ASD intervention education research. We propose a novel joint attention training approach using a Customizable Virtual Human (CVH) and a Virtual Reality (VR) game to assist with joint attention training. Previous work has shown that CVHs potentially help the users with ASD to increase their performance in hand-eye coordination, motivate the users to play longer, as well as improve user experience in a training game. Based upon these discovered CVH benefits, we hypothesize that CVHs may also be beneficial in training joint attention for users with ASD. To test our hypothesis, we developed a CVH with customizable facial features in an educational game - Imagination Drums - and conducted a user study on adolescents with high functioning ASD to investigate the effects of CVHs. We collected users' eye-gaze data and task performance during the game to evaluate the users' joint attention with CVHs and the effectiveness of CVHs compared with Non-Customizable Virtual Humans (NCVHs). The study results showed that the CVH make the participants gaze less at the irrelevant area of the game's storyline (i.e. background), but surprisingly, also provided evidence that participants react slower to the CVH's joint attention bids, compared with NCVH. Overall, the study reveals insights of how users with ASD interact with CVHs and how these interactions affect joint attention.},
	author = {Mei, Chao and Zahed, Bushra T. and Mason, Lee and Ouarles, John},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446242},
	keywords = {Games;Training;Monitoring;Haptic interfaces;Image color analysis;Autism;Face;Customizable virtual human. Autism Spectrum Disorder. 3D interaction;H.5.2 [Information Interfaces and Presentation]: User Interfaces -Evaluation/methodology},
	month = {March},
	pages = {289-296},
	title = {Towards Joint Attention Training for Children with ASD - a VR Game Approach and Eye Gaze Exploration},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446242}}

@inproceedings{8446250,
	abstract = {Alphanumeric text entry is a challenge for Virtual Reality (VR) applications. VR enables new capabilities, impossible in the real world, such as an unobstructed view of the keyboard, without occlusion by the user's physical hands. Several hand representations have been proposed for typing in VR on standard physical keyboards. However, to date, these hand representations have not been compared regarding their performance and effects on presence for VR text entry. Our work addresses this gap by comparing existing hand representations with minimalistic fingertip visualization. We study the effects of four hand representations (no hand representation, inverse kinematic model, fingertip visualization using spheres and video inlay) on typing in VR using a standard physical keyboard with 24 participants. We found that the fingertip visualization and video inlay both resulted in statistically significant lower text entry error rates compared to no hand or inverse kinematic model representations. We found no statistical differences in text entry speed.},
	author = {Grubert, Jens and Witzani, Lukas and Ofek, Eyal and Pahud, Michel and Kranz, Matthias and Kristensson, Per Ola},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446250},
	keywords = {Keyboards;Visualization;Error analysis;Decoding;Virtual reality;Standards;Electronic mail;H.5.2: [User Interfaces - Input devices and strategies.]},
	month = {March},
	pages = {151-158},
	title = {Effects of Hand Representations for Typing in Virtual Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446250}}

@inproceedings{8446280,
	abstract = {This paper studies the combination of tangible objects and wearable haptics for improving the display of stiffness sensations in virtual environments. Tangible objects enable to feel the general shape of objects, but they are often passive or unable to simulate several varying mechanical properties. Wearable haptic devices are portable and unobtrusive interfaces able to generate varying tactile sensations, but they often fail at providing convincing stiff contacts and distributed shape sensations. We propose to combine these two approaches in virtual and augmented reality (VR/AR), becoming able of arbitrarily augmenting the perceived stiffness of real/tangible objects by providing timely tactile stimuli at the fingers. We developed a proof-of-concept enabling to simulate varying elasticity/stiffness sensations when interacting with tangible objects by using wearable tactile modules at the fingertips. We carried out a user study showing that wearable haptic stimulation can well alter the perceived stiffness of real objects, even when the tactile stimuli are not delivered at the contact point. We illustrated our approach both in VR and AR, within several use cases and different tangible settings, such as when touching surfaces, pressing buttons and pistons, or holding an object. Taken together, our results pave the way for novel haptic sensations in VR/AR by better exploiting the multiple ways of providing simple, unobtrusive, and low-cost haptic displays.},
	author = {de Tinguy, Xavier and Pacchierotti, Claudio and Marchal, Maud and L{\'e}cuyer, Anatole},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446280},
	keywords = {Haptic interfaces;Skin;Shape;Belts;Virtual environments;Mechanical factors;Augmented reality;Human-centered computing-Human computer interaction-Interaction devices-Haptic devices},
	month = {March},
	pages = {81-90},
	title = {Enhancing the Stiffness Perception of Tangible Objects in Mixed Reality Using Wearable Haptics},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446280}}

@inproceedings{8446295,
	abstract = {We present the design of a handheld-based interface for collaborative manipulations of 3D objects in mobile augmented reality. Our approach combines touch gestures and device movements for fast and precise control of 7-DOF transformations. Moreover, the interface creates a shared medium where several users can interact through their point-of-view and simultaneously manipulate 3D virtual augmentations. We evaluated our collaborative solution in two parts. First, we assessed our interface in single user mode, comparing the user task performance in three conditions: touch gestures, device movements and hybrid. Then, we conducted a study with 30 participants to understand and classify the strategies that arise while working in pairs, when partners are free to make their task organization. Furthermore, we investigated the effectiveness of simultaneous manipulations compared with the individual approach.},
	author = {Grandi, Jer{\^o}nimo G and Debarba, Henrique G and Bemdt, Iago and Nedel, Luciana and Maciel, Anderson},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446295},
	keywords = {Three-dimensional displays;Collaboration;Task analysis;Augmented reality;Performance evaluation;Cameras;Handheld computers;Human-centered computing-Human computer interaction (HCI)-Interaction techniques;Human-centered computing-Human computer interaction CHCI)-Interaction paradigms-Mixed/augmented reality Human-centered computing-Collaborative and social computing},
	month = {March},
	pages = {49-56},
	title = {Design and Assessment of a Collaborative 3D Interaction Technique for Handheld Augmented Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446295}}

@inproceedings{8446300,
	abstract = {No-Pose (NP) tracking systems rely on a single sensor located at the user's head to determine the position of the head. They estimate the head orientation with inertial sensors and analyze the body motion to compensate their drift. However with orientation drift, VR users implicitly lean their heads and bodies sidewards. Hence, to determine the sensor drift and to explicitly adjust the orientation of the VR display there is a need to understand and consider both the user's head and body orientations. This paper studies the effects of head orientation drift around the yaw axis on the user's absolute head and body orientations when walking naturally in the VR. We study how much drift accumulates over time, how a user experiences and tolerates it, and how a user applies strategies to compensate for larger drifts.},
	author = {Feigl, Tobias and Mutschler, Christopher and Philippsen, Michael},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446300},
	keywords = {Magnetic heads;Head;Resists;Legged locomotion;Tracking;Accelerometers;Magnetometers;VR;orientation drift;head tracking;inertial sensors.: Computing methodologies [Perception] Human-centered computing [Virtual reality]},
	month = {March},
	pages = {409-414},
	title = {Human Compensation Strategies for Orientation Drifts},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446300}}

@inproceedings{8446317,
	abstract = {Interaction tasks in virtual reality (VR) such as three-dimensional (3D) selection or manipulation of objects often suffer from reduced performance due to missing or different feedback provided by VR systems than during corresponding realworld interactions. Vibrotactile and auditory feedback have been suggested as additional perceptual cues complementing the visual channel to improve interaction in VR. However, it has rarely been shown that multimodal feedback improves performance or reduces errors during 3D object selection. Only little research has been conducted in the area of proximity-based multimodal feedback, in which stimulus intensities depend on spatiotemporal relations between input device and the virtual target object. In this paper, we analyzed the effects of unimodal and bimodal feedback provided through the visual, auditory and tactile modalities, while users perform 3D object selections in VEs, by comparing both binary and continuous proximity-based feedback. We conducted a Fitts' Law experiment and evaluated the different feedback approaches. The results show that the feedback types affect ballistic and correction phases of the selection movement, and significantly influence the user performance.},
	author = {Ariza, Oscar and Bruder, Gerd and Katzakis, Nicholas and Steinicke, Frank},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446317},
	keywords = {Three-dimensional displays;Task analysis;Visualization;Haptic interfaces;Feeds;Performance evaluation;Two dimensional displays;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Input Devices and Strategies;Evaluation},
	month = {March},
	pages = {327-334},
	title = {Analysis of Proximity-Based Multimodal Feedback for 3D Selection in Immersive Virtual Environments},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446317}}

@inproceedings{8446320,
	abstract = {We developed and evaluated two-handed gestures on the Microsoft HoloLens to manipulate augmented reality annotations through rotation and scale operations. We explore the design space of bimanual interactions on head-worn AR platforms, with the intention of dedicating two-handed gestures to rotation and scaling manipulations while reserving one-handed interactions to drawing annotations. In total, we implemented five techniques for rotation and scale manipulation gestures on the Microsoft HoloLens: three two-handed techniques, one technique for one-handed rotation and two-handed scale, and one baseline one-handed technique that represents standard HoloLens UI recommendations. Two of the bimanual interaction techniques involve axis separation for rotation whereas the third technique is fully 6DOF and modeled after the successful ``spindle'' approach from 3DUI literature. To evaluate our techniques, we conducted a study with 48 users. We recorded multiple performance metrics for each user on each technique, as well as user preferences. Results indicate that in spite of problems due to field-of-view limitations, certain two-handed techniques perform comparatively to the one-handed baseline technique in terms of accuracy and time. Furthermore, the best-performing two-handed technique outdid all other techniques in terms of overall user preference, demonstrating that bimanual gesture interactions can serve a valuable role in the UI toolbox on head-worn AR devices such as the HoloLens.},
	author = {Chaconas, Nikolas and H{\"o}llerer, Tobias},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446320},
	keywords = {Task analysis;Three-dimensional displays;Augmented reality;Switches;Standards;Mice;Bars;Bimanual;two-handed;gestures;object manipulation;rotation;scale;evaluation;user study;augmented reality;HoloLens: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Input devices and strategies;},
	month = {March},
	pages = {1-8},
	title = {An Evaluation of Bimanual Gestures on the Microsoft HoloLens},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446320}}

@inproceedings{8446352,
	abstract = {Which features and characteristics must virtual agents have to elicit social presence in humans? In order to investigate this objectively, experimental paradigms relying on implicit cognitive phenomena can be employed. In the present study, two aims were reached: First, we built a mixed reality apparatus using affordable consumer VR hardware and other off-the-shelf components to detect implicit cognitive phenomena on the basis of small reaction times with a precision of several milliseconds. Secondly, using this apparatus, we showed that a virtual agent can indeed elicit an implicit phenomenon which was hitherto assumed to be restricted to human participants: social Inhibition of Return (sIOR). sIOR is indicated by an increase in reaction time of about 20 ms that occurs when one subject reaches to the same target to which a co-actor had just responded to [19]. Skarratt, Cole and Kingstone [17] found that only a real co-actor, but not an animated one could induce this inhibitory effect. However, their virtual agent was displayed as a video on a 2D-TV-screen, which was only weakly immersive. In contrast, we found in two experiments that an animated agent presented via a more immersive VR headset could elicit the sIOR phenomenon. Further research has to be done to identify the reasons why this agent - or why VR per se - evokes an sIOR phenomenon. Moreover, the underlying mechanism of why sIOR occurs are not fully understood yet. The current findings open the way to further investigate these questions in experimental settings relying on VR.},
	author = {Wienrich, Carolin and Gross, Richard and Kretschmer, Felix and M{\"u}ller-Plath, Gisela},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446352},
	keywords = {Virtual reality;Cognition;Headphones;Laboratories;Task analysis;Fingers;Light emitting diodes;Social cognition;Social inhibition of return;Agents;Mixed reality;Reaction times;Arduino.: [Human-centered] [Computing] Mixed / augmented reality;Virtual reality;[Computing methodologies] Virtual reality},
	month = {March},
	pages = {191-198},
	title = {Developing and Proving a Framework for Reaction Time Experiments in VR to Objectively Measure Social Interaction with Virtual Agents},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446352}}

@inproceedings{8446364,
	abstract = {Creating realistic animations of virtual humans remains comparatively complex and expensive. This research explores the degree to which animation fidelity affects users' gaze behavior when interacting in virtual reality training simulations that include virtual humans. Participants were randomly assigned to one of three conditions, wherein the virtual patient either: 1) was not animated; 2) played idle animations; or 3) played idle animations, looked at the participant when speaking, and lip-synced speech and facial gestures when conversing with the participant. Each participant's gaze was recorded in an inter-personal interactive patient surveillance simulation. Results suggest that conversational and passive animations elicited visual attention in a similar manner, as compared to the no animation condition. Results also suggest that when participants face critical situations in inter-personal medical simulations, visual attention towards the virtual human decreases while gaze towards goal directed activities increases.},
	author = {Volonte, Matias and Robb, Andrew and Duchowski, Andrew T. and Babu, Sabarish V.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446364},
	keywords = {Animation;Visualization;Solid modeling;Training;Virtual reality;Task analysis;Surveillance;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Animations;Evaluation/methodology;I.3.3 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality;},
	month = {March},
	pages = {25-32},
	title = {Empirical Evaluation of Virtual Human Conversational and Affective Animations on Visual Attention in Inter-Personal Simulations},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446364}}

@inproceedings{8446381,
	abstract = {Augmented reality (AR) applications can leverage the full space of an environment to create immersive experiences. However, most empirical studies of interaction in AR focus on interactions with objects close to the user, generally within arms reach. As objects move farther away, the efficacy and usability of different interaction modalities may change. This work explores AR interactions at a distance, measuring how applications may support fluid, efficient, and intuitive interactive experiences in room-scale augmented reality. We conducted an empirical study (N = 20) to measure trade-offs between three interaction modalities-multimodal voice, embodied freehand gesture, and handhelds devices-for selecting, rotating, and translating objects at distances ranging from 8 to 16 feet (2.4m-4.9m). Though participants performed comparably with embodied freehand gestures and handheld remotes, they perceived embodied gestures as significantly more efficient and usable than device-mediated interactions. Our findings offer considerations for designing efficient and intuitive interactions in room-scale AR applications.},
	author = {Whitlock, Matt and Harnner, Ethan and Brubaker, Jed R. and Kane, Shaun and Szafir, Danielle Albers},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446381},
	keywords = {Task analysis;Usability;Visualization;Pervasive computing;Augmented reality;Electronic mail;Human-centered computing-Interaction design-Interaction design process and methods-User interface design},
	month = {March},
	pages = {41-48},
	title = {Interacting with Distant Objects in Augmented Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446381}}

@inproceedings{8446383,
	abstract = {Virtual Reality (VR) is increasingly used in spatial cognition research, as it offers high experimental control in naturalistic multimodal environments, which is hard to achieve in real-world settings. Although recent technological advances offer a high level of photorealism, locomotion in VR is still restricted because people might not perceive their self-motion as they would in the real world. This might be related to the inability to use embodied spatial orientation processes, which support automatic and obligatory updating of our spatial awareness. Previous research has identified the roles reference frames play in retaining spatial orientation. Here, we propose using visually overlaid rectangular boxes, simulating reference frames in VR, to provide users with a better insight into spatial direction in landmark-free virtual environments. The current mixed-method study investigated how different variations of the visually simulated reference frames might support people in a navigational search task. Performance results showed that the existence of a simulated reference frame yields significant effects on participants completion time and travel distance. Though a simulated CAVE translating with the navigator (one of the simulated reference frames) did not provide significant benefits, the simulated room (another simulated reference frame depicting a rest frame) significantly boosted user performance in the task as well as improved participants preference in the post-experiment evaluation. Results suggest that adding a visually simulated reference frame to VR applications might be a cost-effective solution to the spatial disorientation problem in VR.},
	author = {Nguyen-Vo, Thinh and Riecke, Bernhard E. and Stuerzlinger, Wolfgang},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446383},
	keywords = {Task analysis;Navigation;Visualization;Virtual environments;Resists;Legged locomotion;Cognition;Human-centered computing-Empirical studies in HCI},
	month = {March},
	pages = {415-422},
	title = {Simulated Reference Frame: A Cost-Effective Solution to Improve Spatial Orientation in VR},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446383}}

@inproceedings{8446391,
	abstract = {Compositing virtual objects into photographs with known real world geometry is a common task in mixed reality (MR) applications. This geometry enables rendering of global illumination effects, such as mutual lighting, shadowing, and occlusions between the background photograph and virtual objects. Obtaining high fidelity geometric representations of the real world can be a costly procedure, and is often approximated with depth data. However, it is not clear how much fidelity the depth data should have in order to maintain high visual quality in MR rendering. in this paper, we investigate the relationship between real world depth fidelity and visual quality in MR rendering. We do this by conducting a series of user experiments that measure how seamlessly virtual objects are blended with the background under varying depth resolutions. We independently evaluate the noticeability of multiple composition artifacts that occur with approximate depth. Perceptual thresholds in depth resolution are then obtained for each artifact. The findings can be used to inform trade-off decisions for optimising depth acquisition pipelines in MR applications.},
	author = {Petikam, Lohit and Chalmers, Andrew and Rhee, Taeyhun},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446391},
	keywords = {Rendering (computer graphics);Geometry;Lighting;Virtual reality;Visualization;Three-dimensional displays;Image resolution;Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Perception},
	month = {March},
	pages = {401-408},
	title = {Visual Perception of Real World Depth Map Resolution for Mixed Reality Rendering},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446391}}

@inproceedings{8446403,
	abstract = {With the advantages of small size and light weight, electrical stimulation devices have been investigated for providing haptic feedback in relation to virtual objects. Electrical stimulation devices can directly activate sensory receptors to produce a reaction force or touch sensations. In the current study, we tested a new method of electrically inducing force sensation in the fingertip, presenting haptic feedback designed to alter perceptions of softness, hardness and stickiness. We developed a 3D virtual reality system combined with finger-motion capture and electrical stimulation devices. We conducted two experiments to evaluate our electrical stimulation method and analyzed the effects of electrical stimulation on perception. The first experiment confirmed that participants could distinguish between the directions of the illusory force sensation, reporting whether the stimulation flexed their index finger forward or extended it backward. The second experiment examined the effects of the electric current itself on the intensity of their perception of the softness, hardness and stickiness of a virtual object.},
	author = {Yem, Vibol and Vu, Kevin and Kon, Yuki and Kajimoto, Hiroyuki},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446403},
	keywords = {Electrical stimulation;Force;Electrodes;Haptic interfaces;Current;Thumb;Softness-hardness perception;stickiness perception;electrical stimulation;virtual touch.: H.5.2 [Information Interfaces and Presentation]: User Interfaces-Haptic I/O},
	month = {March},
	pages = {89-96},
	title = {Effect of Electrical Stimulation Haptic Feedback on Perceptions of Softness-Hardness and Stickiness While Touching a Virtual Object},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446403}}

@inproceedings{8446432,
	abstract = {Understanding the sensorimotor control mechanisms that mediate gait compensation during environmental perturbation is a crucial step in developing tailored rehabilitative therapies to restore ambulation in patient populations. Current methods to evaluate the effects of environmental perturbations involve costly systems that physically perturb patients to elicit a compensatory response. Studies have shown that visual feedback alone can elicit dramatic changes in gait; however, the impact of fully immersive visual feedback is not well studied. Here we examined whether a low cost immersive virtual reality (VR) system can elicit perturbation responses similar to a physical disruption. We examined the responses of 11 subjects as they walked through a VR environment consisting of a bridge spanning a lake. While subjects walked on a treadmill mounted to a 6 degree-of-freedom motion base, pseudorandom roll perturbations (3, 6, 11 deg.) were applied visually to the bridge with (VP trials) and without (V trials) the corresponding physical displacement of the motion base. Significant differences were found between normal (unperturbed) walking and normal walking in the VR environment (p<;.05) for average step length, width, and Margin of Stability (MoS). Significant differences were also observed between unperturbed and perturbed walking in the VR environment (p<;0.05 for VP and V trials). While the subjects' responses to visual perturbations were generally lower than to combined visual and physical perturbations, the differences were not statistically significant (p>.05). The results demonstrate that visual perturbations provided in an immersive virtual environment can induce compensatory changes in gait during treadmill walking that are consistent with a physical perturbation. The application of environmental perturbations in VR systems could provide a cost-effective approach for gait rehabilitation in patient populations.},
	author = {Riem, Lara and Van Dehy, Jacob and Onushko, Tanya and Beardsley, Scott},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446432},
	keywords = {Perturbation methods;Visualization;Legged locomotion;Virtual environments;Bridges;Sociology;Statistics;Virtual Reality;Head Mounted Display;Visualization.: H.5.1 [Multimedia Information Systems]: Artificial;augmented;and virtual realities},
	month = {March},
	pages = {128-135},
	title = {Inducing Compensatory Changes in Gait Similar to External Perturbations Using an Immersive Head Mounted Display},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446432}}

@inproceedings{8446441,
	abstract = {Optical See-Through Head-Mounted Displays (OST-HMDs) lose the visibility of virtual contents under bright environment illumination due to their see-through nature. We demonstrate how a liquid crystal (LC) filter attached to an OST-HMD can be used to dynamically increase the perceived brightness of virtual content without impacting the perceived brightness of the real scene. We present a prototype OST-HMD that continuously adjusts the opacity of the LC filter to attenuate the environment light without users becoming aware of the change. Consequently, virtual content appears to be brighter. The proposed approach is evaluated in psychophysical experiments in three scenes, with 16, 31, and 31 participants, respectively. The participants were asked to compare the magnitude of brightness changes of both real and virtual objects, before and after dimming the LC filter over a period of 5, 10, and 20 seconds. The results showed that the participants felt increases in the brightness of virtual objects while they were less conscious of reductions of the real scene luminance. These results provide evidence for the effectiveness of our display design. Our design can be applied to a wide range of OST-HMDs to improve the brightness and hence realism of virtual content in augmented reality applications.},
	author = {Mori, Shohei and Ikeda, Sei and Plopski, Alexander and Sandor, Christian},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446441},
	keywords = {Brightness;Visualization;Prototypes;Lighting;Retina;Optical imaging;Optical attenuators;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality Interaction devices Displays and imagers},
	month = {March},
	pages = {251-258},
	title = {BrightView: Increasing Perceived Brightness of Optical See-Through Head-Mounted Displays Through Unnoticeable Incident Light Reduction},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446441}}

@inproceedings{8446480,
	abstract = {Personal space (PS), the flexible protective zone maintained around oneself, is a key element of everyday social interactions. It, e.g., affects people's interpersonal distance and is thus largely involved when navigating through social environments. However, the PS is regulated dynamically, its size depends on numerous social and personal characteristics and its violation evokes different levels of discomfort and physiological arousal. Thus, gaining more insight into this phenomenon is important. We contribute to the PS investigations by presenting the results of a controlled experiment in a CAVE, focusing on German males in the age of 18 to 30 years. The PS preferences of 27 participants have been sampled while they were approached by either a single embodied, computer-controlled virtual agent (VA) or by a group of three VAs. In order to investigate the influence of a VA's emotions, we altered their facial expression between angry and happy. Our results indicate that the emotion as well as the number of VAs approaching influence the PS: larger distances are chosen to angry VAs compared to happy ones; single VAs are allowed closer compared to the group. Thus, our study is a foundation for social and behavioral studies investigating PS preferences.},
	author = {B{\"o}nsch, Andrea and Radke, Sina and Overath, Heiko and Asch{\'e}, Laura M. and Wendt, Jonathan and Vierjahn, Tom and Habel, Ute and Kuhlen, Torsten W.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446480},
	keywords = {Skin;Navigation;Physiology;Electronic mail;Shape;Brain;Virtual reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;J.4 [Computer Applications]: Social And Behavioral Sciences-Psychology},
	month = {March},
	pages = {199-206},
	title = {Social VR: How Personal Space is Affected by Virtual Agents' Emotions},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446480}}

@inproceedings{8446498,
	abstract = {Taking advantage of motion capture and display technologies, a method giving a user the ability to control the dance motions of a virtual partner in an immersive setup was developed and is presented in this paper. The method utilizes a dance motion dataset containing the motion of both dancers (leader and partner). A hidden Markov model (HMM) was used to learn the structure of the dance motions. The HMM was trained on the motion of a chosen dancer (leader or partner), and during runtime, the system predicts the progress of the chosen dance motion, which corresponds to the progress of the user's motion. The regular structure of the HMM was extended by utilizing a jump state transition, allowing the user to improvise dance motions during the runtime. Since the jump state addition increases the model's complexity, an effort was made to optimize the prediction process to ensure runtime efficiency. A few corrective steps were also implemented to ensure the partner character's motions appear natural. A user study was conducted to understand the naturalness of the synthesized motion as well as the control that the user has on the partner character's synthesized motion.},
	author = {Mousas, Christos},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446498},
	keywords = {Hidden Markov models;Robot kinematics;Virtual reality;Training;Real-time systems;Runtime;Human-centered computing-Interaction paradigms-Virtual reality;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Computing methodologies-Computer graphics-Animation-Motion capture},
	month = {March},
	pages = {57-64},
	title = {Performance-Driven Dance Motion Control of a Virtual Partner Character},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446498}}

@inproceedings{8446539,
	abstract = {Immersive Virtual Environments (IVEs) are becoming more accessible and more widely utilized for training. Previous research has shown that the matching of visual and proprioceptive information is important for calibration. While research has demonstrated that self-avatars can enhance ones' sense of presence and improve distance perception, the effects of self-avatar fidelity on near field distance estimations has yet to be investigated. This study tested the effect of avatar fidelity on the accuracy of distance estimations in the near-field. Performance with a virtual avatar was also compared to real-world performance. Three levels of fidelity were tested; 1) an immersive self-avatar with realistic limbs, 2) a low-fidelity self-avatar showing only joint locations, and 3) end-effector only. The results suggest that reach estimations become more accurate as the visual fidelity of the avatar increases, with accuracy for high fidelity avatars approaching real-world performance as compared to low-fidelity and end-effector conditions. In all conditions reach estimations became more accurate after receiving feedback during a calibration phase.},
	author = {Ebrahimi, Elham and Hartman, Leah S. and Robb, Andrew and Pagano, Christopher C. and Babu, Sabarish V.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446539},
	keywords = {Visualization;Estimation;Avatars;Calibration;Legged locomotion;Tracking;Wrist;Human-centered computing-Visualization-Empirical studies in visualization-;Human-centered computing-Human computer interaction (HCI)-Empirical studies in HCI},
	month = {March},
	pages = {1-8},
	title = {Investigating the Effects of Anthropomorphic Fidelity of Self-Avatars on Near Field Depth Perception in Immersive Virtual Environments},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446539}}

@inproceedings{8446546,
	abstract = {Here we report the first empirical findings on virtual lucidity (VL), a new construct similar to lucidity during dreaming, but regarding awareness that one is having a virtual experience. VL concerns the depth and breadth of this awareness, as well as the extent it affords regulatory monitoring and control. To study VL, we adapted a measure from lucid dreaming research to assess whether more VL predicted lower fear, but not less enjoyment, during a virtual reality (VR) threat scenario of walking, and being asked to step off, a wooden plank seemingly high above a city. We examined predictors of VL and related outcomes across a community sample and lucid dream trainees at a meditation retreat center. In line with hypotheses, higher VL predicted less fear, more enjoyment, and greater likelihood of stepping off the plank. Moreover, a number of dispositional factors predicted greater VL and lower fear. Lucid dream retreatants, engaged in a contemplative practice called illusory form yoga, experienced more VL and less fear compared to nonretreatants, with marginally higher likelihood of stepping off the plank. Finally, VL mediated all significant relations between predictors and outcomes. Results held controlling for presence or fear of heights. We discuss the potential validity and utility of VL, its relation to presence, and examples of how it may inform the development and application of VR and related technologies.},
	author = {Quaglia, Jordan T. and Holecek, Andrew},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446546},
	keywords = {Psychology;Monitoring;Legged locomotion;Virtual environments;Training;Urban areas;Lucid dreaming;meditation;mindfulness;presence;virtual lucidity;virtual reality.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;Augmented;and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences-Psychology},
	month = {March},
	pages = {1-72},
	title = {Lucid Virtual Dreaming: Antecedents and Consequents of Virtual Lucidity During Virtual Threat},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446546}}

@inproceedings{8446574,
	abstract = {While research in the field of augmented reality (AR) has produced many innovative human-computer interaction techniques, some may produce physical and visual perceptions with unforeseen negative impacts on user performance. In a controlled human-subject study we investigated the effects of mismatched physical and visual perception on cognitive load and performance in an AR touching task by varying the physical fidelity (matching vs. non-matching physical shape) and visual mechanism (projector-based vs. HMD-based AR) of the representation. Participants touched visual targets on four corresponding physical-visual representations of a human head. We evaluated their performance in terms of touch accuracy, response time, and a cognitive load task requiring target size estimations during a concurrent (secondary) counting task. After each condition, participants completed questionnaires concerning mental, physical, and temporal demands; stress; frustration; and usability. Results indicated higher performance, lower cognitive load, and increased usability when participants touched a matching physical head-shaped surface and when visuals were provided by a projector from underneath.},
	author = {Hochreiter, Jason and Daher, Salam and Bruder, Gerd and Welch, Greg},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446574},
	keywords = {Task analysis;Three-dimensional displays;Head;Resists;Visualization;Training;Cameras;Human-centered computing-Human computer interaction (HCI)-HCI design and evaluation methods-User studies;Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed/augmented reality;Human-centered computing-Human computer interaction (HCI)-Interaction techniques},
	month = {March},
	pages = {1-386},
	title = {Cognitive and Touch Performance Effects of Mismatched 3D Physical and Visual Perceptions},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446574}}

@inproceedings{8446575,
	abstract = {Introduction. An increasing number of location-based entertainment centers offers the possibility of entering multi-user virtual reality (VR) scenarios. Until now, neither cognition and emotions of users nor team experience have been scientifically evaluated in such an application. The present study investigated the gain of positive social interdependence while experiencing an adventure on the Immersive Deck of Illusion Walk (Berlin, Germany). Method. The preliminary version of a VR group adventure of the company was enriched by a task establishing social interdependence (IDP condition). The impact of IDP on social presence and cooperation (i.e., mutual importance) was evaluated relative to a control task without interdependence (nIDP condition). Results. Social IDP increased social presence and cooperation among participants. Additionally, behavioral involvement (part of presence), certain aspects of the adventure experience, and the affective evaluation during the experience were positively influenced by IDP. Discussion. The present study showed that interdependence can substantially enhance social presence and cooperation (i.e., mutual importance) in a VR setting already characterized by social co-experience. Thus, it revealed one design option (social IDP) to improve the experience, particularly the social experience, of location-based entertainment. Conclusion. The present research addressed one goal of location-based VR hosts to scientifically established design principles for social and collective adventures by supporting the impact of ``collectively mastering an adventurous challenge''. In addition, our evaluation demonstrated that the multi-modal tracking, the free movement, as well as the multi-user features enabled natural interaction with other users and the environment, and thereby engendered a comfortable social experience.},
	author = {Wienrich, C. and Schindler, K. and D{\"o}llinqer, N. and Kock, S. and Traupe, O.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446575},
	keywords = {Virtual reality;Games;Task analysis;Human computer interaction;Psychology;Companies;Entertainment industry;Multi-user VR;Location-based Entertainment;Interdependence;Social Presence;Cooperation;Evaluation.: [HCI]: Collaborative interaction;Virtual Reality;Empirical studies in interaction design;[Human Centered Computing]: Mobile computing;[Hardware]: Haptic devices;[Computing Methodologies]: Tracking},
	month = {March},
	pages = {207-214},
	title = {Social Presence and Cooperation in Large-Scale Multi-User Virtual Reality - The Relevance of Social Interdependence for Location-Based Environments},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446575}}

@inproceedings{8446595,
	abstract = {Fluid artwork refers to works of art based on the aesthetics of fluid motion, such as smoke photography, ink injection into water, and paper marbling. Inspired by such types of art, we created Fluid Sketching as a novel medium for creating 3D fluid artwork in immersive virtual environments. It allows artists to draw 3D fluid-like sketches and manipulate them via six degrees of freedom input devices. Different brush stroke settings are available, varying the characteristics of the fluid. Because of fluids' nature, the diffusion of the drawn fluid sketch is animated, and artists have control over altering the fluid properties and stopping the diffusion process whenever they are satisfied with the current result. Furthermore, they can shape the drawn sketch by directly interacting with it, either with their hand or by blowing into the fluid. We rely on particle advection via curl-noise as a fast procedural method for animating the fluid flow.},
	author = {Eroglu, Sevinc and Gebhardt, Sascha and Schmitz, Patric and Rausch, Dominik and Kuhlen, Torsten Wolfgang},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446595},
	keywords = {Three-dimensional displays;Computational modeling;Fluids;Art;Real-time systems;Mathematical model;Ink;Computing methodologiesComputer graphicsGraphics systems and interfacesVirtual reality;Human-centered computingHuman computer interaction (HCI)Interaction devicesSound-based input / output;Human-centered computingHuman computer interaction (HCI)HCI design and evaluation methodsUser studies},
	month = {March},
	pages = {475-482},
	title = {Fluid SketchingImmersive Sketching Based on Fluid Flow},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446595}}

@inproceedings{8446602,
	abstract = {In this paper, we introduce an online video-based system that actively assists users in assembly tasks. The system guides and monitors the assembly process by providing instructions and feedback on possibly erroneous operations, enabling easy and effective guidance in AR/MR applications. The core of our system is an online video-based assembly parsing method that can understand the assembly process, which is known to be extremely hard previously. Our method exploits the availability of the participating parts to significantly alleviate the problem, reducing the recognition task to an identification problem, within a constrained search space. To further constrain the search space, and understand the observed assembly activity, we introduce a tree-based global-inference technique. Our key idea is to incorporate part-interaction rules as powerful constraints which significantly regularize the search space and correctly parse the assembly video at interactive rates. Complex examples demonstrate the effectiveness of our method.},
	author = {Wang, Bin and Wang, Guofeng and Sharf, Andrei and Li, Yangyan and Zhong, Fan and Qin, Xueying and Cohenor, Daniel and Chen, Baoquan},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446602},
	keywords = {Three-dimensional displays;Solid modeling;Task analysis;Two dimensional displays;Monitoring;Visualization;Shape;Computing methodologies-Computer graphics-Mixed / augmented reality},
	month = {March},
	pages = {459-466},
	title = {Active Assembly Guidance with Online Video Parsing},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446602}}

@inproceedings{8446620,
	abstract = {Many recent head-mounted display applications and games implement a range-restricted variant of teleportation for exploring virtual environments. This travel metaphor referred to as jumping only allows to teleport to locations in the currently visible part of the scene. In this paper, we present a formal description and classification scheme for teleportation techniques and its application to the classification of jumping. Furthermore, we present the results of a user study (N=24) that compared jumping to the more conventional steering with respect to spatial updating and simulator sickness. Our results show that despite significantly faster travel times during jumping, a majority of participants (75%) achieved similar spatial updating accuracies in both conditions (mean difference 0.02$\,^{\circ}$, =5.05$\,^{\circ}$. In addition, jumping induced significantly less simulator sickness, which altogether justifies it as an alternative to steering for the exploration of immersive virtual environments. However, application developers should be aware that spatial updating during jumping may be impaired for individuals.},
	author = {Wei{\ss}ker, Tim and Kunert, Andr{\'e} and Fr{\"o}hlich, Bernd and Kulik, Alexander},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8446620},
	keywords = {Teleportation;Games;Space exploration;Task analysis;Virtual environments;Visualization;I.3.6 [Computer Graphics]: Methodology and TechniquesInteraction techniques H.1.2 [Models and Principles]: User/Machine Systems-Human information processing},
	month = {March},
	pages = {97-104},
	title = {Spatial Updating and Simulator Sickness During Steering and Jumping in Immersive Virtual Environments},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8446620}}

@inproceedings{8447549,
	abstract = {We implemented live-textured geometry model creation with immediate coverage feedback visualizations in AR on the Microsoft HoloLens. A user walking and looking around a physical space can create a textured model of the space, ready for remote exploration and AR collaboration. Out of the box, a HoloLens builds a triangle mesh of the environment while scanning and being tracked in a new environment. The mesh contains vertices, triangles, and normals, but not color. We take the video stream from the color camera and use it to color a UV texture to be mapped to the mesh. Due to the limited graphics memory of the HoloLens, we use a fixed-size texture. Since the mesh generation dynamically changes in real time, we use an adaptive mapping scheme that evenly distributes every triangle of the dynamic mesh onto the fixed-size texture and adapts to new geometry without compromising existing color data. Occlusion is also considered. The user can walk around their environment and continuously fill in the texture while growing the mesh in real-time. We describe our texture generation algorithm and illustrate benefits and limitations of our system with example modeling sessions. Having first-person immediate AR feedback on the quality of modeled physical infrastructure, both in terms of mesh resolution and texture quality, helps the creation of high-quality colored meshes with this standalone wireless device and a fixed memory footprint in real-time.},
	author = {Dong, Samuel and H{\"o}llerer, Tobias},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447549},
	keywords = {Image color analysis;Real-time systems;Geometry;Three-dimensional displays;Cameras;Solid modeling;Rendering (computer graphics);Computing methodologies-Computer graphics-Graphics systems and interfaces-Mixed / augmented reality;Computing methodologies-Computer graphics-Image manipulation-Texturing},
	month = {March},
	pages = {231-237},
	title = {Real-Time Re-Textured Geometry Modeling Using Microsoft HoloLens},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447549}}

@inproceedings{8447550,
	abstract = {This paper presents a novel approach for the augmentation of social behaviors in virtual reality (VR). We designed three visual transformations for behavioral phenomena crucial to everyday social interactions: eye contact, joint attention, and grouping. To evaluate the approach, we let users interact socially in a virtual museum using a large-scale multi-user tracking environment. Using a between-subject design (N = 125) we formed groups of five participants. Participants were represented as simplified avatars and experienced the virtual museum simultaneously, either with or without the augmentations. Our results indicate that our approach can significantly increase social presence in multi-user environments and that the augmented experience appears more thought-provoking. Furthermore, the augmentations seem also to affect the actual behavior of participants with regard to more eye contact and more focus on avatars/objects in the scene. We interpret these findings as first indicators for the potential of social augmentations to impact social perception and behavior in VR.},
	author = {Roth, Daniel and Klelnbeck, Constantin and Feigl, Tobias and Mutschler, Christopher and Latoschik, Marc Erich},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447550},
	keywords = {Avatars;Visualization;Electronic mail;Color;Human computer interaction;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
	month = {March},
	pages = {215-222},
	title = {Beyond Replication: Augmenting Social Behaviors in Multi-User Virtual Realities},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447550}}

@inproceedings{8447552,
	abstract = {This paper presents two new pointing techniques for wiggle 3D displays, which present the 2D projection of 3D content with automatic (rotatory) motion parallax. Standard pointing at targets in wiggle 3D displays is challenging as the content is constantly in motion. The two pointing techniques presented here take advantage of the cursor's current position or the user's gaze direction for collocating the wiggle rotation center and potential targets. We evaluate the performance of the pointing techniques with a novel methodology that integrates 3D distractors into the ISO-9241-9 standard task. The experimental results indicate that the new techniques are significantly more efficient than standard pointing techniques in wiggle 3D displays. Given that we observed no performance variation for different targets, our new techniques seem to negate any interaction performance penalties of wiggle 3D displays.},
	author = {Ortega, Micha{\"e}l and Stuerzlinger, Wolfgang},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447552},
	keywords = {Three-dimensional displays;Task analysis;Two dimensional displays;ISO Standards;Shape;Mice;3D Interaction Technique;Pointing;Eye Tracking;H.5.2. Information Interfaces and Presentation: User Interfaces - Interaction styles;Input devices and strategies},
	month = {March},
	pages = {335-340},
	title = {Pointing at Wiggle 3D Displays},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447552}}

@inproceedings{8447553,
	abstract = {During free exploration of an unknown virtual scene, users often miss important parts, leading to incorrect or incomplete environment knowledge and a potential negative impact on performance in later tasks. This is addressed by wayfinding aids such as compasses, maps, or trails, and automated exploration schemes such as guided tours. However, these approaches either do not actually ensure exploration success or take away control from the user. Therefore, we present an interactive assistance interface to support exploration that guides users to interesting and unvisited parts of the scene upon request, supplementing their own, free exploration. It is based on an automated analysis of object visibility and viewpoint quality and is therefore applicable to a wide range of scenes without human supervision or manual input. In a user study, we found that the approach improves users' knowledge of the environment, leads to a more complete exploration of the scene, and is also subjectively helpful and easy to use.},
	author = {Freitag, Sebastian and Weyers, Benjamin and Kuhlen, Torsten W.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447553},
	keywords = {Visualization;Measurement;Task analysis;Navigation;Three-dimensional displays;Cameras;Histograms;Human-centered computing-Human computer interaction-Interaction paradigms-Virtual reality;Computing methodologies-Computer graphics-Rendering- Visibility},
	month = {March},
	pages = {355-362},
	title = {Interactive Exploration Assistance for Immersive Virtual Environments Based on Object Visibility and Viewpoint Quality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447553}}

@inproceedings{8447554,
	abstract = {We present a comprehensive multitouch input mapping for 3D navigation of multiscale 3D models. In contrast to prior work, our technique offers explicit control over 3D rotation, 3D translation, and uniform scaling with manipulative gestures that do not require graphical widgets. Our proposed technique is consistent with the established RST mapping (rotation, scaling, translation) for 2D mul-titouch input and follows suggestions from prior work on multitouch 3D interaction. Our implementation includes a rendering technique that can reduce perceptual conflicts of 3D touch input on stereoscopic displays. We also report on two user studies that informed the suggested interaction design and confirmed its usability.},
	author = {Kulik, Alexander and Kunert, Andr{\'e} and Keil, Magdalena and Froehlich, Bernd},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447554},
	keywords = {Three-dimensional displays;Two dimensional displays;Navigation;Visualization;Collaboration;Switches;Task analysis;Human-centered computing-Human computer interaction-Interaction techniques-Gestural input},
	month = {March},
	pages = {363-370},
	title = {RST 3D: A Comprehensive Gesture Set for Multitouch 3D Navigation},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447554}}

@inproceedings{8447555,
	abstract = {Natural hand-based interaction should feature hand motion that adapts smoothly to the tracked user's motion, reacts robustly to contact with objects in a virtual environment, and enables dexterous manipulation of these objects. In our work, we enable all these properties thanks to an efficient soft hand simulation model. This model integrates an articulated skeleton, nonlinear soft tissue and frictional contact, to provide the realism necessary for natural interaction. Robust and smooth interaction is made possible by simulating in a single energy minimization framework all the mechanical energy exchanges among elements of the hand: coupling between the hand's skeleton and the user's motion, constraints at skeletal joints, nonlinear soft skin deformation, coupling between the hand's skeleton and the soft skin, frictional contact between the skin and virtual objects, and coupling between a grasped object and other virtual objects. We have put our effort on describing all elements of the hand that provide for realism and natural interaction, while ensuring minimal and bounded computational cost, which is key for smooth and robust interaction. As a result, we accomplish hand simulation as an asset that can be connected to diverse input tracking devices, and seamlessly integrated in game engines for fast deployment in VR applications.},
	author = {Verschoor, Mickeal and Lobo, Daniel and Otaduy, Miguel A.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447555},
	keywords = {Skin;Skeleton;Computational modeling;Couplings;Robustness;Strain;Tracking;Hand Interaction;VR;Simulation: Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality;Human-centered computing-Human computer interaction (HCI)-Interaction techniques},
	month = {March},
	pages = {183-190},
	title = {Soft Hand Simulation for Smooth and Robust Natural Interaction},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447555}}

@inproceedings{8447556,
	abstract = {There are increasing real-time live applications in virtual reality, where it plays an important role in capturing and retargetting 3D human pose. But it is still challenging to estimate accurate 3D pose from consumer imaging devices such as depth camera. This paper presents a novel cascaded 3D full-body pose regression method to estimate accurate pose from a single depth image at 100 fps. The key idea is to train cascaded regressors based on Gradient Boosting algorithm from pre-recorded human motion capture database. By incorporating hierarchical kinematics model of human pose into the learning procedure, we can directly estimate accurate 3D joint angles instead of joint positions. The biggest advantage of this model is that the bone length can be preserved during the whole 3D pose estimation procedure, which leads to more effective features and higher pose estimation accuracy. Our method can be used as an initialization procedure when combining with tracking methods. We demonstrate the power of our method on a wide range of synthesized human motion data from CMU mocap database, Human3.6M dataset and real human movements data captured in real time. In our comparison against previous 3D pose estimation methods and commercial system such as Kinect 2017, we achieve the state-of-the-art accuracy.},
	author = {Xia, Shihong and Zhang, Zihao and Su, Le},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447556},
	keywords = {Three-dimensional displays;Kinematics;Pose estimation;Forestry;Solid modeling;Training;Databases;Computing methodologies-Computer graphics-Animation-Motion Capture;Computing methodologies-Computer graphics-Graphics systems and interfaces-Virtual reality},
	month = {March},
	pages = {431-438},
	title = {Cascaded 3D Full-Body Pose Regression from Single Depth Image at 100 FPS},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447556}}

@inproceedings{8447557,
	abstract = {When using current head-mounted displays (HMDs), users with optical aberrations need to wear the equipment on the top of their own glasses. As both the HMDs and the glasses require to be tightly attached to faces, wearing them together is very inconvenient and uncomfortable, and thus degrades user experiences heavily. In this paper, we propose a real-time image pre-correction technique to correct the aberrations purely by software. Users can take off their own glasses and enjoy the virtual reality (VR) experience through an ordinary HMD freely and comfortably. Furthermore, as our technique is not related to hardware, it is compatible with all the current commercial HMDs. Our technique is based on the observation that the refractive errors majorly cause the ideal retinal image to be convolved by certain kernels. So we pre-correct the image on the display according to the specific aberrations of a user, aiming to maximize the similarity between the convolved retinal image and the ideal image. To achieve real-time performance, we modify the energy function to have linear solutions and implement the optimization fully on GPU. The experiments and the user study indicate that without any changes on hardware, we generate better viewing experience of HMDs for users with optical aberrations.},
	author = {Xu, Feng and Li, Dayang},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447557},
	keywords = {Kernel;Real-time systems;Glass;Optical imaging;Resists;Deconvolution;visual aberration correction;image deconvolution;realtime rendering;image-based rendering: Computing methodologies-Computer graphics-Image manipulation-Image-based rendering;Human-centered computing-Human computer interaction-Interaction techniques},
	month = {March},
	pages = {246-250},
	title = {Software Based Visual Aberration Correction for HMDs},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447557}}

@inproceedings{8447558,
	abstract = {The use of novel displays and interaction resources to support immersive data visualization and improve analytical reasoning is a research trend in the information visualization community. In this work, we evaluate the use of an HMD-based environment for the exploration of multidimensional data, represented in 3D scatterplots as a result of dimensionality reduction (DR). We present a new modeling for this problem, accounting for the two factors whose interplay determine the impact on the overall task performance: the difference in errors introduced by performing dimensionality reduction to 2D or 3D, and the difference in human perception errors under different visualization conditions. This two-step framework offers a simple approach to estimate the benefits of using an immersive 3D setup for a particular dataset. Here, the DR errors for a series of roll call voting datasets when using two or three dimensions are evaluated through an empirical task-based approach. The perception error and overall task performance, on the other hand, are assessed through a comparative user study with 30 participants. Results indicated that perception errors were low and similar in all approaches, resulting in overall performance benefits in both desktop and HMD-based 3D techniques. The immersive condition, however, was found to require less effort to find information and less navigation, besides providing much larger subjective perception of accuracy and engagement.},
	author = {Wagner Filho, Jorge A. and Rey, Marina F. and Freitas, Carla M. D. S. and Nedel, Luciana},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447558},
	keywords = {Data visualization;Three-dimensional displays;Task analysis;Two dimensional displays;Dimensionality reduction;Principal component analysis;Navigation;Immersive visualization;abstract information visualization;dimensionality reduction;3D scatterplots.: H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
	month = {March},
	pages = {483-490},
	title = {Immersive Visualization of Abstract Information: An Evaluation on Dimensionally-Reduced Data Scatterplots},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447558}}

@inproceedings{8447559,
	abstract = {We present Yea Big, Yea High - a 3D user interface for surface selection in virtual environments. The interface extends previous selection interfaces that support exploratory visualization and 3D modeling. While these systems primarily focus on selecting single objects, Yea Big, Yea High allows users to select part of a surface mesh, a common task for data analysis, model editing, or annotation. The selection can be progressively refined by physically indicating a region of interest between a user's hands. We describe the design of the interface and key challenges we encountered. We present findings from a case study exploring design choices and use of the system.},
	author = {Jackson, Bret and Jelke, Brighten and Brown, Gabriel},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447559},
	keywords = {Three-dimensional displays;Shape;User interfaces;Solid modeling;Virtual environments;Surface reconstruction;Task analysis;Human-centered computing-Human computer interaction-Interaction techniques-Gestural input;Human-centered computing-Human computer interaction-Interaction paradigms-Virtual reality},
	month = {March},
	pages = {320-326},
	title = {Yea Big, Yea High: A 3D User Interface for Surface Selection by Progressive Refinement in Virtual Environments},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447559}}

@inproceedings{8447560,
	abstract = {We propose a new approach that utilizes semantic information to register 2D monocular video frames to the world using 3D georeferenced data, for augmented reality driving applications. The geo-registration process uses our predicted vehicle pose to generate a rendered depth map for each frame, allowing 3D graphics to be convincingly blended with the real world view. We also estimate absolute depth values for dynamic objects, up to 120 meters, based on the rendered depth map and update the rendered depth map to reflect scene changes over time. This process also creates opportunistic global heading measurements, which are fused with other sensors, to improve estimates of the 6 degrees-of- freedom global pose of the vehicle over state-of-the-art outdoor augmented reality systems [5]-, [19]. We evaluate the navigation accuracy and depth map quality of our system on a driving vehicle within various large-scale environments for producing realistic augmentations.},
	author = {Chiu, Han-Pang and Murali, Varun and Villamil, Ryan and Kessler, G. Drew and Samarasekera, Supun and Kumar, Rakesh},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447560},
	keywords = {Three-dimensional displays;Sensors;Augmented reality;Semantics;Navigation;Laser radar;Cameras;augmented reality;autonomous navigation;depth estimation;geo-registration;scene understanding},
	month = {March},
	pages = {423-430},
	title = {Augmented Reality Driving Using Semantic Geo-Registration},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447560}}

@inproceedings{8447561,
	abstract = {Work space simulations help trainees acquire skills necessary to perform their tasks efficiently without disrupting the workflow, forgetting important steps during a procedure, or the location of important information. This training can be conducted in Augmented and Virtual Reality (AR, VR) to enhance its effectiveness and speed. When the skills are transferred to the actual application, it is referred to as positive training transfer. However, thus far, it is unclear which training, AR or VR, achieves better results in terms of positive training transfer. We compare the effectiveness of AR and VR for spatial memory training in a control-room scenario, where users have to memorize the location of buttons and information displays in their surroundings. We conducted a within-subject study with 16 participants and evaluated the impact the training had on short-term and long-term memory. Results of our study show that VR outperformed AR when tested in the same medium after the training. In a memory transfer test conducted two days later AR outperformed VR. Our findings have implications on the design of future training scenarios and applications.},
	author = {Caluya, Nicko R. and Plopski, Alexander and Ty, Jayzon F. and Sandor, Christian and Taketomi, Takafumi and Kato, Hirokazu},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447561},
	keywords = {Training;Task analysis;Virtual reality;Resists;Layout;Legged locomotion;Media;H.5.1-Information Interfaces and Presentation: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2-Information Interfaces and Presentation: Multimedia Information Systems-Ergonomics;Evaluation/methodology;Theory and methods},
	month = {March},
	pages = {387-393},
	title = {Transferability of Spatial Maps: Augmented Versus Virtual Reality Training},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447561}}

@inproceedings{8447562,
	abstract = {We present a study of the relative effects of gradual versus instantaneous transition between one's own body and a virtual surrogate body, and between one's real-world environment and a virtual environment. The approach uses a stereo camera attached to an HMD to provide the illusions of virtual body ownership and spatial presence in VR. We conducted the study in a static environment which is similar to the traditional rubber hand experiment platform. Since our transition method is a blending scheme between real and virtual contexts, our study investigates the direct use of real-world information during the transition to increase the dominant visual illusion in a virtual space. We also investigate the use of a conceptual stage, called Limbo, which is a transition phase that evokes anticipation of the virtual world, providing a psychological link between the real and virtual before we enter a totally virtual space. Our study of the transition effect shows that the Limbo state has a significant influence in one's illusions of virtual body ownership (VBOI) and presence.},
	author = {Jung, Sungchul and Wisniewski, Pamela J. and Hughes, Charles E.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8447562},
	keywords = {Resists;Visualization;Cameras;Virtual reality;Legged locomotion;Rubber;Electronic mail;Transition;Virtual Body Ownership;Presence;Perception;User Study.: I.3.7 [Computer Graphics]: Three Dimensional Graphics and RealismVirtual Reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information SystemsArtificial;augmented;virtual realities H.5.1 [Information Interfaces and Presentation]: User InterfacesUser-centered design},
	month = {March},
	pages = {267-272},
	title = {In Limbo: The Effect of Gradual Visual Transition Between Real and Virtual on Virtual Body Ownership Illusion and Presence},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8447562}}

@inproceedings{8448283,
	abstract = {Recent breakthrough in consumer-level virtual reality (VR) devices brings an increasing demand of VR live content. As converting real life content into VR need complex computations, current techniques can not synthesize 360$\,^{\circ}$ 3D VR content with high performance, not to mention real time. We propose an end-to-end system that records a scene using a tripod panoramic rig and broadcasts 360$\,^{\circ}$ stereo panorama videos in real time. The system performs a panorama stitching technique which pre-compute 3 stitching seam candidates for dynamic seam switching in the live broadcasting. This technique achieves high frame rates (>30fps) with minimum foreground cutoff and temporal jittering artifacts. Stereo vision quality is also better preserved by a proposed weighting-based image alignment scheme. We demonstrate the effectiveness of our approach on a variety of videos delivering live events. And our system has been successfully used in broadcasting live shows to mobile phone users on a professional live broadcasting platform with about 390 million user visits per month.},
	author = {Xu, Feng and Zhao, Tianqi and Luo, Bicheng and Dai, Qionghai},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448283},
	keywords = {Stereo vision;Cameras;Switches;Videos;Real-time systems;Broadcasting;Electronic mail;VR live video;360o scene representation;video stitching;image-based rendering: Computing methodologies-Artificial intelligence-Computer vision-Image and video acquisition;Computing methodologies-Computer graphics-Image manipulation-Image-based rendering},
	month = {March},
	pages = {446-9},
	title = {Generating VR Live Videos with Tripod Panoramic Rig},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448283}}

@inproceedings{8448284,
	abstract = {We propose an efficient physics-based method for dexterous `real hand' - `virtual object' interaction in Virtual Reality environments. Our method is based on the Coulomb friction model, and we show how to efficiently implement it in a commodity VR engine for realtime performance. This model enables very convincing simulations of many types of actions such as pushing, pulling, grasping, or even dexterous manipulations such as spinning objects between fingers without restrictions on the objects' shapes or hand poses. Because it is an analytic model, we do not require any prerecorded data, in contrast to previous methods. For the evaluation of our method, we conduction a pilot study that shows that our method is perceived more realistic and natural, and allows for more diverse interactions. Further, we evaluate the computational complexity of our method to show real-time performance in VR environments.},
	author = {H{\"o}ll, Markus and Oberweger, Markus and Arth, Clemens and Lepetit, Vincent},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448284},
	keywords = {Friction;Computational modeling;Grasping;Solid modeling;Three-dimensional displays;Real-time systems;I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling-Physically-based Modeling;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Direct Manipulation},
	month = {March},
	pages = {175-182},
	title = {Efficient Physics-Based Implementation for Realistic Hand-Object Interaction in Virtual Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448284}}

@inproceedings{8448285,
	abstract = {We investigate the self-attribution of distorted pointing movements in immersive virtual reality. Participants had to complete a multidirectional pointing task in which the visual feedback of the tapping finger could be deviated in order to increase or decrease the motor size of a target relative to its visual appearance. This manipulation effectively makes the task easier or harder than the visual feedback suggests. Participants were asked whether the seen movement was equivalent to the movement they performed, and whether they have been successful in the task. We show that participants are often unaware of the movement manipulation, even when it requires higher pointing precision than suggested by the visual feedback. Moreover, subjects tend to self-attribute movements that have been modified to make the task easier more often than movements that have not been distorted. We discuss the implications and applications of our results.},
	author = {Debarba, Henrique G and Khoury, Jad-Nicolas and Perrin, Sami and Herbelin, Bruno and Boulic, Ronan},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448285},
	keywords = {Task analysis;Distortion;Visualization;Haptic interfaces;Indexes;Electronic mail;Virtual reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/methodology},
	month = {March},
	pages = {341-346},
	title = {Perception of Redirected Pointing Precision in Immersive Virtual Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448285}}

@inproceedings{8448286,
	abstract = {We present a VR field trip framework, Kvasir-VR, and assess its two approaches to teacher-guided content. In one approach, networked student groups are guided by a live teacher captured as live-streamed depth camera imagery. The second approach is a standalone (non-networked) version allowing students to individually experience the field trip based on depth camera recordings of the same teacher. Both approaches were tested at two high schools using a VR environment that teaches students about solar energy production via tours of a solar plant. We show that our live networked approach can produce promising test score gains and very high ratings of co-presence, affective attraction, overall opinion, etc. Results show a benefit of live networked VR, as the standalone approach had lower performance in terms of gains and most ratings, although its ratings were still positive. We further consider possible differences of school environment (dedicated vs. integrated classroom), and we conclude with tradeoffs and implications to benefit future design of educational VR.},
	author = {Borst, Christoph W. and Lipari, Nicholas G. and Woodworth, Jason W.},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448286},
	keywords = {Conferences;Virtual reality;Three-dimensional displays;User interfaces;Collaborative VR;education;avatara;Kinect;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;K.3.0 [Computers and Education]: General},
	month = {March},
	pages = {467-474},
	title = {Teacher-Guided Educational VR: Assessment of Live and Prerecorded Teachers Guiding Virtual Field Trips},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448286}}

@inproceedings{8448287,
	abstract = {When a moving object collides with an object at rest, people immediately perceive a causal event: i.e., the first object has launched the second object forwards. However, when the second object's motion is delayed, or is accompanied by a collision sound, causal impressions attenuate and strengthen. Despite a rich literature on causal perception, researchers have exclusively utilized 2D visual displays to examine the launching effect. It remains unclear whether people are equally sensitive to the spatiotemporal properties of observed collisions in the real world. The present study first examined whether previous findings in causal perception with audiovisual inputs can be extended to immersive 3D virtual environments. We then investigated whether perceived causality is influenced by variations in the spatial position of an auditory collision indicator. We found that people are able to localize sound positions based on auditory inputs in VR environments, and spatial discrepancy between the estimated position of the collision sound and the visually observed impact location attenuates perceived causality.},
	author = {Wang, Duotun and Kubrlcht, James and Zhu, Yixin and Lianq, Wei and Zhu, Song-Chun and Jiang, Chenfanfu and Lu, Hongjing},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448287},
	keywords = {Three-dimensional displays;Visualization;Solid modeling;Delays;Spatiotemporal phenomena;Virtual environments;Causal perception;virtual reality;intuitive physics;visual capture;launching},
	month = {March},
	pages = {259-266},
	title = {Spatially Perturbed Collision Sounds Attenuate Perceived Causality in 3D Launching Events},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448287}}

@inproceedings{8448288,
	abstract = {Redirected walking techniques have the potential to provide natural locomotion while users experience large virtual environments. However, when using redirected walking in small physical workspaces, disruptive overt resets are often required. We describe the design of an educational virtual reality experience in which users physically walk through virtual tunnels representative of the World War I battle of Vauquois. Walking in only a 15- by 5-foot tracked space, users are redirected through subtle, narrative-driven resets to walk through a tunnel nearly 50 feet in length. This work contributes approaches and lessons that can be used to provide a seamless and natural virtual reality walking experience in highly constrained physical spaces.},
	author = {Yu, Run and Duer, Zachary and Ogle, Todd and Bowman, Doug A. and Tucker, Thomas and Hicks, David and Choi, Dongsoo and Bush, Zach and Ngo, Huy and Nguyen, Phat and Liu, Xindi},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448288},
	keywords = {Legged locomotion;Virtual environments;Three-dimensional displays;Visualization;Foot;History;Redirected walking;narrative;educational VR.: H.5.1 [Information Interfaces and Presentation (e.g., HCI)]: Multimedia Information Systems-Artificial;Augmented and Virtual Realities},
	month = {March},
	pages = {313-319},
	title = {Experiencing an Invisible World War I Battlefield Through Narrative-Driven Redirected Walking in Virtual Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448288}}

@inproceedings{8448289,
	abstract = {The scarcity of established input methods for augmented reality (AR) head-mounted displays (HMD) motivates us to investigate the performance envelopes of two easily realisable solutions: indirect cursor control via a smartwatch and direct control by in-air touch. Indirect cursor control via a smartwatch has not been previously investigated for AR HMDs. We evaluate these two techniques for carrying out three fundamental user interface actions: target acquisition, goal crossing, and circular steering. We find that in-air is faster than smartwatch (p <; 0.001) for target acquisition and circular steering. We observe, however, that in-air selection can lead to discomfort after extended use and suggest that smartwatch control offers a complementary alternative.},
	author = {Wolf, Dennis and Dudley, John J. and Kristensson, Per Ola},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448289},
	keywords = {Task analysis;User interfaces;Augmented reality;Mathematical model;Navigation;Trajectory;Resists;Fitts' law;steering law;goal crossing;in-air selection;smartwatch;indirect cursor;AR;augmented reality: Human-centered computing-Human computer interaction (HCI)-Interaction paradigms-Mixed / augmented reality},
	month = {March},
	pages = {347-354},
	title = {Performance Envelopes of in-Air Direct and Smartwatch Indirect Control for Head-Mounted Augmented Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448289}}

@inproceedings{8448290,
	abstract = {The recent popularity of consumer-grade virtual reality devices, such as Oculus Rift, HTC Vive, and Fove virtual reality headset, has enabled household users to experience highly immersive virtual environments. We take advantage of the commercial availability of these devices to provide a novel virtual reality-based driving training approach designed to help individuals improve their driving habits in common scenarios. Our approach first identifies improper driving habits of a user when he drives in a virtual city. Then it synthesizes a pertinent training program to help improve the users driving skills based on the discovered improper habits of the user. To apply our approach, a user first goes through a pre-evaluation test from which his driving habits are analyzed. The analysis results are used to drive optimization for synthesizing a training program. This training program is a personalized route which includes different traffic events. When the user drives along this route via a driving controller and an eye-tracking virtual reality headset, the traffic events he encounters will help him to improve his driving habits. To validate the effectiveness of our approach, we conducted a user study to compare our virtual reality-based driving training with other training methods. The user study results show that the participants trained by our approach perform better on average than those trained by other methods in terms of evaluation score and response time and their improvement is more persistent.},
	author = {Lang, Yining and Wei, Liang and Xu, Fang and Zhao, Yibiao and Yu, Lap-Fai},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448290},
	keywords = {Training;Solid modeling;Roads;Vehicles;Virtual environments;Safety;Virtual Reality-Modeling and Simulation-Driver Training Simulator},
	month = {March},
	pages = {297-304},
	title = {Synthesizing Personalized Training Programs for Improving Driving Habits via Virtual Reality},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448290}}

@inproceedings{8448291,
	abstract = {In this paper, we present a novel method for fast generation of furniture arrangements in interior scenes. Our method exploits the benefits of optimization-based approaches for global aesthetic rules and the advantages of procedural approaches for local arrangement of small objects. We generate the furniture arrangements for a given room in two steps: We first optimize the selection and arrangement of furniture objects in a room with respect to aesthetic and functional rules. The infinite trans-dimensional space of furniture layouts is rapidly explored by greedy cost minimization. In the second step, the procedural methods are locally applied in a stochastic fashion to generate important scene details. We demonstrate that our method achieves comparable results to a recent method for automatic interior design in terms of user preferences and that local procedural design enhances the result of optimization-based interior design. Additionally, our method is one order of magnitude faster than the compared method. Finally, the execution times of up to one second show that our method is suitable for generating large-scale indoor virtual environments during runtime.},
	author = {K{\'a}n, Peter and Kaufmann, Hannes},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448291},
	keywords = {Layout;Cost function;Minimization;Three-dimensional displays;Space exploration;Stochastic processes;Computing methodologies-Graphics systems and interfaces-Virtual reality},
	month = {March},
	pages = {491-498},
	title = {Automatic Furniture Arrangement Using Greedy Cost Minimization},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448291}}

@inproceedings{8448292,
	abstract = {We present WoaH, a virtual reality work-at-height simulator aimed at (i) testing whether future workers are able to manage their stress when high up and thus easily detect susceptibility to vertigo, and (ii) training in a typical work-at-height engineering operation. The simulator is composed of a real ladder synchronized in position with a virtual one placed 11 meters above the ground in a virtual environment. Visualization is done through a head-mounted display (HMD). We conducted a first user study evaluating our simulator in terms of cybersickness, perceived realism and anxiety, through both subjective (questionnaires) and objective (electrodermal activity) measurements, and testing whether vibratory cues could enhance the level of anxiety felt. Results indicate that WoaH generates anxiety as expected and is perceived as realistic. Adding vibrations had significant impact on the perceived realism but not on the electro-dermal activity. These first results bring insights to future developments for a deployment in companies dealing with work at height.},
	author = {Di Loreto, C{\'e}dric and Chardonnet, Jean-R{\'e}my and Ryard, Julien and Rousseau, Alain},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448292},
	keywords = {Visualization;Virtual environments;Solid modeling;Navigation;Resists;Companies;Human-centered computingHuman computer interaction (HCI)Interaction paradigmsVirtual reality;Software and its engineeringSoftware organization and propertiesVirtual worlds softwareVirtual worlds training simulations},
	month = {March},
	pages = {281-288},
	title = {WoaH: A Virtual Reality Work-at-Height Simulator},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448292}}

@inproceedings{8448293,
	abstract = {In this paper, we explore the influence of sharing a virtual environment with another user on the sense of embodiment in virtual reality. For this aim, we conducted an experiment where users were immersed in a virtual environment while being embodied in an anthropomorphic virtual representation of themselves. To evaluate the influence of the presence of another user, two situations were studied: either users were immersed alone, or in the company of another user. During the experiment, participants performed a virtual version of the well-known whac-a-mole game, therefore interacting with the virtual environment, while sitting at a virtual table. Our results show that users were significantly more ``efficient'' (i.e., faster reaction times), and accordingly more engaged, in performing the task when sharing the virtual environment, in particular for the more competitive tasks. Also, users experienced comparable levels of embodiment both when immersed alone or with another user. These results are supported by subjective questionnaires but also through behavioural responses, e.g. users reacting to the introduction of a threat towards their virtual body. Taken together, our results show that competition and shared experiences involving an avatar do not influence the sense of embodiment, but can increase user engagement. Such insights can be used by designers of virtual environments and virtual reality applications to develop more engaging applications.},
	author = {Fribourg, Rebecca and Argelaguet, Ferran and Hoyet, Ludovic and L{\'e}cuyer, Anatole},
	booktitle = {2018 IEEE Conference on Virtual Reality and 3D User Interfaces (VR)},
	date-added = {2024-03-18 02:30:06 -0400},
	date-modified = {2024-03-18 02:30:06 -0400},
	doi = {10.1109/VR.2018.8448293},
	keywords = {Avatars;Task analysis;Virtual environments;Visualization;Electronic mail;Games;Human-centered computingHuman computer interaction (HCI)HCI design and evaluation methodsUser studies;Human-centered computingHuman computer interaction (HCI)Interaction paradigmsVirtual reality},
	month = {March},
	pages = {273-280},
	title = {Studying the Sense of Embodiment in VR Shared Experiences},
	year = {2018},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2018.8448293}}

@inproceedings{7892225,
	abstract = {The emotional response a person has to a living space is predominantly affected by light, color and texture as space-making elements. In order to verify whether this phenomenon could be replicated in a simulated environment, we conducted a user study in a six-sided projected immersive display that utilized equivalent design attributes of brightness, color and texture in order to assess to which extent the emotional response in a simulated environment is affected by the same parameters affecting real environments. Since emotional response depends upon the context, we evaluated the emotional responses of two groups of users: inactive (passive) and active (performing a typical daily activity). The results from the perceptual study generated data from which design principles for a virtual living space are articulated. Such a space, as an alternative to expensive built dwellings, could potentially support new, minimalist lifestyles of occupants, defined as the neo-nomads, aligned with their work experience in the digital domain through the generation of emotional experiences of spaces. Data from the experiments confirmed the hypothesis that perceivable emotional aspects of real-world spaces could be successfully generated through simulation of design attributes in the virtual space. The subjective response to the virtual space was consistent with corresponding responses from real-world color and brightness emotional perception. Our data could serve the virtual reality (VR) community in its attempt to conceive of further applications of virtual spaces for well-defined activities.},
	author = {Naz, Asma and Kopper, Regis and McMahan, Ryan P. and Nadin, Mihai},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892225},
	issn = {2375-5334},
	keywords = {Image color analysis;Color;Brightness;Virtual reality;Context;Psychology;Extraterrestrial measurements;Architectural design;affective space;neo-nomads;aesthetics},
	month = {March},
	pages = {3-11},
	title = {Emotional qualities of VR space},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892225}}

@inproceedings{7892226,
	abstract = {The increasing availability of intensely immersive virtual, augmented and mixed reality experiences using head-mounted displays (HMD) has prompted deliberations about the ethical implications of using such technology to resolve technical issues and explore the complex cognitive, behavioral and social dynamics of human `virtuality'. However, little is known about the impact such immersive experiences will have on children (aged 0-18 years). This paper outlines perspectives on child development to present conceptual and practical frameworks for conducting ethical research with children using immersive HMD technologies. The paper addresses not only procedural ethics (gaining institutional approval) but also ethics-in-practice (on-going ethical decision-making).},
	author = {Southgate, Erica and Smith, Shamus P. and Scevak, Jill},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892226},
	issn = {2375-5334},
	keywords = {Ethics;Australia;Decision making;Protocols;Augmented reality;Resists;Ethics;virtual reality;augmented reality;mixed reality;children;adolescents;evaluation;child development},
	month = {March},
	pages = {12-18},
	title = {Asking ethical questions in research using immersive virtual and augmented reality technologies with children and youth},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892226}}

@inproceedings{7892227,
	abstract = {Traditionally in virtual reality systems, head tracking is used in head-mounted displays (HMDs) to allow users to control viewing using 360-degree head and body rotations. Our research explores interaction considerations that enable semi-natural methods of view control that will work for seated use of virtual reality with HMDs when physically turning all the way around is not ideal, such as when sitting on a couch or at a desk. We investigate the use of amplified head rotations so physically turning in a comfortable range can allow viewing of a 360-degree virtual range. Additionally, to avoid situations where the user's neck is turned in an uncomfortable position for an extended period, we also use redirection during virtual movement to gradually realign the user's head position back to the neutral, straight-ahead position. We ran a controlled experiment to evaluate guided head rotation and amplified head rotation without realignment during movement, and we compared both to traditional one-to-one head-tracked viewing as a baseline for reference. After a navigation task, overall errors on spatial orientation tasks were relatively low with all techniques, but orientation effects, sickness, and preferences varied depending on participants' 3D gaming habits. Using the guided rotation technique, participants who played 3D games performed better, reported higher preference scores, and demonstrated significantly lower sickness results compared to non-gamers.},
	author = {Sargunam, Shyam Prathish and Moghadam, Kasra Rahimi and Suhail, Mohamed and Ragan, Eric D.},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892227},
	issn = {2375-5334},
	keywords = {Legged locomotion;Navigation;Resists;Turning;Three-dimensional displays;Virtual reality;Tracking;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems --- Artificial, augmented, and virtual realities},
	month = {March},
	pages = {19-28},
	title = {Guided head rotation and amplified head rotation: Evaluating semi-natural travel and viewing techniques in virtual reality},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892227}}

@inproceedings{7892228,
	abstract = {For many virtual reality applications, a pre-calculated fly-through path is the de facto standard navigation method. Such a path is convenient for users and ensures coverage of critical areas throughout the scene. Traditional applications use constant camera speed, allow for fully user-controlled manual speed adjustment, or use automatic speed adjustment based on heuristics from the scene. We introduce two novel methods for constrained path navigation and exploration in virtual environments which rely on the natural orientation of the user's head during scene exploration. Utilizing head tracking to obtain the user's area of focus, we perform automatic camera speed adjustment to allow for natural off-axis scene examination. We expand this to include automatic camera navigation along the pre-computed path, abrogating the need for any navigational inputs from the user. Our techniques are applicable for any scene with a pre-computed navigation path, including medical applications such as virtual colonoscopy, coronary fly-through, or virtual angioscopy, and graph navigation. We compare the traditional methods (constant speed and manual speed adjustment) and our two methods (automatic speed adjustment and automatic speed/direction control) to determine the effect of speed adjustment on system usability, mental load, performance, and user accuracy. Through this evaluation we observe the effect of automatic speed adjustment compared to traditional methods. We observed no negative impact from automatic navigation, and the users performed as well as with the manual navigation.},
	author = {Mirhosseini, Seyedkoosha and Gutenko, Ievgeniia and Ojal, Sushant and Marino, Joseph and Kaufman, Arie E.},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892228},
	issn = {2375-5334},
	keywords = {Navigation;Cameras;Manuals;Virtual reality;Head;Resists;Acceleration;3D Navigation;tracking;visualization;medicine;immersion;usability;cybersickness;user studies;HMD},
	month = {March},
	pages = {29-36},
	title = {Automatic speed and direction control along constrained navigation paths},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892228}}

@inproceedings{7892229,
	abstract = {Recent breakthroughs in consumer level virtual reality (VR) headsets are creating a growing user-base in demand for immersive, full 3D VR experiences. While monoscopic 360-videos are perhaps the most prevalent type of content for VR headsets, they lack 3D information and thus cannot be viewed with full 6 degree-of-freedom (DOF). We present an approach that addresses this limitation via a novel warping algorithm that can synthesize new views both with rotational and translational motion of the viewpoint. This enables the ability to perform VR playback of input monoscopic 360-videos files in full stereo with full 6-DOF of head motion. Our method synthesizes novel views for each eye in accordance with the 6-DOF motion of the headset. Our solution tailors standard structure-from-motion and dense reconstruction algorithms to work accurately for 360-videos and is optimized for GPUs to achieve VR frame rates (>120 fps). We demonstrate the effectiveness our approach on a variety of videos with interesting content.},
	author = {Huang, Jingwei and Chen, Zhili and Ceylan, Duygu and Jin, Hailin},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892229},
	issn = {2375-5334},
	keywords = {Cameras;Three-dimensional displays;Videos;Image reconstruction;Headphones;Geometry;Tracking;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual reality;I.2.10 [Artificial Intelligence]: Vision and Scene Understanding --- Video analysis;I.4.8 [Image Processing and Computer Vision]: Scene Analysis --- Motion},
	month = {March},
	pages = {37-44},
	title = {6-DOF VR videos with a single 360-camera},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892229}}

@inproceedings{7892230,
	abstract = {The proliferation of head-mounted displays (HMD) in the market means that cinematic virtual reality (CVR) is an increasingly popular format. We explore several metrics that may indicate advantages and disadvantages of CVR compared to traditional viewing formats such as TV. We explored the consumption of panoramic videos in three different display systems: a HMD, a SurroundVideo+ (SV+), and a standard 16:9 TV. The SV+ display features a TV with projected peripheral content. A between-groups experiment of 63 participants was conducted, in which participants watched panoramic videos in one of these three display conditions. Aspects examined in the experiment were spatial awareness, narrative engagement, enjoyment, memory, fear, attention, and a viewer's concern about missing something. Our results indicated that the HMD offered a significant benefit in terms of enjoyment and spatial awareness, and our SV+ display offered a significant improvement in enjoyment over traditional TV. We were unable to confirm the work of a previous study that showed incidental memory may be lower in a HMD over a TV. Drawing attention and a viewer's concern about missing something were also not significantly different between display conditions. It is clear that passive media viewing consists of a complex interplay of factors, such as the media itself, the characteristics of the display, as well as human aspects including perception and attention. While passive media viewing presents many challenges for evaluation, identifying a number of broadly applicable metrics will aid our understanding of these experiences, and allow the creation of better, more engaging CVR content and displays.},
	author = {MacQuarrie, Andrew and Steed, Anthony},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892230},
	issn = {2375-5334},
	keywords = {TV;Streaming media;Virtual reality;Resists;Atmospheric measurements;Particle measurements;Extraterrestrial measurements;Cinematic virtual reality;panoramic video;user study},
	month = {March},
	pages = {45-54},
	title = {Cinematic virtual reality: Evaluating the effect of display type on the viewing experience for panoramic video},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892230}}

@inproceedings{7892231,
	abstract = {In this paper we present an immersive virtual reality user study aimed at investigating how customers perceive and if they would purchase non-standard (i.e. misshaped) fruits and vegetables (FaVs) in supermarkets and hypermarkets. Indeed, food waste is a major issue for the retail sector and a recent trend is to reduce it by selling non-standard goods. An important question for retailers relates to the FaVs' ``level of abnormality'' that consumers would agree to buy. However, this question cannot be tackled using ``classical'' marketing techniques that perform user studies within real shops since fresh produce such as FaVs tend to rot rapidly preventing studies to be repeatable or to be run for a long time. In order to overcome those limitations, we created a virtual grocery store with a fresh FaVs section where 142 participants were immersed using an Oculus Rift DK2 HMD. Participants were presented either ``normal'', ``slightly misshaped'', ``misshaped'' or ``severely misshaped'' FaVs. Results show that participants tend to purchase a similar number of FaVs whatever their deformity. Nevertheless participants' perceptions of the quality of the FaV depend on the level of abnormality.},
	author = {Verhulst, Adrien and Normand, Jean-Marie and Lombart, Cindy and Moreau, Guillaume},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892231},
	issn = {2375-5334},
	keywords = {Three-dimensional displays;Virtual reality;Color;Clothing;Solid modeling;Visualization;Market research;H.5.2 [Information Interfaces and Presentation]: User Interfaces --- Evaluation/Methodology;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual Reality},
	month = {March},
	pages = {55-63},
	title = {A study on the use of an immersive virtual reality store to investigate consumer perceptions and purchase behavior toward non-standard fruits and vegetables},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892231}}

@inproceedings{7892232,
	abstract = {This work presents an evaluation study in which the effects of a penalty-based and a constraint-based haptic rendering algorithm on the user performance and perception are analyzed. A total of N = 24 participants performed in a within-design study three variations of peg-in-hole tasks in a virtual environment after trials in an identically replicated real scenario as a reference. In addition to the two mentioned haptic rendering paradigms, two haptic devices were used, the HUG and a Sigma.7, and the force stiffness was also varied with maximum and half values possible for each device. Both objective (time and trajectory, collision performance, and muscular effort) and subjective ratings (contact perception, ergonomy, and workload) were recorded and statistically analyzed. The results show that the constraint-based haptic rendering algorithm with a lower stiffness than the maximum possible yields the most realistic contact perception, while keeping the visual inter-penetration between the objects roughly at around 15% of that caused by penalty-based algorithm (i.e., non perceptible in many cases). This result is even more evident with the HUG, the haptic device with the highest force display capabilities, although user ratings point to the Sigma.7 as the device with highest usability and lowest workload indicators. Altogether, the paper provides qualitative and quantitative guidelines for mapping properties of haptic algorithms and devices to user performance and perception.},
	author = {Sagardia, Mikel and Hulin, Thomas},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892232},
	issn = {2375-5334},
	keywords = {Rendering (computer graphics);Force;Performance evaluation;Visualization;Trajectory;Force feedback;H.1.2 [Models and Principles]: User/Machine Systems --- Human factors;Human information processing;H.5.2 [Information Systems and Presentation]: User Interfaces --- Haptic I/O;I.3.4. [Computer Graphics]: Graphics Utilities --- Virtual device interfaces},
	month = {March},
	pages = {64-73},
	title = {Evaluation of a penalty and a constraint-based haptic rendering algorithm with different haptic interfaces and stiffness values},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892232}}

@inproceedings{7892233,
	abstract = {We present the design and development of a fully immersive virtual reality (VR) system that can provide prop-based haptic feedback in an infinite virtual environment. It is conceived as a research tool for studying topics related to haptics in VR and based on off-the-shelf components. A robotic arm moves physical props, dynamically matching pose and location of an object in the virtual world. When the user reaches for the virtual object, his or her hands also encounter it in the real physical space. The interaction is not limited to specific body parts and does not rely on an external structure like an exoskeleton. In combination with a locomotion platform for close-to-natural walking, this allows unrestricted haptic interaction in a natural way in virtual environments of unlimited size. We describe the concept, the hardware and software architecture in detail. We establish safety design guidelines for human-robot interaction in VR. Our technical evaluation shows good response times and accuracy. We report on a user study conducted with 34 participants indicating promising results, and discuss the potential of our system.},
	author = {Vonach, Emanuel and Gatterer, Clemens and Kaufmann, Hannes},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892233},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Thumb;Manipulators;Virtual environments;Prop-based virtual reality;encounter-type haptics;passive haptic feedback;fully immersive virtual reality},
	month = {March},
	pages = {74-83},
	title = {VRRobot: Robot actuated props in an infinite virtual environment},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892233}}

@inproceedings{7892234,
	abstract = {Producing sensations of motion in driving simulators often requires using cumbersome and expensive motion platforms. In this article we present a novel and alternative approach for producing self-motion sensations in driving simulations by relying on haptic-feedback. The method consists in applying a force-feedback proportional to the acceleration of the virtual vehicle directly to the hands of the driver, by means of a haptic device attached to the manipulated controller (or a steering wheel). We designed a proof-of-concept based on a standard gamepad physically attached at the extremity of a standard 3DOF haptic display. Haptic effects were designed to match notably the acceleration/braking (longitudinal forces) and left/right turns (lateral forces) of the virtual vehicle. A preliminary study conducted with 23 participants, engaged in gamepad-based active VR navigations in a straight line, showed that haptic motion effects globally improved the involvement and realism of motion sensation for participants with prior experience with haptic devices. Taken together, our results suggest that our approach could be further tested and used in driving simulators in entertainment and/or professional contexts.},
	author = {Bouyer, Guillaume and Chellali, Amine and L{\'e}cuyer, Anatole},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892234},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Acceleration;Three-dimensional displays;Solid modeling;Context;Wheels;Navigation;Driving Simulation;Self-motion;Haptic;Force-feedback},
	month = {March},
	pages = {84-90},
	title = {Inducing self-motion sensations in driving simulators using force-feedback and haptic motion},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892234}}

@inproceedings{7892235,
	abstract = {As the focus of virtual reality technology is shifting from singleperson experiences to multi-user interactions, it becomes increasingly important to accommodate multiple co-located users within a shared real-world space. For locomotion and navigation, the introduction of multiple users moving both virtually and physically creates additional challenges related to potential user-on-user collisions. In this work, we focus on defining the extent of these challenges, in order to apply redirected walking to two users immersed in virtual reality experiences within a shared physical tracked space. Using a computer simulation framework, we explore the costs and benefits of splitting available physical space between users versus attempting to algorithmically prevent user-to-user collisions. We also explore fundamental components of collision prevention such as steering the users away from each other, forced stopping, and user re-orientation. Each component was analyzed for the number of potential disruptions to the flow of the virtual experience. We also develop a novel collision prevention algorithm that reduces overall interruptions by 17.6% and collision prevention events by 58.3%. Our results show that sharing space using our collision prevention method is superior to subdividing the tracked space.},
	author = {Azmandian, Mahdi and Grechkin, Timofey and Rosenberg, Evan Suma},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892235},
	issn = {2375-5334},
	keywords = {Legged locomotion;Tracking;Collision avoidance;Virtual environments;Trajectory;Navigation;Virtual Reality;Locomotion;Redirected Walking},
	month = {March},
	pages = {91-98},
	title = {An evaluation of strategies for two-user redirected walking in shared physical spaces},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892235}}

@inproceedings{7892236,
	abstract = {We developed ``Finger Glove for Augmented Reality'' (FinGAR), which combines electrical and mechanical stimulation to selectively stimulate skin sensory mechanoreceptors and provide tactile feedback of virtual objects. A DC motor provides high-frequency vibration and shear deformation to the whole finger, and an array of electrodes provide pressure and low-frequency vibration with high spatial resolution. FinGAR devices are attached to the thumb, index finger and middle finger. It is lightweight, simple in mechanism, easy to wear, and does not disturb the natural movements of the hand. All of these attributes are necessary for a general-purpose virtual reality system. User study was conducted to evaluate its ability to reproduce sensations of four tactile dimensions: macro roughness, friction, fine roughness and hardness. Result indicated that skin deformation and cathodic stimulation affect macro roughness and hardness, whereas high-frequency vibration and anodic stimulation affect friction and fine roughness.},
	author = {Yem, Vibol and Kajimoto, Hiroyuki},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892236},
	issn = {2375-5334},
	keywords = {Vibrations;Thumb;Skin;Electrodes;DC motors;Electrical stimulation;FinGAR;mechanical stimulation;electrical stimulation;virtual touch},
	month = {March},
	pages = {99-104},
	title = {Wearable tactile device using mechanical and electrical stimulation for fingertip interaction with virtual world},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892236}}

@inproceedings{7892237,
	abstract = {We investigate the effect of vibrotactile feedback delivered to one's feet in an immersive virtual environment (IVE). In our study, participants observed a virtual environment where a virtual human (VH) walked toward the participants and paced back and forth within their social space. We compared three conditions as follows: participants in the ``Sound'' condition heard the footsteps of the VH; participants in the ``Vibration'' condition experienced the vibration of the footsteps along with the sounds; while participants in the ``Mute'' condition were not exposed to sound nor vibrotactile feedback. We found that the participants in the ``Vibration'' condition felt a higher social presence with the VH compared to those who did not feel the vibration. The participants in the ``Vibration'' condition also exhibited greater avoidance behavior while facing the VH and when the VH invaded their personal space.},
	author = {Lee, Myungho and Bruder, Gerd and Welch, Gregory F.},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892237},
	issn = {2375-5334},
	keywords = {Vibrations;Virtual environments;Rubber;Foot;Transducers;Legged locomotion;Back;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Artificial, Augmented, and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences --- Psychology},
	month = {March},
	pages = {105-111},
	title = {Exploring the effect of vibrotactile feedback through the floor on social presence in an immersive virtual environment},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892237}}

@inproceedings{7892238,
	abstract = {We propose an acoustic-VR system that converts acoustic signals of human language (Chinese) to realistic 3D tongue animation sequences in real time. It is known that directly capturing the 3D geometry of the tongue at a frame rate that matches the tongue's swift movement during the language production is challenging. This difficulty is handled by utilizing the electromagnetic articulography (EMA) sensor as the intermediate medium linking the acoustic data to the simulated virtual reality. We leverage Deep Neural Networks to train a model that maps the input acoustic signals to the positional information of pre-defined EMA sensors based on 1,108 utterances. Afterwards, we develop a novel reduced physics-based dynamics model for simulating the tongue's motion. Unlike the existing methods, our deformable model is nonlinear, volume-preserving, and accommodates collision between the tongue and the oral cavity (mostly with the jaw). The tongue's deformation could be highly localized which imposes extra difficulties for existing spectral model reduction methods. Alternatively, we adopt a spatial reduction method that allows an expressive subspace representation of the tongue's deformation. We systematically evaluate the simulated tongue shapes with real-world shapes acquired by MRI/CT. Our experiment demonstrates that the proposed system is able to deliver a realistic visual tongue animation corresponding to a user's speech signal.},
	author = {Luo, Ran and Fang, Qiang and Wei, Jianguo and Lu, Wenhuan and Xu, Weiwei and Yang, Yin},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892238},
	issn = {2375-5334},
	keywords = {Tongue;Speech;Magnetic resonance imaging;Hidden Markov models;Solid modeling;Real-time systems;Three-dimensional displays;H.5.1 [Information Systems]: Multimedia Information Systems --- Artificial, augmented, and virtual realities},
	month = {March},
	pages = {112-121},
	title = {Acoustic VR in the mouth: A real-time speech-driven visual tongue system},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892238}}

@inproceedings{7892239,
	abstract = {An important component of the modeling of sound propagation for virtual reality (VR) is the spatialization of the room impulse response (RIR) for directional listeners. This involves convolution of the listener's head-related transfer function (HRTF) with the RIR to generate a spatial room impulse response (SRIR) which can be used to auralize the sound entering the listener's ear canals. Previous approaches tend to evaluate the HRTF for each sound propagation path, though this is too slow for interactive VR latency requirements. We present a new technique for computation of the SRIR that performs the convolution with the HRTF in the spherical harmonic (SH) domain for RIR partitions of a fixed length. The main contribution is a novel perceptually-driven metric that adaptively determines the lowest SH order required for each partition to result in no perceptible error in the SRIR. By using lower SH order for some partitions, our technique saves a significant amount of computation and is almost an order of magnitude faster than the previous approach. We compared the subjective impact of this new method to the previous one and observe a strong scene-dependent preference for our technique. As a result, our method is the first that can compute high-quality spatial sound for the entire impulse response fast enough to meet the audio latency requirements of interactive virtual reality applications.},
	author = {Schissler, Carl and Stirling, Peter and Mehra, Ravish},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892239},
	issn = {2375-5334},
	keywords = {Harmonic analysis;Ear;Rendering (computer graphics);Computational modeling;Virtual reality;Measurement;Solid modeling;Spatial audio;HRTF;sound propagation;spherical harmonics},
	month = {March},
	pages = {122-130},
	title = {Efficient construction of the spatial room impulse response},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892239}}

@inproceedings{7892240,
	abstract = {We present a system for rapid acquisition of bespoke, animatable, full-body avatars including face texture and shape. A blendshape rig with a skeleton is used as a template for customization. Identity blendshapes are used to customize the body and face shape at the fitting stage, while animation blendshapes allow the face to be animated. The subject assumes a T-pose and a single snapshot is captured using a stereo RGB plus depth sensor rig. Our system automatically aligns a photo texture and fits the 3D shape of the face. The body shape is stylized according to body dimensions estimated from segmented depth. The face identity blendweights are optimised according to image-based facial landmarks, while a custom texture map for the face is generated by warping the input images to a reference texture according to the facial landmarks. The total capture and processing time is under 10 seconds and the output is a light-weight, game-engine-ready avatar which is recognizable as the subject. We demonstrate our system in a VR environment in which each user sees the other users' animated avatars through a VR headset with real-time audio-based facial animation and live body motion tracking, affording an enhanced level of presence and social engagement compared to generic avatars.},
	author = {Malleson, Charles and Kosek, Maggie and Klaudiny, Martin and Huerta, Ivan and Bazin, Jean-Charles and Sorkine-Hornung, Alexander and Mine, Mark and Mitchell, Kenny},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892240},
	issn = {2375-5334},
	keywords = {Face;Avatars;Cameras;Three-dimensional displays;Shape;Solid modeling;Animation;Avatars;Capture;Virtual Reality},
	month = {March},
	pages = {131-140},
	title = {Rapid one-shot acquisition of dynamic VR avatars},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892240}}

@inproceedings{7892241,
	abstract = {The use of first-person self-avatars in immersive virtual environments (VEs) has grown over recent years. It is unknown, however, how visual feedback from a self-avatar influences a user's online actions and subsequent calibration of actions within an immersive VE. The current paper uses a prism throwing adaptation paradigm to test the role of a self-avatar arm or full body on action calibration in a VE. Participants' throwing accuracy to a target on the ground was measured first in a normal viewing environment, then with the visual field rotated clockwise about their vertical axis by 17$\,^{\circ}$ (prism simulation), and then again in the normal viewing environment with the prism distortion removed. Participants experienced either no-avatar, a first-person avatar arm and hand, or a first-person full body avatar during the entire experimental session, in a between-subjects manipulation. Results showed similar throwing error and adaptation during the prism exposure for all conditions, but a reduced aftereffect (displacement with respect to the target in the opposite direction of the prism-exposure) when the avatar arm or full body was present. The results are discussed in the context of how an avatar can provide a visual frame of reference to aid in action calibration.},
	author = {Bodenheimer, Bobby and Creem-Regehr, Sarah and Stefanucci, Jeanine and Shemetova, Elena and Thompson, William B.},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892241},
	issn = {2375-5334},
	keywords = {Avatars;Visualization;Calibration;Legged locomotion;Tracking;Virtual environments;Head;Virtual reality;prism adaptation;self-avatar},
	month = {March},
	pages = {141-147},
	title = {Prism aftereffects for throwing with a self-avatar in an immersive virtual environment},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892241}}

@inproceedings{7892242,
	abstract = {In the past few years, advances have been made on how mixed reality humans (MRHs) can be used for interpersonal communication skills training for medical teams; however, little research has looked at how MRHs can influence communication skills during training. One way to influence communication skills is to leverage MRHs as models of communication behavior. We created a mixed reality medical team training exercise designed to impact communication behaviors that are critical for patient safety. We recruited anesthesia residents to go through an operating room training exercise with MRHs to assess and influence residents' closed loop communication behaviors during medication administration. We manipulated the behavior of the MRHs to determine if the MRHs could influence the residents' closed loop communication behavior. Our results showed that residents' closed loop communications behaviors were influenced by MRHs. Additionally, we found there was a statistically significant difference between groups based on which MRH behavior residents observed. Because the MRHs significantly impacted how residents communicated in simulation, this work expands the boundaries for how VR can be used and demonstrates that MRHs could be used as tools to address complex communication dynamics in a team setting.},
	author = {Cordar, Andrew and Wendling, Adam and White, Casey and Lampotang, Samsun and Lok, Benjamin},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892242},
	issn = {2375-5334},
	keywords = {Surgery;Training;Virtual reality;Solid modeling;Safety;Information exchange;mixed reality;virtual humans;social influence;training},
	month = {March},
	pages = {148-156},
	title = {Repeat after me: Using mixed reality humans to influence best communication practices},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892242}}

@inproceedings{7892243,
	abstract = {Commodity depth cameras, such as the Microsoft Kinect{\textregistered}, have been widely used for the capture and reconstruction of the 3D structure of room-sized dynamic scenes. Camera placement and coverage during capture significantly impact the quality of the resulting reconstruction. In particular, dynamic occlusions and sensor interference have been shown to result in poor resolution and holes in the reconstruction results. This paper presents a novel algorithmic framework and a method for off-line optimization of depth cameras placements for a given 3D dynamic scene, simulated using virtual 3D models. We derive a fitness metric for a particular configuration of sensors by combining factors such as visibility and resolution of the entire dynamic scene with probabilities of interference between sensors. We employ this fitness metric both in a greedy algorithm that determines the number of depth cameras needed to cover the scene, and in a simulated annealing algorithm that optimizes the placements of those sensors. We compare our algorithm's optimized placements with manual sensor placements for a real dynamic scene. We present quantitative assessments using our fitness metric, as well as qualitative assessments to demonstrate that our algorithm not only enhances the resolution and total coverage of the reconstruction, but also fills in voids by avoiding occlusions and sensor interference when compared with the reconstruction of the same scene using mual sensor placement.},
	author = {Chabra, Rohan and Ilie, Adrian and Rewkowski, Nicholas and Cha, Young-Woon and Fuchs, Henry},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892243},
	issn = {2375-5334},
	keywords = {Cameras;Three-dimensional displays;Surface reconstruction;Measurement;Heuristic algorithms;Solid modeling;Computational modeling;G.1.6 [Numerical Analysis]: Optimization --- Global optimization;Simulated annealing;I.4.8 [Computing Methodologies]: Image Processing and Computer Vision --- Reconstruction;Scene Analysis;I.6.3 [Computing Methodologies]: Simulation and Modeling --- Applications;Model Development},
	month = {March},
	pages = {157-166},
	title = {Optimizing placement of commodity depth cameras for known 3D dynamic scene capture},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892243}}

@inproceedings{7892244,
	abstract = {The accurate calibration and registration of a set of color and depth (RGBD) sensors into a shared coordinate system is an essential requirement for 3D surround capturing systems. We present a method to calibrate multiple unsynchronized RGBD-sensors with high accuracy in a matter of minutes by sweeping a tracked checkerboard through the desired capturing space in front of each sensor. Through the sweeping process, a large number of robust correspondences between the depth and the color image as well as the 3D world positions can be automatically established. In order to obtain temporally synchronized correspondences between an RGBD-sensor's data streams and the tracked target's positions we apply an off-line optimization process based on error minimization and a coplanarity constraint. The correspondences are entered into a 3D look-up table which is used during runtime to transform depth and color information into the application's world coordinate system. Our proposed method requires a manual effort of less than one minute per RGBD-sensor and achieves a high calibration accuracy with an average 3D error below 3.5 mm and an average texture reprojection error smaller than 1 pixel.},
	author = {Beck, Stephan and Froehlich, Bernd},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892244},
	issn = {2375-5334},
	keywords = {Calibration;Three-dimensional displays;Adaptive optics;Optical sensors;Optical imaging;Image color analysis;I.4 [Image Processing and computer vision]: Digitization and Image Capture --- Camera calibration},
	month = {March},
	pages = {167-176},
	title = {Sweeping-based volumetric calibration and registration of multiple RGBD-sensors for 3D capturing systems},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892244}}

@inproceedings{7892245,
	abstract = {We propose a facial expression mapping technology between virtual avatars and Head-Mounted Display (HMD) users. HMD allow people to enjoy an immersive Virtual Reality (VR) experience. A virtual avatar can be a representative of the user in the virtual environment. However, the synchronization of the the virtual avatar's expressions with those of the HMD user is limited. The major problem of wearing an HMD is that a large portion of the user's face is occluded, making facial recognition difficult in an HMD-based virtual environment. To overcome this problem, we propose a facial expression mapping technology using retro-reflective photoelectric sensors. The sensors attached inside the HMD measures the distance between the sensors and the user's face. The distance values of five basic facial expressions (Neutral, Happy, Angry, Surprised, and Sad) are used for training the neural network to estimate the facial expression of a user. We achieved an overall accuracy of 88% in recognizing the facial expressions. Our system can also reproduce facial expression change in real-time through an existing avatar using regression. Consequently, our system enables estimation and reconstruction of facial expressions that correspond to the user's emotional changes.},
	author = {Suzuki, Katsuhiro and Nakamura, Fumihiko and Otsuka, Jiu and Masai, Katsutoshi and Itoh, Yuta and Sugiura, Yuta and Sugimoto, Maki},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892245},
	issn = {2375-5334},
	keywords = {Avatars;Resists;Face recognition;Face;Neural networks;Optical sensors;H.5.m. [Information Interfaces and Presentation (e.g. HCI)]: Miscellaneous},
	month = {March},
	pages = {177-185},
	title = {Recognition and mapping of facial expressions to avatar by embedded photo reflective sensors in head mounted display},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892245}}

@inproceedings{7892246,
	abstract = {Modern scientific, engineering and medical computational simulations, as well as experimental and observational data sensing/measuring devices, produce enormous amounts of data. While statistical analysis provides insight into this data, scientific visualization is tactically important for scientific discovery, product design and data analysis. These benefits are impeded, however, when scientific visualization algorithms are implemented from scratch --- a time-consuming and redundant process in immersive application development. This process can greatly benefit from leveraging the state-of-the-art open-source Visualization Toolkit (VTK) and its community. Over the past two (almost three) decades, integrating VTK with a virtual reality (VR) environment has only been attempted to varying degrees of success. In this paper, we demonstrate two new approaches to simplify this amalgamation of an immersive interface with visualization rendering from VTK. In addition, we cover several enhancements to VTK that provide near real-time updates and efficient interaction. Finally, we demonstrate the combination of VTK with both Vrui and OpenVR immersive environments in example applications.},
	author = {O'Leary, Patrick and Jhaveri, Sankhesh and Chaudhary, Aashish and Sherman, William and Martin, Ken and Lonie, David and Whiting, Eric and Money, James and McKenzie, Sandy},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892246},
	issn = {2375-5334},
	keywords = {Rendering (computer graphics);Data visualization;Context;Software;Geometry;Pipelines;Laboratories;Scientific visualization;immersive environments;virtual reality},
	month = {March},
	pages = {186-194},
	title = {Enhancements to VTK enabling scientific visualization in immersive environments},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892246}}

@inproceedings{7892247,
	abstract = {We present MagicToon, an interactive modeling system with mobile augmented reality (AR) that allows children to build 3D cartoon scenes creatively from their own 2D cartoon drawings on paper. Our system consists of two major components: an automatic 2D-to-3D cartoon model creator and an interactive model editor to construct more complicated AR scenes. The model creator can generate textured 3D cartoon models according to 2D drawings automatically and overlay them on the real world, bringing life to flat cartoon drawings. With our interactive model editor, the user can perform several optional operations on 3D models such as copying and animating in AR context through a touchscreen of a handheld device. The user can also author more complicated AR scenes by placing multiple registered drawings simultaneously. The results of our user study have shown that our system is easier to use compared with traditional sketch-based modeling systems and can give more play to children's innovations compared with AR coloring books.},
	author = {Feng, Lele and Yang, Xubo and Xiao, Shuangjiu},
	booktitle = {2017 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:59 -0400},
	date-modified = {2024-03-18 02:29:59 -0400},
	doi = {10.1109/VR.2017.7892247},
	issn = {2375-5334},
	keywords = {Solid modeling;Three-dimensional displays;Two dimensional displays;Color;Augmented reality;Mobile handsets;Atmospheric modeling;2D-to-3D;modeling;augmented reality;mobile devices;user interface;coloring},
	month = {March},
	pages = {195-204},
	title = {MagicToon: A 2D-to-3D creative cartoon modeling system with mobile AR},
	year = {2017},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2017.7892247}}

@inproceedings{7504682,
	abstract = {How do people appropriate their virtual hand representation when interacting in virtual environments? In order to answer this question, we conducted an experiment studying the sense of embodiment when interacting with three different virtual hand representations, each one providing a different degree of visual realism but keeping the same control mechanism. The main experimental task was a Pick-and-Place task in which participants had to grasp a virtual cube and place it to an indicated position while avoiding an obstacle (brick, barbed wire or fire). An additional task was considered in which participants had to perform a potentially dangerous operation towards their virtual hand: place their virtual hand close to a virtual spinning saw. Both qualitative measures and questionnaire data were gathered in order to assess the sense of agency and ownership towards each virtual hand. Results show that the sense of agency is stronger for less realistic virtual hands which also provide less mismatch between the participant's actions and the animation of the virtual hand. In contrast, the sense of ownership is increased for the human virtual hand which provides a direct mapping between the degrees of freedom of the real and virtual hand.},
	author = {Argelaguet, Ferran and Hoyet, Ludovic and Trico, Michael and Lecuyer, Anatole},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504682},
	issn = {2375-5334},
	keywords = {Avatars;Grasping;Virtual environments;Visualization;Synchronous motors;Rubber;Animation;H.5.2 [Information Interfaces and Presentation]: User Interfaces --- Evaluation/Methodology;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual Reality},
	month = {March},
	pages = {3-10},
	title = {The role of interaction in virtual embodiment: Effects of the virtual hand representation},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504682}}

@inproceedings{7504683,
	abstract = {While performing everyday interactions, we often incidentally touch and move objects in subtle ways. These objects are not necessarily directly related to the task at hand, and the movement of an object might even be entirely unintentional. If another person is touching the object at the same time, the movement can transfer through the object and be experienced --- however subtly --- by the other person. For example, when one person hands a drink to another, at some point both individuals will be touching the glass, and consequently exerting small (often unnoticed) forces on the other person. Despite the frequency of such subtle incidental movements of shared objects in everyday interactions, few have examined how these movements affect human-virtual human (VH) interaction. We ran an experiment to assess how presence and social presence are affected when a person experiences subtle, incidental movement through a shared real-virtual object. We constructed a real-virtual room with a table that spanned the boundary between the real and virtual environments. The participant was seated on the real side of the table, which visually extended into the virtual world via a projection screen, and the VH was seated on the virtual side of the table. The two interacted by playing a game of ``Twenty Questions,'' where one player asked the other a series of 20 yes/no questions to deduce what object the other player was thinking about. During the game, the ``wobbly'' group of subjects experienced subtle incidental movements of the real-virtual table: the entire real-virtual table tilted slightly away/toward the subject when the virtual/real human leaned on it. The control group also played the same game, except the table did not wobble. Results indicate that the wobbly group had higher presence and social presence with the virtual human in general, with statistically significant increases in presence, co-presence, and attentional allocation. We present the experiment and results, and discuss some potential implications for virtual human systems and some potential future studies.},
	author = {Lee, Myungho and Kim, Kangsoo and Daher, Salam and Raij, Andrew and Schubert, Ryan and Bailenson, Jeremy and Welch, Greg},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504683},
	issn = {2375-5334},
	keywords = {Electronic mail;Measurement by laser beam;Virtual environments;Games;Training;Haptic interfaces;Force;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Artificial, Augmented and Virtual Realities;J.4 [Computer Applications]: Social and Behavioral Sciences --- Psychology},
	month = {March},
	pages = {11-17},
	title = {The wobbly table: Increased social presence via subtle incidental movement of a real-virtual table},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504683}}

@inproceedings{7504684,
	abstract = {A novel research prototype, called MMSpace, was developed for realistic social telepresence in small group-to-group conversations. MMSpace consists of kinetic display avatars, which can change pose and position by automatically mirroring the remote user's head motions. To fully explore its potential beyond previous alternatives, MMSpace has the following novel features. First, it targets symmetric group-to-group telepresence. Second, the kinetic avatars of MMSpace can produce highly accurate, low latency, and silent physical motions, by using 4-Degree-of-Freedom (DoF) direct-drive actuators, and they can express a wide range of natural human behaviors like head gestures and changing attitudes, as well as indicating the focus of attention. Third, MMSpace supports eye contact between every pair of participants, by integrating i) directional visual attention cues indicated by avatar's kinetic pose change, ii) line-of-sight alignment among the positions of persons, avatars and cameras, and iii) attention-based camera switching, which allows an avatar to always show its owner's face looking directly toward the person that the avatar's owner is looking at. The prototype targets the 2  2 setting, and subjective evaluations based on group discussions indicate that the kinetic display avatar is superior to static displays in various aspects including gaze-awareness, eye-contact, perception of other nonverbal behaviors, mutual understanding, and sense of telepresence.},
	author = {Otsuka, Kazuhiro},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504684},
	issn = {2375-5334},
	keywords = {Kinetic theory;Avatars;Cameras;Face;Visualization;Switches;H1.2 [Models and Princiles]: User/Machine Systems --- Human factors;H4.3 [Information Systems Applications]: Communications Applications --- Computer conferencing, teleconferencing, and videoconferencing},
	month = {March},
	pages = {19-28},
	title = {MMSpace: Kinetically-augmented telepresence for small group-to-group conversations},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504684}}

@inproceedings{7504685,
	abstract = {We present an adaptive data-driven algorithm for interactive crowd simulation. Our approach combines realistic trajectory behaviors extracted from videos with synthetic multi-agent algorithms to generate plausible simulations. We use statistical techniques to compute the movement patterns and motion dynamics from noisy 2D trajectories extracted from crowd videos. These learned pedestrian dynamic characteristics are used to generate collision-free trajectories of virtual pedestrians in slightly different environments or situations. The overall approach is robust and can generate perceptually realistic crowd movements at interactive rates in dynamic environments. We also present results from preliminary user studies that evaluate the trajectory behaviors generated by our algorithm.},
	author = {Kim, Sujeong and Bera, Aniket and Best, Andrew and Chabra, Rohan and Manocha, Dinesh},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504685},
	issn = {2375-5334},
	keywords = {Trajectory;Heuristic algorithms;Solid modeling;Adaptation models;Videos;Computational modeling;Dynamics},
	month = {March},
	pages = {29-38},
	title = {Interactive and adaptive data-driven crowd simulation},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504685}}

@inproceedings{7504686,
	abstract = {An immersive virtual environment is the ideal platform for the planning and training of on-orbit servicing missions, as it provides a flexible and safe environment. In such kind of virtual assembly simulation, grasping virtual objects is one of the most common and natural interactions. However, unlike grasping objects in the real world, it is a non-trivial task in virtual environments, where the primary feedback is visual only. A lot of research investigated ways to provide haptic feedback, such as force, vibrational and electrotactile feedback. Such devices, however, are usually uncomfortable and hard to integrate in projection-based immersive YR systems. In this paper, we present a novel, small and lightweight electro-tactile feedback device, specifically designed for immersive virtual environments. It consists of a small tactor with eight electrodes for each finger and a signal generator attached to the user's hand or arm. Our device can be easily integrated with an existing optical finger tracking system. The study presented in this paper assesses the feasibility and usability of the interaction device. An experiment was conducted in a repeated measures design using the electrotactile feedback modality as independent variable. As benchmark, we chose three typical assembly tasks of a YR simulation for satellite on-orbit servicing missions, including pressing a button, switching a lever switch, and pulling a module from its slot. Results show that electrotactile feedback improved the user's grasping in our virtual on-orbit servicing scenario. The task completion time was significantly lower for all three tasks and the precision of the user's interaction was higher. The workload reported by the participants was significantly lower when using electrotactile feedback. Additionally, users were more confident with their performance while completing the tasks with electrotactile feedback. We describe the device, outline the user study and report the results.},
	author = {Hummel, Johannes and Dodiya, Janki and Center, German Aerospace and Eckardt, Laura and Wolff, Robin and Gerndt, Andreas and Kuhlen, Torsten W.},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504686},
	issn = {2375-5334},
	keywords = {Electrodes;Virtual environments;Grasping;Skin;Thumb;Voltage control;B.4.2 [Input/Output Devices]: Channels and controllers;H.3.4 [Systems and Software]: Performance evaluation;I.3.7 [Three-Dimensional Graphics and Realism]: Virtual Reality},
	month = {March},
	pages = {39-48},
	title = {A lightweight electrotactile feedback device for grasp improvement in immersive virtual environments},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504686}}

@inproceedings{7504687,
	abstract = {This study investigated the implementation of a hand model and contact simulation method for the purpose of improving the reality of object manipulation in a virtual environment. The study focused both on the hand tracking method that takes advantage of nails and also contact simulation using a deformable hand model. The manipulation of an object using a hand, is known to make more frequent use of the fingertips and palm. The proposed method seeks hand form that minimizes the position and orientation errors on those areas. Deformation of the soft tissue of the hand is considered to have an effect on both visual reality and the physical state of contact. In our implementation, the deformation was simulated by FEM and the friction of contact was introduced by the penalty method. In addition, a model that is based on metaballs (or blobs) was employed to represent the smooth surface of the object and to eliminate the problem that derives from polygon modeling. Through experimental implementation, it was proved that object manipulation such as pinching and grasping are possible and that the update rate of simulation can be approximately 50 Hz.},
	author = {Hirota, Koichi and Tagawa, Kazuyoshi},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504687},
	issn = {2375-5334},
	keywords = {Computational modeling;Deformable models;Solid modeling;Skin;Thumb;Sensors;Deformable Hand Model;Manipulation;Hand-Tracking},
	month = {March},
	pages = {49-56},
	title = {Interaction with virtual object using deformable hand},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504687}}

@inproceedings{7504688,
	abstract = {This article introduces model checking as an alternative method to estimate the latency and parallelism of asynchronous Realtime Interactive Systems (RISs). Five typical concurrency and synchronization schemes often found in concurrent Virtual Reality (VR) and computer game systems are identified as use-cases. These use-cases guide the development a) of software primitives necessary for the use-case implementation based on asynchronous RIS architectures and b) of a graphical editor for the specification of various concurrency and synchronization schemes (including the use-cases) based on these primitives. Several model-checking tools are evaluated against typical requirements in the RIS area. As a result, the formal model checking language Rebeca and its model checker RMC are applied to the specification of the use-cases to estimate latency and parallelism for each case. The estimations are compared to measured results achieved by classical profiling from a real-world application. The estimated results of the latencies by model checking approximated the measured results adequately with a minimal difference of 3.9% in the best case and -26.8% in the worst case. It also detected a problematic execution path not covered by the stochastic nature of the measured profiling samples. The estimated results of the degree of parallelization by model checking are approximated with an minimal difference of 9.3% and a maximal difference of -28.8%. Finally, the effort of model checking is compared to the effort of implementing and profiling a RIS.},
	author = {Rehfeld, Stephan and Latoschik, Marc Erich and Tramberend, Henrik},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504688},
	issn = {2375-5334},
	keywords = {Model checking;Concurrent computing;Real-time systems;Computational modeling;Rendering (computer graphics);Engines;Parallel processing;D.2.4 [Software/Program Verification]: Model checking ---;D.2.8 [Metrics]: Performance measures ---;D.4.8 [Performance]: Modeling and prediction ---},
	month = {March},
	pages = {57-66},
	title = {Estimating latency and concurrency of asynchronous real-time interactive systems using model checking},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504688}}

@inproceedings{7504689,
	abstract = {The use of a self-avatar inside an immersive virtual reality system has been shown to have important effects on presence, interaction and perception of space. Based on studies from linguistics and cognition, in this paper we demonstrate that a self-avatar may aid the participant's cognitive processes while immersed in a virtual reality system. In our study participants were asked to memorise pairs of letters, perform a spatial rotation exercise and then recall the pairs of letters. In a between-subject factor they either had an avatar or not, and in a within-subject factor they were instructed to keep their hands still or not. We found that participants who both had an avatar and were allowed to move their hands had significantly higher letter pair recall. There was no significant difference between the other three conditions. Further analysis showed that participants who were allowed to move their hands, but could not see the self-avatar, usually didn't move their hands or stopped moving their hands after a short while. We argue that an active self-avatar may alleviate the mental load of doing the spatial rotation exercise and thus improve letter recall. The results are further evidence of the importance of an appropriate self-avatar representation in immersive virtual reality.},
	author = {Steed, Anthony and Pan, Ye and Zisch, Fiona and Steptoe, William},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504689},
	issn = {2375-5334},
	keywords = {Cognition;Avatars;Solid modeling;Virtual environments;Visualization;Estimation;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities},
	month = {March},
	pages = {67-76},
	title = {The impact of a self-avatar on cognitive load in immersive virtual reality},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504689}}

@inproceedings{7504690,
	abstract = {Understanding how humans make decisions in challenging situations - such as trying to save peoples' lives even though this endangers one's own life - is crucial in optimizing rescue operations and dealing with natural disasters and other crises. The experimental study of these decisions, however, has often been done using text-based surveys, which is known to emphasize rational and reflective judgments. Here, we used virtual reality to investigate decision-making in a real-world context, in which a decision needs to be made intuitively under time pressure - for this we simulated an accident situation in an immersive virtual driving scenario. In the scenario, participants were told to race a course as fast as possible. After training, participants were confronted with the sudden appearance of pedestrians on the race course. We observed three different behaviors: group one ignored the pedestrians and/or hit the accelerator, group two hit the brake, and group three tried to steer the car to avoid pedestrians. We found that most Avoid-group participants had more real and virtual driving experience compared to the other two groups and they also felt more competent during the game as measured by subjective game experience questionnaires. Importantly, results from established personality questionnaires showed that participants who did not brake (therefore potentially harming the pedestrians) had significantly lower scores in perspective-taking and higher scores in psychopathy compared to participants who tried to avoid the accident situation. Our results demonstrate that personality differences to some degree are able to predict intuitive decision-making and that such processes can be studied in a controlled, immersive VR simulation.},
	author = {Ju, Uijong and Kang, June and Wallraven, Christian},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504690},
	issn = {2375-5334},
	keywords = {Brakes;Games;Decision making;Training;Accidents;Solid modeling;Virtual reality;Virtual reality;decision-making;driving Simulation;moral judgments;personality differences},
	month = {March},
	pages = {77-82},
	title = {Personality differences predict decision-making in an accident situation in virtual driving},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504690}}

@inproceedings{7504691,
	abstract = {Augmented Reality (AR) browsers show geo-referenced data in the current view of a user. When the amount of data grows too large, the display quickly becomes cluttered. Clustering items by spatial and semantic attributes can temporarily alleviate the issue, but is not effective against an increasing amount of data. We present an adaptive information density display for AR that balances the amount of presented information against the potential clutter created by placing items on the screen. We use hierarchical clustering to create a level-of-detail structure, in which nodes closer to the root encompass groups of items, while the leaf nodes contain single items. Our method selects items and groups from different levels of this hierarchy based on user-defined preferences and on the amount of visual clutter caused by placing these items. The number of presented items is adapted during user interaction to avoid clutter. We compare our interface to a conventional AR browser interface in a qualitative user study. Users clearly preferred our interface, because it provided a better overview of the data and allowed for easier comparison. In a second study, we evaluated the effect of different degrees of clustering on search and recall tasks. Users generally made fewer errors, when using our interface for a search task, which indicates that the reduced clutter allowed them to stay focused on finding the relevant items.},
	author = {Tatzgern, Markus and Orso, Valeria and Kalkofen, Denis and Jacucci, Giulio and Gamberini, Luciano and Schmalstieg, Dieter},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504691},
	issn = {2375-5334},
	keywords = {Clutter;Data visualization;Visualization;Browsers;Semantics;Electronic mail;Clustering algorithms},
	month = {March},
	pages = {83-92},
	title = {Adaptive information density for augmented reality displays},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504691}}

@inproceedings{7504692,
	abstract = {Full-surround augmented reality, with augmentations spanning the entire human field of view and beyond, is an under-explored topic since there is currently no hardware that can support it. As current AR displays only support relatively small fields of view, most AR applications to-date employ relatively small point-based annotations of the physical world. Anticipating a change in AR capabilities, we experiment with wide-field-of-view annotations that link elements far apart in the visual field. We have built a system that uses full-surround virtual reality to simulate augmented reality with different field of views, with and without tracking artifacts. We conducted a study comparing user performance on five different task groups within an information-seeking scenario, comparing two different fields of view and presence and absence of tracking artifacts. A constrained field of view significantly increased task completion time. We found indications for task time effects of tracking artifacts to vary depending on age.},
	author = {Ren, Donghao and Goldschwendt, Tibor and Chang, YunSuk and H{\"o}llerer, Tobias},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504692},
	issn = {2375-5334},
	keywords = {Augmented reality;Solid modeling;Visualization;Three-dimensional displays;Performance evaluation;Hardware;H.5.1 [Information Interfaces and Presentation (e.g., HCI)]: Multimedia Information Systems --- Artificial, augmented, and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual reality},
	month = {March},
	pages = {93-102},
	title = {Evaluating wide-field-of-view augmented reality with mixed reality simulation},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504692}}

@inproceedings{7504693,
	abstract = {We propose a fast and accurate calibration method for the optical see-through (OST) head-mounted displays (HMD), taking advantage of a low-cost time-of-flight depth-camera. Recently, affordable OST-HMDs and depth-cameras are widely appearing in the commercial market. In order to correctly reflect the user experience into the calibration process, our method demands a user wearing the HMD to repeatedly point at rendered virtual circles with their fingertips. From the repeated calibration data, we perform two stages of full calibration and simplified calibration, to compute key calibration parameters. The full calibration is required when the depth-camera is first installed to the HMD, and afterwards only the simplified calibration is performed whenever a user wears it again. Our experimental results show that the full and simplified calibration can be achieved with 10 and 5 user's repetitions (theoretically 3 and 2 at minimum), which are significantly less than about 20 of the stereo-SPAAM, one of the most popular existing calibration techniques. We also demonstrate that the 3D position errors of our calibration become much quickly smaller than those of the state-of-the-art method.},
	author = {Jun, Hanseul and Kim, Gunhee},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504693},
	issn = {2375-5334},
	keywords = {Calibration;Cameras;Three-dimensional displays;Adaptive optics;Head;Magnetic heads;Mathematical model;Calibration;optical see-through head-mounted display;depth-camera},
	month = {March},
	pages = {103-111},
	title = {A calibration method for optical see-through head-mounted displays with a depth camera},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504693}}

@inproceedings{7504694,
	abstract = {Comfortable, high-quality 3D stereo viewing is becoming a requirement for interactive applications today. Previous research shows that manipulating disparity can alleviate some of the discomfort caused by 3D stereo, but it is best to do this locally, around the object the user is gazing at. The main challenge is thus to develop a gaze predictor in the demanding context of real-time, heavily task-oriented applications such as games. Our key observation is that player actions are highly correlated with the present state of a game, encoded by game variables. Based on this, we train a classifier to learn these correlations using an eye-tracker which provides the ground-truth object being looked at. The classifier is used at runtime to predict object category - and thus gaze - during game play, based on the current state of game variables. We use this prediction to propose a dynamic disparity manipulation method, which provides rich and comfortable depth. We evaluate the quality of our gaze predictor numerically and experimentally, showing that it predicts gaze more accurately than previous approaches. A subjective rating study demonstrates that our localized disparity manipulation is preferred over previous methods.},
	author = {Koulieris, George Alex and Drettakis, George and Cunningham, Douglas and Mania, Katerina},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504694},
	issn = {2375-5334},
	keywords = {Games;Three-dimensional displays;Real-time systems;Stereo image processing;Gaze tracking;Manipulator dynamics;Context;Gaze Prediction;Stereo Grading;Perception},
	month = {March},
	pages = {113-120},
	title = {Gaze prediction using machine learning for dynamic stereo manipulation in games},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504694}}

@inproceedings{7504695,
	abstract = {Difficulties in social interaction, verbal and non-verbal communications as well as repetitive and atypical patterns of behavior, characterizes Autism spectrum disorders (ASD). A number of studies indicated that many children with ASD prefer technology and this preference can be explored to develop systems that may alleviate several challenges of traditional treatment and intervention. As a result, recent advances in computer and robotic technology are ushering in innovative assistive technologies for ASD intervention. The current work presents design, development and a usability study of an adaptive multimodal virtual reality-based social interaction platform for children with ASD. It is hypothesized that endowing a technological system that can detect the processing pattern and mental state of the child using implicit cues from eye tracking and electrophysiological, including peripheral physiological and electroencephalography (EEG), signals and adapt its interaction accordingly is of great importance in assisting and individualizing traditional intervention approaches. The presented VR system is based on a virtual reality based social environment, a school cafeteria, where an individual with ASD interacts with virtual characters. An eye tracker, an EEG monitor and biosensors to measure peripheral electrophysiological signals are integrated with the VR task environment to obtain gaze, EEG signals and several peripheral physiological signals in real-time. In the current work, we show how eye gaze and task performance can be used in real-time to adapt intervention in VR. The other signals are collected for offline analysis. The results from a usability study with 12 subjects with ASD are presented to demonstrate the viability of the proposed concepts within the VR system.},
	author = {Bekele, E. and Wade, J. and Bian, D. and Fan, J. and Swanson, Amy and Warren, Z. and Sarkar, N.},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504695},
	issn = {2375-5334},
	keywords = {Biomedical monitoring;Face;Electroencephalography;Engines;Emotion recognition;Context;Autism;3D social Interaction;multimodal interaction;psychology;usability;VR-based adaptive systems},
	month = {March},
	pages = {121-130},
	title = {Multimodal adaptive social interaction in virtual environment (MASI-VR) for children with Autism spectrum disorders (ASD)},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504695}}

@inproceedings{7504696,
	abstract = {We detail the design, implementation, and an initial evaluation of a virtual reality education and entertainment (edutainment) application called Virtual Environment Interactions (VEnvI). VEnvI is an application in which students learn computer science concepts through the process of choreographing movement for a virtual character using a fun and intuitive interface. In this exploratory study, 54 participants as part of a summer camp geared towards promoting participation of women in science and engineering programmatically crafted a dance performance for a virtual human. A subset of those students participated in an immersive embodied interaction metaphor in VEnvI. In creating this metaphor that provides first-person, embodied experiences using self-avatars, we seek to facilitate engagement, excitement and interest in computational thinking. We qualitatively and quantitatively evaluated the extent to which the activities of the summer camp, programming the dance moves, and the embodied interaction within VEnvI facilitated students' edutainment, presence, interest, excitement, and engagement in computing, and the potential to alter their perceptions of computing and computer scientists. Results indicate that students enjoyed the experience and successfully engaged the virtual character in the immersive embodied interaction, thus exhibiting high telepresence and social presence. Students also showed increased interest and excitement regarding the computing field at the end of their summer camp experience using VEnvI.},
	author = {Parmar, Dhaval and Isaac, Joseph and Babu, Sabarish V. and D'Souza, Nikeetha and Leonard, Alison E. and J{\"o}rg, Sophie and Gundersen, Kara and Daily, Shaundra B.},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504696},
	issn = {2375-5334},
	keywords = {Education;Programming profession;Virtual reality;Computers;Cognition;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual Reality;K.3.2 [Computers and Education]: Computer and Information Science Education --- Computer Science Education},
	month = {March},
	pages = {131-140},
	title = {Programming moves: Design and evaluation of applying embodied interaction in virtual environments to enhance computational thinking in middle school students},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504696}}

@inproceedings{7504697,
	abstract = {This paper presents an experiment conducted in a large-screen immersive virtual environment to evaluate how texting pedestrians respond to permissive traffic alerts delivered via their cell phone. We developed a cell phone app that delivered information to texting pedestrians about when traffic conditions permit safe crossing. We compared gap selection and movement timing in three groups of pedestrians: texting, texting with alerts, and no texting (control). Participants in the control and alert groups chose larger gaps and were more discriminating in their gap choices than participants in the texting group. Both the control and alert groups had more time to spare than the texting group when they exited the roadway even though the alert group timed their entry relative to the lead car less tightly than the control and texting groups. By choosing larger gaps, participants in the alert group were able to compensate for their poorer timing of entry, resulting in a margin of safety that did not differ from those who were not texting. However, they also relied heavily on the alert system and paid less attention to the roadway. The discussion focuses on the potential of assistive technologies based on Vehicle-to-Pedestrian (V2P) communications technology for mitigating pedestrian-motor vehicle crashes.},
	author = {Rahimian, Pooya and O'Neal, Elizabeth E. and Yon, Junghum Paul and Franzen, Luke and Jiang, Yuanyuan and Plumert, Jodie M. and Kearney, Joseph K.},
	booktitle = {2016 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:54 -0400},
	date-modified = {2024-03-18 02:29:54 -0400},
	doi = {10.1109/VR.2016.7504697},
	issn = {2375-5334},
	keywords = {Roads;Vehicles;Cellular phones;Mobile handsets;Virtual environments;Safety;Electronic mail;Pedestrian simulation;Texting;Mobile device use;Pedestrian safety;Connected vehicles technology;Vehicle-to-pedestrian (V2P) communication},
	month = {March},
	pages = {141-149},
	title = {Using a virtual environment to study the impact of sending traffic alerts to texting pedestrians},
	year = {2016},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2016.7504697}}

@inproceedings{7223317,
	abstract = {The need for virtual reality applications for education and training involving bimanual dexterous activities has been increasing in recent years. However, it is unclear how the amount of correspondence between a virtual interaction metaphor to the real-world equivalent, otherwise known as dimensional symmetry, affects bimanual pscyhomotor skills training and how skills learned in the virtual simulation transfer to the real world. How does the number of degrees of freedom enhance or hinder the learning process? Does the increase in dimensional symmetry affect cognitive load? In an empirical evaluation, we compare the effectiveness of a natural 6-DOF interaction metaphor to a simplified 3-DOF metaphor. Our simulation interactively educates users in the step-by-step process of taking a precise measurement using calipers and micrometers in a simulated technical workbench environment. We conducted a usability study to evaluate the user experience and pedagogical benefits using measures including a pre and post cognition questionnaire over all levels of Bloom's taxonomy, workload assessment, system usability, and real world psychomotor assessment tasks. Results from the pre and post cognition questionnaires suggest that learning outcomes improved throughout all levels of Bloom's taxonomy for both conditions, and trends in the data suggest that the 6-DOF metaphor was more effective in real-world skill transference compared to the 3-DOF metaphor.},
	author = {Bertrand, Jeffrey and Brickler, David and Babu, Sabarish and Madathil, Kapil and Zelaya, Melissa and Wang, Tianwei and Wagner, John and Gramopadhye, Anand and Luo, Jun},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223317},
	issn = {2375-5334},
	keywords = {Atmospheric measurements;Particle measurements;Instruments;Training;Metrology;Solid modeling;Bimanual interaction;psychomotor skills education;dimensional symmetry},
	month = {March},
	pages = {3-10},
	title = {The role of dimensional symmetry on bimanual psychomotor skills education in immersive virtual environments},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223317}}

@inproceedings{7223318,
	abstract = {We present the results of a variable information space experiment, targeted at exploring the scalability limits of immersive highresolution, tiled-display walls under physical navigation. Our work is motivated by a lack of evidence supporting the extension of previously established benefits on substantially large, room-shaped displays. Using the Reality Deck, a gigapixel resolution immersive display, as its apparatus, our study spans four display form-factors, starting at 100 megapixels arranged planarly and up to one gi-gapixel in a horizontally immersive setting. We focus on four core tasks: visual search, attribute search, comparisons and pattern finding. We present a quantitative analysis of per-task user performance across the various display conditions. Our results demonstrate improvements in user performance as the display form-factor changes to 600 megapixels. At the 600 megapixel to 1 gigapixel transition, we observe no tangible performance improvements and the visual search task regressed substantially. Additionally, our analysis of subjective mental effort questionnaire responses indicates that subjective user effort grows as the display size increases, validating previous studies on smaller displays. Our analysis of the participants' physical navigation during the study sessions shows an increase in user movement as the display grew. Finally, by visualizing the participants' movement within the display apparatus space, we discover two main approaches (termed ``overview'' and ``detail'') through which users chose to tackle the various data exploration tasks. The results of our study can inform the design of immersive high-resolution display systems and provide insight into how users navigate within these room-sized visualization spaces.},
	author = {Papadopoulos, C. and Mirhosseini, S. and Gutenko, I. and Petkov, K. and Kaufman, A. E. and Laha, B.},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223318},
	issn = {2375-5334},
	keywords = {Navigation;Data visualization;Visualization;Scalability;Timing;Rendering (computer graphics);Wall displays;user studies;display scalability;highresolution display;immersion;navigation;visualization},
	month = {March},
	pages = {11-18},
	title = {Scalability limits of large immersive high-resolution displays},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223318}}

@inproceedings{7223319,
	abstract = {In virtual reality applications, there is an aim to provide real time graphics which run at high refresh rates. However, there are many situations in which this is not possible due to simulation or rendering issues. When running at low frame rates, several aspects of the user experience are affected. For example, each frame is displayed for an extended period of time, causing a high persistence image artifact. The effect of this artifact is that movement may lose continuity, and the image jumps from one frame to another. In this paper, we discuss our initial exploration of the effects of high persistence frames caused by low refresh rates and compare it to high frame rates and to a technique we developed to mitigate the effects of low frame rates. In this technique, the low frame rate simulation images are displayed with low persistence by blanking out the display during the extra time such image would be displayed. In order to isolate the visual effects, we constructed a simulator for low and high persistence displays that does not affect input latency. A controlled user study comparing the three conditions for the tasks of 3D selection and navigation was conducted. Results indicate that the low persistence display technique may not negatively impact user experience or performance as compared to the high persistence case. Directions for future work on the use of low persistence displays for low frame rate situations are discussed.},
	author = {Zielinski, David J. and Rao, Hrishikesh M. and Sommer, Mark A. and Kopper, Regis},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223319},
	issn = {2375-5334},
	keywords = {Navigation;Visualization;Interpolation;Virtual environments;Training;Glass;Complexity theory;virtual environments;low-persistence;simulator sickness;performance;presence},
	month = {March},
	pages = {19-26},
	title = {Exploring the effects of image persistence in low frame rate virtual environments},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223319}}

@inproceedings{7223320,
	abstract = {When walking within an immersive projection environment, accommodation distance, parallax and angular resolution vary according to the distance between the user and the projection walls which can influence spatial perception. As CAVE-like virtual environments get bigger, accurate spatial perception within the projection setup becomes increasingly important for application domains that require the user to be able to naturally explore a virtual environment by moving through the physical interaction space. In this paper we describe an experiment which analyzes how distance estimation is biased when the distance to the screen and parallax vary. The experiment was conducted in a large immersive projection setup with up to ten meter interaction space. The results showed that both the screen distance and parallax have a strong asymmetric effect on distance judgments. We found an increased distance underestimation for positive parallax conditions. In contrast, we found less distance overestimation for negative and zero parallax conditions. We conclude the paper discussing the results with view on future large immersive projection environments.},
	author = {Bruder, Gerd and Sanz, Fernando Argelaguet and Olivier, Anne-Helene and Lecuyer, Anatole},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223320},
	issn = {2375-5334},
	keywords = {Visualization;Retina;Estimation;Convergence;Stereo image processing;Atmospheric measurements;Particle measurements;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Artificial, augmented, and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual reality},
	month = {March},
	pages = {27-32},
	title = {Distance estimation in large immersive projection systems, revisited},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223320}}

@inproceedings{7223321,
	abstract = {The International Air Transport Association forecasts that there will be at least a 30% increase in passenger demand for flights over the next five years. In these circumstances the aircraft industry is looking for new ways to keep passengers occupied, entertained and healthy, and one of the methods under consideration is immersive virtual reality. It is therefore becoming important to understand how motion sickness and presence in virtual reality are influenced by physical motion. We were specifically interested in the use of head-mounted displays (HMD) while experiencing in-flight motions such as turbulence. 50 people were tested in different virtual environments varying in their context (virtual airplane versus magic carpet ride over tropical islands) and the way the physical motion was incorporated into the virtual world (matching visual and auditory stimuli versus no incorporation). Participants were subjected to three brief periods of turbulent motions realized with a motion simulator. Physiological signals (postural stability, heart rate and skin conductance) as well as subjective experiences (sickness and presence questionnaires) were measured. None of our participants experienced severe motion sickness during the experiment and although there were only small differences between conditions we found indications that it is beneficial for both wellbeing and presence to choose a virtual environment in which turbulent motions could be plausible and perceived as part of the scenario. Therefore we can conclude that brief exposure to turbulent motions does not get participants sick.},
	author = {Soyka, Florian and Kokkinara, Elena and Leyrer, Markus and Buelthoff, Heinrich and Slater, Mel and Mohler, Betty},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223321},
	issn = {2375-5334},
	keywords = {Virtual environments;Atmospheric measurements;Particle measurements;Airplanes;Visualization;Physiology;Heart rate;Motion sickness;presence;turbulence;virtual environments;physiological measures},
	month = {March},
	pages = {33-40},
	title = {Turbulent motions cannot shake VR},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223321}}

@inproceedings{7223322,
	abstract = {In mixed reality, real objects can be used to interact with virtual objects. However, unlike in the real world, real objects do not encounter any opposite reaction force when pushing against virtual objects. The lack of reaction force during manipulation prevents users from perceiving the mass of virtual objects. Although this could be addressed by equipping real objects with force-feedback devices, such a solution remains complex and impractical. In this work, we present a technique to produce an illusion of mass without any active force-feedback mechanism. This is achieved by simulating the effects of this reaction force in a purely visual way. A first study demonstrates that our technique indeed allows users to differentiate light virtual objects from heavy virtual objects. In addition, it shows that the illusion is immediately effective, with no prior training. In a second study, we measure the lowest mass difference (JND) that can be perceived with this technique. The effectiveness and ease of implementation of our solution provides an opportunity to enhance mixed reality interaction at no additional cost.},
	author = {Issartel, Paul and Gu{\'e}niat, Florimond and Coquillart, Sabine and Ammi, Mehdi},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223322},
	issn = {2375-5334},
	keywords = {Cloning;Force;Springs;Visualization;Virtual reality;Haptic interfaces;Damping;Mass Perception;Physically-Based Simulation;Mixed Reality;Pseudo-Haptics},
	month = {March},
	pages = {41-46},
	title = {Perceiving mass in mixed reality through pseudo-haptic rendering of Newton's third law},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223322}}

@inproceedings{7223323,
	abstract = {In a Multi-stereoscopic immersive system, several users sharing the same restricted workspace, e.g. a CAVE, may need to perform independent navigation to achieve loosely coupled collaboration tasks for a complex scenario. In this context, a proper navigation paradigm should provide users both an efficient control of virtual navigation and a guarantee of users' safety in the real workspace relative to the display system and between users. In this aim, we propose several alterations of the human joystick metaphor by introducing implicit adaptive control to allow safe individual navigation for multiple users. We conducted a user study with an object-finding task in a double-stereoscopic CAVE-like system to evaluate both users' navigation performance in the virtual world and their behavior in the real workspace under different conditions. The results highlight that the improved paradigm allows two users to navigate independently despite physical system limitations.},
	author = {Chen, Weiya and Ladeveze, Nicolas and Clavel, C{\'e}line and Mestre, Daniel and Bourdot, Patrick},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223323},
	issn = {2375-5334},
	keywords = {Navigation;Vehicles;Tracking;Virtual environments;Three-dimensional displays;Teamwork;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual Reality},
	month = {March},
	pages = {47-54},
	title = {User cohabitation in multi-stereoscopic immersive virtual environment for individual navigation tasks},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223323}}

@inproceedings{7223324,
	abstract = {3D applications appear in every corner of life in the current technology era. There is a need for an ubiquitous 3D input device that works with many different platforms, from head-mounted displays (HMDs) to mobile touch devices, 3DTVs, and even the Cave Automatic Virtual Environments. We present 3DTouch, a novel wearable 3D input device worn on the fingertip for 3D manipulation tasks. 3DTouch is designed to fill the missing gap of a 3D input device that is self-contained, mobile, and universally works across various 3D platforms. This paper presents a low-cost solution to designing and implementing such a device. Our approach relies on a relative positioning technique using an optical laser sensor and a 9-DOF inertial measurement unit. The device employs touch input for the benefits of passive haptic feedback, and movement stability. On the other hand, with touch interaction, 3DTouch is conceptually less fatiguing to use over many hours than 3D spatial input devices. We propose a set of 3D interaction techniques including selection, translation, and rotation using 3DTouch. An evaluation also demonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for subtle touch interaction in 3D space. We envision that modular solutions like 3DTouch opens up a whole new design space for interaction techniques to further develop on. With 3DTouch, we attempt to bring 3D applications a step closer to users.},
	author = {Nguyen, Anh and Banic, Amy},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223324},
	issn = {2375-5334},
	keywords = {Three-dimensional displays;Mice;Tracking;Accuracy;Performance evaluation;Optical sensors;Shape;H.5.2 [Information interfaces and presentation]: User Interfaces --- Graphical user interfaces --- Input devices and strategies},
	month = {March},
	pages = {55-61},
	title = {3DTouch: A wearable 3D input device for 3D applications},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223324}}

@inproceedings{7223325,
	abstract = {Haptic feedback is known to improve 3D interaction in virtual environments but current haptic interfaces remain complex and tailored to desktop interaction. In this paper, we introduce the ``Elastic-Arm'', a novel approach for incorporating haptic feedback in immersive virtual environments in a simple and cost-effective way. The Elastic-Arm is based on a body-mounted elastic armature that links the user's hand to her shoulder. As a result, a progressive resistance force is perceived when extending the arm. This haptic feedback can be incorporated with various 3D interaction techniques and we illustrate the possibilities offered by our system through several use cases based on well-known examples such as the Bubble technique, Redirected Touching and pseudo-haptics. These illustrative use cases provide users with haptic feedback during selection and navigation tasks but they also enhance their perception of the virtual environment. Taken together, these examples suggest that the Elastic-Arm can be transposed in numerous applications and with various 3D interaction metaphors in which a mobile hap-tic feedback can be beneficial. It could also pave the way for the design of new interaction techniques based on ``human-scale'' egocentric haptic feedback.},
	author = {Achibet, Merwan and Girard, Adrien and Talvas, Anthony and Marchal, Maud and L{\'e}cuyer, Anatole},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223325},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Virtual environments;Three-dimensional displays;Navigation;Visualization;Force;Prototypes;H.5.2 [Information Interfaces and Presentation]: User Interfaces --- Interaction styles;Haptic I/O},
	month = {March},
	pages = {63-68},
	title = {Elastic-Arm: Human-scale passive haptic feedback for augmenting interaction and perception in virtual environments},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223325}}

@inproceedings{7223326,
	abstract = {We demonstrate a generalizable method for unified multitouch detection and response on a human head-shaped surface with a rear-projection animated 3D face. The method helps achieve hands-on touch-sensitive training with dynamic physical-virtual patient behavior. The method, which is generalizable to other non-parametric rear-projection surfaces, requires one or more infrared (IR) cameras, one or more projectors, IR light sources, and a rear-projection surface. IR light reflected off of human fingers is captured by cameras with matched IR pass filters, allowing for the localization of multiple finger touch events. These events are tightly coupled with the rendering system to produce auditory and visual responses on the animated face displayed using the projector(s), resulting in a responsive, interactive experience. We illustrate the applicability of our physical prototype in a medical training scenario.},
	author = {Hochreiter, Jason and Daher, Salam and Nagendran, Arjun and Gonzalez, Laura and Welch, Greg},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223326},
	issn = {2375-5334},
	keywords = {Cameras;Three-dimensional displays;Sensors;Head;Training;Rendering (computer graphics);H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Animations, Artificial, Augmented, and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual Reality;I.3.8 [Computer Graphics]: Applications},
	month = {March},
	pages = {69-74},
	title = {Touch sensing on non-parametric rear-projection surfaces: A physical-virtual head for hands-on healthcare training},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223326}}

@inproceedings{7223327,
	abstract = {In this paper, we investigate obstacle avoidance behavior during real walking in a large immersive projection setup. We analyze the walking behavior of users when avoiding real and virtual static obstacles. In order to generalize our study, we consider both anthropomorphic and inanimate objects, each having his virtual and real counterpart. The results showed that users exhibit different locomotion behaviors in the presence of real and virtual obstacles, and in the presence of anthropomorphic and inanimate objects. Precisely, the results showed a decrease of walking speed as well as an increase of the clearance distance (i. e., the minimal distance between the walker and the obstacle) when facing virtual obstacles compared to real ones. Moreover, our results suggest that users act differently due to their perception of the obstacle: users keep more distance when the obstacle is anthropomorphic compared to an inanimate object and when the orientation of anthropomorphic obstacle is from the profile compared to a front position. We discuss implications on future large shared immersive projection spaces.},
	author = {Sanz, Ferran Argelaguet and Olivier, Anne-Helene and Bruder, Gerd and Pettr{\'e}, Julien and L{\'e}cuyer, Anatole},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223327},
	issn = {2375-5334},
	keywords = {Legged locomotion;Collision avoidance;Trajectory;Visualization;Virtual environments;Glass;Analysis of variance;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Artificial, Augmented, and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual Reality},
	month = {March},
	pages = {75-80},
	title = {Virtual proxemics: Locomotion in the presence of obstacles in large immersive projection environments},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223327}}

@inproceedings{7223328,
	abstract = {Individuals tend to find realistic walking speeds too slow when relying on treadmill walking or Walking-In-Place (WIP) techniques for virtual travel. This paper details three studies investigating the effects of visual display properties and gain presentation mode on the perceived naturalness of virtual walking speeds: The first study compared three different degrees of peripheral occlusion; the second study compared three different degrees of perceptual distortion produced by varying the geometric field of view (GFOV); and the third study compared three different ways of presenting visual gains. All three studies compared treadmill walking and WIP locomotion. The first study revealed no significant main effects of peripheral occlusion. The second study revealed a significant main effect of GFOV, suggesting that the GFOV size may be inversely proportional to the degree of underestimation of the visual speed. The third study found a significant main effect of gain presentation mode. Allowing participants to interactively adjust the gain led to a smaller range of perceptually natural gains and this approach was significantly faster. However, the efficiency may come at the expense of confidence. Generally the lower and upper bounds of the perceptually natural speeds were higher for treadmill walking than WIP. However, not all differences were statistically significant.},
	author = {Nilsson, Niels Christian and Serafin, Stefania and Nordahl, Rolf},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223328},
	issn = {2375-5334},
	keywords = {Legged locomotion;Visualization;Optical distortion;Analysis of variance;Virtual environments;Distortion;Adaptive optics;H.1.2 [Information Systems]: User/Machine Systems --- Human factors;I.3.7 [Computer Graphics]: Three-Dimenshional Graphics and Realism --- Virtual Reality},
	month = {March},
	pages = {81-88},
	title = {The effect of visual display properties and gain presentation mode on the perceived naturalness of virtual walking speeds},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223328}}

@inproceedings{7223329,
	abstract = {Latency (i.e., time delay) in a Virtual Environment is known to disrupt user performance, presence and induce simulator sickness. However, can we utilize the effects caused by experiencing latency to benefit virtual rehabilitation technologies? We investigate this question by conducting an experiment that is aimed at altering gait by introducing latency applied to one side of a self-avatar with a front-facing mirror. This work was motivated by previous findings where participants altered their gait with increasing latency, even when participants failed to notice considerably high latencies as 150ms or 225ms. In this paper, we present the results of a study that applies this novel technique to average healthy persons (i.e., to demonstrate the feasibility of the approach before applying it to persons with disabilities). The results indicate a tendency to create asymmetric gait in persons with symmetric gait when latency is applied to one side of their self-avatar. Thus, the study shows the potential of applying one-sided latency in a self-avatar, which could be used to develop asymmetric gait rehabilitation techniques.},
	author = {Samaraweera, Gayani and Perdomo, Alex and Quarles, John},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223329},
	issn = {2375-5334},
	keywords = {Mirrors;Avatars;Legged locomotion;Visualization;Foot;Optical feedback;Optical sensors;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities --- Evaluation/methodology},
	month = {March},
	pages = {89-96},
	title = {Applying latency to half of a self-avatar's body to change real walking patterns},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223329}}

@inproceedings{7223330,
	abstract = {Dynamic Projection Mapping, projection-based AR for a moving object without misalignment by a high-speed optical axis controller by rotational mirrors, has a trade-off between stability of highspeed tracking and high visibility for a variety of projection content. In this paper, a system that will provide robust high-speed tracking without any markers on objects against illumination changes, including projected images, is realized by introducing a retroreflective background with the optical axis controller for Dynamic Projection Mapping. Low-intensity episcopic light is projected with Projection Mapping content, and the light reflected from the background is sufficient for high-speed cameras but is nearly invisible to observers. In addition, we introduce adaptive windows and peripheral weighted erosion to maintain accurate high-speed tracking. Under low light conditions, we examined the visual performance of diffuse reflection and retroreflection from both camera and observer viewpoints. We evaluated stability relative to illumination and disturbance caused by non-target objects. Dynamic Projection Mapping with partially well-lit content in a low-intensity light environment is realized by our proposed system.},
	author = {Sueishi, Tomohiro and Oku, Hiromasa and Ishikawa, Masatoshi},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223330},
	issn = {2375-5334},
	keywords = {Cameras;High-speed optical techniques;Optical imaging;Lighting;Target tracking;Optical control;Mirrors;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Artificial, augmented, and virtual realities;I.4.8 [Image Processing and Computer Vision]: Scene Analysis --- Tracking},
	month = {March},
	pages = {97-104},
	title = {Robust high-speed tracking against illumination changes for dynamic projection mapping},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223330}}

@inproceedings{7223331,
	abstract = {This paper presents the design and implementation of a system for simulating mixed reality in setups combining mobile devices and large backdrop displays. With a mixed reality simulator, one can perform usability studies and evaluate mixed reality systems while minimizing confounding variables. This paper describes how mobile device AR design factors can be flexibly and systematically explored without sacrificing the touch and direct unobstructed manipulation of a physical personal MR display. First, we describe general principles to consider when implementing a mixed reality simulator, enumerating design factors. Then, we present our implementation which utilizes personal mobile display devices in conjunction with a large surround-view display environment. Standing in the center of the display, a user may direct a mobile device, such as a tablet or head-mounted display, to a portion of the scene, which affords them a potentially annotated view of the area of interest. The user may employ gesture or touch screen interaction on a simulated augmented camera feed, as they typically would in video-see-through mixed reality applications. We present calibration and system performance results and illustrate our system's flexibility by presenting the design of three usability evaluation scenarios.},
	author = {Rodrigue, Mathieu and Waranis, Andrew and Wood, Tim and H{\"o}llerer, Tobias},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223331},
	issn = {2375-5334},
	keywords = {Virtual reality;Cameras;Lenses;Smart phones;Servers;Mobile communication;Augmented reality;virtual reality;large displays;immersive displays;mobile device;input device;interaction techniques},
	month = {March},
	pages = {105-110},
	title = {Mixed reality simulation with physical mobile display devices},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223331}}

@inproceedings{7223332,
	abstract = {In virtual reality, hybrid virtual environment (HVE) systems provide the immersed user with multiple interactive representations of the virtual world, and can be effectively used for 3D interaction tasks with highly diverse requirements. We present a new HVE metaphor called Object Impersonation that allows the user to not only manipulate a virtual object from outside, but also become the object, and maneuver from inside. This approach blurs the line between travel and object manipulation, leading to efficient cross-task interaction in various task scenarios. Using a tablet- and HMD-based HVE system, two different designs of Object Impersonation were implemented, and compared to a traditional, non-hybrid 3D interface for three different object manipulation tasks. Results indicate improved task performance and enhanced user experience with the added orientation control from the object's point of view. However, they also revealed higher cognitive overhead to attend to both interaction contexts, especially without sufficient reference cues in the virtual environment.},
	author = {Wang, Jia and Lindeman, Robert W.},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223332},
	issn = {2375-5334},
	keywords = {Three-dimensional displays;Context;Virtual environments;Avatars;Integrated circuits;Space exploration;Hybrid virtual environments;cross-task interaction;3D user interface;tablet interface;virtual reality},
	month = {March},
	pages = {111-118},
	title = {Object impersonation: Towards effective interaction in tablet- and HMD-based hybrid virtual environments},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223332}}

@inproceedings{7223333,
	abstract = {Paper documents such as passports, visas and banknotes are frequently checked by inspection of security elements. In particular, view-dependent elements such as holograms are interesting, but the expertise of individuals performing the task varies greatly. Augmented Reality systems can provide all relevant information on standard mobile devices. Hologram verification still takes long and causes considerable load for the user. We aim to address this drawback by first presenting a work flow for recording and automatic matching of hologram patches. Several user interfaces for hologram verification are presented, aiming to noticeably reduce verification time. We evaluate the most promising interfaces in a user study with prototype applications running on off-the-shelf hardware. Our results indicate that there is a significant difference in capture time between interfaces but that users do not prefer the fastest interface.},
	author = {Hartl, Andreas and Grubert, Jens and Reinbacher, Christian and Arth, Clemens and Schmalstieg, Dieter},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223333},
	issn = {2375-5334},
	keywords = {Visualization;Mobile communication;User interfaces;Navigation;Mobile handsets;Inspection;Manuals;Document inspection;holograms;augmented reality;user interfaces;mobile devices},
	month = {March},
	pages = {119-126},
	title = {Mobile user interfaces for efficient verification of holograms},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223333}}

@inproceedings{7223334,
	abstract = {We present an efficient approach for probeless light estimation and coherent rendering of Augmented Reality in dynamic scenes. This approach can handle dynamically changing scene geometry and dynamically changing light sources in real time with a single mobile RGB-D sensor and without relying on an invasive lightprobe. We jointly filter both in-view dynamic geometry and outside-view static geometry. The resulting reconstruction provides the input for efficient global illumination computation in image-space. We demonstrate that our approach can deliver state-of-the-art Augmented Reality rendering effects for scenes that are more scalable and more dynamic than previous work.},
	author = {Gruber, Lukas and Ventura, Jonathan and Schmalstieg, Dieter},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223334},
	issn = {2375-5334},
	keywords = {Geometry;Lighting;Estimation;Rendering (computer graphics);Cameras;Image reconstruction;Image color analysis;Augmented reality;photometric registration;radiance transfer},
	month = {March},
	pages = {127-134},
	title = {Image-space illumination for augmented reality in dynamic environments},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223334}}

@inproceedings{7223335,
	abstract = {We propose a novel approach to generate 4D light field in the physical world for lighting reproduction. The light field is generated by projecting lighting images on a lens array. The lens array turns the projected images into a controlled anisotropic point light source array which can simulate the light field of a real scene. In terms of acquisition, we capture an array of light probe images from a real scene, based on which an incident light field is generated. The lens array and the projectors are geometric and photometrically calibrated, and an efficient resampling algorithm is developed to turn the incident light field into the images projected onto the lens array. The reproduced illumination, which allows per-ray lighting control, can produce realistic lighting result on real objects, avoiding the complex process of geometric and material modeling. We demonstrate the effectiveness of our approach with a prototype setup.},
	author = {Zhou, Zhong and Yu, Tao and Qiu, Xiaofeng and Yang, Ruigang and Zhao, Qinping},
	booktitle = {2015 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:47 -0400},
	date-modified = {2024-03-18 02:29:47 -0400},
	doi = {10.1109/VR.2015.7223335},
	issn = {2375-5334},
	keywords = {Lenses;Lighting;Arrays;Spatial resolution;Light sources;Probes;Calibration;Light field projection;virtual reality;lighting reproduction;realistic lighting;matting and compositing},
	month = {March},
	pages = {135-142},
	title = {Light field projection for lighting reproduction},
	year = {2015},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2015.7223335}}

@inproceedings{6802042,
	abstract = {In this paper we introduce a novel approach for stereoscopic rendering of virtual environments with a wide Field-of-View (FoV) up to 360$\,^{\circ}$. Handling such a wide FoV implies the use of non-planar projections and generates specific problems such as for rasterization and clipping of primitives. We propose a novel pre-clip stage specifically adapted to geometric approaches for which problems occur with polygons spanning across the projection discontinuities. Our approach integrates seamlessly with immersive virtual reality systems as it is compatible with stereoscopy, head-tracking, and multi-surface projections. The benchmarking of our approach with different hardware setups could show that it is well compliant with real-time constraint, and capable of displaying a wide range of FoVs. Thus, our geometric approach could be used in various VR applications in which the user needs to extend the FoV and apprehend more visual information.},
	author = {Ardouin, J{\'e}r{\^o}me and L{\'e}cuyer, Anatole and Marchal, Maud and Marchand, Eric},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802042},
	issn = {2375-5334},
	keywords = {Rendering (computer graphics);Stereo image processing;Geometry;Pipelines;Real-time systems;Equations;Image color analysis;H.5.2 [Information Interfaces and Presentation]: User Interfaces --- user-centered design;I.3.7 [Computer Graphics];Three-Dimensional Graphics and Realism --- Virtual Reality},
	month = {March},
	pages = {3-8},
	title = {Stereoscopic rendering of virtual environments with wide Field-of-Views up to 360$\,^{\circ}$},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802042}}

@inproceedings{6802043,
	abstract = {In the domain of large-scale visualization instruments, hybrid reality environments (HREs) are a recent innovation that combines the best-in-class capabilities of immersive environments, with the best-in-class capabilities of ultra-high-resolution display walls. HREs create a seamless 2D/3D environment that supports both information-rich analysis as well as virtual reality simulation exploration at a resolution matching human visual acuity. Co-located research groups in HREs tend to work on a variety of tasks during a research session (sometimes in parallel), and these tasks require 2D data views, 3D views, linking between them and the ability to bring in (or hide) data quickly as needed. In this paper we present Omegalib, a software framework that facilitates application development on HREs. Omegalib is designed to support dynamic reconfigurability of the display environment, so that areas of the display can be interactively allocated to 2D or 3D workspaces as needed. Compared to existing frameworks and toolkits, Omegalib makes it possible to have multiple immersive applications running on a cluster-controlled display system, have different input sources dynamically routed to applications, and have rendering results optionally redirected to a distributed compositing manager. Omegalib supports pluggable front-ends, to simplify the integration of third-party libraries like OpenGL, OpenSceneGraph, and the Visualization Toolkit (VTK). We present examples of applications developed with Omegalib for the 74-megapixel, 72-tile CAVE2{\texttrademark} system, and show how a Hybrid Reality Environment proved effective in supporting work for a co-located research group in the environmental sciences.},
	author = {Febretti, Alessandro and Nishimoto, Arthur and Mateevitsi, Victor and Renambot, Luc and Johnson, Andrew and Leigh, Jason},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802043},
	issn = {2375-5334},
	keywords = {Three-dimensional displays;Runtime;Rendering (computer graphics);Collaboration;Visualization;Operating systems;Multi-view;Tiled Displays;Cluster;Immersive Environments;Middleware},
	month = {March},
	pages = {9-14},
	title = {Omegalib: A multi-view application framework for hybrid reality display environments},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802043}}

@inproceedings{6802044,
	abstract = {Photorealistic Augmented Reality (AR) requires knowledge of the scene geometry and environment lighting to compute photometric registration. Recent work has introduced probeless photometric registration, where environment lighting is estimated directly from observations of reflections in the scene rather than through an invasive probe such as a reflective ball. However, computing the dense radiance transfer of a dynamically changing scene is computationally challenging. In this work, we present an improved radiance transfer sampling approach, which combines adaptive sampling in image and visibility space with robust caching of radiance transfer to yield real time framerates for photorealistic AR scenes with dynamically changing scene geometry and environment lighting.},
	author = {Gruber, Lukas and Langlotz, Tobias and Sen, Pradeep and H{\"o}herer, Tobias and Schmalstieg, Dieter},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802044},
	issn = {2375-5334},
	keywords = {Rendering (computer graphics);Geometry;Lighting;Estimation;Cameras;Light sources;Real-time systems;Augmented reality;photometric registration;radiance transfer},
	month = {March},
	pages = {15-20},
	title = {Efficient and robust radiance transfer for probeless photorealistic augmented reality},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802044}}

@inproceedings{6802045,
	abstract = {Augmented Reality (AR) applications require knowledge about the real world environment in which they are used. This knowledge is often gathered while developing the AR application and stored for future uses of the application. Consequently, changes to the real world lead to a mismatch between the previously recorded data and the real world. New capturing techniques based on dense Simultaneous Localization and Mapping (SLAM) not only allow users to capture real world scenes at run-time, but also enables them to capture changes of the world. However, instead of using previously recorded and prepared scenes, users must interact with an unprepared environment. In this paper, we present a set of new interaction techniques that support users in handling captured real world environments. The techniques present virtual viewpoints of the scene based on a scene analysis and provide natural transitions between the AR view and virtual viewpoints. We demonstrate our approach with a SLAM based prototype that allows us to capture a real world scene and describe example applications of our system.},
	author = {Tatzgern, Markus and Grasset, Raphael and Kalkofen, Denis and Schmalstieg, Dieter},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802045},
	issn = {2375-5334},
	keywords = {Cameras;Switches;Navigation;Image reconstruction;Geometry;Semantics;Simultaneous localization and mapping;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Artificial, augmented and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces --- Interaction styles},
	month = {March},
	pages = {21-26},
	title = {Transitional Augmented Reality navigation for live captured scenes},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802045}}

@inproceedings{6802046,
	abstract = {Annotations of objects in 3D environments are commonly controlled using view management techniques. State-of-the-art view management strategies for external labels operate in 2D image space. This creates problems, because the 2D view of a 3D scene changes over time, and temporal behavior of elements in a 3D scene is not obvious in 2D image space. We propose managing the placement of external labels in 3D object space instead. We use 3D geometric constraints to achieve label placement that fulfills the desired objectives (e.g., avoiding overlapping labels), but also behaves consistently over time as the viewpoint changes. We propose two geometric constraints: a 3D pole constraint, where labels move along a 3D pole sticking out from the annotated object, and a plane constraint, where labels move in a dominant plane in the world. This formulation is compatible with standard optimization approaches for labeling, but overcomes the lack of temporal coherence.},
	author = {Tatzgern, Markus and Kalkofen, Denis and Grasset, Raphael and Schmalstieg, Dieter},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802046},
	issn = {2375-5334},
	keywords = {Three-dimensional displays;Layout;Cameras;Image resolution;Vectors;Force;Gears;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Artificial, augmented and virtual realities},
	month = {March},
	pages = {27-32},
	title = {Hedgehog labeling: View management techniques for external labels in 3D space},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802046}}

@inproceedings{6802047,
	abstract = {Imagine you are facing a mirror, seeing at the same time both your real body and a virtual display of your brain in activity and perfectly superimposed to your real image ``inside your real skull''. In this paper, we introduce a novel augmented reality paradigm called ``Mind-Mirror'' which enables the experience of seeing ``through your own head'', visualizing your brain ``in action and in situ''. Our approach relies on the use of a semi-transparent mirror positioned in front of a computer screen. A virtual brain is displayed on screen and automatically follows the head movements using an optical face-tracking system. The brain activity is extracted and processed in real-time with the help of an electroencephalography cap (EEG) worn by the user. A rear view is also proposed thanks to an additional webcam recording the rear of the user's head. The use of EEG classification techniques enables to test a Neurofeedback scenario in which the user can train and progressively learn how to control different mental states, such as ``concentrated'' versus ``relaxed''. The results of a user study comparing a standard visualization used in Neurofeedback to our approach showed that the Mind-Mirror could be successfully used and that the participants have particularly appreciated its innovation and originality. We believe that, in addition to applications in Neurofeedback and Brain-Computer Interfaces, the Mind-Mirror could also be used as a novel visualization tool for education, training or entertainment applications.},
	author = {Mercier-Ganady, Jonathan and Lotte, Fabien and Loup-Escande, Emilie and Marchal, Maud and L{\'e}cuyer, Anatole},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802047},
	issn = {2375-5334},
	keywords = {Brain;Electroencephalography;Mirrors;Head;Neurofeedback;Visualization;Surface topography;Augmented Reality;EEG;Brain Activity;Mirror;Visualization;BCI},
	month = {March},
	pages = {33-38},
	title = {The Mind-Mirror: See your brain in action in your head using EEG and augmented reality},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802047}}

@inproceedings{6802048,
	abstract = {In this paper, we introduce a system to capture the enhanced 3D structure of a room-sized dynamic scene with commodity depth cameras such as Microsoft Kinects. It is challenging to capture the entire dynamic room. First, the raw data from depth cameras are noisy due to the conflicts of the room's large volume and cameras' limited optimal working distance. Second, the severe occlusions between objects lead to dramatic missing data in the captured 3D. Our system incorporates temporal information to achieve a noise-free and complete 3D capture of the entire room. More specifically, we pre-scan the static parts of the room offline, and track their movements online. For the dynamic objects, we perform non-rigid alignment between frames and accumulate data over time. Our system also supports the topology changes of the objects and their interactions. We demonstrate the success of our system with various situations.},
	author = {Dou, Mingsong and Fuchs, Henry},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802048},
	issn = {2375-5334},
	keywords = {Three-dimensional displays;Image color analysis;Cameras;Heuristic algorithms;Tracking;Topology;Calibration},
	month = {March},
	pages = {39-44},
	title = {Temporally enhanced 3D capture of room-sized dynamic scenes with commodity depth cameras},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802048}}

@inproceedings{6802049,
	abstract = {We present a novel VR solution for Reminiscence Therapy (RT), developed jointly by a group of memory clinicians and computer scientists. RT involves the discussion of past activities, events or experiences with others, often with the aid of tangible props which are familiar items from the past; it is a popular intervention in dementia care. We introduce an immersive VR system designed for RT, which allows easy presentation of familiar environments. In particular, our system supports highly-realistic Image-Based Rendering in an immersive setting. To evaluate the effectiveness and utility of our system for RT, we perform a study with healthy elderly participants to test if our VR system can help with the generation of autobiographical memories. We adapt a verbal Autobiographical Fluency protocol to our VR context, in which elderly participants are asked to generate memories based on images they are shown. We compare the use of our image-based system for an unknown and a familiar environment. The results of our study show that the number of memories generated for a familiar environment is higher than that for an unknown environment using our system. This indicates that IBR can convey familiarity of a given scene, which is an essential requirement for the use of VR in RT. Our results also show that our system is as effective as traditional RT protocols, while acceptability and motivation scores demonstrate that our system is well tolerated by elderly participants.},
	author = {Chapoulie, Emmanuelle and Guerchouche, Rachid and Petit, Pierre-David and Chaurasia, Gaurav and Robert, Philippe and Drettakis, George},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802049},
	issn = {2375-5334},
	keywords = {Senior citizens;Navigation;Rendering (computer graphics);Cameras;Three-dimensional displays;Security;Reminiscence Therapy;Image-Based Rendering;Virtual Reality;Immersive Environments;Autobiographical Fluency},
	month = {March},
	pages = {45-50},
	title = {Reminiscence Therapy using Image-Based Rendering in VR},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802049}}

@inproceedings{6802050,
	abstract = {Selection of moving targets is a common, yet complex task in human-computer interaction (HCI) and virtual reality (VR). Predicting user intention may be beneficial to address the challenges inherent in interaction techniques for moving-target selection. This article extends previous models by integrating relative head-target and hand-target features to predict intended moving targets. The features are calculated in a time window ending at roughly two-thirds of the total target selection time and evaluated using decision trees. With two targets, this model is able to predict user choice with up to ~ 72% accuracy on general moving-target selection tasks and up to ~ 78% by also including task-related target properties.},
	author = {Casallas, Juan Sebastian and Oliver, James H. and Kelly, Jonathan W. and Merienne, Frederic and Garbaya, Samir},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802050},
	issn = {2375-5334},
	keywords = {Accuracy;Predictive models;Decision trees;Three-dimensional displays;Solid modeling;Virtual reality;Human computer interaction;H.5.2 [Information interfaces and presentation]: User Interfaces --- Interaction Styles, Theory and methods;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism --- Virtual Reality;I.5.4 [Pattern Recognition]: Applications},
	month = {March},
	pages = {51-56},
	title = {Using relative head and hand-target features to predict intention in 3D moving-target selection},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802050}}

@inproceedings{6802051,
	abstract = {This study evaluates the effectiveness of an AR-based context-aware assembly support system with AR visualization modes proposed in object assembly. Although many AR-based assembly support systems have been proposed, few keep track of the assembly status in real-time and automatically recognize error and completion states at each step. Naturally, the effectiveness of such context-aware systems remains unexplored. Our test-bed system displays guidance information and error detection information corresponding to the recognized assembly status in the context of building block (LEGO) assembly. A user wearing a head mounted display (HMD) can intuitively build a building block structure on a table by visually confirming correct and incorrect blocks and locating where to attach new blocks. We proposed two AR visualization modes, one of them that displays guidance information directly overlaid on the physical model, and another one in which guidance information is rendered on a virtual model adjacent to the real model. An evaluation was conducted to comparatively evaluate these AR visualization modes as well as determine the effectiveness of context-aware error detection. Our experimental results indicate the visualization mode that shows target status next to real objects of concern outperforms the traditional direct overlay under moderate registration accuracy and marker-based tracking.},
	author = {Khuong, Bui Minh and Kiyokawa, Kiyoshi and Miller, Andrew and La Viola, Joseph J. and Mashita, Tomohiro and Takemura, Haruo},
	booktitle = {2014 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:40 -0400},
	date-modified = {2024-03-18 02:29:40 -0400},
	doi = {10.1109/VR.2014.6802051},
	issn = {2375-5334},
	keywords = {Assembly;Visualization;Real-time systems;Solid modeling;Educational institutions;Three-dimensional displays;Augmented reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Artificial, augmented and virtual realities;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems --- Evaluation/methodology},
	month = {March},
	pages = {57-62},
	title = {The effectiveness of an AR-based context-aware assembly support system in object assembly},
	year = {2014},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2014.6802051}}

@inproceedings{6549347,
	abstract = {In Augmented Reality (AR), careless augmentations can easily lead to information overflow. Especially on small screen devices, only a limited amount of information can be displayed comprehensively. Compact visualization filters data by reducing redundancies and creating a layout of the remaining information. Previously, this approach was applied to create static compact explosion diagrams. In this paper, we extend the approach to annotations, which are a major source of information in AR, and create compact layouts of annotations and annotated explosion diagrams. We present methods to transfer compact visualizations to dynamic AR settings and achieve interactive frame rates even on limited-resource hardware, such as mobile phones. Moreover, we create temporally coherent and scene-aware layouts.},
	author = {Tatzgern, Markus and Kalkofen, Denis and Schmalstieg, Dieter},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549347},
	issn = {2375-5334},
	keywords = {Layout;Explosions;Data visualization;Optimization;Cameras;Augmented reality;Redundancy;Augmented Reality;Visualization},
	month = {March},
	pages = {3-6},
	title = {Dynamic compact visualizations for augmented reality},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549347}}

@inproceedings{6549348,
	abstract = {We introduce a taxonomy for mixed simulation focusing on mixed simulators with physical exteriors augmented with virtual underlays for practicing medical procedures such as central venous access (CVA). We used CT and MRI imaging and 3D printing to develop anatomically authentic mixed simulators, i.e., exact physical and/or virtual replicas of their human models. Embedded 6 DOF magnetic sensors monitor tracked instruments during simulated procedures, facilitating after action review or self-debriefing. We implemented automated scoring algorithms that include tracking and grading of near misses. After 28 anesthesia residents trained with the CVA simulator, incidence of pneumothorax and arterial puncture in the simulated environment dropped from 11 % to 7% and 13% to 7%, respectively.},
	author = {Larnpotang, Samsun and Lizdas, David and Rajon, Didier and Luria, Isaac and Gravenstein, Nikolaus and Bisht, Yashwant and Schwab, Wilhelm and Friedman, William and Bova, Frank and Robinson, Albert},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549348},
	issn = {2375-5334},
	keywords = {Needles;Solid modeling;Three-dimensional displays;Veins;Cameras;Training;Taxonomy;Mixed simulation;taxonomy;virtual underlays;medical procedural simulators},
	month = {March},
	pages = {7-10},
	title = {Mixed simulators: Augmented physical simulators with virtual underlays},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549348}}

@inproceedings{6549349,
	abstract = {We introduce the concept of the infinite canvas as a metaphor for the immersive visual exploration of very large image datasets using a natural walking interface. The interface allows the user to move along the display surface and to be continuously exposed to new data, essentially exploring the horizontal axis of an arbitrarily long canvas. Our system provides a spiral navigation interface that shows a compressed immersive overview of the data and facilitates the rapid and fluid transition to distant points within the infinite canvas. We demonstrate the implementation of the infinite canvas in the world's first 1.5 billion pixel tiled immersive display.},
	author = {Petkov, Kaloian and Papadopoulos, Charilaos and Kaufman, Arie E.},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549349},
	issn = {2375-5334},
	keywords = {Spirals;Navigation;Legged locomotion;Visualization;Data visualization;Three-dimensional displays;Virtual environments},
	month = {March},
	pages = {11-14},
	title = {Visual exploration of the infinite canvas},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549349}}

@inproceedings{6549350,
	abstract = {This paper describes the impact of display resolution and luminance on the responses of participants in a behavioral study that used a projection-based Immersive Virtual Reality System. The scenario was a virtual bar where participants witnessed a violent attack of one person on another due to an argument about support for a soccer club. The major response variable was the number of interventions made by participants. The study was between-groups with 10 participants in two groups pre-upgrade and post-upgrade, both in the same 4-screen Cave-like system. However, the post-upgrade group experienced the scenario with projectors that had a significantly higher level of resolution and luminance than those experienced by the pre-upgrade group. The results show that, other things being equal, the number of both verbal and physical interventions was greater amongst those in the post-upgrade group compared to the pre-upgrade group.},
	author = {Rovira, Aitor and Swapp, David and Southern, Richard and Zhang, Jian J. and Slater, Mel},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549350},
	issn = {2375-5334},
	keywords = {Educational institutions;Virtual environments;Image resolution;Safety;Cathode ray tubes;User studies;large format displays;Cave system;virtual characters;presence;bystander behavior},
	month = {March},
	pages = {15-18},
	title = {The impact of enhanced projector display on the responses of people to a violent scenario in immersive virtual reality},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549350}}

@inproceedings{6549351,
	abstract = {In this paper we present a novel interaction framework for augmented reality, and demonstrate its application in an interactive AR exposure treatment system for the fear of spiders. We use data from the Microsoft Kinect to track and model real world objects in the AR environment, enabling realistic interaction between them and virtual content. Objects are tracked in three dimensions using the Iterative Closest Point algorithm and a point cloud model of the objects is incrementally developed. The approximate motion and shape of each object in the scene serve as inputs to the AR application. Very few restrictions are placed on the types of objects that can be used. In particular, we do not require objects to be marked in a certain way in order to be recognized, facilitating natural interaction. To demonstrate our interaction framework we present an AR exposure treatment system where virtual spiders can walk up, around, or behind real objects and can be carried, prodded and occluded by the user. We also discuss improvements we are making to the interaction framework and its potential for use in other applications.},
	author = {Corbett-Davies, Sam and Dunser, Andreas and Green, Richard and Clark, Adrian},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549351},
	issn = {2375-5334},
	keywords = {Augmented reality;Three-dimensional displays;Iterative closest point algorithm;Tracking;Cameras;Real-time systems;Computational modeling;Augmented reality;3D interaction;Kinect;environment awareness;exposure treatment},
	month = {March},
	pages = {19-22},
	title = {An advanced interaction framework for augmented reality based exposure treatment},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549351}}

@inproceedings{6549352,
	abstract = {In this paper we propose a general-purpose telepresence system design that can be adapted to a wide range of scenarios and present a framework for a proof-of-concept prototype. The prototype system allows users to see remote participants and their surroundings merged into the local environment through the use of an optical see-through head-worn display. Real-time 3D acquisition and head tracking allows the remote imagery to be seen from the correct point of view and with proper occlusion. A projector-based lighting control system permits the remote imagery to appear bright and opaque even in a lit room. Immersion can be adjusted across the VR continuum. Our approach relies only on commodity hardware; we also experiment with wider field of view custom displays.},
	author = {Maimone, Andrew and Yang, Xubo and Dierk, Nate and State, Andrei and Dou, Mingsong and Fuchs, Henry},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549352},
	issn = {2375-5334},
	keywords = {Three-dimensional displays;Optical imaging;Optical sensors;Lighting control;Prototypes;Adaptive optics;Geometry;teleconferencing;augmented reality;virtual reality},
	month = {March},
	pages = {23-26},
	title = {General-purpose telepresence with head-worn optical see-through displays and projector-based lighting},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549352}}

@inproceedings{6549353,
	abstract = {One of the problems in using head-mounted displays (HMDs) is that the virtual images shown through the HMDs are usually distorted due to their optical distortions. In order to correctly compensate the optical distortions through a pre distortion technique, accurate values of the distortion parameters are required. Although several distortion calibration methods have been developed in prior work, these methods suffer a few critical limitations. In this paper, we proposed a method for robustly estimating the optical distortion parameters of both immersive and (optical) see-through HMDs, without the limitations of existing methods. The proposed method is based on photogrammetry and considers not only the radial distortion but also the tangential distortion. Its effectiveness was evaluated through an experiment conducted with two different types of HMDs, an optical see-through head-mounted projection display and a commercially available non-see-through HMD. According to the experimental results, the proposed method showed significantly lower reprojection error than a previous method proposed by Owen et al. In addition, when both the radial and tangential distortions were considered, the proposed method was significantly more effective than when only the radial distortion was considered.},
	author = {Lee, Sangyoon and Hua, Hone},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549353},
	issn = {2375-5334},
	keywords = {Optical distortion;Cameras;Calibration;Lenses;Mathematical model;Optical imaging;Nonlinear distortion;Head-mounted display;optical distortion;display calibration;photogrammetry},
	month = {March},
	pages = {27-30},
	title = {A robust camera-based method for optical distortion calibration of head-mounted displays},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549353}}

@inproceedings{6549354,
	abstract = {Characters with body materials that are different from that of humans, such as metal robots or rubber people, frequently appear in movies and comics. While the abilities of their synthetic bodies can be easily observed from their actions, their somatic sensations are more difficult to appreciat e. Our aim in this work is to simulate the alteration of the material of the human body by means of vibrotactile feedback. The feedback represents the properties of the materials and is periodically applied to the elbow joint in synchrony with the elbow angle. This simulated sensation of having a different body material gives us the feeling of those characters. This technique can also be applied to improve maneuverability in the teleoperation of master-slave systems because it gives the operator a robot-like sensation.},
	author = {Kurihara, Yosuke and Hachisu, Taku and Sato, Michi and Fukushima, Shogo and Kajimoto, Hiroyuki},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549354},
	issn = {2375-5334},
	keywords = {Materials;Elbow;Vibrations;Rubber;Haptic interfaces;Aluminum;Joints;Body sense;material;vibrotactile feedback;virtual reality},
	month = {March},
	pages = {31-34},
	title = {Virtual alteration of body material by periodic vibrotactile feedback},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549354}}

@inproceedings{6549355,
	abstract = {Recent virtual grasping approaches involve physical simulation and virtual couplings between tracked and virtual hand configurations. We introduce a nonuniform coupling in which the stiffness of thumb coupling is scaled relative to that of other digits. This shifts the position of grasped objects in the hand, which may impact grasp and release performance. We graphically illustrate the effects on grasped object position, and we experimentally measure impact on object motion during grasp release. In addition to basic nonuniform scaling, we propose adaptive scaling to account for the number and depth of digits involved in multi-finger grasps. We show that one particular choice of adaptive coupling results in a tradeoff of increased object position consistency and decreased release consistency. The knowledge gained from our study will enable researchers to optimize couplings in future work.},
	author = {Borst, Christoph W. and Prachyabrued, Mores},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549355},
	issn = {2375-5334},
	keywords = {Thumb;Couplings;Grasping;Springs;Three-dimensional displays;Force;Virtual grasping;virtual coupling;grasping forces},
	month = {March},
	pages = {35-38},
	title = {Nonuniform and adaptive coupling stiffness for virtual grasping},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549355}}

@inproceedings{6549356,
	abstract = {In this work we present a new scalable map for middle-scale locative games. Our map is built upon the recent development of fiducial markers, specifically, the random dot markers. We propose a simple solution, i.e., using a grid of compound markers, to address the scalability problem. Our highly scalable approach is able to generate a middle-scale map on which multiple players can stand and position themselves via mobile cameras in real time. We show how a classic computer game can be effectively adapted to our middle-scale gaming platform.},
	author = {Chen, Lu and Fu, Hongbo and Andy Li, Wing Ho and Taj, Chiew-Lan},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549356},
	issn = {2375-5334},
	keywords = {Games;Cameras;Compounds;Accuracy;Augmented reality;Memory management;H.5.l [Information Interfaces And Presentation]: Multimedia Information Systems-Artificial, augmented and virtual realities;I.4.9 [Computing Methodologies]: Image Processing and Computer Vision-Applications},
	month = {March},
	pages = {39-42},
	title = {Scalable maps of random dots for middle-scale locative mobile games},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549356}}

@inproceedings{6549357,
	abstract = {Augmented Reality systems that run interactively and in real time, using high quality graphical displays and sensational cues, can create the illusion of virtual objects appearing to be real. This paper presents the design, implementation, and evaluation of BurnAR, a novel demonstration which enables users to experience the illusion of seeing their own hands burning, which we achieve by overlaying virtual flames and smoke on their hands. Surprisingly, some users reported an involuntary warming sensation of their hands. Based on these comments, we hypothesized that stimulation of multiple sensory modalities presented in this AR environment can induce an involuntary experience in an additional sensory pathway: observation of virtual flames resulting in a heat sensation. This cross-modal transfer, known as virtual synesthe-sia, is a temporary experience which affects some people who are not synesthetes and only lasts for a short time during the illusory experience. To verify our hypothesis, we conducted an exploratory study where participants experienced the BurnAR demonstration under controlled conditions.},
	author = {Weir, Peter and Sandor, Christian and Swoboda, Matt and Nguyen, Thanh and Eck, Ulrich and Reitmayr, Gerhard and Day, Arindam},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549357},
	issn = {2375-5334},
	keywords = {Heating;Visualization;Virtual environments;Three-dimensional displays;Augmented reality;Streaming media;H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems-[Artificial, augmented and virtual realities];H.l.2. [Information Systems]: Models and Principles-[Human factors]},
	month = {March},
	pages = {43-46},
	title = {Burnar: Involuntary heat sensations in augmented reality},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549357}}

@inproceedings{6549358,
	abstract = {The typical registration process in augmented reality CAR) consists of three independent consecutive stages: static calibration, dynamic tracking, and graphics overlay. The result is that the real-virtual registration is ``open loop''-inaccurate calibration or tracking leads to misregistration that is seen by the users but not the system. To cope with this, we propose a general approach to ``close the loop'' in the displayed appearance by using the visual feedback of registration for pose tracking to achieve accurate registration. Specifically, a model-based method is introduced to simultaneously track and augment real objects in a closed-loop fashion, where the model is comprised of the combination of the real object to be tracked and the virtual object to be rendered. This method is applicable to paradigms including video-based AR, projector-based AR, and diminished reality. Both qualitative and quantitative experiments are presented to demonstrate the feasibility and effectiveness of our approach.},
	author = {Zheng, Feng and Schubert, Ryan and Weich, Greg},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549358},
	issn = {2375-5334},
	keywords = {Calibration;Three-dimensional displays;Mathematical model;Solid modeling;Cameras;Equations;Augmented reality;Closed-loop registration;visual feedback;tracking;projector-based AR;video-based AR;diminished reality},
	month = {March},
	pages = {47-50},
	title = {A general approach for closed-loop registration in AR},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549358}}

@inproceedings{6549359,
	abstract = {Flight simulation training devices, unless equipped with high-fidelity, panoramic field of view visual displays, are restricted in their usage for training of visual flight maneuvers. An exploratory experiment was conducted in a fixed-based simulator with eight pi-lots. The common, static triple display was tested against a single display augmented with amplified head pitch and yaw rotations. Participants started airborne in a visual circuit pattern and performed a base-to-final turn and full stop landing whilst keeping a visual scan for traffic in the vicinity. Results showed that there was a significant difference between displays: turns were initiated closer to the centerline with larger bank angles on the augmented single display. There was no degradation in vertical and cross-track error and neither in response time with the augmented setup. It also did not result in any significant extra workload or simulator sick-ness. These findings demonstrate the potential value of upgrading flight training devices and remotely piloted vehicle ground stations to improve their utility and pave the way for future research into this domain.},
	author = {Ngoc, Luan Le and Kalawsky, Roy S.},
	booktitle = {2013 IEEE Virtual Reality (VR)},
	date-added = {2024-03-18 02:29:35 -0400},
	date-modified = {2024-03-18 02:29:35 -0400},
	doi = {10.1109/VR.2013.6549359},
	issn = {2375-5334},
	keywords = {Visualization;Training;Aircraft;Virtual reality;Atmospheric measurements;Particle measurements;Turning;Amplified head rotations;FOV;flight simulator;virtual reality;visual scanning;pilot training},
	month = {March},
	pages = {51-54},
	title = {Evaluating usability of amplified head rotations on base-to-final turn for flight simulation training devices},
	year = {2013},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2013.6549359}}

@inproceedings{5759416,
	abstract = {Models of interaction tasks are quantitative descriptions of relationships between human temporal performance and the spatial characteristics of the interactive tasks. Examples include Fitts' law for modeling the pointing task and Accot and Zhai's steering law for the path steering task, etc. Models can be used as guidelines to design efficient user interfaces and quantitatively evaluate interaction techniques and input devices. In this paper, we introduce a 3D object pursuit interaction task, in which users are required to continuously track a moving target in a virtual environment. The entire movement of the task is broken into a tracking phase and a correction phase. For each phase, we propose a model that has been verified by two experiments. As the experimental results show, the time for the tracking phase is fixed once a task has been established, while the time for the correction phase usually varies according to some characteristics of the task. It can be modeled as a function of path length, target width and the velocity with which the target moves. The proposed model can be used to quantitatively evaluate the efficiency of user interfaces that involve the interaction with moving objects.},
	author = {Liu, Lei and van Liere, Robert},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759416},
	issn = {2375-5334},
	keywords = {Mathematical model;Target tracking;Solid modeling;Three dimensional displays;Equations;Data models},
	month = {March},
	pages = {3-10},
	title = {Modeling object pursuit for 3D interactive tasks in virtual reality},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759416}}

@inproceedings{5759430,
	abstract = {We developed a new hand model for increasing the robustness of finger-based manipulations of virtual objects. Each phalanx of our hand model consists of a number of deformable soft bodies, which dynamically adapt to the shape of grasped objects based on the applied forces. Stronger forces directly result in larger contact areas, which increase the friction between hand and object as would occur in reality. For a robust collision-based soft body simulation, we extended the lattice-shape matching algorithm to work with adaptive stiffness values, which are dynamically derived from force and velocity thresholds. Our implementation demonstrates that this approach allows very precise and robust grasping, manipulation and releasing of virtual objects and performs in real-time for a variety of complex scenarios. Additionally, laborious tuning of object and friction parameters is not necessary for the wide range of objects that we typically grasp with our hands.},
	author = {Jacobs, Jan and Froehlich, Bernd},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759430},
	issn = {2375-5334},
	keywords = {Grasping;Robustness;Force;Friction;Adaptation model;Shape},
	month = {March},
	pages = {11-18},
	title = {A soft hand model for physically-based manipulation of virtual objects},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759430}}

@inproceedings{5759431,
	abstract = {We present a novel technique implementing barehanded interaction with virtual 3D content by employing a time-of-flight camera. The system improves on existing 3D multi-touch systems by working regardless of lighting conditions and supplying a working volume large enough for multiple users. Previous systems were limited either by environmental requirements, working volume, or computational resources necessary for realtime operation. By employing a time-of-flight camera, the system is capable of reliably recognizing gestures at the finger level in real-time at more than 50 fps with commodity computer hardware using our newly developed precision hand and finger-tracking algorithm. Building on this algorithm, the system performs gesture recognition with simple constraint modeling over statistical aggregations of the hand appearances in a working volume of more than 8 cubic meters. Two iterations of user tests were performed on a prototype system, demonstrating the feasibility and usability of the approach as well as providing first insights regarding the acceptance of true barehanded touch-based 3D interaction.},
	author = {Hackenberg, Georg and McCall, Rod and Broll, Wolfgang},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759431},
	issn = {2375-5334},
	keywords = {Fingers;Feature extraction;Three dimensional displays;Pixel;Cameras;Transforms;Real time systems;3D user interfaces;hand tracking;computer vision;multi-touch;gesture recognition},
	month = {March},
	pages = {19-26},
	title = {Lightweight palm and finger tracking for real-time 3D gesture control},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759431}}

@inproceedings{5759432,
	abstract = {The parameter estimation variance of the Single Point Active Alignment Method (SPAAM) is studied through an experiment where 11 subjects are instructed to create alignments using an Optical See-Through Head Mounted Display (OSTHMD) such that three separate correspondence point distributions are acquired. Modeling the OSTHMD and the subject's dominant eye as a pinhole camera, findings show that a correspondence point distribution well distributed along the user's line of sight yields less variant parameter estimates. The estimated eye point location is studied in particular detail. The findings of the experiment are complemented with simulated data which show that image plane orientation is sensitive to the number of correspondence points. The simulated data also illustrates some interesting properties on the numerical stability of the calibration problem as a function of alignment noise, number of correspondence points, and correspondence point distribution.},
	author = {Axholt, Magnus and Cooper, Matthew D. and Skoglund, Martin A. and Ellis, Stephen R. and O'Connell, Stephen D. and Ynnerman, Anders},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759432},
	issn = {2375-5334},
	keywords = {Cameras;Calibration;Noise;Pixel;Transmission line matrix methods;Optical sensors;Optical imaging;single point active alignment method;camera resectioning;calibration;optical see-through head mounted display;augmented reality},
	month = {March},
	pages = {27-34},
	title = {Parameter estimation variance of the single point active alignment method in optical see-through head mounted display calibration},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759432}}

@inproceedings{5759433,
	abstract = {This paper presents a novel approach for detecting and tracking markers with randomly scattered dots for augmented reality applications. Compared with traditional markers with square pattern, our random dot markers have several significant advantages for flexible marker design, robustness against occlusion and user interaction. The retrieval and tracking of these markers are based on geometric feature based keypoint matching and tracking. We experimentally demonstrate that the discriminative ability of forty random dots per marker is applicable for retrieving up to one thousand markers.},
	author = {Uchiyama, Hideaki and Saito, Hideo},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759433},
	issn = {2375-5334},
	keywords = {Virtual reality},
	month = {March},
	pages = {35-38},
	title = {Random dot markers},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759433}}

@inproceedings{5759434,
	abstract = {Motion perception in immersive virtual reality environments significantly differs from the real world. For example, previous work has shown that users tend to underestimate travel distances in immersive virtual environments (VEs). As a solution to this problem, some researchers propose to scale the mapped virtual camera motion relative to the tracked real-world movement of a user until real and virtual motion appear to match, i. e., real-world movements could be mapped with a larger gain to the VE in order to compensate for the underestimation. Although this approach usually results in more accurate self-motion judgments by users, introducing discrepancies between real and virtual motion can become a problem, in particular, due to misalignments of both worlds and distorted space cognition. In this paper we describe a different approach that introduces apparent self-motion illusions by manipulating optic flow fields during movements in VEs. These manipulations can affect self-motion perception in VEs, but omit a quantitative discrepancy between real and virtual motions. We introduce four illusions and show in experiments that optic flow manipulation can significantly affect users' self-motion judgments. Furthermore, we show that with such manipulation of optic flow fields the underestimation of travel distances can be compensated.},
	author = {Bruder, Gerd and Steinicke, Frank and Wieland, Phil},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759434},
	issn = {2375-5334},
	keywords = {Visualization;Optical sensors;Cameras;Integrated optics;Blindness;Optical distortion;Detectors;Self-motion perception;visual illusions;optic flow},
	month = {March},
	pages = {39-46},
	title = {Self-motion illusions in immersive virtual reality environments},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759434}}

@inproceedings{5759435,
	abstract = {While modern dynamic driving simulators equipped with six degrees-of-freedom (6-DOF) hexapods and X-Y platforms have improved realism, mechanical limitations prevent them from offering a fully realistic driving experience. Solutions are often sought in the ''washout'' algorithm, with linear accelerations simulated by an empirically chosen combination of translation and tilt-coordination, based on the incapacity of otolith organs to distinguish between inclination of the head and linear acceleration. In this study, we investigated the most effective combination of tilt and translation to provide a realistic perception of movement. We tested 3 different braking intensities (deceleration), each with 5 inverse proportional tilt/translation ratios. Subjects evaluated braking intensity using an indirect method corresponding to a 2-Alternative-Forced-Choice Paradigm. We find that perceived intensity of braking depends on the tilt/translation ratio used: for small and average decelerations (0.6 and 1.0m/s2), increased tilt yielded an increased overestimation of braking, inverse proportionally with intensity; for high decelerations (1.4m/s2), on half the conditions braking was overestimated with more tilt than translation and underestimated with more translation than tilt. We define a mathematical function describing the relationship between tilt, translation and the desired level of deceleration, intended as a supplement to motion cueing algorithms, that should improve the realism of driving simulations.},
	author = {Stratulat, Anca and Roussarie, Vincent and Vercher, Jean-Louis and Bourdin, Christophe},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759435},
	issn = {2375-5334},
	keywords = {Acceleration;Visualization;Solid modeling;Dynamics;Heuristic algorithms;Driver circuits;Roads;Dynamic driving simulator;braking perception;washout algorithm;tilt-coordination;vestibular system;multisen-sory integration},
	month = {March},
	pages = {47-50},
	title = {Improving the realism in motion-based driving simulators by adapting tilt-translation technique to human perception},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759435}}

@inproceedings{5759436,
	abstract = {In a mixed-reality (MR) environment, the appearance of touchable objects can be changed by superimposing a computer-generated image (CGI) onto them (MR visual stimulation). At the same time, when humans sense the hardness of real objects, it is known that their perception is influenced not only by tactile information but also by visual information. In this paper, we studied the psychophysical influence on the sense of hardness by using a real object that has a CGI superimposed on it. In this experiment, we deform in an extreme way the CGI animation on the real object, while the subject pushes the real object using his/her finger. The results of the experiments found that human subjects sensed different hardnesses by emphasizing the dent deformation of the CGI animation.},
	author = {Hirano, Yuichi and Kimura, Asako and Shibata, Fumihisa and Tamura, Hideyuki},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759436},
	issn = {2375-5334},
	keywords = {Visualization;Animation;Virtual reality;Psychology;Humans;Fingers;Haptic interfaces;Mixed Reality;Sense of Hardness;Psychophysical Influence;Visual Stimulation},
	month = {March},
	pages = {51-54},
	title = {Psychophysical influence of mixed-reality visual stimulation on sense of hardness},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759436}}

@inproceedings{5759437,
	abstract = {We report on a user study evaluating Redirected Free Exploration with Distractors (RFED), a large-scale, real-walking, locomotion interface, by comparing it to Walking-in-Place (WIP) and Joystick (JS), two common locomotion interfaces. The between-subjects study compared navigation ability in RFED, WIP, and JS interfaces in VEs that are more than two times the dimensions of the tracked space. The interfaces were evaluated based on navigation and wayfinding metrics and results suggest that participants using RFED were significantly better at navigating and wayfinding through virtual mazes than participants using walking-in-place and joystick interfaces. Participants traveled shorter distances, made fewer wrong turns, pointed to hidden targets more accurately and more quickly, and were able to place and label targets on maps more accurately. Moreover, RFED participants were able to more accurately estimate VE size.},
	author = {Peck, Tabitha C. and Fuchs, Henry and Whitton, Mary C.},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759437},
	issn = {2375-5334},
	keywords = {Navigation;Tracking;Legged locomotion;Training;Bars;Cognitive science;Turning},
	month = {March},
	pages = {55-62},
	title = {An evaluation of navigational ability comparing Redirected Free Exploration with Distractors to Walking-in-Place and joystick locomotio interfaces},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759437}}

@inproceedings{5759438,
	abstract = {In this paper, we present a unified approach for a drift-free and jitter-reduced vision-aided navigation system. This approach is based on an error-state Kalman filter algorithm using both relative (local) measurements obtained from image based motion estimation through visual odometry, and global measurements as a result of landmark matching through a pre-built visual landmark database. To improve the accuracy in pose estimation for augmented reality applications, we capture the 3D local reconstruction uncertainty of each landmark point as a covariance matrix and implicity rely more on closer points in the filter. We conduct a number of experiments aimed at evaluating different aspects of our Kalman filter framework, and show our approach can provide highly-accurate and stable pose both indoors and outdoors over large areas. The results demonstrate both the long term stability and the overall accuracy of our algorithm as intended to provide a solution to the camera tracking problem in augmented reality applications.},
	author = {Oskiper, Taragay and Chiu, Han-Pang and Zhu, Zhiwei and Samaresekera, Supun and Kumar, Rakesh},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759438},
	issn = {2375-5334},
	keywords = {Cameras;Three dimensional displays;Visualization;Kalman filters;Measurement uncertainty;Mathematical model;Augmented reality},
	month = {March},
	pages = {63-70},
	title = {Stable vision-aided navigation for large-area augmented reality},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759438}}

@inproceedings{5759439,
	abstract = {We present a recognition-driven navigation system for large-scale 3D virtual environments. The proposed system contains three parts, virtual environment reconstruction, feature database building and recognition-based navigation. The virtual environment is reconstructed automatically with LIDAR data and aerial images. The feature database is composed of image patches with features and registered location and orientation information. The database images are taken at different distances from the scenes with various viewing angles, and these images are then partitioned into smaller patches. When a user navigates the real world with a handheld camera, the captured image is used to estimate its location and orientation. These location and orientation information are also reflected in the virtual environment. With the proposed patch approach, the recognition is robust to large occlusions and can be done in real time. Experiments show that our proposed navigation system is efficient and well synchronized with real world navigation.},
	author = {Guan, Wei and You, Suya and Neumann, Ulrich},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759439},
	issn = {2375-5334},
	keywords = {Navigation;Cameras;Databases;Three dimensional displays;Virtual environment;Buildings;Visualization},
	month = {March},
	pages = {71-74},
	title = {Recognition-driven 3D navigation in large-scale virtual environments},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759439}}

@inproceedings{5759440,
	abstract = {The use of Touch Screen Interaction (TSI) for 3-D interaction entails both the addition of new haptic cues and the separation of the manipulation of the Degrees of Freedom (DoF) of the task: a 3 DoF task must be transformed into a 2-D+1-D task to be completed using a touch screen. In this paper, we investigate the impact of these two factors in the context of a 3-D positioning task. Our goal is to identify their respective influence on subjective preferences and performance measurements. To that purpose, we conducted an experimental comparison of five positioning techniques, isolating the influence of each of these two factors. The results we obtained suggest that the addition of haptic cues does not influence the user precision. However, the decomposition of the task has a strong influence on accuracy. More precisely, separating the manipulation of the depth dimension leads to an increased precision while isolating other dimensions does not influence the results. To explain this result, we realised a behavioural analysis of the data. This study suggests that the differences in the performance may be linked to the perceptual structure of the techniques. A technique isolating the manipulation of the depth seems to have a more adapted perceptual structure than a technique separating the height, even if those two dimensions are equally involved in the realisation of the task.},
	author = {Veit, Manuel and Capobianco, Antonio and Bechmann, Dominique},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759440},
	issn = {2375-5334},
	keywords = {DH-HEMTs;Delta modulation;Haptic interfaces;Performance evaluation;Indexes;Mice;Virtual reality;Interaction;Virtual Reality;Degrees of Freedom;User Study},
	month = {March},
	pages = {75-82},
	title = {An experimental analysis of the impact of Touch Screen Interaction techniques for 3-D positioning tasks},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759440}}

@inproceedings{5759441,
	abstract = {Using virtual reality for understanding sports performance allows for systematic investigation of human sensorimotor capabilities and meanwhile promotes the design and comparison of realistic immersive platforms. In this paper, we propose a virtual reality-based experimental design for studying the human ability to intercept spinning balls deflected by the Magnus effect. Compared to the previous approaches, we focused on a tight perception-action coupling. Experienced and novice subjects immersed in a 3D soccer stadium were asked to head realistically simulated balls, free kicked with and without sidespin. Consistent with the former studies, qualitative results show that the interception performance systematically relates to both the ball sidespin direction and arrival position for all the subjects, either experienced or not. However, contrary to those former studies where subjects answered only pseudo-verbally, experienced and novice groups differentiate in quantitative performances, supporting that expertise likely appears when perception is coupled to action. Further analyses will be needed to extract the different information-movement relationships governing the behaviors of experienced subjects and novices.},
	author = {Hoinville, Thierry and Naceri, Abdeldiallil and Ortiz, Jes{\'u}s and Bernier, Emmanuel and Chellali, Ryad},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759441},
	issn = {2375-5334},
	keywords = {Trajectory;Spinning;Humans;Head;Couplings;Three dimensional displays;Virtual reality},
	month = {March},
	pages = {83-86},
	title = {Performances of experienced and novice sportball players in heading virtual spinning soccer balls},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759441}}

@inproceedings{5759442,
	abstract = {International Workshop Agreement 3 organized by the International Standard Organization calls for more research to determine simple objective ways to assess susceptibility to visually induce motion sickness (VIMS) without making viewers sick (So and Ujike, 2010). This study examines the use of measurable optokinetic afternystagmus (OKAN) parameters to predict susceptibility to VIMS. Eighteen participants were recruited. They were exposed to a sickness provoking virtual rotating drum (210 degrees field-of-view) with striped patterns rotating at 60 degrees per second for 30 minutes (Phase 1). Sickness data were collected before, during, and after the exposure. These participants were invited back for OKAN measurements at least two weeks after Phase 1 was completed to minimize any adaption effect (Phase 2). Out of the 18 participants, 10 participants (i.e., 55%) exhibited consistent patterns of OKAN. Correlations between the time constants of OKAN and levels of VIMS experienced by the same viewers were found. The possibility of using OKAN as an objective indicator of the susceptibility to visually induced motion sickness is discussed.},
	author = {Guo, Cuiting and Ji, Jennifer and So, Richard},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759442},
	issn = {2375-5334},
	keywords = {Correlation;Particle measurements;Atmospheric measurements;Electrooculography;Head;Virtual reality;Distance measurement;OKAN;Visually induced motion sickness;Velocity storage mechanism;Motion sickness},
	month = {March},
	pages = {87-90},
	title = {Could OKAN be an objective indicator of the susceptibility to visually induced motion sickness?},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759442}}

@inproceedings{5759443,
	abstract = {We present a novel bimanual body-directed travel technique, PenguFly (PF), and compare it with two standard travel-by-pointing techniques by conducting a between-subject experiment in a CAVE. In PF, the positions of the user's head and hands are projected onto the ground, and travel direction and speed are computed based on direction and magnitude of the vector from the midpoint of the projected hand positions to the projected head position. The two baseline conditions both use a single hand to control the direction, with speed controlled discretely by button pushes with the same hand in one case, and continuously by the distance between the hands in the other case. Users were asked to travel through a simple virtual world and collect virtual coins within a set time. We found no significant differences between travel conditions for reported presence or usability, but a significant increase in nausea with PF. Total travel distance was significantly higher for the baseline condition with discrete speed selection, whereas travel accuracy in terms of coin-to-distance ratio was higher with PF.},
	author = {von Kapri, Anette and Rick, Tobias and Feiner, Steven},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759443},
	issn = {2375-5334},
	keywords = {Particle measurements;Accuracy;Atmospheric measurements;Virtual reality;Testing;Usability;Legged locomotion},
	month = {March},
	pages = {91-94},
	title = {Comparing steering-based travel techniques for search tasks in a CAVE},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759443}}

@inproceedings{5759444,
	abstract = {In this paper we investigate the utility of an interactive, desktop-based virtual reality (VR) system for training personnel in hazardous working environments. Employing a novel software model, CRAM (Course Resource with Active Materials), we asked participants to learn a specific aircraft maintenance task. The evaluation sought to identify the type of familiarization training that would be most useful prior to hands on training, as well as after, as skill maintenance. We found that participants develop an increased awareness of hazards when training with stimulating technology - in particular (1) interactive, virtual simulations and (2) videos of an instructor demonstrating a task - versus simply studying (3) a set of written instructions. The results also indicate participants desire to train with these technologies over the standard written instructions. Finally, demographic data collected during the evaluation elucidates future directions for VR systems to develop a more robust and stimulating hazard training environment.},
	author = {Stocker, Catherine and Sunshine-Hill, Ben and Drake, John and Perera, Ian and Kider, Joseph T. and Badler, Norman I.},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759444},
	issn = {2375-5334},
	keywords = {Training;Videos;Aircraft;Lifting equipment;Hazards;Maintenance engineering;Materials;Virtual Reality;Training;Interactive Environments;Evaluation/Methodology},
	month = {March},
	pages = {95-102},
	title = {CRAM it! A comparison of virtual, live-action and written training systems for preparing personnel to work in hazardous environments},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759444}}

@inproceedings{5759445,
	abstract = {Enabling users to shape 3-D boxes in immersive virtual environments is a non-trivial problem. In this paper, a new family of techniques for creating rectangular boxes of arbitrary position, orientation, and size is presented and evaluated. These new techniques are based solely on position data, making them different from typical, existing box shaping techniques. The basis of the proposed techniques is a new algorithm for constructing a full box from just three of its corners. The evaluation of the new techniques compares their precision and completion times in a 9 degree-of-freedom (DoF) docking experiment against an existing technique, which requires the user to perform the rotation and scaling of the box explicitly. The precision of the users' box construction is evaluated by a novel error metric measuring the difference between two boxes. The results of the experiment strongly indicate that for precision docking of 9 DoF boxes, some of the proposed techniques are significantly better than ones with explicit rotation and scaling. Another interesting result is that the number of DoF simultaneously controlled by the user significantly influences the precision of the docking.},
	author = {Stenholt, Rasmus and Madsen, Claus B.},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759445},
	issn = {2375-5334},
	keywords = {Equations;Virtual environment;Mice;Measurement;Face;Shape;Resource management;3-D UI;interaction techniques;docking;3-D boxes;HMD},
	month = {March},
	pages = {103-110},
	title = {Shaping 3-D boxes: A full 9 degree-of-freedom docking experiment},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759445}}

@inproceedings{5759446,
	abstract = {The knowledge of the propagation behavior of radio waves is a fundamental prerequisite for planning and optimizing mobile radio networks. Propagation effects are usually simulated numerically, since real-world measurement campaigns are time-consuming and expensive. Automatic planning algorithms can explore a vast amount of network configurations to find good deployment schemes. However, complex urban scenarios demand for a great emphasis on site-specific details in the propagation environment which are often not covered by automatic approaches. Therefore, we have combined the simulation of radio waves with an interactive exploration and modification of the propagation environment in a virtual reality prototype application. By coupling real-time simulation and manipulation tasks we can provide an uninterrupted user-centered workflow.},
	author = {Rick, Tobias and von Kapri, Anette and Kuhlen, Torsten},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759446},
	issn = {2375-5334},
	keywords = {Solid modeling;Computational modeling;Buildings;Cities and towns;Real time systems;Interference;Virtual reality},
	month = {March},
	pages = {111-114},
	title = {A virtual reality system for the simulation and manipulation of wireless communication networks},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759446}}

@inproceedings{5759447,
	abstract = {We introduce a general technique for blending imagery from multiple projectors on a tracked, moving, non-planar object. Our technique continuously computes visibility of pixels over the surfaces of the object and dynamically computes the per-pixel weights for each projector. This approach supports smooth transitions between areas of the object illuminated by different number of projectors, down to the illumination contribution of individual pixels within each polygon. To achieve real-time performance, we take advantage of graphics hardware, implementing much of the technique with a custom dynamic blending shader program within the GPU associated with each projector. We demonstrate the technique with some tracked objects being illuminated by three projectors.},
	author = {Lincoln, Peter and Welch, Greg and Fuchs, Henry},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759447},
	issn = {2375-5334},
	keywords = {Pixel;Calibration;Computational modeling;Sea surface;Rendering (computer graphics);Real time systems;Solid modeling},
	month = {March},
	pages = {115-118},
	title = {Continual surface-based multi-projector blending for moving objects},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759447}}

@inproceedings{5759448,
	abstract = {A new device has been developed for generating airflow field and odor-concentration distribution in a real environment for presenting to the user. This device is called a multi-sensorial field (MSF) display. When two fans are placed facing each other, the airflows generated by them collide with each other and are radially deflected on a plane perpendicular to the original airflow direction. By utilizing the deflected airflow, the MSF display can present the airflow blowing from the front to the user without placing fans in front of the user. The directivity of the airflow deflection can be controlled by placing nozzles on the fans to adjust the cross-sectional shape of the airflow jets coming from the fans. The MSF display can also generate odor-concentration distribution in a real environment by introducing odor vapors into the airflow generated by the fans. The user can freely move his/her head and sniff at various locations in the generated odor distribution. The results of preliminary sensory tests are presented to show the potential of the MSF display.},
	author = {Matsukura, Haruka and Nihei, Tomohiko and Ishida, Hiroshi},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759448},
	issn = {2375-5334},
	keywords = {Fans;Olfactory;Monitoring;Virtual reality;Gas detectors;Velocity control;Wind display;olfactory display;airflow field;odor distribution},
	month = {March},
	pages = {119-122},
	title = {Multi-sensorial field display: Presenting spatial distribution of airflow and odor},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759448}}

@inproceedings{5759449,
	abstract = {Haptic interaction with virtual objects is a major concern in the virtual reality field. There are many physically-based efficient models that enable the simulation of a specific type of media, e.g. fluid volumes, deformable and rigid bodies. However, combining these often heterogeneous algorithms in the same virtual world in order to simulate and interact with different types of media can be a complex task. In this paper, we propose the first haptic rendering technique for the simulation and the interaction with multistate media, namely fluids, deformable bodies and rigid bodies, in real-time and with 6DoF haptic feedback. Based on the Smoothed-Particle Hydrodynamics (SPH) physical model for all three types of media, our method avoids the complexity of dealing with different algorithms and their coupling. We achieve high update rates while simulating a physically-based virtual world governed by fluid and elasticity theories, and show how to render interaction forces and torques through a 6DoF haptic device.},
	author = {Cirio, Gabriel and Marchal, Maud and Le Gentil, Aur{\'e}lien and L{\'e}cuyer, Anatole},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759449},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Computational modeling;Solid modeling;Deformable models;Media;Rendering (computer graphics);Couplings},
	month = {March},
	pages = {123-126},
	title = {``Tap, squeeze and stir'' the virtual world: Touching the different states of matter through 6DoF haptic interaction},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759449}}

@inproceedings{5759450,
	abstract = {In this paper, we propose a pseudo-gustatory display based on the cross-modal interactions that underlie the perception of flavor. Although several studies on visual, auditory, haptic, and olfactory displays have been conducted, gustatory displays have seldom been studied. This scarcity has been attributed to the fact that synthesizing arbitrary taste from basic tastants is difficult. On the other hand, it has been noted that the perception of taste is influenced by visual cues, auditory cues, smell, the trigeminal system, and touch. In our research, we aim at utilizing this influence between modalities for realizing a "pseudo-gustatory" system that enables the user to experience various tastes without changing the chemical composition of foods. Based on this concept, we built a "Meta Cookie" system to change the perceived taste of a cookie by overlaying visual and olfactory information onto a real cookie which an AR marker pattern. We performed an experiment that investigates how people experience the flavor of a plain cookie by using our system. The result suggests that our system can change the perceived taste based on the effect of the cross-modal interaction of vision, olfaction and gustation.},
	author = {Narumi, Takuji and Kajinami, Takashi and Nishizaka, Shinya and Tanikawa, Tomohiro and Hirose, Michitaka},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759450},
	issn = {2375-5334},
	keywords = {Olfactory;Cameras;Visualization;Chemicals;Mouth;Virtual reality;Gustatory Display;Pseudo-gustation;Olfactory Display;Cross-modal Integration;Augmented Reality},
	month = {March},
	pages = {127-130},
	title = {Pseudo-gustatory display system based on cross-modal integration of vision, olfaction and gustation},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759450}}

@inproceedings{5759451,
	abstract = {We present a study that compares finger-based direct interaction to controller-based ray interaction in a CAVE as well as in head-mounted displays. We focus on interaction tasks within reach of the users' arms and hands and explore various feedback methods including visual, pressure-based tactile and vibro-tactile feedback. Furthermore, we enhanced a precise finger tracking device with a direct pinch-detection mechanism to improve the robustness of grasp detection. Our results indicate that finger-based interaction is generally preferred if the functionality and ergonomics of manually manipulated virtual artifacts has to be assessed. However, controller-based interaction is often faster and more robust. In projection-based environments finger-based interaction almost reaches the task completion times and the subjective robustness of controller-based interaction if the grasping heuristics relies on our direct pinch detection. It also improves significantly by adding tactile feedback, while visual feedback proves sufficient in head-mounted displays. Our findings provide a guideline for the design of fine grain finger-based interfaces.},
	author = {Moehring, Mathias and Froehlich, Bernd},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759451},
	issn = {2375-5334},
	keywords = {Grasping;Visualization;Tactile sensors;Robustness;Performance evaluation;Optical feedback},
	month = {March},
	pages = {131-138},
	title = {Effective manipulation of virtual objects within arm's reach},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759451}}

@inproceedings{5759452,
	abstract = {This study tries to increase the effects of odors by controlling the odor emissions. We developed an olfactory feedback system to maintain user's concentration effectively based on biological information. The system presumes the level of user's concentration based on an electrocardiogram analysis. The analysis is carried out using variance of R-R intervals and frequency spectra. According to the presumed concentration level, the system presents an odor with an awakening effect to maintain user's concentration. We performed experiments in which subjects did simple tasks with this system, changing the method of controlling odor emissions. We compared the results of each method and evaluated the effectiveness of our olfactory feedback system and the methods of controlling odor emissions. The comparison of the results showed that presenting an odor when a user loses his/her concentration is effective to decrease errors of addition tasks.},
	author = {Miyaura, Masaaki and Narumi, Takuji and Nishimura, Kunihiro and Tanikawa, Tomohiro and Hirose, Michitaka},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759452},
	issn = {2375-5334},
	keywords = {Olfactory;Rail to rail inputs;Hafnium;Tracking;Pixel},
	month = {March},
	pages = {139-142},
	title = {Olfactory feedback system to improve the concentration level based on biological information},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759452}}

@inproceedings{5759453,
	abstract = {Current VR systems such as the CAVE provide an effective platform for the immersive exploration of large 3D data. A major limitation is that in most cases at least one display surface, such as a ceiling or a back wall, is missing due to space, access or cost constraints. This partially-immersive visualization results in a substantial loss of visual information that may be acceptable for some applications; however, it becomes a major obstacle for critical tasks that need to utilize the users' entire field of vision. We have developed a conformal rendering pipeline for the visualization of datasets on partially-immersive platforms. The angle-preserving conformal mapping approach is used to render the 360$\,^{\circ}$ view directly onto arbitrary display configurations. It has the desirable property of preserving shapes locally, which is important for identifying shape-based features in the data. Our conformal visualization technique is applicable to rasterization, volume rendering and real-time raytracing, and in contrast to image-based retargeting approaches, it constructs accurate, artifact-free stereoscopic images. We demonstrate our conformal visualization pipeline in the 5-sided CAVE for Immersive Virtual Colonoscopy, as well as architectural and abstract data exploration. Our user study shows that on the visual polyp detection task, conformal visualization leads to improved sensitivity at comparable examination times against the traditional rendering approach.},
	author = {Petkov, Kaloian and Papadopoulos, Charilaos and Zhang, Min and Kaufman, Arie E. and Gu, Xianfeng},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759453},
	issn = {2375-5334},
	keywords = {Rendering (computer graphics);Measurement;Data visualization;Shape;Visualization;Conformal mapping;Geometry},
	month = {March},
	pages = {143-150},
	title = {Conformal visualization for partially-immersive platforms},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759453}}

@inproceedings{5759454,
	abstract = {The aim of Redirected Walking (RDW) is to redirect a person along their path of travel in a Virtual Environment (VE) in order to increase the virtual space that can be explored in a given tracked area. Among other techniques, the user is redirected on a curved real-world path while visually walking straight in the VE (curvature gain). In this paper, we describe two experiments we conducted to test and extend RDW techniques. In Experiment 1, we measured the effect of walking speed on the detection threshold for curvature of the walking path. In a head-mounted display (HMD) VE, we found a decreased sensitivity for curvature for the slowest walking speed. When participants walked at 0.75 m/s, their detection threshold was approximately 0.1m-1 (radius of approximately 10m). In contrast, for faster walking speeds (>;1.0m/s), we found a significantly lower detection threshold of approximately 0.036m-1 (radius of approximately 27m). In Experiment 2, we implemented many well known redirection techniques into one dynamic RDW application. We integrated a large virtual city model and investigated RDW for free exploration. Further, we implemented a dynamic RDW controller which made use of the results from Experiment 1 by dynamically adjusting the applied curvature gain depending on the actual walking velocity of the user. In addition, we investigated the possible role of avatars to slow the users down or make them rotate their heads while exploring. Both the dynamic curvature gain controller and the avatar controller were evaluated in Experiment 2. We measured the average distance that was walked before reaching the boundaries of the tracked area. The mean walked distance was significantly larger in the condition where the dynamic gain controller was applied. This distance increased from approximately 15m for static gains to approximately 22m for dynamic gains. This did not come at the cost of an increase in simulator sickness. Applying the avatar controller did reveal an effect on walking distance or simulator sickness.},
	author = {Neth, Christian T. and Souman, Jan L. and Engel, David and Kloos, Uwe and B{\"u}lthoff, Heinrich H. and Mohler, Betty J.},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759454},
	issn = {2375-5334},
	keywords = {Legged locomotion;Avatars;Sensitivity;Virtual environment;Particle measurements;Cybernetics;Atmospheric measurements},
	month = {March},
	pages = {151-158},
	title = {Velocity-dependent dynamic curvature gain for redirected walking},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759454}}

@inproceedings{5759455,
	abstract = {We present change blindness redirection, a novel technique for allowing the user to walk through an immersive virtual environment that is considerably larger than the available physical workspace. In contrast to previous redirection techniques, this approach, based on a dynamic environment model, does not introduce any visual-vestibular conflicts from manipulating the mapping between physical and virtual motions, nor does it require breaking presence to stop and explicitly reorient the user. We conducted two user studies to evaluate the effectiveness of the change blindness illusion when exploring a virtual environment that was an order of magnitude larger than the physical walking space. Despite the dynamically changing environment, participants were able to draw coherent sketch maps of the environment structure, and pointing task results indicated that they were able to maintain their spatial orientation within the virtual world. Only one out of 77 participants across both both studies definitively noticed that a scene change had occurred, suggesting that change blindness redirection provides a remarkably compelling illusion. Secondary findings revealed that a wide field-of-view increases pointing accuracy and that experienced gamers reported greater sense of presence than those with little or no experience with 3D video games.},
	author = {Suma, Evan A. and Clark, Seth and Krum, David and Finkelstein, Samantha and Bolas, Mark and Warte, Zachary},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759455},
	issn = {2375-5334},
	keywords = {Virtual environment;Legged locomotion;Three dimensional displays;Blindness;Games;Visualization;Monitoring;virtual environments;redirection;change blindness},
	month = {March},
	pages = {159-166},
	title = {Leveraging change blindness for redirection in virtual environments},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759455}}

@inproceedings{5759456,
	abstract = {When viewed from below, a user's feet cast shadows onto the floor screen of an under-floor projection system, such as a six-sided CAVE. Tracking those shadows with a camera provides enough information for calculating a user's ground-plane location, foot orientation, and footstep events. We present Shadow Walking, an unencumbered locomotion technique that uses shadow tracking to sense a user's walking direction and step speed. Shadow Walking affords virtual locomotion by detecting if a user is walking in place. In addition, Shadow Walking supports a sidestep gesture, similar to the iPhone's pinch gesture. In this paper, we describe how we implemented Shadow Walking and present a preliminary assessment of our new locomotion technique. We have found Shadow Walking provides advantages of being unencumbered, inexpensive, and easy to implement compared to other walking-in-place approaches. It also has potential for extended gestures and multi-user locomotion.},
	author = {Zielinski, David J. and McMahan, Ryan P. and Brady, Rachael B.},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759456},
	issn = {2375-5334},
	keywords = {Legged locomotion;Pixel;Foot;Floors;Tracking;Cameras;Virtual environment;Virtual Reality;Navigation;Shadow Walking},
	month = {March},
	pages = {167-170},
	title = {Shadow walking: An unencumbered locomotion technique for systems with under-floor projection},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759456}}

@inproceedings{5759457,
	abstract = {This article presents a platform for software technology research in the area of intelligent Realtime Interactive Systems. Simulator X is targeted at Virtual Reality, Augmented Reality, Mixed Reality, and computer games. It provides a foundation and testbed for a variety of different application models. The current research architecture is based on the actor model to support fine grained concurrency and parallelism. Its design follows the minimize coupling and maximize cohesion software engineering principle. A distributed world state and execution scheme is combined with an object-centered world view based on an entity model. Entities conceptually aggregate properties internally represented by state variables. An asynchronous event mechanism allows intra- and interprocess communication between the simulation actors. An extensible world interface uses an ontology-based semantic annotation layer to provide a coherent world view of the resulting distributed world state and execution scheme to application developers. The world interface greatly simplifies configurability and the semantic layer provides a solid foundation for the integration of different Artificial Intelligence components. The current architecture is implemented in Scala using the Java virtual machine. This choice additionally fosters low-level scalability, portability, and reusability.},
	author = {Latoschik, Marc Erich and Tramberend, Henrik},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759457},
	issn = {2375-5334},
	keywords = {Object oriented modeling;Solid modeling;Virtual reality;Semantics;Computer architecture;Artificial intelligence},
	month = {March},
	pages = {171-174},
	title = {Simulator X: A scalable and concurrent architecture for intelligent realtime interactive systems},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759457}}

@inproceedings{5759458,
	abstract = {In this paper we describe the Mixed Reality system that we are developing to facilitate a real-world application, that of collaborative remote troubleshooting of broken office devices. The architecture of the system is centered on a 3D virtual representation of the device augmented with status data of the actual device coming from its internal sensors. The purpose of this paper is to illustrate how this approach supports the interactions required by the remote collaborative troubleshooting activity whilst taking into account technical constraints that come from a real world application. We believe it constitutes an interesting opportunity for using Mixed Reality in this domain.},
	author = {Roulland, Frederic and Castellani, Stefania and Valobra, Pascal and Ciriza, Victor and O'Neill, Jacki and Deng, Ye},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759458},
	issn = {2375-5334},
	keywords = {Three dimensional displays;Collaboration;Solid modeling;Semantics;Collaborative work;Performance evaluation;Maintenance engineering;device troubleshooting;Mixed Reality;3D modeling;collaborative systems},
	month = {March},
	pages = {175-178},
	title = {Mixed reality for supporting office devices troubleshooting},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759458}}

@inproceedings{5759459,
	abstract = {Reconstruction plates (deformable metal plates) are widely used in reduction and internal fixation surgeries for bone fractures at sites with irregular or individually different anatomical morphology. Traditional surgical procedures require intra-operative manual implant templating, which often leads to prolonged operations with unnecessary damage or hemorrhage. In this paper, we present a novel approach that uses computer graphics and augmented reality (AR) techniques to assist preoperative implant templating, substantially improving these surgical procedures. We exploit the symmetry of the human body to virtually reconstruct the broken skeletal structure using intact contralateral bones. Then 3D models of virtual implants are generated along a drawn path and rendered in an AR environment, to guide preoperative implant templating, thus reducing surgical invasiveness and operation duration. A successful clinical application is presented to demonstrate the effectiveness of our method.},
	author = {Fangyang, Shen and Yue, Shen and Yue, Qi},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759459},
	issn = {2375-5334},
	keywords = {Implants;Surgery;Bones;Three dimensional displays;Solid modeling;Computational modeling;Computed tomography},
	month = {March},
	pages = {179-182},
	title = {AR aided implant templating for unilateral fracture reduction and internal fixation surgery},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759459}}

@inproceedings{5759460,
	abstract = {This work introduces techniques to facilitate large-scale Augmented Reality (AR) experiences in unprepared outdoor environments. We develop a shape-based object detection framework that works with limited texture and can robustly handle extreme illumination and occlusion issues. The contribution of this work is a purely geometric approach for detecting marker-like objects under difficult and realistic outdoor conditions. We demonstrate these techniques for mobile AR experiences by detecting and tracking star-shaped pentagrams embedded in the Hollywood Walk of Fame at 30Hz on a Nokia N900 phone.},
	author = {Korah, Thommen and Wither, Jason and Tsai, Yun-Ta and Azuma, Ronald},
	booktitle = {2011 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:29 -0400},
	date-modified = {2024-03-18 02:29:29 -0400},
	doi = {10.1109/VR.2011.5759460},
	issn = {2375-5334},
	keywords = {Image segmentation;Shape;Feature extraction;Image edge detection;Robustness;Pixel;Lighting},
	month = {March},
	pages = {183-186},
	title = {Mobile Augmented Reality at the Hollywood Walk of Fame},
	year = {2011},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2011.5759460}}

@inproceedings{6180866,
	abstract = {We present a new algorithm to simulate a variety of crowd behaviors using the Discrete Choice Model (DCM). DCM has been widely studied in econometrics to examine and predict customers' or households' choices. Our DCM formulation can simulate virtual agents' goal selection and we highlight our algorithm by simulating heterogeneous crowd behaviors: evacuation, shopping, and rioting scenarios.},
	author = {Liu, Wenxi and Lau, Rynson and Manocha, Dinesh},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180866},
	issn = {2375-5334},
	keywords = {Computational modeling;Solid modeling;Navigation;Negative feedback;Heuristic algorithms;Decision making;Collision avoidance;H.5.1 [Information Interfaces and Presentation];Multimedia Information Systems --- Animations},
	month = {March},
	pages = {3-6},
	title = {Crowd simulation using Discrete Choice Model},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180866}}

@inproceedings{6180867,
	abstract = {One augmented reality approach is to use digital projectors to alter the appearance of a physical scene, avoiding the need for head-mounted displays or special goggles. Instead, spatial augmented reality (SAR) systems depend on having sufficient light radiance to compensate the surface's colors to those of a target visualization. However, standard SAR systems in dark room settings may suffer from insufficient light radiance causing bright colors to exhibit unexpected color shifts, resulting in a misleading visualization. We introduce a SAR framework which focuses on minimally altering the appearance of arbitrarily shaped and colored objects to exploit the presence of environment/room light as an additional light source to achieve compliancy for bright colors. While previous approaches have compensated for environment light, none have explicitly exploited the environment light to achieve bright, previously incompliant colors. We implement a full working system and compared our results to solutions achievable with standard SAR systems.},
	author = {Law, Alvin J. and Aliaga, Daniel G.},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180867},
	issn = {2375-5334},
	keywords = {Visualization;Image color analysis;Color;Lighting;Mathematical model;Equations;Radiometry;spatial-augmented reality;projector-based displays;interaction design;mobile and ubiquitous visualization},
	month = {March},
	pages = {7-10},
	title = {Spatial augmented reality for environmentally-lit real-world objects},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180867}}

@inproceedings{6180868,
	abstract = {Studying what design features are necessary and effective for educational virtual environments (VEs), we focused on two design issues: level of environmental detail and method of navigation. In a controlled experiment, participants studied animal facts distributed among different locations in an immersive VE. Participants viewed the information as either an automated tour through the environment or with full navigational control. The experiment also compared two levels of environmental detail: a sparse environment with only the animal fact cards and a detailed version that also included landmark items and ground textures. The experiment tested memory and understanding of the animal information. Though neither environmental detail nor navigation type significantly affected learning outcomes, the results suggest that manual navigation may have negatively affected the learning activity. Also, learning scores were correlated with both spatial ability and video game usage, suggesting that educational VEs may not be an appropriate presentation method for some learners.},
	author = {Ragan, Eric D. and Huber, Karl J. and Laha, Bireswar and Bowman, Doug A.},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180868},
	issn = {2375-5334},
	keywords = {Navigation;Animals;Virtual environments;Three dimensional displays;Manuals;Layout;virtual environments;virtual worlds;navigation;landmarks;educational software;learning},
	month = {March},
	pages = {11-14},
	title = {The effects of navigational control and environmental detail on learning in 3D virtual environments},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180868}}

@inproceedings{6180869,
	abstract = {We present a room-sized telepresence system for informal gatherings rather than conventional meetings. Unlike conventional systems which constrain participants to sit in fixed positions, our system aims to facilitate casual conversations between people in two sites. The system consists of a wall of large flat displays at each of the two sites, showing a panorama of the remote scene, constructed from a multiplicity of color and depth cameras. The main contribution of this paper is a solution that ameliorates the eye contact problem during conversation in typical scenarios while still maintaining a consistent view of the entire room for all participants. We achieve this by using two sets of cameras - a cluster of ''Panorama Cameras'' located at the center of the display wall and are used to capture a panoramic view of the entire room, and a set of ''Personal Cameras'' distributed along the display wall to capture front views of nearby participants. A robust segmentation algorithm with the assistance of depth cameras and an image synthesis algorithm work together to generate a consistent view of the entire scene. In our experience this new approach generates fewer distracting artifacts than conventional 3D reconstruction methods, while effectively correcting for eye gaze.},
	author = {Dou, Mingsong and Shi, Ying and Frahm, Jan-Michael and Fuchs, Henry and Mauchly, Bill and Marathe, Mod},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180869},
	issn = {2375-5334},
	keywords = {Cameras;Image segmentation;Three dimensional displays;Switches;Image color analysis;Graphics processing unit;Color;I.4.6 [Computing Methodologies]: Image Processing and Computer Vision --- Segmentation;H.4.3 [Information Systems Application];Communications Applications --- teleconferencing},
	month = {March},
	pages = {15-18},
	title = {Room-sized informal telepresence system},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180869}}

@inproceedings{6180871,
	abstract = {The concepts of immersion and presence focus on the environment in a virtual environment. We instead focus on embodied conversational agents (ECAs). ECAs occupy the virtual environment as interactive partners. We propose that the ECA analogues of immersion and presence are physicality and social presence. We performed a study to determine the effect of an ECA's physicality on social presence and eliciting realistic behavior from the user. The results showed that increasing physicality can elicit realistic behavior and increase social presence but there was also an interaction effect with plausibility.},
	author = {Chuah, Joon Hao and Robb, Andrew and White, Casey and Wendling, Adam and Lampotang, Samsun and Kopper, Regis and Lok, Benjamin},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180871},
	issn = {2375-5334},
	keywords = {Virtual environments;Humans;Measurement;Physiology;Biomedical imaging;Legged locomotion},
	month = {March},
	pages = {19-22},
	title = {Increasing agent physicality to raise social presence and elicit realistic behavior},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180871}}

@inproceedings{6180872,
	abstract = {This work presents an evaluation study of two different collision feedback modalities for virtual assembly verification: visual and force feedback. Forty-three subjects performed several assembly tasks (peg-in-hole, narrow passage) designed with two levels of difficulty. The used haptic rendering algorithm is based on voxel and point data-structures. Both objective - time and collision performance - and subjective measures have been recorded and analyzed. The comparison of the feedback modalities revealed a clear and highly significant superiority of force feedback in virtual assembly scenarios. The objective data shows that whereas the assembly time is similar in most cases for both conditions, force collision feedback yields significantly smaller collision forces, which indicate higher assembly precision. The subjective ratings of the participants define the force feedback condition as the most appropriate for determining clearances and correcting collision configurations, being the best suited modality to predict mountability.},
	author = {Sagardia, Mikel and Weber, Bernhard and Hulin, Thomas and Hirzinger, Gerd and Preusche, Carsten},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180872},
	issn = {2375-5334},
	keywords = {Assembly;Visualization;Force feedback;Force;Virtual reality;High definition video;H.1.2 [Human information processing];[H.5.2]: Haptic I/O;I.3.4. [Virtual device interfaces]},
	month = {March},
	pages = {23-26},
	title = {Evaluation of visual and force feedback in virtual assembly verifications},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180872}}

@inproceedings{6180873,
	abstract = {While training participants to assemble a 3D wooden burr puzzle, we compared results of training in a stereoscopic, head tracked virtual assembly environment utilizing haptic devices and data gloves with real world training. While virtual training took participants about three times longer, the group that used the virtual environment was able to assemble the physical test puzzle about three times faster than the group trained with the physical puzzle. We present several possible cognitive explanations for these results and our plans for future exploration of the factors that improve the effectiveness of virtual process training over real world experience.},
	author = {Oren, Mike and Carlson, Patrick and Gilbert, Stephen and Vance, Judy M.},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180873},
	issn = {2375-5334},
	keywords = {Training;Assembly;Virtual environments;Haptic interfaces;Testing;Virtual groups;Assembly;virtual reality;training;haptics;cognition},
	month = {March},
	pages = {27-30},
	title = {Puzzle assembly training: Real world vs. virtual environment},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180873}}

@inproceedings{6180874,
	abstract = {Most virtual reality simulators have a serious flaw: Users tend to get easily lost and disoriented as they navigate. According to the prevailing opinion, this is because of the lack of actual physical motion to match the visually simulated motion: E.g., using HMD-based VR, Klatzky et al. [1] showed that participants failed to update visually simulated rotations unless they were accompanied by physical rotation of the observer, even if passive. If we use more naturalistic environments (but no salient landmarks) instead of just optic flow, would physical motion cues still be needed to prevent disorientation? To address this question, we used a paradigm inspired by Klatzky et al.: After visually displayed passive movements along curved streets in a city environment, participants were asked to point back to where they started. In half of the trials the visually displayed turns were accompanied by a matching physical rotation. Results showed that adding physical motion cues did not improve pointing performance. This suggests that physical motions might be less important to prevent disorientation if visuals are naturalistic enough. Furthermore, unexpectedly two participants consistently failed to update the visually simulated heading changes, even when they were accompanied by physical rotations. This suggests that physical motion cues do not necessarily improve spatial orientation ability in VR (by inducing obligatory spatial updating). These findings have noteworthy implications for the design of effective motion simulators.},
	author = {Sigurdarson, Salvar and Milne, Andrew P. and Feuereissen, Daniel and Riecke, Bernhard E.},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180874},
	issn = {2375-5334},
	keywords = {Visualization;Turning;Virtual environments;Psychology;Cognition;Navigation;virtual reality;visual cognition;spatial updating},
	month = {March},
	pages = {31-34},
	title = {Can physical motions prevent disorientation in naturalistic VR?},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180874}}

@inproceedings{6180875,
	abstract = {When we locomote through real or virtual environments, self-to-object relationships constantly change. Nevertheless, in real environments we effortlessly maintain an ongoing awareness of roughly where we are with respect to our immediate surrounds, even in the absence of any direct perceptual support (e.g., in darkness or with eyes closed). In virtual environments, however, we tend to get lost far more easily. Why is that? Research suggests that physical motion cues are critical in facilitating this ``automatic spatial updating'' of the self-to-surround relationships during perspective changes. However, allowing for full physical motion in VR is costly and often unfeasible. Here, we demonstrated for the first time that the mere illusion of self-motion (``circular vection'') can provide a similar benefit as actual self-motion: While blindfolded, participants were asked to imagine facing new perspectives in a well-learned room, and point to previously-learned objects. As expected, this task was difficult when participants could not physically rotate to the instructed perspective. Performance was significantly improved, however, when they perceived illusory self-rotation to the novel perspective (even though they did not physically move). This circular vection was induced by a combination of rotating sound fields (``auditory vection'') and biomechanical vection from stepping along a carrousel-like rotating floor platter. In summary, illusory self-motion was shown to indeed facilitate perspective switches and thus spatial orientation. These findings have important implications for both our understanding of human spatial cognition and the design of more effective yet affordable VR simulators. In fact, it might ultimately enable us to relax the need for physical motion in VR by intelligently utilizing self-motion illusions.},
	author = {Riecke, Bernhard E. and Feuereissen, Daniel and Rieser, John J. and McNamara, Timothy P.},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180875},
	issn = {2375-5334},
	keywords = {Biomechanics;Visualization;Psychology;Educational institutions;Solid modeling;Interference;Headphones;Spatial Updating;Self-Motion Illusion;Vection;VR},
	month = {March},
	pages = {35-38},
	title = {Self-motion illusions (vection) in VR --- Are they good for anything?},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180875}}

@inproceedings{6180876,
	abstract = {This paper describes a sensor-fusion-based wireless walking-in-place (WIP) interaction technique. We devised a new human-walking detection algorithm that is based on a sensor-fusion using both acceleration and magnetic sensors integrated within a smart phone. Our sensor-fusion approach can be useful for the cases when the detection capability of a single sensor is limited to a certain range of walking speeds, when a system power source is limited, and/or when computation power is limited. The proposed algorithm is versatile enough to handle possible data-loss and random delay in the wireless communication environment, resulting in reduced wireless communication load and computation overhead. The initial study demonstrated that the algorithm can detect dynamic speeds of human walking. The algorithm can be implemented on any mobile device equipped with magnetic and acceleration sensors.},
	author = {Kim, Ji-Sun and Gra{\v c}anin, Denis and Quek, Francis},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180876},
	issn = {2375-5334},
	keywords = {Legged locomotion;Acceleration;Smart phones;Wireless communication;Magnetic sensors;Wireless sensor networks;Humans;Navigation technique;walking in place;virtual environment;sensor fusion},
	month = {March},
	pages = {39-42},
	title = {Sensor-fusion walking-in-place interaction technique using mobile devices},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180876}}

@inproceedings{6180877,
	abstract = {Natural walking can provide a compelling experience in immersive virtual environments, but it remains an implementation challenge due to the physical space constraints imposed on the size of the virtual world. The use of redirection techniques is a promising approach that relaxes the space requirements of natural walking by manipulating the user's route in the virtual environment, causing the real world path to remain within the boundaries of the physical workspace. In this paper, we present and apply a novel taxonomy that separates redirection techniques according to their geometric flexibility versus the likelihood that they will be noticed by users. Additionally, we conducted a user study of three reorientation techniques, which confirmed that participants were less likely to experience a break in presence when reoriented using the techniques classified as subtle in our taxonomy. Our results also suggest that reorientation with change blindness illusions may give the impression of exploring a more expansive environment than continuous rotation techniques, but at the cost of negatively impacting spatial knowledge acquisition.},
	author = {Suma, Evan A. and Bruder, Gerd and Steinicke, Frank and Krum, David M. and Bolas, Mark},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180877},
	issn = {2375-5334},
	keywords = {Virtual environments;Legged locomotion;Taxonomy;Thyristors;Optical character recognition software;Blindness;Virtual environments;redirection;taxonomy},
	month = {March},
	pages = {43-46},
	title = {A taxonomy for deploying redirection techniques in immersive virtual environments},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180877}}

@inproceedings{6180878,
	abstract = {This study incorporated a dual-task paradigm, in which participants were asked to perform basic locomotion tasks with one of three interfaces while remembering a sequence of either spatial or verbal items. Interfaces varied in similarity to natural body movements. Stopping performance was compromised when concurrently remembering a spatial, but not verbal, sequence. Also users exhibited lower performance on spatial memory tasks while using more unnatural locomotion interfaces. These results confirm that semi-natural locomotion interfaces require spatial working memory resources and thus locomotion interfaces compete with ongoing spatial tasks, as opposed to those requiring verbal resources or general attention resources.},
	author = {Marsh, William E. and Putnam, Marisa and Kelly, Jonathan W. and Dark, Veronica J. and Oliver, James H.},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180878},
	issn = {2375-5334},
	keywords = {Memory management;Legged locomotion;Navigation;Psychology;Virtual environments;Solid modeling;H5.2 [Information interfaces and presentation]: User Interfaces --- Input devices and strategies},
	month = {March},
	pages = {47-50},
	title = {The cognitive implications of semi-natural virtual locomotion},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180878}}

@inproceedings{6180879,
	abstract = {We present a method for reducing interference between multiple structured light-based depth sensors operating in the same spectrum with rigidly attached projectors and cameras. A small amount of motion is applied to a subset of the sensors so that each unit sees its own projected pattern sharply, but sees a blurred version of the patterns of other units. If high spacial frequency patterns are used, each sensor sees its own pattern with higher contrast than the patterns of other units, resulting in simplified pattern disambiguation. An analysis of this method is presented for a group of commodity Microsoft Kinect color-plus-depth sensors with overlapping views. We demonstrate that applying a small vibration with a simple motor to a subset of the Kinect sensors results in reduced interference, as manifested as holes and noise in the depth maps. Using an array of six Kinects, our system reduced interference-related missing data from from 16.6% to 1.4% of the total pixels. Another experiment with three Kinects showed an 82.2% percent reduction in the measurement error introduced by interference. A side-effect is blurring in the color images of the moving units, which is mitigated with post-processing. We believe our technique will allow inexpensive commodity depth sensors to form the basis of dense large-scale capture systems.},
	author = {Maimone, Andrew and Fuchs, Henry},
	booktitle = {2012 IEEE Virtual Reality Workshops (VRW)},
	date-added = {2024-03-18 02:29:24 -0400},
	date-modified = {2024-03-18 02:29:24 -0400},
	doi = {10.1109/VR.2012.6180879},
	issn = {2375-5334},
	keywords = {Cameras;Interference;Sensors;Measurement uncertainty;Prototypes;Arrays;Color;motion;input devices;sharpening and deblurring},
	month = {March},
	pages = {51-54},
	title = {Reducing interference between multiple structured light depth sensors using motion},
	year = {2012},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2012.6180879}}

@inproceedings{5444782,
	abstract = {This paper extends 3D lens techniques. Interactive 3D lenses, often called volumetric lenses, provide users with alternative views of datasets within spatially bounded regions of interest (focus) while maintaining the surrounding overview (context). In contrast to previous multi-pass rendering work, we discuss the strengths, limitations, and performance cost of a single-pass technique. For a substantial range of effects, it supports several interactive composable lenses at interactive frame rates without performance loss during increasing lens intersections or manipulations. Other cases, for which this performance cannot be achieved, are also discussed. Finally, we illustrate possible applications of our lens system, especially new Time Warp lenses for exploring time-varying datasets in interactive VR.},
	author = {Tiesel, Jan-Phillip and Borst, Christoph W. and Das, Kaushik and Habib, Emad},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444782},
	issn = {2375-5334},
	keywords = {Lenses;Spatiotemporal phenomena;Rendering (computer graphics);Virtual reality;Data visualization;Time varying systems;Computer graphics;Costs;Performance loss;Application software;Magic Lens;volumetric lens},
	month = {March},
	pages = {235-242},
	title = {Single-pass 3D lens rendering and spatiotemporal ``Time Warp'' example},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444782}}

@inproceedings{5444783,
	abstract = {In this paper we present a GPU implementation to accurately select 3D objects based on their silhouettes by a pointing device with six degrees of freedom (6DOF) in a virtual environment (VE). We adapt a 2D picking metaphor to 3D selection in VE's by changing the projection and view matrices according to the position and orientation of a 6DOF pointing device and rendering a conic selection volume to an off-screen pixel buffer. This method works for triangulated as well as volume rendered objects, no explicit geometric representation is required.},
	author = {Rick, Tobias and von Kapri, Anette and Kuhlen, Torsten},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444783},
	issn = {2375-5334},
	keywords = {Virtual environment;Rendering (computer graphics);Computer graphics;Virtual reality;Mice;Geometry;Brain;Shape;Jitter;Statistics;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality},
	month = {March},
	pages = {243-246},
	title = {GPU implementation of 3D object selection by conic volume techniques in virtual environments},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444783}}

@inproceedings{5444784,
	abstract = {We present a novel, real-time, markerless vision-based tracking system, employing a rigid orthogonal configuration of two pairs of opposing cameras. Our system uses optical flow over sparse features to overcome the limitation of vision-based systems that require markers or a pre-loaded model of the physical environment. We show how opposing cameras enable cancellation of common components of optical flow leading to an efficient tracking algorithm. Experiments comparing our device with an electromagnetic tracker show that its average tracking accuracy is 80% over 185 frames, and it is able to track large range motions even in outdoor settings.},
	author = {Gupta, Prince and da Vitoria Lobo, Niels and Laviola, Joseph J.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444784},
	issn = {2375-5334},
	keywords = {Cameras;Image motion analysis;Tracking;Optical sensors;Robot vision systems;Optical devices;Layout;Application software;Simultaneous localization and mapping;Real time systems;Optical Flow;Polar Correlation;Multi Camera;Markerless},
	month = {March},
	pages = {223-226},
	title = {Markerless tracking using Polar Correlation of camera optical flow},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444784}}

@inproceedings{5444785,
	abstract = {Simulating grass field in real-time has many applications, such as in virtual reality and games. Modeling accurate grass-grass, grass-object and grass-wind interactions requires a high computational cost. In this paper, we present a method to simulate grass field in real-time by considering grass field as a two dimensional grid-based continuum and shifting the complex interactions to the dynamics of continuum. We adopt the wave simulation as the numerical model for the dynamics of continuum which represents grass-grass interaction. We propose a procedural approach to handle grass-object and grass-wind interactions as external force that updates the wave simulation. The proposed method can be efficiently implemented on a GPU. As a result, massive amounts of grass can interact with moving objects and wind in real-time.},
	author = {Chen, Kan and Johan, Henry},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444785},
	issn = {2375-5334},
	keywords = {Computational modeling;Animation;Virtual reality;Computational efficiency;Computer graphics;Virtual environment;Large-scale systems;Physics;Geometry;Numerical simulation;Real-time animation;grass;continuum simulation},
	month = {March},
	pages = {227-234},
	title = {Real-time continuum grass},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444785}}

@inproceedings{5444786,
	abstract = {We present a novel method for the real-time creation and tracking of panoramic maps on mobile phones. The maps generated with this technique are visually appealing, very accurate and allow drift-free rotation tracking. This method runs on mobile phones at 30 Hz and has applications in the creation of panoramic images for offline browsing, for visual enhancements through environment mapping and for outdoor Augmented Reality on mobile phones.},
	author = {Wagner, Daniel and Mulloni, Alessandro and Langlotz, Tobias and Schmalstieg, Dieter},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444786},
	issn = {2375-5334},
	keywords = {Mobile handsets;Cameras;Robustness;Application software;Intelligent sensors;Accelerometers;Simultaneous localization and mapping;Augmented reality;Virtual reality;Computer vision;Panorama creation;Tracking;Mobile phone},
	month = {March},
	pages = {211-218},
	title = {Real-time panoramic mapping and tracking on mobile phones},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444786}}

@inproceedings{5444787,
	abstract = {Although hand tracking algorithm has been widely used in virtual reality and HCI system, it is still a challenging problem in vision-based research area. Due to the robustness and real-time requirements in VR applications, most hand tracking algorithms require special device to achieve satisfactory results. In this paper, we propose an easy-to-use and inexpensive approach to track the hands accurately with a single normal webcam. Outstretched hand is detected by contour & curvature based detection techniques to initialize the tracking region. Robust multi-cue hand tracking is then achieved by velocity-weighted features and color cue. Experiments show that the proposed multi-cue hand tracking approach achieves continuous real-time results even for the situation of cluttered background. The approach fulfills the speed and accuracy requirements of frontal-view vision-based human computer interactions.},
	author = {Pan, Zhigeng and Li, Yang and Zhang, Mingmin and Sun, Chao and Guo, Kangde and Tang, Xing and Zhou, Steven Zhiying},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444787},
	issn = {2375-5334},
	keywords = {Computer vision;Cameras;Robustness;Virtual reality;Human computer interaction;Skin;Object detection;Application software;Real time systems;Chaos;Hand tracking;3D Interaction;Gesture detection},
	month = {March},
	pages = {219-222},
	title = {A real-time multi-cue hand tracking algorithm based on computer vision},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444787}}

@inproceedings{5444788,
	abstract = {While label placement algorithms are generally successful in managing visual clutter by preventing label overlap, they can also cause significant label movement in dynamic displays. This study investigates motion detection thresholds for various types of label movement in realistic and complex virtual environments, which can be helpful for designing less salient and disturbing algorithms. Our results show that label movement in stereoscopic depth is shown to be less noticeable than similar lateral monoscopic movement, inherent to 2D label placement algorithms. Furthermore, label movement can be introduced more readily into the visual periphery (over 15$\,^{\circ}$ eccentricity) because of reduced sensitivity in this region. Moreover, under the realistic viewing conditions that we used, motion of isolated labels is more easily detected than that of overlapping labels. This perhaps counterintuitive finding may be explained by visual masking due to the visual clutter arising from the label overlap. The quantitative description of the findings presented in this paper should be useful not only for label placement applications, but also for any cluttered AR or VR application in which designers wish to control the users' visual attention, either making text labels more or less noticeable as needed.},
	author = {Peterson, Stephen D. and Axholt, Magnus and Cooper, Matthew and Ellis, Stephen R.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444788},
	issn = {2375-5334},
	keywords = {Motion detection;Displays;Virtual reality;Virtual environment;Application software;User interfaces;Air traffic control;Humans;NASA;Algorithm design and analysis;H.5.2 [Information Systems]: User Interfaces;I.3 [Computing Methodologies]: Computer Graphics},
	month = {March},
	pages = {203-206},
	title = {Detection thresholds for label motion in visually cluttered displays},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444788}}

@inproceedings{5444789,
	abstract = {The concept of affordance, introduced by the psychologist James Gibson, can be defined as the functional utility of an object, a surface or an event. The purpose of this article was to evaluate the perception of affordances in virtual environments (VE). In order to test this perception, we considered the affordances for standing on a virtual slanted surface. The participants were asked to judge whether a virtual slanted surface supported upright stance. The perception was investigated by manipulating the texture of the slanted surface (Wooden texture vs. Ice texture). Results showed an effect of the texture: the perceptual boundary (or critical angle) with the Ice texture was significantly lower than with the Wooden texture. These results reveal that perception of affordances for standing on a slanted surface in virtual reality is possible and comparable to previous studies conducted in real environments.},
	author = {Regia-Corte, Tony and Marchal, Maud and L{\'e}cuyer, Anatole},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444789},
	issn = {2375-5334},
	keywords = {Virtual reality;Psychology;Testing;Ice;Surface texture;Legged locomotion;Apertures;Velocity measurement;Virtual environment;Multimedia systems;H.5.1 Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, augmented and virtual realities;H.1.2 [Information Systems]: User/Machine Systems-human factors;human information processing},
	month = {March},
	pages = {207-210},
	title = {Can you stand on virtual grounds? A study on postural affordances in virtual reality},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444789}}

@inproceedings{5444790,
	abstract = {In visual perception, change blindness describes the phenomenon that persons viewing a visual scene may apparently fail to detect significant changes in that scene. These phenomena have been observed in both computer generated imagery and real-world scenes. Several studies have demonstrated that change blindness effects occur primarily during visual disruptions such as blinks or saccadic eye movements. However, until now the influence of stereoscopic vision on change blindness has not been studied thoroughly in the context of visual perception research. In this paper we introduce change blindness techniques for stereoscopic projection systems, providing the ability to substantially modify a virtual scene in a manner that is difficult for observers to perceive. We evaluate techniques for passive and active stereoscopic viewing and compare the results to those of monoscopic viewing conditions. For stereoscopic viewing conditions, we found that change blindness phenomena occur with the same magnitude as in monoscopic viewing conditions. Furthermore, we have evaluated the potential of the presented techniques for allowing abrupt, and yet significant, changes of a stereoscopically displayed virtual reality environment.},
	author = {Steinicke, Frank and Bruder, Gerd and Hinrichs, Klaus and Willemsen, Pete},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444790},
	issn = {2375-5334},
	keywords = {Blindness;Layout;Virtual reality;Humans;Visual perception;Computer science;Visual system;Virtual environment;Visualization;Computer graphics;Visual perception;stereoscopic viewing;change blindness},
	month = {March},
	pages = {187-194},
	title = {Change blindness phenomena for stereoscopic projection systems},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444790}}

@inproceedings{5444791,
	abstract = {A number of studies have reported that distance judgments are underestimated in virtual environments (VE) when compared to those made in the real world. Studies have also reported that providing users with visual feedback in the VE improves their distance perception and made them feel more immersed in the virtual world. In this study, we investigated the effect of tactile feedback and visual manipulation of the VE on egocentric distance perception. In contrast to previous studies which have focused on task-specific and error-corrective feedback (for example, providing knowledge about the errors in distance estimations), we demonstrate that exploratory feedback is sufficient for reducing errors in distance estimation. In Experiment 1, the effects of different types of feedback (visual, tactile and visual plus tactile) on distance judgments were studied. Tactile feedback was given to participants as they explored and touched objects in a VE. Results showed that distance judgments improved in the VE regardless of the type of sensory feedback provided. In Experiment 2, we presented a real world environment to the participants and then situated them in a VE that was either a replica or an altered representation of the real world environment. Results showed that participants made significant underestimation in their distance judgments when the VE was not a replica of the physical space. We further found that providing both visual and tactile feedback did not reduce distance compression in such a situation. These results are discussed in the light of the nature of feedback provided and how assumptions about the VE may affect distance perception in virtual environments.},
	author = {Ahmed, Farahnaz and Cohen, Joseph D. and Binder, Katherine S. and Fennema, Claude L.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444791},
	issn = {2375-5334},
	keywords = {Feedback;Virtual environment;Psychology;Educational institutions;Computer graphics;Space technology;Extraterrestrial measurements;Computer science;Estimation error;Virtual reality;Immersive virtual reality;Egocentric distance perception},
	month = {March},
	pages = {195-202},
	title = {Influence of tactile feedback and presence on egocentric distance perception in virtual environments},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444791}}

@inproceedings{5444792,
	abstract = {In this paper we present a Mixed Reality (MR) teleconferencing application based on Second Life (SL) and the OpenSim virtual world. Augmented Reality (AR) techniques are used for displaying virtual avatars of remote meeting participants in real physical spaces, while Augmented Virtuality (AV), in form of video based gesture detection, enables capturing of human expressions to control avatars and to manipulate virtual objects in virtual worlds. The use of Second Life for creating a shared augmented space to represent different physical locations allows us to incorporate the application into existing infrastructure. The application is implemented using open source Second Life viewer, ARToolKit and OpenCV libraries.},
	author = {Kantonen, Tuomas and Woodward, Charles and Katz, Neil},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444792},
	issn = {2375-5334},
	keywords = {Virtual reality;Teleconferencing;Second Life;Avatars;Augmented virtuality;Augmented reality;Video sharing;Object detection;Humans;Libraries;mixed reality;virtual worlds;Second Life;teleconferencing;immersive virtual environments;collaborative augmented reality},
	month = {March},
	pages = {179-182},
	title = {Mixed reality in virtual world teleconferencing},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444792}}

@inproceedings{5444793,
	abstract = {Collaborative virtual environment has been limited on static or rigid 3D models, due to the difficulties of real-time streaming of large amounts of data that is required to describe motions of 3D deformable models. Streaming shape deformations of complex 3D models arising from a remote user's manipulations is a challenging task. In this paper, we present a framework based on spectral transformation that encodes surface deformations in a frequency format to successfully meet the challenge, and demonstrate its use in a distributed virtual environment. Our research contributions through this framework include: i) we reduce the data size to be streamed for surface deformations since we stream only the transformed spectral coefficients and not the deformed model; ii) we propose a mapping method to allow models with multi-resolutions to have the same deformations simultaneously; iii) our streaming strategy can tolerate loss without the need for special handling of packet loss. Our system guarantees real-time transmission of shape deformations and ensures the smooth motions of 3D models. Moreover, we achieve very effective performance over real Internet conditions as well as a local LAN. Experimental results show that we get low distortion and small delays even when surface deformations of large and complicated 3D models are streamed over lossy networks.},
	author = {Tang, Ziying and Rong, Guodong and Guo, Xiaohu and Prabhakaran, B.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444793},
	issn = {2375-5334},
	keywords = {Shape;Collaboration;Virtual environment;Deformable models;High performance computing;Computer networks;Multiresolution analysis;Simultaneous localization and mapping;Spatial resolution;Internet;Collaborative Interaction;Distributed Virtual Reality;3D Shape Deformation},
	month = {March},
	pages = {183-186},
	title = {Streaming 3D shape deformations in collaborative virtual environment},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444793}}

@inproceedings{5444794,
	abstract = {In this paper, we investigate the effects of viewing conditions and rotation methods on different types of collaborative tasks in a tabletop AR environment in which two users are co-located. The viewing condition means how the manipulation of a tabletop world by one user is shown in the other users' view and the rotation method means what type of input devices is used to rotate the tabletop world for alternative orientations. Our experiment considered two different viewing conditions-consistent view and inconsistent view and two different rotation methods-direct turn and indirect turn. In the experiment, a 3D display environment called Stereoscopic Collaboration in Augmented and Projective Environments (SCAPE) was utilized as a test environment, and two tasks were considered: Lego-like block building task and text label selection task. The former was designed for synchronous and referring-strong type, and the latter was designed for asynchronous and orientation-strong type. As dependent variables, various objective and subjective measurements including task completion time, quality of task result, turn angle, and questionnaire were measured. According to the results, the viewing conditions had significant effects on several objective and subjective measurements. On task completion time, their effect for the synchronous task was opposite to that for the asynchronous task. On the other hand, the rotation methods had significant effects only on turn angle.},
	author = {Lee, Sangyoon and Hua, Hong},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444794},
	issn = {2375-5334},
	keywords = {Collaboration;Computer displays;Educational institutions;Time measurement;Virtual reality;Computer interfaces;Physics computing;Visualization;Layout;Three dimensional displays;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-artificial, augmented, and virtual realities;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces-collaborative computing, evaluation/methodology;H.1.2 [Models and Principles]: User/Machine Systems-human factors},
	month = {March},
	pages = {163-170},
	title = {Effects of viewing conditions and rotation methods in a collaborative tabletop AR environment},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444794}}

@inproceedings{5444795,
	abstract = {A collaborative virtual environment (CVE) allows remote users to access and modify shared data through networks, such as the Internet. However, when the users are connected via the Internet, the network latency problem may become significant and affect the performance of user interactions. Existing works to address the network latency problem mainly focus on developing motion prediction methods that appear statistically accurate for certain applications. However, it is often not known how reliable they are in a CVE. In this work, we study the sources of error introduced by a motion predictor and propose to address the errors by estimating the error bounds of each prediction made by the motion predictor. Without loss of generality, we discuss how we may estimate the upper and lower error bounds based on a particular motion predictor. Finally, we evaluate the effectiveness of our method extensively through a number of experiments and show the effectiveness of using the estimated error bound in an area-based visibility culling algorithm for DVE navigation.},
	author = {Lau, Rynson W. H. and Lee, Kenneth},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444795},
	issn = {2375-5334},
	keywords = {Estimation error;Motion estimation;Delay;IP networks;Virtual environment;Error correction;Geometry;Prefetching;Collaboration;Computer errors;collaborative virtual environments;network latency;motion prediction;prediction error},
	month = {March},
	pages = {171-178},
	title = {On error bound estimation for motion prediction},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444795}}

@inproceedings{5444796,
	abstract = {A system to synthesize in real-time the sound of footsteps on different materials is presented. The system is based on microphones which allow the user to interact with his own footwear. This solution distinguishes our system from previous efforts that require specific shoes enhanced with sensors. The microphones detect real footsteps sounds from users, from which the ground reaction force (GRF) is estimated. Such GRF is used to control a sound synthesis engine based on physical models. Evaluations of the system in terms of sound validity and fidelity of interaction are described.},
	author = {Nordahl, Rolf and Serafin, Stefania and Turchet, Luca},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444796},
	issn = {2375-5334},
	keywords = {Virtual reality;Microphones;Footwear;Real time systems;Acoustic materials;Sensor systems;Acoustic sensors;Force sensors;Control system synthesis;Engines;sound synthesis;physical models;footsteps sounds;auditory perception},
	month = {March},
	pages = {147-153},
	title = {Sound synthesis and evaluation of interactive footsteps for virtual reality applications},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444796}}

@inproceedings{5444797,
	abstract = {In this paper we present a novel technique to calibrate multiple casually aligned projectors on a fiducial-free cylindrical curved surface using a single camera. We impose two priors to the cylindrical display: (a) cylinder is a vertically extruded surface; and (b) the aspect ratio of the rectangle formed by the four corners of the screen is known. Using these priors, we can estimate the display's 3D surface geometry and camera extrinsic parameters using a single image without any explicit display to camera correspondences. Using the estimated camera and display properties, we design a novel deterministic algorithm to recover the intrinsic and extrinsic parameters of each projector using a single projected pattern seen by the camera which is then used to register the images on the display from any arbitrary viewpoint making it appropriate for virtual reality systems. Finally, our method can be extended easily to handle sharp corners - making it suitable for the common CAVE like VR setup. To the best of our knowledge, this is the first method that can achieve accurate geometric auto-calibration of multiple projectors on a cylindrical display without performing an extensive stereo reconstruction.},
	author = {Sajadi, Behzad and Majumder, Aditi},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444797},
	issn = {2375-5334},
	keywords = {Cameras;Three dimensional displays;Computer displays;Surface reconstruction;Geometry;Virtual reality;Image reconstruction;Computer science;Algorithm design and analysis;Computer graphics;Multi-Projector Displays;Tiled Displays;Calibration;Registration},
	month = {March},
	pages = {155-162},
	title = {Auto-calibration of cylindrical multi-projector systems},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444797}}

@inproceedings{5444798,
	abstract = {In this paper the virtual reality augmented cycling kit (VRACK) a mechatronic rehabilitation system with an interactive virtual environment is presented. It was designed as a modular system that can convert most bicycles in virtual reality (VR) cycles. Novel hardware components embedded with sensors were implemented on a stationary exercise bicycle to monitor physiological and biomechanical parameters of participants while immersing them in a virtual reality simulation providing the user with visual, auditory and haptic feedback. This modular and adaptable system attaches to commercially-available stationary bicycle systems and interfaces with a personal computer for simulation and data acquisition processes. The bicycle system includes novel handle bars based on hydraulic pressure sensors and innovative pedals that monitor lower extremity kinetics and kinematics. Parameters monitored by these systems are communicated to a practitioner's interface screen and can be amplified before entering its virtual environment. The first prototype of the system was successful in demonstrating that a modular mechatronic kit can monitor and record kinetic, kinematic and physiologic parameters of riders.},
	author = {Ranky, Richard and Sivak, Mark and Lewis, Jeffrey and Gade, Venkata and Deutsch, Judith E. and Mavroidis, Constantinos},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444798},
	issn = {2375-5334},
	keywords = {Virtual reality;Bicycles;Computerized monitoring;Condition monitoring;Mechatronics;Virtual environment;Biomedical monitoring;Computational modeling;Kinetic theory;Kinematics;Immersive Gaming;3D Interaction for VR;Haptics;Non-Visual Interfaces;Exergaming;Rehabilitation},
	month = {March},
	pages = {135-138},
	title = {VRACK --- virtual reality augmented cycling kit: Design and validation},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444798}}

@inproceedings{5444799,
	abstract = {We present a new interaction handling model for physics-based sound synthesis in virtual environments. A new three-level surface representation for describing object shapes, visible surface bumpiness, and microscopic roughness (e.g. friction) is proposed to model surface contacts at varying resolutions for automatically simulating rich, complex contact sounds. This new model can capture various types of surface interaction, including sliding, rolling, and impact with a combination of three levels of spatial resolutions. We demonstrate our method by synthesizing complex, varying sounds in several interactive scenarios and a game-like virtual environment. The three-level interaction model for sound synthesis enhances the perceived coherence between audio and visual cues in virtual reality applications.},
	author = {Ren, Zhimin and Yeh, Hengchin and Lin, Ming C.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444799},
	issn = {2375-5334},
	keywords = {Virtual environment;Rough surfaces;Surface roughness;Auditory displays;Friction;Computational modeling;Virtual reality;Feedback;Real time systems;Rendering (computer graphics);H.5.5 [Sound and Music Computing]: Modeling-Systems;H.5.2 [User Interface]: Auditory feedback},
	month = {March},
	pages = {139-146},
	title = {Synthesizing contact sounds between textured models},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444799}}

@inproceedings{5444800,
	abstract = {In the present paper, we describe a virtual reality application developed for the study of unilateral spatial neglect, a post-stroke neurological disorder that results in failure to respond to stimuli presented contralaterally to the damaged hemisphere. Recently, it has been proposed that patients with unilateral spatial neglect experience sensorimotor decorrelation in the affected space. Consequently, it is possible that since the sensorimotor experience in the affected space is perturbed, patients avoid this space, which results in neglect behavior. Here, we evaluate this hypothesis using a virtual reality application built on the base of the Stringed Haptic Workbench, a large-scale visuo-haptic system. The results provide support for the hypothesis and demonstrate that the proposed application is suitable for the envisioned goal.},
	author = {Tsirlin, Inna and Dupierrix, Eve and Chokron, Sylvie and Ohlmann, Theophile and Coquillart, Sabine},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444800},
	issn = {2375-5334},
	keywords = {Virtual reality;Haptic interfaces;Testing;Decorrelation;Displays;Large-scale systems;Virtual environment;Multimedia systems;Information systems;Psychology;spatial neglect;multimodal virtual environments;neurological disorders;visuo-haptic display},
	month = {March},
	pages = {127-130},
	title = {Multimodal virtual reality application for the study of unilateral spatial neglect},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444800}}

@inproceedings{5444801,
	abstract = {Would a room in which the walls appear to come to life, be a pleasant place and would it help or hinder concentration? On the doorstep of life-like architecture we use virtual reality to begin to answer these questions. We are brought to this doorstep through the potential convergence of emerging adaptive, reactive and organic architecture and the approaches that have made life like virtual agents both engaging and useful. The scene is set by introducing the concept of Adaptive Appraisive Architecture, in which a building could appear to exhibit life like appearance and behaviour. While being impressive would such a building be pleasant and useful? Before building the physical structure or coding the artificial intelligence this paper measures the impact of being within a room with moving walls on experience and performance. To do this we gave test subjects two jigsaw puzzles and placed them within a life size simulation where the walls move then remain static. The impact of this difference is measured on experience through questionnaire and post interview. The impact on performance is measured in terms of the amount of the puzzle solved. The relevance of the results are not constrained to adaptive architecture as they add to the body of knowledge that relate distractions to concentration.},
	author = {Adi, Mohamad Nadim and Roberts, David},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444801},
	issn = {2375-5334},
	keywords = {Buildings;Computer graphics;Computer architecture;Virtual reality;Computer displays;Painting;Animation;Biological system modeling;Computational modeling;Computer simulation;Architectural Design;Construction;experimental methods;large-format displays;Flow;Concentration;Adaptive;Intelligent;Social},
	month = {March},
	pages = {131-134},
	title = {Can you help me concentrate room?},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444801}}

@inproceedings{5444802,
	abstract = {Non-photorealistically rendered (NPR) immersive virtual environments (IVEs) can facilitate conceptual design in architecture by enabling preliminary design sketches to be previewed and experienced at full scale, from a first-person perspective. However, it is critical to ensure the accurate spatial perception of the represented information, and many studies have shown that people typically underestimate distances in most IVEs, regardless of rendering style. In previous work we have found that while people tend to judge distances more accurately in an IVE that is a high-fidelity replica of their concurrently occupied real environment than in an IVE that it is a photorealistic representation of a real place that they've never been to, significant distance estimation errors re-emerge when the replica environment is represented in a NPR style. We have also previously found that distance estimation accuracy can be improved, in photo-realistically rendered novel virtual environments, when people are given a fully tracked, high fidelity first person avatar self-embodiment. In this paper we report the results of an experiment that seeks to determine whether providing users with a high-fidelity avatar self-embodiment in a NPR virtual replica environment will enable them to perceive the 3D spatial layout of that environment more accurately. We find that users who are given a first person avatar in an NPR replica environment judge distances more accurately than do users who experience the NPR replica room without an embodiment, but not as accurately as users whose distance judgments are made in a photorealistically rendered virtual replica room. Our results provide a partial solution to the problem of facilitating accurate distance perception in NPR virtual environments, while supporting and expanding the scope of previous findings that giving people a realistic avatar self-embodiment in an IVE can help them to interpret what they see through an HMD in a way that is more similar to how they would interpret a corresponding visual stimulus in the real world.},
	author = {Phillips, Lane and Ries, Brian and Kaeding, Michael and Interrante, Victoria},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444802},
	issn = {2375-5334},
	keywords = {Avatars;Virtual environment;Computer graphics;Rendering (computer graphics);Estimation error;Space technology;Head;Displays;Statistics;Computer science;spatial perception;immersive virtual environments;non-photorealistic rendering;first-person avatars},
	month = {March},
	pages = {115-1148},
	title = {Avatar self-embodiment enhances distance perception accuracy in non-photorealistic immersive virtual environments},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444802}}

@inproceedings{5444803,
	abstract = {Nowadays, applications of virtual reality (VR) and computer games use human characters models with ever-increasing sophistication. Additional challenges are posed by applications, such as life-simulation computer games (The Sims, Spore, etc.), internet-based virtual worlds (Second Life) and animation movies, that require simulation of kinship and interaction between isolated populations with well defined ethnic characteristics. The main difficulty in those situations is to generate models automatically, which are physically similar to a given population or family. In this paper, human reproduction is mimicked to produce character models, which inherit genetic characteristics from their ancestors. Unlike morphing techniques, in our method, it is possible that a genetic characteristic from an ancestor be manifested only after a few generations.},
	author = {Vieira, Roberto C. Cavalcante and Vidal, Creto Augusto and Cavalcante-Neto, Joaquim B.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444803},
	issn = {2375-5334},
	keywords = {Genetics;Character generation;Computational modeling;Application software;Virtual reality;Humans;Fungi;Internet;Second Life;Animation;Modeling of virtual characters;Genetic inheritance;Reproductive simulation},
	month = {March},
	pages = {119-126},
	title = {Simulation of genetic inheritance in the generation of virtual characters},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444803}}

@inproceedings{5444804,
	abstract = {We present the development and evaluation of the Virtual Experience Test (VET). The VET is a survey instrument used to measure holistic virtual environment experiences based upon the five dimensions of experiential design: sensory, cognitive, affective, active, and relational. Experiential Design (ED) is a holistic approach to enhance presence in virtual environments that goes beyond existing presence theory (i.e. a focus on the sensory aspects of VE experiences) to include affective and cognitive factors. To evaluate the VET, 62 participants played the commercial video game Mirror's Edge. After gameplay both the VET and the ITC-Sense of Presence Inventory (ITC-SOPI) were administered. A principal component analysis was performed on the VET and it was determined that the actual question clustering coincided with the proposed dimensions of experiential design. Furthermore, scores from the VET were shown to have a significant relationship with presence scores on the ITC-SOPI. The results of this research produced a validated measure of holistic experience that could be used to evaluate virtual environments. Furthermore, our experiment indicates that virtual environments utilizing holistic designs can result in significantly higher presence.},
	author = {Chertoff, Dustin B. and Goldiez, Brian and LaViola, Joseph J.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444804},
	issn = {2375-5334},
	keywords = {Virtual environment;Instruments;Games;Multimedia systems;Information systems;Automatic testing;Intelligent sensors;Automation;Principal component analysis;Virtual reality;Presence;Experiential Design;Virtual Environments},
	month = {March},
	pages = {103-110},
	title = {Virtual Experience Test: A virtual environment evaluation questionnaire},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444804}}

@inproceedings{5444805,
	abstract = {This paper reports an experiment that investigated people's body ownership of an avatar that was observed in a virtual mirror. Twenty subjects were recruited in a within-groups study where 10 first experienced a virtual character that synchronously reflected their upper-body movements as seen in a virtual mirror, and then an asynchronous condition where the mirror avatar displayed prerecorded actions, unrelated to those of the participant. The other 10 subjects experienced the conditions in the opposite order. In both conditions the participant could carry out actions that led to elevation above ground level, as seen from their first person perspective and correspondingly in the mirror. A rotating virtual fan eventually descended to 2 m above the ground. The hypothesis was that synchronous mirror reflection would result in higher subjective sense of ownership. A questionnaire analysis showed that the body ownership illusion was significantly greater for the synchronous than asynchronous condition. Additionally participants in the synchronous condition avoided collision with the descending fan significantly more often than those in the asynchronous condition. The results of this experiment are put into context within similar experiments on multisensory correlation and body ownership within cognitive neuroscience.},
	author = {Gonz{\'a}lez-Franco, Mar and P{\'e}rez-Marcos, Daniel and Spanlang, Bernhard and Slater, Mel},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444805},
	issn = {2375-5334},
	keywords = {Mirrors;Reflection;Virtual environment;Rubber;Virtual reality;Computer graphics;Avatars;Synchronous motors;Large scale integration;Computer science;rubber hand illusion;body ownership;agency;virtual reality},
	month = {March},
	pages = {111-114},
	title = {The contribution of real-time mirror reflections of motor actions on virtual body ownership in an immersive virtual environment},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444805}}

@inproceedings{5444806,
	abstract = {Sketching leverages human skills for various purposes. In-Place Augmented Reality Sketching experiences build on the intuitiveness and flexibility of hand sketching for tasks like content creation. In this paper we explore the design space of In-Place Augmented Reality Sketching, with particular attention to content authoring in games. We propose a contextual model that offers a framework for the exploration of this design space by the research community. We describe a sketch-based AR racing game we developed to demonstrate the proposed model. The game is developed on top of our shape recognition and 3D registration library for mobile AR.},
	author = {Hagbi, Nate and Grasset, Rapha{\"e}l and Bergig, Oriel and Billinghurst, Mark and El-Sana, Jihad},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444806},
	issn = {2375-5334},
	keywords = {Augmented reality;User interfaces;Virtual reality;Humans;Space technology;Space exploration;Image reconstruction;Context modeling;Shape;Software libraries;In-Place Augmented Reality Sketching;Tangible Interaction;User Interface;Sketch Interaction},
	month = {March},
	pages = {91-94},
	title = {In-Place Sketching for content authoring in Augmented Reality games},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444806}}

@inproceedings{5444807,
	abstract = {The rubber hand illusion is a simple illusion where participants can be induced to report and behave as if a rubber hand is part of their body. The induction is usually done by an experimenter tapping both a rubber hand prop and the participant's real hand: the touch and visual feedback of the taps must be synchronous and aligned to some extent. The illusion is usually tested by several means including a physical threat to the rubber hand. The response to the threat can be measured by galvanic skin response (GSR): those that have the illusion showed a marked rise in GSR. Based on our own and reported experiences with immersive virtual reality (IVR), we ask whether a similar illusion is induced naturally within IVR? Does the participant report and behave as if the virtual arm is part of their body? We show that participants in a HMD-based IVR who see a virtual body can experience similar responses to threats as those in comparable rubber hand illusion experiments. We show that these responses can be negated by replacing the virtual body with an abstract cursor representing the hand, and that the responses are stable under some gradual forced distortion of tracker space so that proprioceptive and visual information are not matched.},
	author = {Yuan, Ye and Steed, Anthony},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444807},
	issn = {2375-5334},
	keywords = {Rubber;Virtual reality;Galvanizing;Skin;Neuroscience;Testing;Stress;Protocols;Computer science;Educational institutions;Rubber-hand illusion;immersive virtual reality;virtual body;galvanic skin response;body image;body schema},
	month = {March},
	pages = {95-102},
	title = {Is the rubber hand illusion induced by immersive virtual reality?},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444807}}

@inproceedings{5444808,
	abstract = {It is well-documented that natural lighting conditions and real-world backgrounds affect the usability of optical see-through augmented reality (AR) displays in outdoor environments. In many cases, outdoor environmental conditions can dramatically alter users' color perception of user interface elements, by for example, washing out text or icon colors. As a result, users' semantic interpretation of interface elements can be compromised, rendering interface designs useless or counter-productive --- an especially critical problem in application domains where color encoding is critical, such as military or medical visualization. In this paper, we present our experiences designing and constructing an optical AR testbed that emulates outdoor lighting conditions and allows us to measure the combined color of real-world backgrounds and virtual colors as projected through an optical see-through display. We present a formalization of color blending in AR, which supports further research on perceived color in AR displays. We describe an engineering study where we measure the color of light that reaches an optical see-through display user's eye under systematically varied virtual and real-world conditions. Our results further quantify the effect of lighting and background color on the color of virtual graphics, and specifically quantify how virtual colors change based on different real-world backgrounds.},
	author = {Gabbard, Joseph L. and Swan, J. Edward and Zedlitz, Jason and Winchester, Woodrow W.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444808},
	issn = {2375-5334},
	keywords = {Biomedical optical imaging;Displays;Usability;Augmented reality;User interfaces;Rendering (computer graphics);Encoding;Visualization;Optical design;Testing;Outdoor Augmented Reality;Optical See-through Display;User Interface Design;Color Perception},
	month = {March},
	pages = {79-86},
	title = {More than meets the eye: An engineering study to empirically examine the blending of real and virtual color spaces},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444808}}

@inproceedings{5444809,
	abstract = {We propose a novel interface called Twinkle for interacting with an arbitrary physical surface using a handheld projector and a camera. When a user flashes a projection light on an object, the projected images react as if the user touched the object with the light. The handheld device recognizes the features of the physical environment and displays images and sounds that are generated in real-time according to the user's motion and collisions of projected images with objects. We realize this system by using several image-processing techniques and a collision detection algorithm. We also use an acceleration sensor to compensate for the image processing. In this paper, we explain the principle of interacting with a physical surface. Then, we describe the implementation of the prototype system and some application examples.},
	author = {Yoshida, Takumi and Hirobe, Yuki and Nii, Hideaki and Kawakami, Naoki and Tachi, Susumu},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444809},
	issn = {2375-5334},
	keywords = {Cameras;Handheld computers;Image recognition;Displays;Detection algorithms;Acceleration;Image sensors;Acoustic sensors;Image processing;Prototypes;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, augmented, and virtual realities},
	month = {March},
	pages = {87-90},
	title = {Twinkle: Interacting with physical surfaces using handheld projector},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444809}}

@inproceedings{5444810,
	abstract = {Clicking is a key feature any interaction input system needs to provide. In the case of 3D input devices, such a feature is often difficult to provide (e.g. vision-based, or tracking systems for free-hand interaction do not natively provide any button). In this work, we show that it is actually possible to build an application that provides two classical interaction tasks (selection, and pick-release), without any button-like feature. Our method is based on trajectory and kinematic gesture analysis. In a preliminary study we exhibit the principle of the method. Then, we detail an algorithm to discriminate selection, pick and release tasks using kinematic criteria. We present a controlled experiment that validates our method with an average success rate equal to 90.1% across all conditions.},
	author = {Choumane, Ali and Casiez, G{\'e}ry and Grisoni, Laurent},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444810},
	issn = {2375-5334},
	keywords = {Kinematics;Acceleration;Speech recognition;Keyboards;Calibration;User interfaces;Games;Motion estimation;Motion analysis;Diversity reception;H.5.2 [User Interfaces]: Input devices and strategies},
	month = {March},
	pages = {67-70},
	title = {Buttonless clicking: Intuitive select and pick-release through gesture analysis},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444810}}

@inproceedings{5444811,
	abstract = {Tracking is a major issue of virtual and augmented reality applications. Single object tracking on monocular video streams is fairly well understood. However, when it comes to multiple objects, existing methods lack scalability and can recognize only a limited number of objects. Thanks to recent progress in feature matching, state-of-the-art image retrieval techniques can deal with millions of images. However, these methods do not focus on real-time video processing and can not track retrieved objects. In this paper, we present a method that combines the speed and accuracy of tracking with the scalability of image retrieval. At the heart of our approach is a bi-layer clustering process that allows our system to index and retrieve objects based on tracks of features, thereby effectively summarizing the information available on multiple video frames. As a result, our system is able to track in real-time multiple objects, recognized with low delay from a database of more than 300 entries.},
	author = {Pilet, Julien and Saito, Hideo},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444811},
	issn = {2375-5334},
	keywords = {Image retrieval;Target tracking;Scalability;Cameras;Quantization;Augmented reality;Streaming media;Information retrieval;Object detection;Vocabulary},
	month = {March},
	pages = {71-78},
	title = {Virtually augmenting hundreds of real pictures: An approach based on learning, retrieval, and tracking},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444811}}

@inproceedings{5444812,
	abstract = {Many Virtual Environments require walking interfaces to explore virtual worlds much larger than available real-world tracked space. We present a model for generating virtual locomotion speeds from Walking-In-Place (WIP) inputs based on walking biomechanics. By employing gait principles, our model - called Gait-Understanding-Driven Walking-In-Place (GUD WIP) - creates output speeds which better match those evident in Real Walking, and which better respond to variations in step frequency, including realistic starting and stopping. The speeds output by our implementation demonstrate considerably less within-step fluctuation than a good current WIP system - Low-Latency, Continuous-Motion (LLCM) WIP - while still remaining responsive to changes in user input. We compared resulting speeds from Real Walking, GUD WIP, and LLCM-WIP via user study: The average output speeds for Real Walking and GUD WIP respond consistently with changing step frequency - LLCM-WIP is far less consistent. GUD WIP produces output speeds that are more locally consistent (smooth) and step-frequency-to-walk-speed consistent than LLCM-WIP.},
	author = {Wendt, Jeremy D. and Whitton, Mary C. and Brooks, Frederick P.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444812},
	issn = {2375-5334},
	keywords = {Legged locomotion;Virtual environment;Frequency;Computer graphics;Delay;Biomechanics;Virtual reality;Foot;Fluctuations;Data analysis;H.5.1 [Information Interfaces and Presentation]: Multimeda Information Systems-Artificial, augmented, and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Input devices and strategies;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimenshional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {51-58},
	title = {GUD WIP: Gait-Understanding-Driven Walking-In-Place},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444812}}

@inproceedings{5444813,
	abstract = {We present a systematic study on the recognition of 3D gestures using spatially convenient input devices. Specifically, we examine the linear acceleration-sensing Nintendo Wii Remote coupled with the angular velocity-sensing Nintendo Wii MotionPlus. For the study, we created a 3D gesture database, collecting data on 25 distinct gestures totalling 8500 gestures samples. Our experiment explores how the number of gestures and the amount of gestures samples used to train two commonly used machine learning algorithms, a linear and AdaBoost classifier, affect overall recognition accuracy. We examined these gesture recognition algorithms with user dependent and user independent training approaches and explored the affect of using the Wii Remote with and without the Wii MotionPlus attachment. Our results show that in the user dependent case, both the Ad-aBoost and linear classification algorithms can recognize up to 25 gestures at over 90% accuracy, with 15 training samples per gesture, and up to 20 gestures at over 90% accuracy, with only five training samples per gesture. In particular, all 25 gestures could be recognized at over 99% accuracy with the linear classifier using 15 training samples per gesture, with the Wii Remote coupled with the Wii MotionPlus. In addition, both algorithms can recognize up to nine gestures at over 90% accuracy using a user independent training database with 100 samples per gesture. The Wii MotionPlus attachment played a significant role in improving accuracy in both the user dependent and independent cases.},
	author = {Hoffman, Michael and Varcholik, Paul and LaViola, Joseph J.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444813},
	issn = {2375-5334},
	keywords = {Acceleration;Machine learning algorithms;Computer interfaces;Accelerometers;Gyroscopes;Design methodology;Robustness;Spatial databases;Classification algorithms;Pattern recognition;I.6.3 [Computing Methodologies]: Methodologies and Techniques-Interaction Techniques;I.5.2 [Pattern Recognition]: Design Methodology-Classifier Design and Evaluation;K.8 [Computing Milieux]: Personal Computing-Games},
	month = {March},
	pages = {59-66},
	title = {Breaking the status quo: Improving 3D gesture recognition with spatially convenient input devices},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444813}}

@inproceedings{5444814,
	abstract = {Games use high quality graphics and pre-crafted visual and auditive effects to create user experiences. Virtual environments offer new navigation and interaction methods, immersive installations, and haptic and olfactoric output. We introduce an extension to VR systems, which makes it possible to use a wide range of multimodal effects from gaming and VR to be activated and modified on a per object basis at runtime. To access, manipulate, and add effects to the objects of a scene intuitively, our extension realizes an abstract, hierarchical scene concept using multimodal objects. Multiple effects can be added to each object and the parameters of each effect can be manipulated online. Fading of effects and bundling of multiple effects for multiple objects are more advanced features of the system.},
	author = {Haringer, Matthias and Beckhaus, Steffi},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444814},
	issn = {2375-5334},
	keywords = {Layout;Virtual reality;Virtual environment;Mood;Graphics;Navigation;Engines;Runtime;Tuners;Haptic interfaces;virtual environments;shaders;effects;multimodality},
	month = {March},
	pages = {43-46},
	title = {Effect based scene manipulation for multimodal VR systems},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444814}}

@inproceedings{5444815,
	abstract = {Most of today's mobile internet devices contain facilities to display maps of the user's surroundings with points of interest embedded into the map. Other researchers have already explored complementary, egocentric visualizations of these points of interest using mobile mixed reality. Being able to perceive the point of interest in detail within the user's current context is desirable, however, it is challenging to display off-screen or occluded points of interest. We have designed and implemented space-distorting visualizations to address these situations. While this class of visualizations has been extensively studied in information visualization, we are not aware of any attempts to apply them to augmented or mixed reality. Based on the informal user feedback that we have gathered, we have performed several iterations on our visualizations. We hope that our initial results can inspire other researchers to also investigate space-distorting visualizations for mixed and augmented reality.},
	author = {Sandor, Christian and Dey, Arindam and Cunningham, Andrew and Barbier, Sebastien and Eck, Ulrich and Urquhart, Donald and Marner, Michael R. and Jarvis, Graeme and Rhee, Sang},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444815},
	issn = {2375-5334},
	keywords = {Visualization;Virtual reality;Displays;Mobile computing;Image reconstruction;Solid modeling;Internet;Feedback;Augmented reality;Wearable computers;H.5.1. [Information Interfaces and Presentation]: Multimedia Information Systems-[Artificial, augmented and virtual realities];I.3.6 [Computer Graphics]: Methodology and Techniques-[Interaction Techniques]},
	month = {March},
	pages = {47-50},
	title = {Egocentric space-distorting visualizations for rapid environment exploration in mobile mixed reality},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444815}}

@inproceedings{5444816,
	abstract = {Users in virtual environments often find navigation more difficult than in the real world. Our new locomotion interface, Improved Redirection with Distractors (IRD), enables users to walk in larger-than-tracked space VEs without predefined waypoints. We compared IRD to the current best interface, really walking, by conducting a user study measuring navigational ability. Our results show that IRD users can really walk through VEs that are larger than the tracked space and can point to targets and complete maps of VEs no worse than when really walking.},
	author = {Peck, Tabitha C. and Fuchs, Henry and Whitton, Mary C.},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444816},
	issn = {2375-5334},
	keywords = {Navigation;Virtual environment;Visualization;Virtual reality;Displays;Mobile computing;Image reconstruction;Solid modeling;Internet;Feedback;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial-augmented, and virtual realities},
	month = {March},
	pages = {35-38},
	title = {Improved Redirection with Distractors: A large-scale-real-walking locomotion interface and its effect on navigation in virtual environments},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444816}}

@inproceedings{5444817,
	abstract = {Olfactory displays which exist now can only present the set of scents which was prepared beforehand because a set of "primary odors" has not been found. In this paper, we focus on development of an olfactory display using cross modality which can represent more patterns of scents than the patterns of scents prepared. Due to cross modal effect between vision and olfaction, human tends to feel not olfactory but visual stimulation as scents. First, we asked subjects to smell various aroma chemicals and evaluate their similarity. Based on the data of similarity among aromas, we built a map of smell distance. Next, we selected a few aroma chemicals from the smell distance map and implemented a visual and olfactory display. Then we conducted an assessment experiment of the display. We presented various pictures and the selected aroma chemicals to subjects and asked them what kind of scent they smelled like. In this experiment, we succeed in giving subjects feeling more patterns of scents than the number of selected aroma chemicals. Also, we succeed in making not olfactory stimulation by aroma chemicals but visual stimulation by pictures as scents. In particular, we find that the visual effect on olfactory sensation is more strong when the distance between the picture and the aroma chemicals is close than when the distance is far.},
	author = {Nambu, Aiko and Narumi, Takuji and Nishimura, Kunihiro and Tanikawa, Tomohiro and Hirose, Michitaka},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444817},
	issn = {2375-5334},
	keywords = {Olfactory;Virtual reality;Chemical elements;Auditory displays;Haptic interfaces;Humans;Visual effects;Multimedia systems;User interfaces;Olfactory display;multimodal interface;cross modality},
	month = {March},
	pages = {39-42},
	title = {Visual-olfactory display using olfactory sensory map},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444817}}

@inproceedings{5444818,
	abstract = {In this paper, we quantify the effects of two physical affordances on user interaction in an augmented virtual environment: the grounding of datasets on a workbench and the augmentation of a Magic Lens interface tool. We investigated the effects of these affordances on subjects' performance and behavior in an information gathering task. Our results indicated that grounding had a significant main effect and that there was significant interaction between grounding and interface factors. Specifically, subjects tended to perform better with some level of affordance than none at all, although performance with grounded datasets decreased in the presence of augmented Magic Lens. Further, both affordance factors influenced behavior by reducing head mobility.},
	author = {Brown, Leonard D. and Hua, Hong},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444818},
	issn = {2375-5334},
	keywords = {Virtual environment;Grounding;Lenses;Haptic interfaces;Displays;Data visualization;Educational institutions;Optical feedback;Optical sensors;Chromium;Usability;augmented reality;interface;affordance},
	month = {March},
	pages = {23-26},
	title = {An evaluation of physical affordances in augmented virtual environments: Dataset grounding and Magic Lens},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444818}}

@inproceedings{5444819,
	abstract = {Natural Interaction in virtual environments is a key requirement for the virtual validation of functional aspects in automotive product development processes. Natural Interaction is the metaphor people encounter in reality: the direct manipulation of objects by their hands. To enable this kind of Natural Interaction, we propose a pseudo-physical metaphor that is both plausible enough to provide realistic interaction and robust enough to meet the needs of industrial applications. Our analysis of the most common types of objects in typical automotive scenarios guided the development of a set of refined grasping heuristics to support robust finger-based interaction of multiple hands and users. The objects' behavior in reaction to the users' finger motions is based on pseudo-physical simulations, which also take various types of constrained objects into account. In dealing with real-world scenarios, we had to introduce the concept of Normal Proxies, which extend objects with appropriate normals for improved grasp detection and grasp stability. An expert review revealed that our interaction metaphors allow for an intuitive and reliable assessment of several functionalities of objects found in a car interior.},
	author = {Moehring, Mathias and Froehlich, Bernd},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444819},
	issn = {2375-5334},
	keywords = {Virtual reality;Grasping;Robustness;Automotive engineering;Robust stability;Fingers;Object detection;Virtual environment;Hardware;Electrical equipment industry;Direct Interaction;Immersive Applications},
	month = {March},
	pages = {27-34},
	title = {Enabling functional validation of virtual cars through Natural Interaction metaphors},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444819}}

@inproceedings{5444820,
	abstract = {It is extremely challenging to run controlled studies comparing multiple Augmented Reality (AR) systems. We use an AR simulation approach, in which a Virtual Reality (VR) system is used to simulate multiple AR systems. To investigate the validity of this approach, in our first experiment we carefully replicated a well-known study by Ellis et al. using our simulator, obtaining comparable results. We include a discussion on general issues we encountered with replicating a prior study. In our second experiment further exploring the validity of AR simulation, we investigated the effects of simulator latency on the results from experiments conducted in an AR simulator. We found simulator latency to have a significant effect on 3D tracing, however there was no interaction between simulator latency and artificial latency. Based on the results from these two experiments, we conclude that simulator latency is not inconsequential in determining task performance. Simulating visual registration is not sufficient to simulate the overall perception of registration errors in an AR system. We also need to keep simulator latency at a minimum. We discuss the impact of these results on the use of the AR simulation approach.},
	author = {Lee, Cha and Bonebrake, Scott and Bowman, Doug A. and H{\"o}llerer, Tobias},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444820},
	issn = {2375-5334},
	keywords = {Delay;Virtual reality;Computational modeling;Hardware;Graphics;Augmented reality;Computer displays;Degradation;Software performance;Jitter;I.3.7 [Three-Dimensional Graphics and Realism]: Virtual Reality-AR Simulation;I.3.6 [Methodology and Techniques]: Device independence-Replication},
	month = {March},
	pages = {11-18},
	title = {The role of latency in the validity of AR simulation},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444820}}

@inproceedings{5444821,
	abstract = {This paper describes a framework which uses augmented reality for evaluating the performance of mobile computer vision systems. Computer vision systems use primarily image data to interpret the surrounding world, e.g to detect, classify and track objects. The performance of mobile computer vision systems acting in unknown environments is inherently difficult to evaluate since, often, obtaining ground truth data is problematic. The proposed novel framework exploits the possibility to add virtual agents into a real data sequence collected in an unknown environment, thus making it possible to efficiently create augmented data sequences, including ground truth, to be used for performance evaluation. Varying the content in the data sequence by adding different virtual agents is straightforward, making the proposed framework very flexible. The method has been implemented and tested on a pedestrian detection system used for automotive collision avoidance. Preliminary results show that the method has potential to replace and complement physical testing, for instance by creating collision scenarios, which are difficult to test in reality.},
	author = {Nilsson, Jonas and {\"O}dblom, Anders C. E. and Fredriksson, Jonas and Zafar, Adeel and Ahmed, Fahim},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444821},
	issn = {2375-5334},
	keywords = {Mobile computing;Computer vision;Augmented reality;Virtual reality;System testing;Vehicle safety;Road safety;Application software;Road vehicles;Remotely operated vehicles;Augmented reality;computer vision;performance evaluation;active safety;collision avoidance},
	month = {March},
	pages = {19-22},
	title = {Performance evaluation method for mobile computer vision systems using augmented reality},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444821}}

@inproceedings{5444836,
	abstract = {This paper presents a solution for the photorealistic rendering of synthetic objects into dynamic real scenes, in Augmented Reality applications. In order to achieve this goal, an Image Based Lighting approach is used, where environment maps with different levels of glossiness are generated for each virtual object in the scene at every frame. Due to this, illumination effects, such as color bleeding and specular reflections, can be simulated for virtual objects in a consistent way. A unifying sampling method for the spherical harmonics transformation pass is also used. It is independent of map format and does not need to apply different weights for each sample. The developed technique is combined with an extended version of Lafortune Spatial BRDF, featuring Fresnel effect and an innovative tangent rotation parameterization. The solution is evaluated in various Augmented Reality case studies, where other features like shadowing and lens effects are also exploited.},
	author = {Pessoa, Saulo and Moura, Guilherme and Lima, Jo{\~a}o and Teichrieb, Veronica and Kelner, Judith},
	booktitle = {2010 IEEE Virtual Reality Conference (VR)},
	date-added = {2024-03-18 02:29:17 -0400},
	date-modified = {2024-03-18 02:29:17 -0400},
	doi = {10.1109/VR.2010.5444836},
	issn = {2375-5334},
	keywords = {Augmented reality;Lighting;Layout;Computer graphics;Virtual reality;Rendering (computer graphics);Hemorrhaging;Optical reflection;Sampling methods;Shadow mapping;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, Augmented, and Virtual Realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Color, Shading, Shadowing, and Texture},
	month = {March},
	pages = {3-10},
	title = {Photorealistic rendering for Augmented Reality: A global illumination and BRDF solution},
	year = {2010},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2010.5444836}}

@inproceedings{4810990,
	abstract = {In this paper we introduce a "spatialized haptic rendering" technique to enhance 6DOF haptic manipulation of virtual objects with impact position information using vibrations. This rendering technique uses our perceptive ability to determine the contact position by using the vibrations generated by the impact. In particular, the different vibrations generated by a beam are used to convey the impact position information. We present two experiments conducted to tune and evaluate our spatialized haptic rendering technique. The first experiment investigates the vibration parameters (amplitudes/frequencies) needed to enable an efficient discrimination of the force patterns used for spatialized haptic rendering. The second experiment is an evaluation of spatialized haptic rendering during 6DOF manipulation. Taken together, the results suggest that spatialized haptic rendering can be used to improve the haptic perception of impact position in complex 6DOF interactions.},
	author = {Sreng, Jean and Lecuyer, Anatole and Andriot, Claude and Arnaldi, Bruno},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4810990},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Vibrations;Rendering (computer graphics);Virtual reality;Frequency;Force feedback;Rough surfaces;Surface roughness;Probes;Multimedia systems;Haptic rendering;force-feedback;vibration;spatialization;6DOF;contact;impact;open-loop;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;Augmented;and Virtual Realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Haptics I/O},
	month = {March},
	pages = {3-9},
	title = {Spatialized Haptic Rendering: Providing Impact Position Information in 6DOF Haptic Simulations Using Vibrations},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4810990}}

@inproceedings{4810992,
	abstract = {Immersive virtual environments (IVEs) allow participants to interact with their 3D surroundings using natural hand gestures. Previous work shows that the addition of haptic feedback cues improves performance on certain 3D tasks. However, we believe this is not true for all situations. Depending on the difficulty of the task, we suggest that we should expect differences in the ballistic movement of our hands when presented with different types of haptic force-feedback conditions. We investigated how hard, soft and no haptic force-feedback responses, experienced when in contact with the surface of an object, affected user performance on a task involving selection of multiple targets. To do this, we implemented a natural egocentric selection interaction technique by integrating a two-handed large-scale force-feedback device in to a CAVETM-like IVE system. With this, we performed a user study where we show that participants perform selection tasks best when interacting with targets that exert soft haptic force-feedback cues. For targets that have hard and no force-feedback properties, we highlight certain associated hand movement that participants make under these conditions, that we hypothesise reduce their performance.},
	author = {Pawar, Vijay M and Steed, Anthony},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4810992},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Haptics;3D selection;task performance;two-handed interaction;force-feedback;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Haptics;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques},
	month = {March},
	pages = {11-18},
	title = {Evaluating the Influence of Haptic Force-Feedback on 3D Selection Tasks using Natural Egocentric Gestures},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4810992}}

@inproceedings{4810993,
	abstract = {This paper describes a global interactive scheme including fast motion planning and real time guiding force for 3D CAD part assembly or disassembly tasks. For real time purpose, the motion planner is divided into different steps. First, a preliminary workspace discretization is done without time limitations at the beginning of the simulation. Then, using those computed data, a second part tries to find a collision free path in real time. Once the path is found, an haptic artificial force is applied constraining the user on the path. The user can then influence the planner by not following the path and automatically order a new path research. The performance of this haptic assistance is measured on a test simulation based on an ALSTOM power components assembly simulation.},
	author = {Ladeveze, Nicolas and Fourquet, Jean Yves and Puel, Bernard and Taix, Michel},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4810993},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Assembly;Path planning;Virtual reality;Computational modeling;Testing;Layout;Motion planning;Computer graphics;Design automation;Virtual Reality;Motion planning;Octree;A star;Rapidly Exploring Deterministic Tree;Haptics;I.2.9 [Artificial Intelligence]: Robotics-Workcell organization and planning;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism - Virtual reality-;H.5.2 [User Interfaces]: Haptic I/O-;[J.6]: Computer-Aided Design- Computer Aided Design;J.2 [Physical Sciences And Engineering]: Engineering-},
	month = {March},
	pages = {19-25},
	title = {Haptic Assembly and Disassembly Task Assistance using Interactive Path Planning},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4810993}}

@inproceedings{4810995,
	abstract = {Designing low end-to-end latency system architectures for virtual reality is still an open and challenging problem. We describe the design, implementation and evaluation of a client-server depth-image warping architecture that updates and displays the scene graph at the refresh rate of the display. Our approach works for scenes consisting of dynamic and interactive objects. The end-to-end latency is minimized as well as smooth object motion generated. However, this comes at the expense of image quality inherent to warping techniques. We evaluate the architecture and its design trade-offs by comparing latency and image quality to a conventional rendering system. Our experience with the system confirms that the approach facilitates common interaction tasks such as navigation and object manipulation.},
	author = {Smit, Ferdi and van Liere, Robert and Beck, Stephan and Froehlich, Bernd},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4810995},
	issn = {2375-5334},
	keywords = {Virtual reality;Delay;Image quality;Layout;Displays;File servers;Rendering (computer graphics);Computer graphics;Computer architecture;Navigation;I.3.3 [Computer Graphics]: Picture/Image Generation-Display Algorithms;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {27-34},
	title = {An Image-Warping Architecture for VR: Low Latency versus Image Quality},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4810995}}

@inproceedings{4810996,
	abstract = {We present a novel calibration framework for multi-projector displays that achieves continuous geometric calibration by estimating and refining the poses of all projectors in an ongoing fashion during actual display use. Our framework provides scalability by operating as a distributed system of "intelligent" projector units: projectors augmented with rigidly-mounted cameras, and paired with dedicated computers. Each unit interacts asynchronously with its peers, leveraging their combined computational power to cooperatively estimate the poses of all of the projectors. In cases where the projection surface is static, our system is able to continuously refine all of the projector poses, even when they change simultaneously.},
	author = {Johnson, Tyler and Welch, Greg and Fuchs, Henry and la Force, Eric and Towles, Herman},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4810996},
	issn = {2375-5334},
	keywords = {Calibration;Computer displays;Shape;Computer graphics;Virtual reality;Smart cameras;Distributed computing;Robustness;Optical filters;Image motion analysis;Projector displays;continuous calibration;I.3.3 [Computer Graphics]: Picture/Image Generation-Display Algorithms;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {35-42},
	title = {A Distributed Cooperative Framework for Continuous Multi-Projector Pose Estimation},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4810996}}

@inproceedings{4810997,
	abstract = {Several critical limitations exist in the currently available tracking technologies for fully-enclosed virtual reality (VR) systems. While several 6DOF tracking projects such as Hedgehog [24] have successfully demonstrated excellent accuracy, precision, and robustness within moderate budgets, these projects still include elements of hardware that can interfere with the user's visual experience. The objective of this project is to design a tracking solution for fully-enclosed VR displays that achieves comparable performance to available commercial solutions but without any artifacts that can obscure the user's view. JanusVF is a tracking solution involving a cooperation of both the hardware sensors and the software rendering system. A small, high-resolution camera is worn on the user's head, but faces backwards (180 degree rotation about vertical from the user's perspective). After acquisition of the initial state, the VR rendering software draws specific fiducial markers with known size and absolute position inside the VR scene. These virtual markers are only drawn behind the user and in view of the camera. These fiducials are tracked by ARToolkitPlus [25] and integrated by a single-constraint-at-a-time (SCAAT) [26] filter algorithm to update the head pose. Early experiments in a six-sided CAVE-like system show performance that is comparable to alternative commercial technologies.},
	author = {Hutson, Malcolm and White, Steven and Reiners, Dirk},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4810997},
	issn = {2375-5334},
	keywords = {Navigation;Virtual reality;Hardware;Cameras;Robustness;Displays;Sensor systems;Software systems;Layout;Filters;I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Stereo;Tracking;H.5.2 [Information Interfaces and Presentation (I.7)]: User Interfaces (D.2.2;H.1.2;I.3.6)-Evaluation/methodology;Input devices and strategies;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality},
	month = {March},
	pages = {43-50},
	title = {JanusVF: Accurate Navigation Using SCAAT and Virtual Fiducials},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4810997}}

@inproceedings{4810998,
	abstract = {Investment into multi-wall immersive virtual environments is often motivated by the potential for small groups of users to work collaboratively, yet most systems only allow for stereographic rendering from a single viewpoint. This paper discusses approaches for supporting copresent head-tracked users in an immersive projection environment, such as the CAVEtrade, without relying on additional projection and frame-multiplexing technology. The primary technique presented here is called image blending and consists of rendering independent views for each head-tracked user to an off-screen buffer and blending the images into a final composite view using view-vector incidence angles as weighting factors. Additionally, users whose view-vectors intersect a projection screen at similar locations are grouped into a view-cluster. Clustered user views are rendered from the average head position and orientation of all users in that cluster. The clustering approach minimizes users' exposure to undesirable display artifacts such as inverted stereo pairs and nonlinear object projections by distributing projection error over all tracked viewers. These techniques have the added advantage that they can be easily integrated into existing systems with minimally increased hardware and software requirements. We compare image blending and view clustering with previously published techniques and discuss possible implementation optimizations and their tradeoffs.},
	author = {Marbach, Jonathan},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4810998},
	issn = {2375-5334},
	keywords = {Rendering (computer graphics);Collaboration;Collaborative work;Hardware;Virtual reality;Clustering algorithms;Head;Polarization;Layout;Geology;Immersive Virtual Reality;Multi-Viewer Images;Collaboration;Geometry Shader;I.3.3 Picture/Image Generation - Display Algorithms;Viewing Algorithms;I.3.7 Three-Dimensional Graphics and Realism - Virtual Reality},
	month = {March},
	pages = {51-54},
	title = {Image Blending and View Clustering for Multi-Viewer Immersive Projection Environments},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4810998}}

@inproceedings{4810999,
	abstract = {We tested users' depth perception of virtual objects in our mobile augmented reality (AR) system in both indoor and outdoor environments using a depth matching task. The indoor environment is characterized by strong linear perspective cues; we attempted to re-create these cues in the outdoor environment. In the indoor environment, we found an overall pattern of underestimation of depth that is typical for virtual environments and AR systems. However, in the outdoor environment, we found that subjects overestimated depth. In addition, our synthetic linear perspective cues met with a measure of success, leading users to reduce their estimate of the depth of distant objects. We describe the experimental procedure, analyze the data, present the results of the study, and discuss the implications for mobile, outdoor AR systems.},
	author = {Livingston, Mark A. and Ai, Zhuming and Swan, J. Edward and Smallman, Harvey S.},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4810999},
	issn = {2375-5334},
	keywords = {Augmented reality;Displays;Laboratories;Indoor environments;Virtual environment;Virtual reality;Layout;System testing;Data analysis;Multimedia systems;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/Methodology;H.1.2 [Models and Principles]: User/Machine Systems-Human factors},
	month = {March},
	pages = {55-62},
	title = {Indoor vs. Outdoor Depth Perception for Mobile Augmented Reality},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4810999}}

@inproceedings{4811000,
	abstract = {We present an application of interactive global illumination and spatially augmented reality to architectural daylight modeling that allows designers to explore alternative designs and new technologies for improving the sustainability of their buildings. Images of a model in the real world, captured by a camera above the scene, are processed to construct a virtual 3D model. To achieve interactive rendering rates, we use a hybrid rendering technique, leveraging radiosity to simulate the inter-reflectance between diffuse patches and shadow volumes to generate per-pixel direct illumination. The rendered images are then projected on the real model by four calibrated projectors to help users study the daylighting illumination. The virtual heliodon is a physical design environment in which multiple designers, a designer and a client, or a teacher and students can gather to experience animated visualizations of the natural illumination within a proposed design by controlling the time of day, season, and climate. Furthermore, participants may interactively redesign the geometry and materials of the space by manipulating physical design elements and see the updated lighting simulation.},
	author = {Sheng, Yu and Yapo, Theodore C. and Young, Christopher and Cutler, Barbara},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811000},
	issn = {2375-5334},
	keywords = {Augmented reality;Daylighting;Lighting;Rendering (computer graphics);Buildings;Cameras;Layout;Computer graphics;Hybrid power systems;Animation;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Radiosity;Virtual Reality;H.5.1 [Information Interfaces and Presentation (HCI)]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
	month = {March},
	pages = {63-70},
	title = {Virtual Heliodon: Spatially Augmented Reality for Architectural Daylighting Design},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811000}}

@inproceedings{4811001,
	abstract = {This article introduces explosion diagrams to augmented reality (AR) applications. It presents algorithms to seamlessly integrate an object's explosion diagram into a real world environment, including the AR rendering of relocated objects textured with live video and the restoration of visual information which are hidden behind relocated objects. It demonstrates several types of visualizations for convincing AR explosion diagrams and it discusses visualizations of exploded parts as well as visual links conveying their relocation direction. Furthermore, we show the integration of our rendering and visualization techniques in an AR framework, which is able to automatically compute a diagram's layout and an animation of its corresponding explosion.},
	author = {Kalkofen, Denis and Tatzgern, Markus and Schmalstieg, Dieter},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811001},
	issn = {2375-5334},
	keywords = {Explosions;Augmented reality;Object overlay and spatial layout techniques;realtime rendering;mediated and diminished reality;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;E.1 [Data Structures]: Graphs and Networks;I.3.6 [Computer Graphics]: Methodology and Techniques-Graphics data structures and data types},
	month = {March},
	pages = {71-78},
	title = {Explosion Diagrams in Augmented Reality},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811001}}

@inproceedings{4811002,
	abstract = {Augmented reality x-ray vision allows users to see through walls and view real occluded objects and locations. We present an augmented reality x-ray vision system that employs multiple view modes to support new visualizations that provide depth cues and spatial awareness to users. The edge overlay visualization provides depth cues to make hidden objects appear to be behind walls, rather than floating in front of them. Utilizing this edge overlay, the tunnel cut-out visualization provides details about occluding layers between the user and remote location. Inherent limitations of these visualizations are addressed by our addition of view modes allowing the user to obtain additional detail by zooming in, or an overview of the environment via an overhead exocentric view.},
	author = {Avery, Benjamin and Sandor, Christian and Thomas, Bruce H.},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811002},
	issn = {2375-5334},
	keywords = {Augmented reality;X-ray imaging;Visualization;Machine vision;Layout;Rendering (computer graphics);Wearable computers;Computer graphics;Image reconstruction;Virtual reality;Outdoor Augmented Reality;Wearable Computers;Image-Based Rendering;Visualization;Depth Perception;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism - Virtual Reality;J.9.e [Mobile Applications]: Wearable computers and body area networks},
	month = {March},
	pages = {79-82},
	title = {Improving Spatial Perception for Augmented Reality X-Ray Vision},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811002}}

@inproceedings{4811003,
	abstract = {In face-to-face collaboration, eye gaze is used both as a bidirectional signal to monitor and indicate focus of attention and action, as well as a resource to manage the interaction. In remote interaction supported by immersive collaborative virtual environments (ICVEs), embodied avatars representing and controlled by each participant share a virtual space. We report on a study designed to evaluate methods of avatar eye gaze control during an object-focused puzzle scenario performed between three networked CAVEtrade-like systems. We compare tracked gaze, in which avatars' eyes are controlled by head-mounted mobile eye trackers worn by participants, to a gaze model informed by head orientation for saccade generation, and static gaze featuring non-moving eyes. We analyse task performance, subjective user experience, and interactional behaviour. While not providing statistically significant benefit over static gaze, tracked gaze is observed as the highest performing condition. However, the gaze model resulted in significantly lower task performance and increased error rate.},
	author = {Steptoe, William and Oyekoya, Oyewole and Murgia, Alessio and Wolff, Robin and Rae, John and Guimaraes, Estefania and Roberts, David and Steed, Anthony},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811003},
	issn = {2375-5334},
	keywords = {Avatars;Collaboration;Virtual environment;Eyes;Remote monitoring;Resource management;Design methodology;Control systems;Performance evaluation;Performance analysis;Immersive Collaborative Virtual Environments;Eye Tracking;Avatars;Eye Gaze;Behavioural Realism;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;H.4.3 [Information Systems Applications]: Communications Applications-Computer conferencing;teleconferencing;and videoconferencing;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Animation},
	month = {March},
	pages = {83-90},
	title = {Eye Tracking for Avatar Eye Gaze Control During Object-Focused Multiparty Interaction in Immersive Collaborative Virtual Environments},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811003}}

@inproceedings{4811004,
	abstract = {The goal of our work is to develop a programmatically controlled peer to ride with a human subject for the purpose of studying how social interactions influence riding behavior. The peer is controlled through a combination of reactive controllers that determine the gross motion of the virtual bicycle, action-based controllers that animate the virtual bicyclist and generate verbal behaviors, and a keyboard interface that allows an experimenter to initiate the virtual bicyclist's actions during the course of an experiment. The virtual bicyclist's repertoire of behaviors includes road following, riding alongside the human rider, stopping at intersections, and crossing intersections through specified gaps. The virtual cyclist engages the human subject through gaze, gesture, and verbal interactions. We describe the structure of the behavior code and report the results of a pilot study examining how 10- and 12-year-old children interact with a peer cyclist. Results of the pilot study showed that the presence of the peer had a significant influence on the size of the gaps taken as well as time left to spare between the participant and the trailing car in the crossed gap.},
	author = {Babu, Sabarish and Grechkin, Timofey and Chihak, Benjamin and Ziemer, Christine and Kearney, Joseph and Cremer, James and Plumert, Jodie},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811004},
	issn = {2375-5334},
	keywords = {Humans;Bicycles;Injuries;Traffic control;Motion control;Peer to peer computing;Road accidents;Decision making;Psychology;Animation;Virtual Humans;Applied Perception;3D Human-Computer Interaction;K.3 Computers and Education;H.5 Information Interfaces and Presentation},
	month = {March},
	pages = {91-98},
	title = {A Virtual Peer for Investigating Social Influences on Children's Bicycling},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811004}}

@inproceedings{4811005,
	abstract = {This paper proposes virtual social perspective-taking (VSP). In VSP, users are immersed in an experience of another person to aid in understanding the person's perspective. Users are immersed by 1) providing input to user senses from logs of the target person's senses, 2) instructing users to act and interact like the target, and 3) reminding users that they are playing the role of the target. These guidelines are applied to a scenario where taking the perspective of others is crucial - the medical interview. A pilot study (n = 16) using this scenario indicates VSP elicits reflection on the perspectives of others and changes behavior in future, similar social interactions. By encouraging reflection and change, VSP advances the state-of-the-art in training social interactions with virtual experiences.},
	author = {Raij, Andrew and Kotranza, Aaron and Lind, D. Scott and Lok, Benjamin},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811005},
	issn = {2375-5334},
	keywords = {Avatars;Humans;Reflection;Virtual reality;Computer graphics;Medical diagnostic imaging;Breast;Cancer;Senior citizens;Educational institutions;Immersion;Virtual Reality;Human-Computer Interaction;Virtual Humans;Mixed Reality;Tangible Interfaces;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {99-102},
	title = {Virtual Experiences for Social Perspective-Taking},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811005}}

@inproceedings{4811006,
	abstract = {In many technical support systems, live support agents help end-users resolve issues with computer software/hardware and appliances/gadgets in the real world. The dominant mode of communication in such systems is still the telephone, while instant messaging, video communication, and remote take-over have emerged as additional modalities in recent years. In contrast to these, 3D avatar based visual co-presence offers a unique combination of gestural interaction, shared reality, agent multitasking, and anonymity. Our paper argues that such 3D co-presence is viable in computer chat/remote help sessions, as well as real world support over camera equipped devices, offering an attractive alternative and enhancement to today's support modalities.},
	author = {Guven, Sinem and Podlaseck, Mark and Pingali, Gopal},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811006},
	issn = {2375-5334},
	keywords = {Avatars;Virtual reality;Virtual environment;Second Life;Telephony;Video sharing;Cameras;Context;Streaming media;Software;Co-presence;virtual reality;augmented reality;interaction;technical support;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities;H.5.2 [User Interfaces]: Interaction styles},
	month = {March},
	pages = {103-106},
	title = {Exploring Co-presence for Next Generation Technical Support},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811006}}

@inproceedings{4811007,
	abstract = {How do users of virtual environments perceive virtual space? Many experiments have explored this question, but most of these have used head-mounted immersive displays. This paper reports an experiment that studied large-screen immersive displays at medium-field distances of 2 to 15 meters. The experiment measured ego-centric depth judgments in a CAVE, a tiled display wall, and a real-world outdoor field as a control condition. We carefully modeled the outdoor field to make the three environments as similar as possible. Measuring egocentric depth judgments in large-screen immersive displays requires adapting new measurement protocols; the experiment used timed imagined walking, verbal estimation, and triangulated blind walking. We found that depth judgments from timed imagined walking and verbal estimation were very similar in all three environments. However, triangulated blind walking was accurate only in the out-door field; in the large-screen immersive displays it showed under-estimation effects that were likely caused by insufficient physical space to perform the technique. These results suggest using timed imagined walking as a primary protocol for assessing depth perception in large-screen immersive displays. We also found that depth judgments in the CAVE were more accurate than in the tiled display wall, which suggests that the peripheral scenery offered by the CAVE is helpful when perceiving virtual space.},
	author = {Klein, Eric and Swan, J. Edward and Schmidt, Gregory S. and Livingston, Mark A. and Staadt, Oliver G.},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811007},
	issn = {2375-5334},
	keywords = {Protocols;Large screen displays;Legged locomotion;Virtual environment;Layout;Particle measurements;Observers;Face detection;Calibration;Area measurement;Distance Perception;Egocentric Depth Perception;Virtual Environments;Large-Screen Immersive Displays;I.2.10 [Artifical Intelligence]: Vision and Scene Understanding-Perceptual Reasoning;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Ergonomics},
	month = {March},
	pages = {107-113},
	title = {Measurement Protocols for Medium-Field Distance Perception in Large-Screen Immersive Displays},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811007}}

@inproceedings{4811009,
	abstract = {Augmented reality (AR) displays often reduce the visual capabilities of the user. This reduction can be measured both objectively and through user studies. We acquired objective measurements with a color meter and conducted two user studies for each of two key measurements. First was the combined effect of resolution and display contrast, which equate to the visual acuity and apparent brightness. The combined effect may be captured by the contrast sensitivity function and measured through analogs of optometric exams. We expanded the number of commercial devices tested in previous studies, including higher resolution and video-overlay AR displays. We found patterns of reduced contrast sensitivity similar to previous work; however, we saw that all displays enabled users to achieve the maximum possible acuity with at least moderate levels of contrast. The second measurement was the perception of color. Objective measurements showed a distortion of color, notably in the blue region of color space. We devised a color matching task to quantify the distortion of color perception, finding that the displays themselves were poor at showing colors in the blue region of color space and that the perceptual distortion of such colors was even greater than the objective distortion. We noted significantly different distortions and variability between displays.},
	author = {Livingston, Mark A. and Barrow, Jane H. and Sibley, Ciara M.},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811009},
	issn = {2375-5334},
	keywords = {Augmented reality;Protocols;Legged locomotion;Large screen displays;Virtual environment;Layout;Particle measurements;Observers;Face detection;Calibration;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/Methodology;H.1.2 [Models and Principles]: User/Machine Systems-Human factors},
	month = {March},
	pages = {115-122},
	title = {Quantification of Contrast Sensitivity and Color Perception using Head-worn Augmented Reality Displays},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811009}}

@inproceedings{4811010,
	abstract = {In this paper we present the multi-display environment Deskotheque, which combines personal and tiled projected displays into a continuous teamspace. Its main distinguishing factor is a fine-grained spatial (i. e., both geometric and topological) model of the display layout. Using this model, Deskotheque allows seamless mouse pointer navigation and application window sharing across the multi-display environment. Geometric compensation of casually aligned multi-projector displays supports a wide range of display configurations. Mouse pointer redirection and window migration are tightly integrated into the windowing system, while geometric compensation of projected imagery is accomplished by a 3D compositing window manager. Thus, Deskotheque provides sharing of unmodified desktop application windows across display and workstation boundaries without compromising hardware-accelerated rendering of 2D or 3D content on projected tiled displays with geometric compensation.},
	author = {Pirchheim, Christian and Waldner, Manuela and Schmalstieg, Dieter},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811010},
	issn = {2375-5334},
	keywords = {Model driven engineering;Mice;Navigation;Solid modeling;Computer displays;Two dimensional displays;Three dimensional displays;Collaborative work;Electronic mail;User interfaces;Multi-Display Environment;Geometric Display Compensation;Collaboration;Mouse Pointer Navigation;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces-Collaborative computing;I.3.3 [Computer Graphics]: Picture/Image Generation-Display algorithms;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Windowing systems},
	month = {March},
	pages = {123-126},
	title = {Deskotheque: Improved Spatial Awareness in Multi-Display Environments},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811010}}

@inproceedings{4811011,
	abstract = {We are investigating the utility of a projection-based stereoscopic two-user system for applications in the automotive industry. In this paper we compare real-world pointing to pointing at virtual objects in a two-user scenario. In our study we investigated the following situation: One user points at an object while the second person has to guess the referenced object. Our results indicate that pointing at objects in a virtual world is much less precise than pointing in the real world even for a well calibrated stereoscopic two-user system. Pointing techniques like outlining an object or pointing from a distance produce more errors than the corresponding real-world techniques, but they are less error prone than interactions requiring to touch a virtual object. Our findings are a first step to qualify direct interaction techniques in a multi-user projection-based system.},
	author = {Salzmann, Holger and Moehring, Mathias and Froehlich, Bernd},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811011},
	issn = {2375-5334},
	keywords = {Virtual environment;Computer graphics;Automotive engineering;Collaborative work;Computer errors;Collaboration;Head;Displays;Assembly systems;Virtual reality;mixed reality;multi-user systems;collaborative virtual environment;human-machine interfaces;co-location;I.3.1 [Computer Graphics]: Hardware Architecture-Three-dimensional displays;I.3.6 [Computer Graphics]: Methodology and Techniques-Ergonomics;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality},
	month = {March},
	pages = {127-130},
	title = {Virtual vs. Real-World Pointing in Two-User Scenarios},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811011}}

@inproceedings{4811012,
	abstract = {This paper reports one experiment conducted to evaluate the influence of oscillating camera motions on the perception of traveled distances in virtual environments. In the experiment, participants viewed visual projections of translations along straight paths. They were then asked to reproduce the traveled distance during a navigation phase using keyboard keys. Each participant had to complete the task (1) with linear camera motion, and (2) with oscillating camera motion that simulates the visual flow generated by natural human walking. Taken together, our preliminary results suggest that oscillating camera motions allow a more accurate distance reproduction for short traveled distances.},
	author = {Terziman, Leo and Lecuyer, Anatole and Hillaire, Sebastien and Wiener, Jan M.},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811012},
	issn = {2375-5334},
	keywords = {Cameras;Virtual environment;Legged locomotion;Humans;Virtual reality;Navigation;Computer graphics;Cognitive science;Keyboards;Multimedia systems;camera motion;walking;perception;distance;firstperson-navigation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Input devices and strategies;Interaction styles;User-centered design;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality},
	month = {March},
	pages = {131-134},
	title = {Can Camera Motions Improve the Perception of Traveled Distance in Virtual Environments?},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811012}}

@inproceedings{4811013,
	abstract = {Eye gaze is an important and widely studied non-verbal resource in co-located social interaction. When we attempt to support tele-presence between people, there are two main technologies that can be used today: video-conferencing (VC) and collaborative virtual environments (CVEs). In VC, one can observe eye-gaze behaviour but practically the targets of eye-gaze are only correct if the participants remain relatively still. We attempt to support eye-gaze behaviour in an unconstrained manner by integrating eye-trackers into an Immersive CVE (ICVE) system. This paper aims to show that while both ICVE and VC allow people to discern being looked at and what else is looked at, when someone gazes into their space from another location, ICVE alone can continue to do this as people move. The conditions of aligned VC, ICVE, eye-gaze enabled ICVE and co-location are compared. The impact of factors of alignment, lighting, resolution, and perspective distortion are minimised through a set of pilot experiments, before a formal experiment records results for optimal settings. Results show that both VC and ICVE support eye-gaze in constrained situations, but only ICVE supports movement of the observer. We quantify the mis-judgements that are made and discuss how our findings might inform research into supporting eye-gaze through interpolated free viewpoint video based methods.},
	author = {Roberts, David and Wolff, Robin and Rae, John and Steed, Anthony and Aspin, Rob and McIntyre, Moira and Pena, Adriana and Oyekoya, Oyewole and Steptoe, Will},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811013},
	issn = {2375-5334},
	keywords = {Collaboration;Virtual environment;Videoconference;Virtual colonoscopy;Cameras;Avatars;Computer graphics;Displays;Video sharing;Head;Immersive Collaborative Virtual Environments;Eye-Tracking;Gaze-Tracking;Video Conferencing;Tele-presence;H.5.1 [Communications Applications]: Computer conferencing, teleconferencing, and videoconferencing;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism: Virtual Reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism: Animation},
	month = {March},
	pages = {135-142},
	title = {Communicating Eye-gaze Across a Distance: Comparing an Eye-gaze enabled Immersive Collaborative Virtual Environment, Aligned Video Conferencing, and Being Together},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811013}}

@inproceedings{4811014,
	abstract = {Due to the intrinsic subtlety and dynamics of eye movements, automated generation of natural and engaging eye motion has been a challenging task for decades. In this paper we present an effective technique to synthesize natural eye gazes given a head motion sequence as input, by statistically modeling the innate coupling between gazes and head movements. We first simultaneously recorded head motions and eye gazes of human subjects, using a novel hybrid data acquisition solution consisting of an optical motion capture system and off-the-shelf video cameras. Then, we statistically learn gaze-head coupling patterns using a dynamic coupled component analysis model. Finally, given a head motion sequence as input, we can synthesize its corresponding natural eye gazes based on the constructed gaze-head coupling model. Through comparative user studies and evaluations, we found that comparing with the state of the art algorithms in eye motion synthesis, our approach is more effective to generate natural gazes correlated with given head motions. We also showed the effectiveness of our approach for gaze simulation in two-party conversations.},
	author = {Ma, Xiaohan and Deng, Zhigang},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811014},
	issn = {2375-5334},
	keywords = {Magnetic heads;Computer graphics;Optical recording;Cameras;Coupled mode analysis;Avatars;Humans;Data acquisition;Facial animation;Virtual reality;Gaze-Head Coupling;Eye Motion;Facial Animation;Digital Avatars;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Animation;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Graphical user interfaces (GUI)},
	month = {March},
	pages = {143-150},
	title = {Natural Eye Motion Synthesis by Modeling Gaze-Head Coupling},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811014}}

@inproceedings{4811015,
	abstract = {Trials on the transmission of olfactory information together with audio/visual information are currently underway. However, a problem exists in that continuous emission of scent leaves scent in the air causing human olfactory adaptation. To resolve this problem, we aimed at minimizing the quantity of scent ejected using an ink-jet olfactory display developed. Following the development of a breath sensor for breath synchronization, we next developed an olfactory ejection system to present scent on each inspiration. We then measured human olfactory characteristics in order to determine the most suitable method for presenting scent on an inspiration. Experiments revealed that the intensity of scent perceived by the user was altered by differences in the presentation method even when the quantity of scent was unchanged. We present here a method of odor presentation that most effectively minimizes the ejection quantities.},
	author = {Sato, Junta and Ohtsu, Kaori and Bannai, Yuichi and Okada, Ken-ichi},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811015},
	issn = {2375-5334},
	keywords = {Olfactory;Humans;Computer displays;Auditory displays;Sensor phenomena and characterization;Sensor systems;Anthropometry;Virtual reality;Nose;Control systems;Olfactory information;Human Olfactory Characteristics;olfactory display;ink-jet;pulse ejection;breath sensor;H5.2 [INFORMATION INTERFACES AND PRESENTATION]: User Interfaces-User-centered design;H.1.2 [MODELS AND PRINCIPLES]: User/Machine Systems-Human factors},
	month = {March},
	pages = {151-158},
	title = {Effective Presentation Technique of Scent Using Small Ejection Quantities of Odor},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811015}}

@inproceedings{4811016,
	abstract = {A variety of smells can be realized by blending multiple odor components using an olfactory display. Since a set of odor components to cover the entire range of smells has not yet been known, we studied a method of selecting odor components using a large-scale mass spectrum database. Basis vectors corresponding to odor components were extracted by the NMF (nonnegative matrix factorization) method. Then, the recipe of the target odor was obtained using the nonnegative least-squares method. The basis vectors were successfully obtained from 10,000 compounds within a tolerable error. Moreover, the mass spectra of 104 odors composed of 322 compounds could be approximated using 32-50 basis vectors.},
	author = {Nakamoto, Takamichi and Murakami, Keisuke},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811016},
	issn = {2375-5334},
	keywords = {Olfactory;Displays;Databases;Virtual reality;Mass spectroscopy;Solenoids;Valves;Educational institutions;Data engineering;Weight control;Olfactory display;Mass spectrometry;NMF method;nonnegative least squares;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems -Artificial;Augmented;and Virtual Realities},
	month = {March},
	pages = {159-162},
	title = {Selection Method of Odor Components for Olfactory Display Using Mass Spectrum Database},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811016}}

@inproceedings{4811017,
	abstract = {Posttraumatic Stress Disorder (PTSD) is reported to be caused by traumatic events that are outside the range of usual human experience including (but not limited to) military combat, violent personal assault, being kidnapped or taken hostage and terrorist attacks. Initial data suggests that at least 1 out of 5 Iraq War veterans are exhibiting symptoms of depression, anxiety and PTSD. Virtual Reality (VR) delivered exposure therapy for PTSD has been previously used with reports of positive outcomes. The current paper is a follow-up to a paper presented at IEEE VR2006 and will present the rationale and description of a VR PTSD therapy application (Virtual Iraq) and present the findings from its use with active duty service members since the VR2006 presentation. Virtual Iraq consists of a series of customizable virtual scenarios designed to represent relevant Middle Eastern VR contexts for exposure therapy, including a city and desert road convoy environment. User-centered design feedback needed to iteratively evolve the system was gathered from returning Iraq War veterans in the USA and from a system deployed in Iraq and tested by an Army Combat Stress Control Team. Results from an open clinical trial using Virtual Iraq at the Naval Medical Center-San Diego with 20 treatment completers indicate that 16 no longer met PTSD diagnostic criteria at post-treatment, with only one not maintaining treatment gains at 3 month follow-up.},
	author = {Yeh, Shih-Ching and Newman, Brad and Liewer, Matt and Pair, Jarrell and Treskunov, Anton and Reger, Greg and Rothbaum, Barbara and Difede, JoAnn and Spitalnick, Josh and McLay, Rob and Parsons, Thomas and Rizzo, Albert},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811017},
	issn = {2375-5334},
	keywords = {Stress;Medical treatment;Virtual reality;Humans;Terrorism;Cities and towns;Roads;User centered design;Feedback;USA Councils;Virtual Reality;Posttraumatic Stress Disorder;PTSD;Exposure Therapy;Full Spectrum Warrior;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;I.6.3 [Computing Methodologies]: Simulation and Modeling-Applications;J.3 [Computer Applications]: Life and Medical Sciences-Health;J.4 [Computer Applications]: Social and Behavioral Sciences-Psychology},
	month = {March},
	pages = {163-170},
	title = {A Virtual Iraq System for the Treatment of Combat-Related Posttraumatic Stress Disorder},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811017}}

@inproceedings{4811018,
	abstract = {Civil Support Teams (CST) must be ready to respond to a variety of potential situations involving dangerous materials. For many of these materials, standard real-world training methods can be successfully employed. Training involving radiological agents, however, poses a greater challenge than for other agents due to a lack of materials that can suitably mimic the situation without the danger of the real material. To address the need of providing a good training system for learning how to behave when responding to a radiological threat, we have developed a CST immersive training system. Our system simulates a radiological threat in a virtual environment and allows users to practice surveying the threat using virtual representations of the world and necessary equipment. We developed novel multi-user interaction techniques to enable simultaneous training for two CST members. The 92nd CST tested the system and provided feedback throughout the development process. The team learned to use the system with little coaching, quickly learned to navigate and interact via wand controls, and ultimately performed a successful demonstration of a radiological survey using our system for their superior officers.},
	author = {Koepnick, Steven and Norpchen, Derek and Sherman, William R. and Coming, Daniel S.},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811018},
	issn = {2375-5334},
	keywords = {Olfactory;Displays;Databases;Virtual reality;Mass spectroscopy;Solenoids;Valves;Educational institutions;Data engineering;Weight control;3D user interfaces;collaborative virtual reality;immersive training;I.3.7 [COMPUTER GRAPHICS]: Three-Dimensional Graphics and Realism-Virtual Reality;I.6.3 [SIMULATION AND MODELING]: Applications-;K.3.1 [COMPUTERS AND EDUCATION]: Computer Uses in Education-Collaborative Learning},
	month = {March},
	pages = {171-174},
	title = {Immersive Training for Two-Person Radiological Surveys},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811018}}

@inproceedings{4811019,
	abstract = {Touch is a powerful component of human communication, yet has been largely absent in communication between humans and virtual humans (VHs). This paper expands on recent work which allowed unidirectional touch from human to VH, by evaluating bidirectional touch as a new channel for nonverbal communication. A VH augmented with a haptic interface is able to touch her interaction partner using a pseudo-haptic touch or an active-haptic touch from a co-located mechanical arm. Within the context of a simulated doctor-patient interaction, two user studies (n = 54) investigate how touch can be used by both human and VH to communicate. Results show that human-to-VH touch is used for the same communication purposes as human-to-human touch, and that VH-to-human touch (pseudo-haptic and active-haptic) allows the VH to communicate with its human interaction partner. The enhanced nonverbal communication provided by bidirectional touch has the potential to solve difficult problems in VH research, such as disambiguating user speech, enforcing social norms, and achieving rapport with VHs.},
	author = {Kotranza, Aaron and Lok, Benjamin and Pugh, Carla M. and Lind, D. Scott},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811019},
	issn = {2375-5334},
	keywords = {Humans;Haptic interfaces;Computer graphics;Virtual reality;Back;Educational institutions;Context modeling;Speech enhancement;Breast;Medical simulation;Virtual humans;haptics;tangible interfaces;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism - Virtual Reality;I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction techniques},
	month = {March},
	pages = {175-178},
	title = {Virtual Humans That Touch Back: Enhancing Nonverbal Communication with Virtual Humans through Bidirectional Touch},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811019}}

@inproceedings{4811020,
	abstract = {Virtual reality exposure therapy (VRET) is becoming an increasing commonplace technique for the treatment of a wide range of psychological disorders, such as phobias. Effective virtual reality systems are suggested to invoke presence, which in term elicits an emotional response, helping to lead a successful treatment outcome. However, a number of problems are apparent: (1) the expense of traditional virtual reality systems hampers their widespread adoption; (2) the depth of research into several disorders is still limited in depth; and (3) the understanding of presence and its relation to delivery mechanism and treatment outcome is still not entirely understood. We implemented and experimentally investigated an immersive VRET prototype system for the treatment of claustrophobia, a system that combines affordability, robustness and practicality while providing presence and effectiveness in treatment. The prototype system was heuristically evaluated and a controlled treatment scenario experiment using a non-clinical sample was performed. In the following, we describe the background, system concept and implementation, the tests and future directions.},
	author = {Bruce, Morgan and Regenbrecht, Holger},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811020},
	issn = {2375-5334},
	keywords = {Virtual reality;Medical treatment;System testing;Psychology;Robustness;Prototypes;Interference;Virtual environment;Hardware;Controllability;Claustrophobia;Virtual Reality Exposure Therapy (VRET);XNA;H.5.1 [Multimedia Information Systems]: Artificial, augmented, and virtual realities, Evaluation/methodology;H.5.2 [User Interfaces]: Prototyping;H.1.2 [User/Machine Systems]: Software psychology},
	month = {March},
	pages = {179-182},
	title = {A virtual reality claustrophobia therapy system - implementation and test},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811020}}

@inproceedings{4811021,
	abstract = {We present a novel concept, Virtualized Traffic, to reconstruct and visualize continuous traffic flows from discrete spatio-temporal data provided by traffic sensors or generated artificially to enhance a sense of immersion in a dynamic virtual world. Given the positions of each car at two recorded locations on a highway and the corresponding time instances, our approach can reconstruct the traffic flows (i.e. the dynamic motions of multiple cars over time) in between the two locations along the highway for immersive visualization of virtual cities or other environments. Our algorithm is applicable to high-density traffic on highways with an arbitrary number of lanes and takes into account the geometric, kinematic, and dynamic constraints on the cars. Our method reconstructs the car motion that automatically minimizes the number of lane changes, respects safety distance to other cars, and computes the acceleration necessary to obtain a smooth traffic flow subject to the given constraints. Furthermore, our framework can process a continuous stream of input data in real time, enabling the users to view virtualized traffic events in a virtual world as they occur.},
	author = {van den Berg, Jur and Sewall, Jason and Lin, Ming and Manocha, Dinesh},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811021},
	issn = {2375-5334},
	keywords = {Traffic control;Data visualization;Road transportation;Automated highways;Kinematics;Layout;Image reconstruction;Virtual reality;Computer science;Cities and towns;I.6.3 [Computing Methodologies]: Simulation and Modeling-Applications},
	month = {March},
	pages = {183-190},
	title = {Virtualized Traffic: Reconstructing Traffic Flows from Discrete Spatio-Temporal Data},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811021}}

@inproceedings{4811022,
	abstract = {We introduce new features for the broad phase algorithm sweep and prune that increase scalability for large virtual reality environments and allow for efficient AABB (axis-aligned bounding boxes) insertion and removal to support dynamic object creation and destruction. We introduce a novel segmented interval list structure that allows AABB insertion and removal without requiring a full sort of the axes. This algorithm is well-suited to large environments in which many objects are not moving at once. We analyze and test implementations of sweep and prune that include subdivision, batch insertion and removal, and segmented interval lists. Our tests show these techniques provide higher performance than previous sweep and prune methods, and perform better than octrees in temporally coherent environments.},
	author = {Tracy, Daniel J. and Buss, Samuel R. and Woods, Bryan M.},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811022},
	issn = {2375-5334},
	keywords = {Large-scale systems;Virtual reality;Computer graphics;Acceleration;Scalability;Testing;Geometry;Mathematics;Cognitive science;Performance evaluation;I.3.6 [Computer Graphics]: Methodology and Techniques-Graphics data structures and data types;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {191-198},
	title = {Efficient Large-Scale Sweep and Prune Methods with AABB Insertion and Removal},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811022}}

@inproceedings{4811023,
	abstract = {This research effort focuses on the historically-difficult problem of creating large-scale (city size) scene models from sensor data, including rapid extraction and modeling of geometry models. The solution to this problem is sought in the development of a novel modeling system with a fully automatic technique for the extraction of polygonal 3D models from LiDAR (Light Detection And Ranging) data. The result is an accurate 3D model representation of the real-world as shown in Figure 1. We present and evaluate experimental results of our approach for the automatic reconstruction of large U.S. cities.},
	author = {Poullis, Charalambos and You, Suya},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811023},
	issn = {2375-5334},
	keywords = {Cities and towns;Image reconstruction;Solid modeling;Geometry;Large-scale systems;Laser radar;Virtual environment;Layout;Data mining;Virtual reality;I.3.5 [Computational Geometry and Object Modeling]: Boundary representations-Geometric algorithms;languages;and systems;Modeling packages;I.3.7 [Three-Dimensional Graphics and Realism]: Virtual reality-Visible line/surface algorithms},
	month = {March},
	pages = {199-202},
	title = {Automatic Creation of Massive Virtual Cities},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811023}}

@inproceedings{4811024,
	abstract = {In order to increase a user's sense of presence in an artificial environment some researchers propose a gradual transition from reality to the virtual world instead of immersing users into the virtual world directly. One approach is to start the VR experience in a virtual replica of the physical space to accustom users to the characteristics of VR, e.g., latency, reduced field of view or tracking errors, in a known environment. Although this procedure is already applied in VR demonstrations, until now it has not been verified whether the usage of such a transitional environment - as transition between real and virtual environment - increases someone's sense of presence. We have observed subjective, physiological and behavioral reactions of subjects during a fully-immersive flight phobia experiment under two different conditions: the virtual flight environment was displayed immediately, or subjects visited a transitional environment before entering the virtual flight environment. We have quantified to what extent a gradual transition to the VE via a transitional environment increases the level of presence. We have found that subjective responses show significantly higher scores for the user's sense of presence, and that subjects' behavioral reactions change when a transitional environment is shown first. Considering physiological reactions, no significant difference could be found.},
	author = {Steinicke, Frank and Bruder, Gerd and Hinrichs, Klaus and Steed, Anthony and Gerlach, Alexander L.},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811024},
	issn = {2375-5334},
	keywords = {Virtual reality;Computer displays;Virtual environment;Visualization;Computer graphics;Computer science;Psychology;Humans;Educational institutions;Delay;Virtual reality;presence;transitional environment;virtual portals},
	month = {March},
	pages = {203-210},
	title = {Does a Gradual Transition to the Virtual World increase Presence?},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811024}}

@inproceedings{4811025,
	abstract = {As users of head-tracked head-mounted display systems move their heads, latency causes unnatural scene motion. We 1) analyzed scene motion due to latency and head motion, 2) developed a mathematical model relating latency, head motion, scene motion, and perception thresholds, 3) developed procedures to determine perceptual thresholds of scene-velocity and latency without the need for a head-mounted display or a low-latency system, and 4), for six subjects under a specific set of conditions, we measured scene-velocity and latency thresholds and compared the relationship between these thresholds. Resulting PSEs (min 10 ms) and JNDs (min 3 ms) of latency thresholds are in a similar range reported by Ellis and Adelstein. The results are a step toward enabling scientists and engineers to determine latency requirements before building immersive virtual environments using head-mounted display systems.},
	author = {Jerald, Jason and Whitton, Mary},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811025},
	issn = {2375-5334},
	keywords = {Delay;Layout;Magnetic heads;Motion analysis;Computer graphics;Computer displays;Virtual environment;Computer science;Mathematical model;Motion measurement;H.1.2 [Models and Principles]: User/Machine Systems-Human Factors;I.3.6 [Computer Graphics]: Methodology and Techniques-Ergonomics and Interaction Techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;I.3.m [Computer Graphics]: Miscellaneous-Perception},
	month = {March},
	pages = {211-218},
	title = {Relating Scene-Motion Thresholds to Latency Thresholds for Head-Mounted Displays},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811025}}

@inproceedings{4811026,
	abstract = {The study of aimed movements has a long history, starting at least as far back as 1899 when Wood-worth proposed a two-component model in which aimed movements are broken into an initial ballistic phase and an additional control phase. In this paper, we use Wood-worth's model for experimentally comparing aimed movements in the real world with those in a virtual environment. Trajectories from real world movements have been collected and compared to trajectories of movements taken from a virtual environment. From this, we show that significant temporal differences arise in both the ballistic and control phases, but the difference is much larger in the control phase; users' improvement is relatively greater in the virtual world than in the real world. They progress more in ballistic phase in the real world, but more in correction phase in the virtual world. These results allow us to better understand the pointing tasks in virtual environments.},
	author = {Liu, Lei and van Liere, Robert and Nieuwenhuizen, Catharina and Martens, Jean-Bernard},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811026},
	issn = {2375-5334},
	keywords = {Virtual reality;H5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H5.2 [Information Interfaces and Presentation]: User Interfaces-Interaction styles;User-centered design},
	month = {March},
	pages = {219-222},
	title = {Comparing Aimed Movements in the Real World and in Virtual Reality},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811026}}

@inproceedings{4811027,
	abstract = {Besides visual validations of virtual car models, immersive applications like a Virtual Seating Buck enable car designers and engineers to decide product related issues without building expensive hardware prototypes. For replacing real models, it is mandatory that decision makers can rely on VR-based findings. However, especially when using a Head Mounted Display, users complain about an unnatural perception of space. Such misperceptions have already been reported in literature where several evaluation methods have been proposed for researching possible causes. Unfortunately, most of the methods do not represent the scenarios usually found in the automotive industry, since they focus on too large distances of five to fifteen meters. In this paper, we present an evaluation scenario adapted to size and distance perception within the reach of the user. With this method, we analyzed our standard setups and found a systematic error that is lower than aberrations reported by earlier research work. Furthermore, we tried to mitigate perception errors by a Depth of Field Blur applied to the virtual images.},
	author = {Moehring, Mathias and Gloystein, Antje and Doerner, Ralf},
	booktitle = {2009 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:11 -0400},
	date-modified = {2024-03-18 02:29:11 -0400},
	doi = {10.1109/VR.2009.4811027},
	issn = {2375-5334},
	keywords = {Automotive engineering;Aerospace industry;Virtual reality;Hardware;Design engineering;Virtual prototyping;Head;Displays;Standards development;Extraterrestrial measurements;HMD;Experimental Method;Immersion;Perception;I.3.7 Three-Dimensional Graphics and Realism;H.5.2 User Interfaces},
	month = {March},
	pages = {223-226},
	title = {Issues with Virtual Space Perception within Reaching Distance: Mitigating Adverse Effects on Applications Using HMDs in the Automotive Industry},
	year = {2009},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2009.4811027}}

@inproceedings{4480743,
	abstract = {This paper proposes to generate and provide wide field of view (FOV) augmented reality (AR) imagery by mosaicing images from smaller fields of moving views in "desktop" tangible AR (DTAR) environments. AR systems usually offer a limited FOV into the interaction space, constrained by the FOV of the camera and/or the display, which causes serious usability problems especially when the interaction space is large and many tangible props/markers are used. This problem is more apparent in DTAR environments in which an upright frontal display is used, instead of a head mounted display. This can be solved partly by placing the camera at a relatively far location or by using multiple cameras and increasing the working FOV. However, as for the former solution, the large distance between the interaction space and the fixed camera decreases the tracking and recognition reliability of the tangible markers, and the latter solution introduces significant additional set-up, cost, and computational load. Thus, we propose to use a mosaiced image to provide wide FOV AR imagery. We experimentally compare our solution, i.e. to offer the entire view of the interaction space at once, to other nominal AR set-ups. The experimental results show that, despite some amounts of visual artifacts due to the imperfect mosaicing, the proposed solution can improve task performance and usability for a typical DTAR system. Our findings should contribute to making AR systems more practical and usable for the mass.},
	author = {Jeon, Seokhee and Kim, Gerard J.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480743},
	issn = {2375-5334},
	keywords = {Augmented reality;Cameras;Virtual reality;Usability;Head;Computer displays;Switches;Computer science;Haptic interfaces;Computational efficiency;Mosaicing;Augmented Reality;Desktop AR;Usability;Interaction Space;H.5.1 [INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
	month = {March},
	pages = {3-10},
	title = {Providing a Wide Field of View for Effective Interaction in Desktop Tangible Augmented Reality},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480743}}

@inproceedings{4480744,
	abstract = {In this paper, we propose a novel imaging system that enables the capture of photos and videos with sparse informational pixels. Our system is based on the projection and detection of 3D optical tags. We use an infrared (IR) projector to project temporally-coded (blinking) dots onto selected points in a scene. These tags are invisible to the human eye, but appear as clearly visible time-varying codes to an IR photosensor. As a proof of concept, we have built a prototype camera system (consisting of co-located visible and IR sensors) to simultaneously capture visible and IR images. When a user takes an image of a tagged scene using such a camera system, all the scene tags that are visible from the system's viewpoint are detected. In addition, tags that lie in the field of view but are occluded, and ones that lie just outside the field of view, are also automatically generated for the image. Associated with each tagged pixel is its 3D location and the identity of the object that the tag falls on. Our system can interface with conventional image recognition methods for efficient scene authoring, enabling objects in an image to be robustly identified using cheap cameras, minimal computations, and no domain knowledge. We demonstrate several applications of our system, including, photo-browsing, e-commerce, augmented reality, and objection localization.},
	author = {Li Zhang and Subramaniam, Neesha and Lin, Robert and Raskar, Ramesh and Nayar, Shree},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480744},
	issn = {2375-5334},
	keywords = {Pixel;Layout;Cameras;Optical imaging;Videos;Optical detectors;Optical sensors;Humans;Prototypes;Infrared image sensors;Optical tags;infrared tags;projected fiducial markers;temporal coding;tagged pixels;browsing;tracking;retagging;mixed and augmented reality},
	month = {March},
	pages = {11-18},
	title = {Capturing Images with Sparse Informational Pixels using Projected 3D Tags},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480744}}

@inproceedings{4480745,
	abstract = {One of the main goals of anywhere augmentation is the development of automatic algorithms for scene acquisition in augmented reality systems. In this paper, we present Envisor, a system for online construction of environment maps in new locations. To accomplish this, Envisor uses vision-based frame to frame and landmark orientation tracking for long-term, drift-free registration. For additional robustness, a gyroscope/compass orientation unit can optionally be used for hybrid tracking. The tracked video is then projected into a cubemap frame by frame. Feedback is presented to the user to help avoid gaps in the cubemap, while any remaining gaps are filled by texture diffusion. The resulting environment map can be used for a variety of applications, including shading of virtual geometry and remote presence.},
	author = {DiVerdi, Stephen and Wither, Jason and Hollerer, Tobias},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480745},
	issn = {2375-5334},
	keywords = {Virtual reality;Cameras;Layout;Tracking;Augmented reality;Robustness;Gyroscopes;Feedback;Geometry;Computer graphics;I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Tracking;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {19-26},
	title = {Envisor: Online Environment Map Construction for Mixed Reality},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480745}}

@inproceedings{4480746,
	abstract = {Mixed reality's (MR) ability to merge real and virtual spaces is applied to merging different knowledge types, such as abstract and concrete knowledge. To evaluate whether the merging of knowledge types can benefit learning, MR was applied to an interesting problem in anesthesia machine education. The virtual anesthesia machine (VAM) is an interactive, abstract 2D transparent reality simulation of the internal components and invisible gas flows of an anesthesia machine. It is widely used in anesthesia education. However when presented with an anesthesia machine, some students have difficulty transferring abstract VAM knowledge to the concrete real device. This paper presents the augmented anesthesia machine (AAM). The AAM applies a magic-lens approach to combine the VAM simulation and a real anesthesia machine. The AAM allows students to interact with the real anesthesia machine while visualizing how these interactions affect the internal components and invisible gas flows in the real world context. To evaluate the AAM's learning benefits, a user study was conducted. Twenty participants were divided into either the VAM (abstract only) or AAM (concrete+abstract) conditions. The results of the study show that MR can help users bridge their abstract and concrete knowledge, thereby improving their knowledge transfer into real world domains.},
	author = {Quarles, John and Lampotang, Samsun and Fischler, Ira and Fishwick, Paul and Lok, Benjamin},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480746},
	issn = {2375-5334},
	keywords = {Virtual reality;Merging;Concrete;Anesthesia;Active appearance model;Fluid flow;Machine learning;Visualization;Bridges;Knowledge transfer;J.3 [Computer Applications]: Life and Medical Sciences - Health;Mixed Reality;Modeling and Simulation;Anesthesiology;Psychology;User Studies},
	month = {March},
	pages = {27-34},
	title = {A Mixed Reality Approach for Merging Abstract and Concrete Knowledge},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480746}}

@inproceedings{4480747,
	abstract = {Augmented reality (AR) is the mixing of computer-generated stimuli with real-world stimuli. In this paper, we present results from a controlled, empirical study comparing three ways of delivering spatialized audio for AR applications: a speaker array, headphones, and a bone-conduction headset. Analogous to optical-see-through AR in the visual domain, hear-through AR allows users to receive computer-generated audio using the bone-conduction headset, and real-world audio using their unoccluded ears. Our results show that subjects achieved the best accuracy using a speaker array physically located around the listener when stationary sounds were played, but that there was no difference in accuracy between the speaker array and the bone-conduction device for sounds that were moving, and that both devices outperformed standard headphones for moving sounds. Subjective comments by subjects following the experiment support this performance data.},
	author = {Lindeman, Robert W. and Noma, Haruo and de Barros, Paulo Goncalves},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480747},
	issn = {2375-5334},
	keywords = {Augmented reality;Bones;Character generation;Ear;Headphones;Loudspeakers;Virtual reality;Frequency;Microphones;Irrigation;Augmented reality;audio;bone conduction;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Audio input/output;Artificial, augmented, and virtual realities},
	month = {March},
	pages = {35-42},
	title = {An Empirical Study of Hear-Through Augmented Reality: Using Bone Conduction to Deliver Spatialized Audio},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480747}}

@inproceedings{4480748,
	abstract = {The precision with which users can maintain boresight alignment between visual targets at different depths is recorded for 24 subjects using two different boresight targets. Subjects' normal head stability is established using their Romberg coefficients. Weibull distributions are used to describe the probabilities of the magnitude of head positional errors and the three dimensional cloud of errors is displayed by orthogonal two dimensional density plots. These data will lead to an understanding of the limits of user introduced calibration error in augmented reality systems.},
	author = {Axholt, Magnus and Peterson, Stephen and Ellis, Stephen R.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480748},
	issn = {2375-5334},
	keywords = {Calibration;Stability;Eyes;Magnetic heads;Augmented reality;Space technology;Humans;Computer displays;NASA;Weibull distribution;boresight;line of sight;calibration;postural sway;augmented reality;H.5.2 [Information Systems]: User Interfaces;H.1.2 [User/Machine Systems]: Human factors},
	month = {March},
	pages = {43-46},
	title = {User Boresighting for AR Calibration: A Preliminary Analysis},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480748}}

@inproceedings{4480749,
	abstract = {This paper describes the use of user's focus point to improve some visual effects in virtual environments (VE). First, we describe how to retrieve user's focus point in the 3D VE using an eye-tracking system. Then, we propose the adaptation of two rendering techniques which aim at improving users' sensations during first-person navigation in VE using his/her focus point: (1) a camera motion which simulates eyes movement when walking, i.e., corresponding to vestibulo-ocular and vestibulocollic reflexes when the eyes compensate body and head movements in order to maintain gaze on a specific target, and (2) a depth-of-field (DoF) blur effect which simulates the fact that humans perceive sharp objects only within some range of distances around the focal distance. Second, we describe the results of an experiment conducted to study users' subjective preferences concerning these visual effects during first-person navigation in VE. It showed that participants globally preferred the use of these effects when they are dynamically adapted to the focus point in the VE. Taken together, our results suggest that the use of visual effects exploiting users' focus point could be used in several VR applications involving first- person navigation such as the visit of architectural site, training simulations, video games, etc.},
	author = {Hillaire, Sebastien and Lecuyer, Anatole and Cozot, Remi and Casiez, Gery},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480749},
	issn = {2375-5334},
	keywords = {Cameras;Virtual environment;Visual effects;Navigation;Biological system modeling;Eyes;Legged locomotion;Humans;Virtual reality;Games;eye-tracking;visual feedback;depth-of-field blur;camera motion;focus point;first-person-navigation;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Interaction styles;User-centered design},
	month = {March},
	pages = {47-50},
	title = {Using an Eye-Tracking System to Improve Camera Motions and Depth-of-Field Blur Effects in Virtual Environments},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480749}}

@inproceedings{4480750,
	abstract = {Access control is an important aspect of shared virtual environments. Resource access may not only depend on prior authorization, but also on context of usage such as distance or position in the scene graph hierarchy. In virtual worlds that allow user-created content, participants must be able to define and exchange access rights to control the usage of their creations. Using object capabilities, fine-grained access control can be exerted on the object level. We describe our experiences in the application of the object-capability model for access control to object-manipulation tasks common to collaborative virtual environments. We also report on a prototype implementation of an object-capability safe virtual environment that allows anonymous, dynamic exchange of access rights between users, scene elements, and autonomous actors.},
	author = {Scheffler, Martin and Springer, Jan P. and Froehlich, Bernd},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480750},
	issn = {2375-5334},
	keywords = {Virtual environment;Access control;Permission;Layout;Virtual prototyping;Vehicle dynamics;Computer security;Information security;Object oriented programming;Computer graphics;Object Capabilities;Security;Virtual Environments;D.1.5 [Programming Techniques]: Object-Oriented Programming;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;K.6.5 [Computing Milieux]: Management of Computing and Information Systems-Security and Protection},
	month = {March},
	pages = {51-58},
	title = {Object-Capability Security in Virtual Environments},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480750}}

@inproceedings{4480751,
	abstract = {We have developed techniques called mobile group dynamics (MGDs), which help groups of people to work together while they travel around large-scale virtual environments. MGDs explicitly showed the groups that people had formed themselves into, and helped people move around together and communicate over extended distances. The techniques were evaluated in the context of an urban planning application, by providing one batch of participants with MGDs and another with an interface based on conventional collaborative virtual environments (CVEs). Participants with MGDs spent nearly twice as much time in close proximity (within 10m of their nearest neighbor), communicated seven times more than participants with a conventional interface, and exhibited real-world patterns of behavior such as staying together over an extended period of time and regrouping after periods of separation. The study has implications for CVE designers, because it shows how MGDs improves groupwork in CVEs.},
	author = {Dodds, Trevor J. and Ruddle, Roy A.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480751},
	issn = {2375-5334},
	keywords = {Large-scale systems;Collaboration;Virtual environment;Collaborative work;Virtual reality;Collaborative software;Computer interfaces;Computer networks;Distributed computing;Mobile computing;Collaborative interaction;experimental methods;distributed VR;usability;C.2.4 [Computer-Computer Communication Networks]: Distributed Systems-Distributed applications;H.1.2 [Models and Principles]: User/Machine Systems-Human factors;Software psychology;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented and virtual realities;H.5.3 [Information Interfaces and Presentation]: Group and Organization Interfaces-Collaborative computing;Computer-supported cooperative work;Synchronous interaction;I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {59-66},
	title = {Mobile Group Dynamics in Large-Scale Collaborative Virtual Environments},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480751}}

@inproceedings{4480752,
	abstract = {Massively Multiplayer Online Worlds (MMOs) are persistent virtual environments where people play, experiment and socially interact. In this paper, we demonstrate that MMOs also provide a powerful platform for Augmented Reality (AR) applications, where we blend together locations in physical space with corresponding places in the virtual world. We introduce the notion of AR stages, which are persistent, evolving spaces that encapsulate AR experiences in online three-dimensional virtual worlds. We discuss the concepts and technology necessary to use an MMO for AR, including a novel set of design concepts aimed at keeping such a system easy to learn and use. By leveraging the features of the commercial MMO Second Life, we have created a powerful AR authoring environment accessible to a large, diverse set of users.},
	author = {Lang, Tobias and Maclntyre, Blair and Zugaza, Iker Jamardo},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480752},
	issn = {2375-5334},
	keywords = {Augmented reality;Space technology;Second Life;Graphics;Collaborative work;Virtual environment;Virtual reality;User interfaces;Multimedia systems;Information systems;Augmented Reality;Virtual Reality;Massively Multi-player Online Worlds;User Interfaces;Second Life;I.3.7 [Three-Dimensional Graphics and Realism]: Virtual reality-;H.5.1 [Multimedia Information Systems]: Artificial;augmented;and virtual realities-},
	month = {March},
	pages = {67-70},
	title = {Massively Multiplayer Online Worlds as a Platform for Augmented Reality Experiences},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480752}}

@inproceedings{4480753,
	abstract = {Research into collaborative mixed reality (MR) or augmented reality has recently been active. Previous studies showed that MR was preferred for collocated collaboration while immersive virtual reality was preferred for remote collaboration. The main reason for this preference is that the physical object in remote space cannot be handled directly. However, MR using tangible objects is still attractive for remote collaborative systems, because MR enables seamless interaction with real objects enhanced by virtual information with the sense of touch. Here we introduce "tangible replicas"(dual objects that have the same shape, size, and surface), and propose a symmetrical model for remote collaborative MR. The result of experiments shows that pointing and drawing functions on the tangible replica work well despite limited shared information.},
	author = {Yamamoto, Shun and Tamaki, Hidekazu and Okajima, Yuta and Okada, Kenichi and Bannai, Yuichi},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480753},
	issn = {2375-5334},
	keywords = {Collaboration;Virtual reality;Collaborative work;Feedback;Virtual environment;Haptic interfaces;Biological system modeling;Environmental factors;Augmented reality;Displays;Mixed Reality;remote collaboration;collaborative interaction;usability},
	month = {March},
	pages = {71-74},
	title = {Symmetric Model of Remote Collaborative MR Using Tangible Replicas},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480753}}

@inproceedings{4480754,
	abstract = {The automotive industry uses physical seating bucks, which are minimal mockups of a car interior, to assess various aspects of the planned interior early in the development process. In a virtual seating buck, users wear a head-mounted display (HMD) which overlays a virtual car interior on a physical seating buck. We have developed a two-user virtual seating buck system, which allows two users to take the role of the driver and co-driver respectively. Both users wear tracked head-mounted displays and see the virtual car interior from the respective view points enabling them to properly interact with the interface elements of a car. We use this system for the development, test and evaluation of novel human-machine interface concepts for future car models. We provide each user with an avatar, since the two co-located users need to see each others' actions. Our evaluation of different head and hand models for representing the two users indicate that the user representations and motions should be as realistic as possible even though the focus is on testing interface elements operated by the users' fingers. The participants of our study also expressed that they clearly prefer the two-user seating buck over a single-user system since it directly supports the face-to-face discussions of features and problems of a newly developed interface.},
	author = {Salzmann, Holger and Froehlich, Bernd},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480754},
	issn = {2375-5334},
	keywords = {Man machine systems;Avatars;Computer graphics;Testing;Virtual reality;Fingers;Automotive engineering;Displays;Virtual environment;Hardware;mixed reality;multi-user systems;collaborative virtual environment;human-machine interfaces;co-location;I.3.1 [Computer Graphics]: Hardware Architecture-Three-dimensional displays;I.3.6 [Computer Graphics]: Methodology and Techniques-Ergonomics;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality},
	month = {March},
	pages = {75-82},
	title = {The Two-User Seating Buck: Enabling Face-to-Face Discussions of Novel Car Interface Concepts},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480754}}

@inproceedings{4480755,
	abstract = {In this paper we present an augmented reality (AR) application for industrial building acceptance. Building acceptance is the process of comparing as-planned documentation with the factory that was actually built. A self-supported mobile AR device, the AR-planar, is used to facilitate this comparison by overlaying 3D models on top of a video image. The suitability of this approach is assessed using an expert heuristic in a real factory, and furthermore the usability of the AR-planar in comparison to other AR systems was examined in a complementary user study.},
	author = {Schoenfelder, Ralph and Schmalstieg, Dieter},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480755},
	issn = {2375-5334},
	keywords = {Augmented reality;Production facilities;Buildings;Construction industry;Strategic planning;Virtual manufacturing;Documentation;Usability;Virtual reality;User interfaces;H.5.1 [Information interfaces and presentation]: Multimedia Information Systems - Artificial, augmented, and virtual realities;H.5.2 [Information interfaces and presentation]: User Interfaces;J.7 [Computers in other systems]: Industrial control;ergonomics;user study;augmented reality},
	month = {March},
	pages = {83-90},
	title = {Augmented Reality for Industrial Building Acceptance},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480755}}

@inproceedings{4480756,
	abstract = {This paper proposes after-action review (AAR) with human-virtual human (H-VH) experiences. H-VH experiences are seeing increased use in training for real-world, H-H experiences. To improve training, the users of H-VH experiences need to review, evaluate, and get feedback on them. AAR enables users to review their H- VH interaction, evaluate their actions, and receive feedback on how to improve future real-world, H-H experiences. The Interpersonal Scenario Visualizer (IPSViz), an AAR tool for H-VH experiences, is presented. IPSViz allows medical students to review their interactions with VH patients. To enable review, IPSViz generates spatial, temporal, and social visualizations of H- VH interactions. Visualizations are generated by treating the interaction as a set of signals. Interaction signals are captured, logged, and processed to generate visualizations for review, evaluation and feedback. In a study (N=27), reviewing the visualizations helped students become self-aware of their actions with a virtual human and gain insight into how to improve interactions with real humans.},
	author = {Raij, Andrew B. and Lok, Benjamin C.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480756},
	issn = {2375-5334},
	keywords = {Humans;Visualization;Feedback;Medical diagnostic imaging;Signal generators;Virtual environment;Video recording;Signal processing;Virtual reality;Computer graphics;Virtual Humans;Information Visualization;H.5.1 [INFORMATION INTERFACES AND PRESENTATION]: Multimedia Information Systems-Artificial;augmented;and virtual realities;I.3.7 [COMPUTER GRAPHICS]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {91-98},
	title = {IPSViz: An After-Action Review Tool for Human-Virtual Human Experiences},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480756}}

@inproceedings{4480757,
	abstract = {Virtual human (VH) experiences are receiving increased attention for training real-world interpersonal scenarios. Communication in interpersonal scenarios consists of not only speech and gestures, but also relies heavily on haptic interaction--interpersonal touch. By adding haptic interaction to VH experiences, the bandwidth of human-VH communication can be increased to approach that of human-human communication. To afford haptic interaction, a new species of embodied agent is proposed--mixed reality humans (MRHs). A MRH is a virtual human embodied by a tangible interface that shares the same registered space. The tangible interface affords the haptic interaction that is critical to effective simulation of interpersonal scenarios. We applied MRHs to simulate a virtual patient requiring a breast cancer screening (medical interview and physical exam). The design of the MRH patient is presented. This paper also presents the results of a pilot study in which eight (n = 8) physician-assistant students performed a clinical breast exam on the MRH patient. Results show that when afforded haptic interaction with a MRH patient, users demonstrated interpersonal touch and social engagement similarly to interacting with a human patient.},
	author = {Kotranza, Aaron and Lok, Benjamin},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480757},
	issn = {2375-5334},
	keywords = {Humans;Virtual reality;Haptic interfaces;Medical simulation;Computer graphics;Bandwidth;Biological tissues;Speech recognition;Breast cancer;Merging;Tangible interfaces;virtual humans;mixed reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism - Virtual Reality;I.3.6 [Computer Graphics]: Methodology and Techniques - Interaction techniques},
	month = {March},
	pages = {99-106},
	title = {Virtual Human + Tangible Interface = Mixed Reality Human An Initial Exploration with a Virtual Breast Exam Patient},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480757}}

@inproceedings{4480758,
	abstract = {This paper presents a complete framework for creating a speech-enabled avatar from a single image of a person. Our approach uses a generic facial motion model which represents deformations of a prototype face during speech. We have developed an HMM-based facial animation algorithm which takes into account both lexical stress and coarticulation. This algorithm produces realistic animations of the prototype facial surface from either text or speech. The generic facial motion model can be transformed to a novel face geometry using a set of corresponding points between the prototype face surface and the novel face. Given a face photograph, a small number of manually selected features in the photograph are used to deform the prototype face surface. The deformed surface is then used to animate the face in the photograph. We show several examples of avatars that are driven by text and speech inputs.},
	author = {Bitouk, Dmitri and Nayar, Shree K.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480758},
	issn = {2375-5334},
	keywords = {Avatars;Facial animation;Prototypes;Speech synthesis;Hidden Markov models;Deformable models;Stress;Computer graphics;Software prototyping;Solid modeling;H.5.2 [Information Interfaces and Presentation]: Multimedia Information Systems-Animations;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Animation},
	month = {March},
	pages = {107-110},
	title = {Creating a Speech Enabled Avatar from a Single Photograph},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480758}}

@inproceedings{4480759,
	abstract = {In collaborative virtual environments, the visual representation of avatars has been shown to be an important determinant of participant behaviour and response. We explored the influence of varying conditions of eye-representation in our high-fidelity avatar by measuring how accurately people can identify the avatar's point-of- regard (direction of gaze), together with subjective authenticity assessments of the avatar's behaviour and visual representation. The first of two variables investigated was socket-deformation, which is to say that our avatar's eyelids, eyebrows and surrounding areas morphed realistically depending on eye-rotation. The second was vergence of our avatar's eyes to the exact point-of-regard. Our results suggest that the two variables significantly influence the accuracy of point-of-regard identification. This accuracy is highly dependent on the combination of viewing-angle and the point-of-regard itself. We found that socket-deformation in particular has a highly positive impact on the perceived authenticity of our avatar's overall appearance, and when judging just the eyes. However, despite favourable subjective ratings, overall performance during the point-of-regard identification task was actually worse with the highest quality avatar. This provides more evidence that as we move forward to using higher fidelity avatars, there will be a tradeoff between supporting realism of representation and supporting the actual communicative task.},
	author = {Steptoe, William and Steed, Anthony},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480759},
	issn = {2375-5334},
	keywords = {Avatars;Computer graphics;Eyes;Humans;Collaboration;Virtual environment;Virtual reality;Computer science;Educational institutions;Eyelids;Virtual reality;collaborative virtual environments;avatars;social presence;behavioural realism;representation;eye-gaze;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented and virtual realities;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Animation;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;I.3.7 [Computer Graphics]: General-Human Factors},
	month = {March},
	pages = {111-114},
	title = {High-Fidelity Avatar Eye-Representation},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480759}}

@inproceedings{4480760,
	abstract = {Vection is defined as the compelling sensation of illusory self- motion elicited by a moving sensory, usually visual, stimulus. This paper presents collected introspective data on the experience of linear, circular, and curvilinear vection. We evaluate the differences between twelve different trajectories and the influence of the floor projection on the illusion of self-motion. All of the simulated self- motions examined are of a constant velocity, except for a brief simulated initial acceleration. First, we find that linear translations to the left and right are perceived as the least convincing, while linear down is perceived as the most convincing of the linear trajectories. Second, we find that the floor projection significantly improves the introspective measures of linear vection experienced in a photorealistic three-dimensional town. Finally, we find that while linear forward vection is not perceived to be very convincing, curvilinear forward vection is reported to be as convincing as circular vection. Considering our experimental results, our suggestions for simulators and VE applications where vection is desirable is to increase the number of curvilinear trajectories (as opposed to linear ones) and, if possible, add floor projection in order to improve the illusory sense of self-motion.},
	author = {Trutoiu, Laura C. and Mohler, Betty and Schulte-Pelkum, Jorg and Bulthoff, Heinrich H.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480760},
	issn = {2375-5334},
	keywords = {Virtual environment;Cybernetics;Acceleration;Virtual reality;Image motion analysis;Educational institutions;Cities and towns;Biomedical optical imaging;Optical sensors;Laboratories;K.6.1 [Virtual Reality]: Self Motion Perception-Virtual Reality;K.7.m [Optic Flow]: -},
	month = {March},
	pages = {115-120},
	title = {Circular, Linear, and Curvilinear Vection in a Large-screen Virtual Environment with Floor Projection},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480760}}

@inproceedings{4480761,
	abstract = {Virtual environments (VEs) that use a real-walking locomotion interface have typically been restricted in size to the area of the tracked lab space. Techniques proposed to lift this size constraint, enabling real walking in VEs that are larger than the tracked lab space, all require reorientation techniques (ROTs) in the worst-case situation--when a user is close to walking out of the tracked space. We propose a new ROT using distractors--objects in the VE for the user to focus on while the VE rotates --and compare our method to current ROTs through two user studies. Our findings show ROTs using distractors were preferred and ranked more natural by users. Users were also less aware of the rotating VE, when ROTs with distractors were used.},
	author = {Peck, Tabitha C. and Whitton, Mary C. and Fuchs, Henry},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480761},
	issn = {2375-5334},
	keywords = {Legged locomotion;Virtual environment;Turning;Humans;Virtual reality;Computer graphics;Multimedia systems;Bicycles;USA Councils;Loudspeakers;Virtual Environments;Walking;Locomotion;User Studies;Reorientation Techniques;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Evaluation/methodology;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {121-127},
	title = {Evaluation of Reorientation Techniques for Walking in Large Virtual Environments},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480761}}

@inproceedings{4480763,
	abstract = {In this paper, we report the results of a pilot study designed to evaluate the impact of signs as navigation aids in virtual worlds. Test subjects were divided into three groups (no aid, a dynamic electronic map, and signs) and asked to search a virtual building four times for six differently colored spheres. The spheres were in the same locations each time, and subjects were allowed to locate them in any order. A statistical analysis of the data revealed that on the first and second trials subjects took nearly four times as long to find the spheres with no aid present, compared to with maps and signs. We then compared only the sign and map conditions. Overall, subjects who navigated the world with the aid of signs were significantly faster than those who were provided with a map. While more research into the use of signs in virtual worlds is necessary, these results indicate that for at least some environments subjects are able to locate targets more quickly when using signs than maps.},
	author = {Cliburn, Daniel C. and Rilea, Stacy L.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480763},
	issn = {2375-5334},
	keywords = {Navigation;Computer graphics;Virtual reality;Computer interfaces;Pervasive computing;Military computing;Decision making;Cities and towns;Road transportation;Hospitals;Virtual Environment;Signs;Navigation;I.3.7 [Computer Graphics]: Three Dimensional Graphics and Realism - Virtual Reality;H.5.2 [Information Interfaces and Presentation (e.g.;HCI)]: User Interfaces - Evaluation/Methodology},
	month = {March},
	pages = {129-132},
	title = {Showing Users the Way: Signs in Virtual Worlds},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480763}}

@inproceedings{4480764,
	abstract = {This paper compares a large-screen display to a non-stereo head-mounted display (HMD) for a virtual human (VH) experience. As VH experiences are increasingly being applied to training, it is important to understand the effect of immersive displays on user interaction with VHs. Results are reported from a user study (n=27) of 10 minute human-VH interactions in a VH experience which allows medical students to practice communication skills with VH patients. Results showed that student self-ratings of empathy, a critical doctor-patient communication skill, were significantly higher in the HMD; however, when compared to observations of student behavior, students using the large-screen display were able to more accurately reflect on their use of empathy. More work is necessary to understand why the HMD inhibits students' ability to self-reflect on their use of empathy.},
	author = {Johnsen, Kyle and Lok, Benjamin},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480764},
	issn = {2375-5334},
	keywords = {Humans;Large screen displays;Computer displays;Virtual reality;Three dimensional displays;Biomedical imaging;Computer graphics;Wires;Head;Natural languages;virtual humans;embodied agents;display comparison;medical education;immersive virtual environments;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism Virtual Reality;H.5.2 [Information Systems and Presentation]: User Interfaces - Evaluation},
	month = {March},
	pages = {133-136},
	title = {An Evaluation of Immersive Displays for Virtual Human Experiences},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480764}}

@inproceedings{4480765,
	abstract = {We present a new visual-inertial tracking device for augmented and virtual reality applications. The paper addresses two fundamental issues of such systems. The first one concerns the definition and modelling of the sensor fusion. Much work has been done in this area and several models for exploiting the data of the gyroscopes and linear accelerometers have been proposed. However, the respective advantages of each model and in particular the benefits of the integration of the accelerometer data in the filter are still unclear. The paper therefore provides an evaluation of different models with special investigation of the effects of using accelerometers on the tracking performance. The second contribution is about the development of an image processing approach that does not require special landmarks but uses natural features. Our solution relies on a 3D model of the scene that enables to predict the appearances of the features by rendering the model using the prediction data of the sensor fusion filter. The feature localisation is robust and accurate mainly because local lighting is also estimated. The final system is evaluated with help of ground-truth and real data. High stability and accuracy is demonstrated also for large environments.},
	author = {Bleser, Gabriele and Stricker, Didier},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480765},
	issn = {2375-5334},
	keywords = {Image processing;Sensor fusion;Accelerometers;Filters;Predictive models;Virtual reality;Gyroscopes;Layout;Rendering (computer graphics);Robustness;augmented reality;markerless camera tracking;model-based tracking;inertial sensors;sensor fusion;extended;1.4.8 [Image processing and computer vision]: Scene analysis-Motion;Photometry;Sensor fusion;Tracking;I.2.10 [Artificial intelligence]: Vision and Scene Understanding 3D/stereo scene analysis;Intensity;color;photometry;and thresholding;Motion;Texture;Video analysis;I.5.5 [Pattern recognition]: Applications-Computer vision;G. 3 [Probability and statistics]: Markov processes;Probabilistic algorithms;Robust regression;Stochastic processes;I.2.9 [Artificial intelligence]: Robotics- Kinematics and dynamics;Sensors;I.3.m [Computer graphics]: Miscellaneous-Augmented Reality;Algorithms;Experimentation;Performance;Theory;Verification},
	month = {March},
	pages = {137-144},
	title = {Advanced tracking through efficient image processing and visual-inertial sensor fusion},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480765}}

@inproceedings{4480766,
	abstract = {We describe a novel markerless camera tracking approach and user interaction methodology for augmented reality (AR) on unprepared tabletop environments. We propose a real-time system architecture that combines two types of feature tracking methods. Distinctive image features of the scene are detected and tracked frame- to-frame by computing optical flow. In order to achieve real-time performance, multiple operations are processed in a multi-threaded manner for capturing a video frame, tracking features using optical flow, detecting distinctive invariant features, and rendering an output frame. We also introduce a user interaction for establishing a global coordinate system and for locating virtual objects in the AR environment. A user's bare hand is used for the user interface by estimating a camera pose relative to the user's outstretched hand. We evaluate the speed and accuracy of our hybrid feature tracking approach, and demonstrate a proof-of-concept application for enabling AR in unprepared tabletop environments using hands for interaction.},
	author = {Lee, Taehee and Hollerer, Tobias},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480766},
	issn = {2375-5334},
	keywords = {Augmented reality;Cameras;Optical computing;Image motion analysis;Real time systems;Computer architecture;Layout;Optical detectors;Computer vision;Rendering (computer graphics);position and orientation tracking technology;vision-based registration and tracking;interaction techniques for MR/AR;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis},
	month = {March},
	pages = {145-152},
	title = {Hybrid Feature Tracking and User Interaction for Markerless Augmented Reality},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480766}}

@inproceedings{4480767,
	abstract = {The rapid and efficient creation of virtual environments has become a crucial part of virtual reality applications. In particular, civil and defense applications often require and employ detailed models of operations areas for training, simulations of different scenarios, planning for natural or man-made events, monitoring, surveillance, games and films. A realistic representation of the large-scale environments is therefore imperative for the success of such applications since it increases the immersive experience of its users and helps reduce the difference between physical and virtual reality. However, the task of creating such large-scale virtual environments still remains a time-consuming and manual work. In this work we propose a novel method for the rapid reconstruction of photorealistic large-scale virtual environments. First, a novel parameterized geometric primitive is presented for the automatic building detection, identification and reconstruction of building structures. In addition, buildings with complex roofs containing non-linear surfaces are reconstructed interactively using a nonlinear primitive. Secondly, we present a rendering pipeline for the composition of photorealistic textures which unlike existing techniques it can recover missing or occluded texture information by integrating multiple information captured from different optical sensors (ground, aerial and satellite).},
	author = {Poullis, Charalambos and You, Suya and Neumann, Ulrich},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480767},
	issn = {2375-5334},
	keywords = {Large-scale systems;Virtual environment;Buildings;Virtual reality;Discrete event simulation;Surveillance;Surface reconstruction;Pipelines;Optical sensors;Satellites;Large-scale Modeling;Texturing;Multiple Sensory input;Photorealistic Virtual Environments},
	month = {March},
	pages = {153-160},
	title = {Rapid Creation of Large-scale Photorealistic Virtual Environments},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480767}}

@inproceedings{4480768,
	abstract = {This paper discusses the implementation of an automultiscopic omni-directional display. By rotating a flat-panel display whose viewing angle is strictly limited, or has sharp directivity, different and independent views are presented to different viewpoints. Design parameters including the resolution of viewpoints and update rate for viewers are clarified, and the effect of the refresh rate of display panels, speed of rotation, directivity of the panel on these parameters are discussed. Based on the discussion; the implementation of a prototype is reported. A directional filter using lenticular and slit array is devised, and its characteristic is evaluated. Also, a rendering algorithm that takes into account the effect of rotation of the display panel is proposed. Through experiments the feasibility of the proposed approach is confirmed.},
	author = {Hirota, Koichi and Tagawa, Kazuyoshi and Suzuki, Yasuhiro},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480768},
	issn = {2375-5334},
	keywords = {Flat panel displays;Filters;Prototypes;Three dimensional displays;Large screen displays;Rendering (computer graphics);Image converters;Lenses;Holography;Chromium;B.4.2 [Input/Output and Data Communications]: Input/Output Devices - Image display;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial, augmented, and virtual realities;automultiscopic omni-directional display;visual display;projection transformation;rendering},
	month = {March},
	pages = {161-168},
	title = {Automultiscopic display by revolving flat-panel displays},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480768}}

@inproceedings{4480769,
	abstract = {We present a new technique for managing visual clutter caused by overlapping labels in complex information displays. This technique, "label layering", utilizes stereoscopic disparity as a means to segregate labels in depth for increased legibility and clarity. By distributing overlapping labels in depth, we have found that selection time during a visual search task in situations with high levels of overlap is reduced by four seconds or 24%. Our data show that the depth order of the labels must be correlated with the distance order of their corresponding objects. Since a random distribution of stereoscopic disparity in contrast impairs performance, the benefit is not solely due to the disparity-based image segregation. An algorithm using our label layering technique accordingly could be an alternative to traditional label placement algorithms that avoid label overlap at the cost of distracting motion, symbology dimming or label size reduction.},
	author = {Peterson, Stephen and Axholt, Magnus and Ellis, Stephen R.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480769},
	issn = {2375-5334},
	keywords = {Displays;Clutter;Poles and towers;Aircraft;User interfaces;Air traffic control;Aerospace control;Technology management;Space technology;Humans;Label placement;user interfaces;stereoscopic displays;augmented reality;air traffic control;H.5.2 [Information Systems]: User Interfaces;I.3 [Computing Methodologies]: Computer Graphics},
	month = {March},
	pages = {169-176},
	title = {Managing Visual Clutter: A Generalized Technique for Label Segregation using Stereoscopic Disparity},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480769}}

@inproceedings{4480770,
	abstract = {Multi-frame rate rendering is a parallel rendering technique that renders interactive parts of the scene on one graphics card while the rest of the scene is rendered asynchronously on a second graphics card. The resulting color and depth images of both render processes are composited and displayed. This paper presents advanced multi-frame rate rendering techniques, which remove limitations of the original approach and reduce artifacts. The interactive manipulation of light sources and their parameters affects the entire scene. Our multi-GPU deferred shading splits the rendering task into a rasterization and lighting pass and distributes the passes to the appropriate graphics card to enable light manipulations at high frame rates independent of the geometry complexity of the scene. We also developed a parallel volume rendering technique, which allows the manipulation of objects inside a translucent volume at high frame rates. Due to the asynchronous nature of multi-frame rate rendering artifacts may occur during the migration of objects from the slow to the fast graphics card, and vice versa. We show how proper state management can be used to avoid these artifacts almost completely. These techniques were developed in the context of a single-system multi-GPU setup, which considerably simplifies the implementation and increases performance.},
	author = {Springer, Jan P. and Lux, Christopher and Reiners, Dirk and Froehlich, Bernd},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480770},
	issn = {2375-5334},
	keywords = {Rendering (computer graphics);Layout;Computer graphics;Computer displays;Computer networks;Image generation;Petroleum;Gas industry;Color;Light sources;Multi-Frame Rate Rendering;Multi-GPU Systems;3D Interaction;F.1.2 [Computation by Abstract Devices]: Modes of ComputationInteractive and Reactive Computation;I.3.2 [Computer Graphics]: Graphics SystemsDistributed/Network Graphics;I.3.3 [Computer Graphics]: Picture/Image Generation- Display Algorithms;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {177-184},
	title = {Advanced Multi-Frame Rate Rendering Techniques},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480770}}

@inproceedings{4480771,
	abstract = {For safety and operability of drivers while operating a vehicle, it is very important to obtain a wide field of vision. However, the space available for setting up windows is limited. Therefore, we propose a ";transparent cockpit,"; in which the image of a blind spot is displayed on the inner wall of the vehicle using a retro-reflective projection technology. In this system, the internal components of the vehicle, such as the doors and floor, are virtually transparent, and the blind spot is clearly visible, as observed from a window. In this paper, we describe the implementation of a prototype of the proposed system and demonstrate its effectiveness by an evaluation experiment.},
	author = {Yoshida, Takumi and Jo, Kensei and Minamizawa, Kouta and Nii, Hideaki and Kawakami, Naoki and Tachi, Susumu},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480771},
	issn = {2375-5334},
	keywords = {Displays;Vehicle safety;Space technology;Augmented reality;Vehicle driving;Prototypes;Virtual reality;Cameras;Floors;Multimedia systems;Augmented reality;display technology;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities},
	month = {March},
	pages = {185-188},
	title = {Transparent Cockpit: Visual Assistance System for Vehicle Using Retro-reflective Projection Technology},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480771}}

@inproceedings{4480772,
	abstract = {Various virtual and augmented reality systems include volumetric lenses, an extension of 2D magic lenses to 3D volumes in which effects are applied to scene elements. We present a new 3D volumetric lens rendering system that differs fundamentally from other approaches and that is the first to address efficient real-time composition of multiple 3D lenses. A lens factory module composes chainable shader programs for rendering composite visual styles and geometry of intersection regions. Geometry is handled by Boolean combinations of region tests in fragment shaders, which allows both convex and non-convex CSG volumes for lens shape. Efficiency is further addressed by a region analyzer module and by broad-phase culling. Finally, we consider the handling of order effects for composed 3D lenses.},
	author = {Best, Christopher M. and Borst, Christoph W.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480772},
	issn = {2375-5334},
	keywords = {Lenses;Layout;Geometry;Production facilities;Real time systems;Testing;Shape;Computer graphics;Rendering (computer graphics);Augmented reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;H.5.2 [Information Interfaces And Presentation]: User Interfaces-Interaction Styles;Windowing Systems;Magic Lens;volumetric lens},
	month = {March},
	pages = {189-192},
	title = {New Rendering Approach for Composable Volumetric Lenses},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480772}}

@inproceedings{4480773,
	abstract = {In our experience, novel ideas for 3D interaction techniques greatly outpace developers' ability to implement them, despite the potential benefit of these ideas. We believe this is due to the inherent implementation complexity of 3D interfaces, without sufficient support from methods and tools. Believing a developer-centric representation could overcome this problem, we investigated developer practices, artifacts and language. This resulted in the theory of concept-oriented design and Chasm, a prototype realization of the theory. The key feature of concept-oriented design is its use of developer-centric representations to create a multi-tiered implementation, ranging from an envisioned behavior expressed in conversational language to low-level code. Evaluation of Chasm by domain experts and its use in multiple case studies has demonstrated that concept-oriented design in Chasm enabled developers to represent an exponential growth in 3D interface complexity with only a linear growth in implementation complexity. Positive comments by developers further support the developer-centric representation.},
	author = {Wingrave, Chadwick A. and Bowman, Doug A.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480773},
	issn = {2375-5334},
	keywords = {User interfaces;Software engineering;Virtual reality;Prototypes;Model driven engineering;Software design;Computer aided software engineering;Design methodology;Computer graphics;Multimedia systems;3D interaction;implementation;User Interface Description Language;Model-Driven Engineering;D.2.2 [Software Engineering]: Design Tools and Techniques---Computer-aided software engineering (CASE), Object-oriented design methods, User interfaces;I.3.6 [Computer Graphics] Methodology and Techniques---Languages;H.5.1 [Information Interfaces and Presentation] Multimedia Information Systems---Artificial, augmented, and virtual realities;H.5.2. [Information Interfaces and Presentation] User Interfaces---Graphical User Interfaces},
	month = {March},
	pages = {193-200},
	title = {Tiered Developer-Centric Representations for 3D Interfaces: Concept-Oriented Design in Chasm},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480773}}

@inproceedings{4480774,
	abstract = {Recent advanced interface technologies allow the user to interact with different spaces such as Virtual Reality (VR), Augmented Reality (AR) and Ubiquitous Computing (UC) spaces. Previously, human computer interaction (HCI) issues in VR, AR and UC have been largely carried out in separate communities. Here, we combine these three interaction spaces into a single interaction space, called Tangible Space. We propose the VARU framework which is designed for rapid prototyping of a tangible space application. It is designed to provide extensibility, flexibility and scalability. Depending on the available resources, the user could interact with either the virtual, physical or mixed environment. By having the VR, AR and UC spaces in a single platform, it gives us the possibility to explore different types of collaboration across the different spaces. As a result, we present our prototype application which is built using the VARU framework.},
	author = {Irawati, Sylvia and Ahn, Sangchul and Kim, Jinwook and Ko, Heedong},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480774},
	issn = {2375-5334},
	keywords = {Prototypes;Virtual reality;Space technology;Human computer interaction;Collaboration;Application software;Augmented reality;Ubiquitous computing;Computer displays;Switches;C.2.4 [Computer Communication Networks]: Distributed Systems - Client/server;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial, Augmented and Virtual Realities;framework;virtual reality;augmented reality;ubiquitous computing;tangible space},
	month = {March},
	pages = {201-208},
	title = {VARU Framework: Enabling Rapid Prototyping of VR, AR and Ubiquitous Applications},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480774}}

@inproceedings{4480775,
	abstract = {Our goal is to investigate new interaction techniques, using the haptic modality for CAD applications. An important issue in CAD systems is the modification of the Boundary Representation (B-Rep) of 3D objects. However, this fundamental task is only possible if a geometric element (vertex, edge, face) has already been selected. This paper focuses on several haptic selection methods to solve specific geometric constraints (differentiation of topological entities, variable density of the geometry, concavities) during the edition of a CAD model. Evaluation studies are carried out to compare the impact of these haptic methods on user performance. The results of these experiments are used to create a global haptic selection method for CAD editing.},
	author = {Picon, Flavien and Ammi, Mehdi and Bourdot, Patrick},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480775},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Design automation;Virtual reality;History;Computer graphics;Computer interfaces;Force feedback;Assembly;Geometry;Solid modeling;H.5.2 [Information interfaces and presentation]: Haptic I/O;I.3.6 [Computer Graphics]: Interaction Techniques I.3.7 [Computer Graphics]: Virtual Reality;J.6 [Computer-Aided Engineering]: Computer Aided Design;Haptic selection methods;CAD models},
	month = {March},
	pages = {209-212},
	title = {Case Study of Haptic Methods for Selection on CAD Models},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480775}}

@inproceedings{4480776,
	abstract = {We describe M4, the multi-modal mesh manipulation system, which aims to provide a more intuitive desktop interface for freeform manipulation of 3D meshes. The system combines interactive 3D graphics with haptic force feedback and provide several virtual tools for the manipulation of 3D objects represented by irregular triangle meshes. The current functionality includes mesh painting with pressure dependent brush size and paint preview, mesh cutting via drawing a poly-line on the model and two types of mesh deformations. We use two phantoms, either in a co-located haptic/3D-stereo setup or as a fish tank VR setup with a 3D flat panel. In our system, the second hand assists the manipulation of the object, either by ";holding"; the mesh or by affecting the manipulation directly. While the connection of 3D artists and designers to such a direct interaction system may be obvious, we are also investigating its potential benefits for landscape architects and other users of spatial geoscience data. Feedback from an upcoming user study will evaluate the benefits of this system and its tools for these different user groups.},
	author = {Faeth, Adam and Oren, Michael and Sheller, Jonathan and Godinez, Sean and Harding, Chris},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480776},
	issn = {2375-5334},
	keywords = {Painting;Virtual reality;Haptic interfaces;Graphics;Force feedback;Brushes;Paints;Deformable models;Imaging phantoms;Marine animals;H.5.2 Haptic I/O;I.3.7 Three-Dimensional Graphics and Realism - Virtual Reality;I.3.4 Graphics Utilities - Graphic Editors;I.3.6 Methodology and Techniques - Interaction techniques;Haptics;H3D;3D graphics;X3D;shaders;digital shapes;surface mesh;geometric modeling;deformation;cutting},
	month = {March},
	pages = {213-216},
	title = {Cutting, Deforming and Painting of 3D meshes in a Two Handed Viso-haptic VR System},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480776}}

@inproceedings{4480777,
	abstract = {In this paper we propose a novel approach to augmenting human memory based on spatial and graphic information using wearable and smartphone devices. Mnemonics is a technique for memorizing a number of unstructured items that has been known for more than two millennia and was used in ancient Greece. Although its utility is remarkable, acquiring the skill to take advantage of mnemonics is generally difficult. In this study we propose a new method of increasing the effectiveness of classic mnemonics by facilitating the process of memorizing and applying mnemonics. The spatial electronic mnemonics (SROM) proposed here is partly based on an ancient technique that utilizes locations and images that reflect the characteristics of human memory. We first present the design of the SROM as a working hypothesis that augments traditional mnemonics using a portable computer. Then an augmented virtual memory peg (vMPeg) that incorporates a graphic numeral and a photograph of a location is introduced as a first implementation for generating a vMPeg. In an experiment, subjects exhibited remarkable retention of the vMPegs over a long interval. The second phase of placing items to remember on a generated vMPeg was also examined in a preliminary experiment, which also indicated good subject performance. In addition to evaluating the SROM by anylysing the scores for correct recall, a subjective evaluation was performed to investigate the nature of the SROM.},
	author = {Ikei, Yasushi and Ota, Hirofumi},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480777},
	issn = {2375-5334},
	keywords = {Humans;Wearable computers;Graphics;Computer aided instruction;Pervasive computing;Portable computers;Virtual reality;Application software;Computer interfaces;Electronic mail;Memory augmentation;Wearable computer;Visual images;Space and place;VR application;H.5.m [Information Interfaces and Presentation]: Miscellaneous;K.3.1 [Computer Uses in Education]: Computer-assisted instruction (CAI)},
	month = {March},
	pages = {217-224},
	title = {Spatial Electronic Mnemonics for Augmentation of Human Memory},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480777}}

@inproceedings{4480778,
	abstract = {The use of virtual environments for training is strongly stimulated by important needs for training on sensitive equipments. Yet, developing such an application is often done without reusing existing components, which requires a huge amount of time. We present in this paper a full authoring platform to facilitate the development of both new virtual environments and pedagogical information for procedural training. This platform, named GVT (generic virtual training) relies on innovative models and provides authoring tools which allow capitalizing on the developments realized. We present a generic model named STORM, used to describe reusable behaviors for 3D objects and reusable interactions between those objects. We also present a scenario language named LORA which allows non computer scientists to author various and complex sequences of tasks in a virtual scene. Based on those models, as an industrial validation with Nexter-Group, more than fifty operational scenarios of maintenance training on military equipments have been realized so far. We have also set up an assessment campaign, and we expose in this paper the first results which show that GVT enables trainees to learn procedures efficiently. The platform keeps on evolving and training on collaborative procedures will soon be available.},
	author = {Gerbaud, Stephanie and Mollet, Nicolas and Ganier, Franck and Arnaldi, Bruno and Tisseau, Jacques},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480778},
	issn = {2375-5334},
	keywords = {Virtual environment;Industrial training;Virtual reality;Computer graphics;Collaborative work;Storms;Military computing;Layout;Defense industry;Military equipment;virtual environments;virtual reality;training;procedure;behavioral objects;collaboration;D.2.13 [Software Engineering]: Reusable Software-Reuse models;I.6.5 [Simulation and Modeling]: Model Development-Modeling methodologies;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality;K.3.1 [Computers and Education]: Computer Uses in Education- Collaborative learning},
	month = {March},
	pages = {225-232},
	title = {GVT: a platform to create virtual environments for procedural training},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480778}}

@inproceedings{4480779,
	abstract = {We have developed a distance education system for developing skills in endoscopic paranasal sinus surgery, to enable efficient remote training of novices in manual skills such as their standing position and posture, and the insertion angle/depth and holding of surgical instruments. The system uses a precise model of human paranasal sinuses and the "HyperMirror" (HM) telecommunication interface. HM is a virtual mirror allowing clear visualization of differences in manual operation between the trainee and remote expert. This paper outlines the proposed system and describes remote training experiments between two locations 200 miles apart. In the experiments, two expert surgeons trained 17 novices for 40 to 60 min on probing of the nasofrontal duct and aspiration of the maxillary sinus, and subjectively evaluated their manual skills. The results showed that most of the novices improved their manual skills and were able to complete each procedure.},
	author = {Kumagai, Toru and Yamashita, Juli and Morikawa, Osamu and Yokoyama, Kazunori and Fujimaki, Shin'ichi and Konishi, Taku and Ishimasa, Hiroshi and Murata, Hideyuki and Tomoda, Koichi},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480779},
	issn = {2375-5334},
	keywords = {Distance learning;Surgery;Surgical instruments;Manuals;Mirrors;Computer science education;Electronic switching systems;Industrial training;Humans;Computer interfaces;Endoscopic sinus surgery;manual skill;remote training;surgical education;K.3.1 [Computer and Education]: Computer Uses in Education - Distance learning;H5.1 [Information Interface and Presentation]: User Interfaces - Screen design;J.3 [Computer Applications]: Life and Medical Sciences - Health},
	month = {March},
	pages = {233-236},
	title = {Distance education system for teaching manual skills in endoscopic paranasal sinus surgery using "hypermirror" telecommunication interface},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480779}}

@inproceedings{4480780,
	abstract = {Most projector-based immersive displays have numerous problems relating to complexity, space, and cost. We present a technique for rendering perspectively correct images using a casual arrangement of a projector, a mirror and a display surface. Our technique renders an arbitrarily wide field of view using an efficient GPU-based single-pass rendering algorithm. The rendering algorithm is preceded by a one-time camera-based geometric correction calibration step. As we will describe, this technique can be implemented with inexpensive, commodity hardware and using readily available display surfaces. Thus, this technique enables immersive projection systems to be used in casual locations, such as a classroom or even in the home.},
	author = {Yuen, Nancy P. Y. and Thibault, William C.},
	booktitle = {2008 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:06 -0400},
	date-modified = {2024-03-18 02:29:06 -0400},
	doi = {10.1109/VR.2008.4480780},
	issn = {2375-5334},
	keywords = {Rendering (computer graphics);Calibration;Computer displays;Cameras;Geometry;Costs;Mirrors;Hardware;Shape;Computer vision;display algorithms;viewing algorithms;camera calibration;projector-camera systems;I.3.3 [Computing Methodologies]: Computer Graphics-Picture/Image Generation;I.4.1 [Image Processing and Computer Vision]: Digitization and Image Capture-Camera Calibration},
	month = {March},
	pages = {237-240},
	title = {Inexpensive Immersive Projection},
	year = {2008},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2008.4480780}}

@inproceedings{4160999,
	abstract = {Even in state-of-the-art virtual reality (VR) setups, participants often feel lost when navigating through virtual environments. In psychological experiments, such disorientation is often compensated for by extensive training. The current study investigated participants' sense of direction by means of a rapid point-to-origin task without any training or performance feedback. This allowed us to study participants' intuitive spatial orientation in VR while minimizing the influence of higher cognitive abilities and compensatory strategies. After visually displayed passive excursions along one-or two-segment trajectories, participants were asked to point back to the origin of locomotion "as accurately and quickly as possible". Despite using a high-quality video projection with a 84deg times 63deg field of view, participants' overall performance was rather poor. Moreover, six of the 16 participants exhibited striking qualitative errors, i.e., consistent left-right confusions that have not been observed in comparable real world experiments. Taken together, this study suggests that even an immersive high-quality video projection system is not necessarily sufficient for enabling natural spatial orientation in VR. We propose that a rapid point-to-origin paradigm can be a useful tool for evaluating and improving the effectiveness of VR setups in terms of enabling natural and unencumbered spatial orientation and performance.},
	author = {Riecke, Bernhard E. and Wiener, Jan M.},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352457},
	issn = {2375-5334},
	keywords = {Virtual reality;Navigation;Optical feedback;Virtual environment;Psychology;Legged locomotion;Turning;Optical sensors;Image motion analysis;Large screen displays;ego-motion simulation;human factors;navigation;point-to-origin;psychophysics;spatial orientation;spatial updating;triangle completion;Virtual Reality;H.1.2 [Models and Principles]: User/Machine SystemsHuman factors, Human information processing;H.5.1 [Information Interfaces and Presentation, (e.g. HCI]: Multimedia Information SystemsArtificial, augmented, and virtual realities;J.4 [Social and Behavioral Sciences]: Psychology},
	month = {March},
	pages = {3-10},
	title = {Can People Not Tell Left from Right in VR? Point-to-origin Studies Revealed Qualitative Errors in Visual Path Integration},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352457}}

@inproceedings{4161000,
	abstract = {Enabling veridical spatial perception in immersive virtual environments (IVEs) is an important yet elusive goal, as even the factors implicated in the often-reported phenomenon of apparent distance compression in HMD-based IVEs have yet to be satisfactorily elucidated. In recent experiments [e.g. 3], we have found that participants appear less prone to significantly underestimate egocentric distances in HMD-based IVEs, relative to in the real world, in the special case that they unambiguously know, through first-hand observation, that the presented virtual environment is a high fidelity 3D model of their concurrently occupied real environment. We had hypothesized that this increased veridicality might be due to participants having a stronger sensation of `presence' in the IVE under these conditions of co-location, which state of mind leads them to act on their visual input in the IVE similarly as they would in the real world (the presence hypothesis). However, alternative hypotheses are also possible. Primary among these is the visual calibration hypothesis: participants could be relying on metric information gleaned from their exposure to the real environment to calibrate their judgments of sizes and distances in the matched virtual environment. It is important to disambiguate between the presence and visual calibration hypotheses because they suggest different directions for efforts to facilitate veridical distance perception in general (non-co-located) IVEs. In this paper, we present the results of an experiment that seeks novel insight into this question.},
	author = {Interrante, Victoria and Lindquist, Jason and Anderson, Lee},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352458},
	issn = {2375-5334},
	keywords = {Virtual environment;Calibration;Computer graphics;Displays;Computer science;Computer architecture;Degradation;Chromium;Virtual reality;USA Councils;egocentric distance perception;immersive virtual environments},
	month = {March},
	pages = {11-18},
	title = {Elucidating Factors that can Facilitate Veridical Spatial Perception in Immersive Virtual Environments},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352458}}

@inproceedings{4161001,
	abstract = {This paper presents a comparison of various classification methods for the problem of recognizing grasp types involved in object manipulations performed with a data glove. Conventional wisdom holds that data gloves need calibration in order to obtain accurate results. However, calibration is a time-consuming process, inherently user-specific, and its results are often not perfect. In contrast, the present study aims at evaluating recognition methods that do not require prior calibration of the data glove, by using raw sensor readings as input features and mapping them directly to different categories of hand shapes. An experiment was carried out, where test persons wearing a data glove had to grasp physical objects of different shapes corresponding to the various grasp types of the Schlesinger taxonomy. The collected data was analyzed with 28 classifiers including different types of neural networks, decision trees, Bayes nets, and lazy learners. Each classifier was analyzed in six different settings, representing various application scenarios with differing generalization demands. The results of this work are twofold: (1) We show that a reasonably well to highly reliable recognition of grasp types can be achieved - depending on whether or not the glove user is among those training the classifier - even with uncalibrated data gloves. (2) We identify the best performing classification methods for recognition of various grasp types. To conclude, cumbersome calibration processes before productive usage of data gloves can be spared in many situations.},
	author = {Heumer, Guido and Amor, Heni Ben and Weber, Matthias and Jung, Bernhard},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352459},
	issn = {2375-5334},
	keywords = {Data gloves;Calibration;Shape;Sensor phenomena and characterization;Testing;Taxonomy;Data analysis;Classification tree analysis;Neural networks;Decision trees;Data Glove;Calibration;Grasp Recognition;Classification Methods;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-[Virtual Reality]},
	month = {March},
	pages = {19-26},
	title = {Grasp Recognition with Uncalibrated Data Gloves - A Comparison of Classification Methods},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352459}}

@inproceedings{4161002,
	abstract = {In this paper we introduce new user interface concepts for fish tank virtual reality (VR) systems based on autostereoscopic (AS) display technologies. Such AS displays allow to view stereoscopic content without requiring special glasses. Unfortunately, until now simultaneous monoscopic and stereoscopic display was not possible. Hence prior work on fish tank VR systems focussed either on 2D or 3D interactions. In this paper we introduce so called interscopic interaction concepts providing an improved working experience, which enable great potentials in terms of the interaction between 2D elements, which may be displayed either in monoscopic or stereoscopic, e.g., GUI items, and the 3D virtual environment usually displayed stereoscopically. We present a framework which is based on a software layer between the operating system and its graphical user interface supporting the display of both mono- as well as stereoscopic content in arbitrary regions of an autostereoscopic display. The proposed concepts open up new vistas for the interaction in environments where essential parts of the GUI are displayed monoscopically and other parts are rendered stereoscopically. We address some essential issues of such fish tank VR systems and introduce intuitive interaction concepts which we have realized},
	author = {Stenicke, Frank and Ropinski, Timo and Bruder, Gerd and Hinrichs, Klaus},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352460},
	issn = {2375-5334},
	keywords = {User interfaces;Marine animals;Virtual reality;Graphical user interfaces;Computer displays;Mice;Visualization;Glass;Virtual environment;Costs;fish tank VR;autostereoscopic displays;interscopic user interfaces;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Graphical user interfaces (GUI);Interaction styles},
	month = {March},
	pages = {27-34},
	title = {Interscopic User Interface Concepts for Fish Tank Virtual Reality Systems},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352460}}

@inproceedings{4161003,
	abstract = {A challenge in presenting augmenting information in outdoor augmented reality (AR) settings lies in the broad range of uncontrollable environmental conditions that may be present, specifically large-scale fluctuations in natural lighting and wide variations in likely backgrounds or objects in the scene. In this paper, we present a active AR testbed that samples the user's field of view, and collects outdoor illuminance values at the participant's position. The main contribution presented herein is a user-based study (conducted using the testbed) that examined the effects on user performance of four outdoor background textures, four text colors, three text drawing styles, and two text drawing style algorithms for a text identification task using an optical, see-through AR system. We report significant effects for all these variables, and discuss design guidelines and ideas for future work},
	author = {Gabbard, Joseph L. and Swan, J. Edward and Hix, Deborah and Kim, Si-Jung and Fitch, Greg},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352461},
	issn = {2375-5334},
	keywords = {Augmented reality;Graphical user interfaces;Testing;Design engineering;Optical sensors;Engineering drawings;Systems engineering and theory;Large-scale systems;Fluctuations;Virtual reality;Outdoor Augmented Reality;Optical See-Through Display;Text Drawing Styles;Text Legibility;Empirical Study},
	month = {March},
	pages = {35-42},
	title = {Active Text Drawing Styles for Outdoor Augmented Reality: A User-Based Study and Design Implications},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352461}}

@inproceedings{4161004,
	abstract = {A major roadblock for using augmented reality in many medical and industrial applications is the fact that the user cannot take full advantage of the 3D virtual data. This usually requires the user to move the virtual object, which disturbs the real/virtual alignment, or to move his head around the real objects, which is not always possible and/or practical. This problem becomes more dramatic when a single camera is used for monitor based augmentation, such as in augmented laparoscopic surgery. In this paper we introduce an interaction and 3D visualization paradigm, which presents a new solution to this old problem. The interaction paradigm uses an interactive virtual mirror positioned into the augmented scene, which allows easy and complete interactive visualization of 3D virtual data. This paper focuses on the exemplary application of such visualization techniques to laparoscopic interventions. A large number of such interventions aims at regions inside a specific organ, e.g. blood vessels to be clipped for tumor resection. We use high-resolution intra-operative imaging data generated by a mobile C-arm with cone-beam CT imaging capability. Both the C-arm and the laparoscope are optically tracked and registered in a common world coordinate frame. After patient positioning, port placement, and carbon dioxide insufflation, a C-arm volume is reconstructed during patient exhalation and superimposed in real time on the laparoscopic live video without any need for an additional patient registration procedure. To overcome the missing perception of 3D depth and shape when rendering virtual volume data directly on top of the organ's surface view, we introduce the concept of a laparoscopic virtual mirror: A virtual reflection plane within the live laparoscopic video, which is able to visualize a reflected side view of the organ and its interior. This enables the surgeon to observe the 3D structure of, for example, blood vessels by moving the virtual mirror within the augmented monocular view of the laparoscope.},
	author = {Navab, Nassir and Feuerstein, Marco and Bichlmeier, Christoph},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352462},
	issn = {2375-5334},
	keywords = {Laparoscopes;Mirrors;Augmented reality;Data visualization;Biomedical imaging;Blood vessels;High-resolution imaging;Optical imaging;Biomedical monitoring;Head;Interactive AR Visualization;Depth Perception;Medical Augmented Reality;Laparoscopic Surgery;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems-Artificial;augmented;and virtual realities;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Interaction styles;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction techniques;J.3 [Life and Medical Sciences]},
	month = {March},
	pages = {43-50},
	title = {Laparoscopic Virtual Mirror New Interaction Paradigm for Monitor Based Augmented Reality},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352462}}

@inproceedings{4161005,
	abstract = {While an important factor in depth perception, the occlusion effect in 3D environments also has a detrimental impact on tasks involving discovery, access, and spatial relation of objects in a 3D visualization. A number of interactive techniques have been developed in recent years to directly or indirectly deal with this problem using a wide range of different approaches. In this paper, we build on previous work on mapping out the problem space of 3D occlusion by defining a taxonomy of the design space of occlusion management techniques in an effort to formalize a common terminology and theoretical framework for this class of interactions. We classify a total of 25 different techniques for occlusion management using our taxonomy and then go on to analyze the results, deriving a set of five orthogonal design patterns for effective reduction of 3D occlusion. We also discuss the "gaps" in the design space, areas of the taxonomy not yet populated with existing techniques, and use these to suggest future research directions into occlusion management.},
	author = {Elmqvist, Niklas and Tsigas, Philippas},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352463},
	issn = {2375-5334},
	keywords = {Taxonomy;Humans;Pattern analysis;Quality management;Visualization;Space exploration;Electronic mail;Environmental management;Technology management;Engineering management;occlusion management;occlusion reduction;taxonomy;design patterns;visual cues;depth perception},
	month = {March},
	pages = {51-58},
	title = {A Taxonomy of 3D Occlusion Management Techniques},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352463}}

@inproceedings{4161006,
	abstract = {In this paper, the effects of three visual factors: scene complexity, stereovision and motion parallax on correct perception of a virtual object's size were analyzed in an immersive virtual environment. We designed a controlled experiments set to incorporate visual conditions that reflected all twelve different configuration combinations of the three visual factors. Under each visual condition, subject performed the task of making judgments of the sizes of a virtual object displayed at five different distances from him/her. A total number of eighteen subjects participated in our study. The subjects' judgments and the corresponding actual sizes of the virtual object were recorded. Based on the collected data, two quantitative measures of subjects' performance were derived and analyzed. The results of our experiments were consistent across the majority of the subject population and suggested that scene complexity and stereovision could have significant impact on the performance of a user of virtual environments to make correct judgments on a virtual object's size. On the contrary, motion parallax, either produced by the virtual environment or by the observer, might not be a significant factor in determining that performance},
	author = {Luo, Xun and Kenyon, Robert and Kamper, Derek and Sandin, Daniel and DeFanti, Thomas},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352464},
	issn = {2375-5334},
	keywords = {Layout;Virtual environment;Visualization;Virtual reality;Motion analysis;Computer graphics;Retina;Displays;Biomedical engineering;Performance analysis;Measurement;Human Factors;Virtual Reality;Size Constancy;Visual Factors;Effects},
	month = {March},
	pages = {59-66},
	title = {The Effects of Scene Complexity, Stereovision, and Motion Parallax on Size Constancy in a Virtual Environment},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352464}}

@inproceedings{4161007,
	abstract = {Because marker-based position tracking system is inexpensive and easy to use, it has the potential to make the system more feasible for homes or businesses and broaden the current suite of augmented reality (AR) techniques. To apply marker-based systems to homes or businesses, blending markers with environments naturally is important. Our innovative approach to marker-based 3D position tracking uses seamless patterns encrypted with positional data. Although users can obtain 3D positional data by processing marker images similar to many existing marker-based systems, our markers are designed with interior decoration in mind. That way they can be installed in walls, floors, or ceilings. Unlike existing systems whose fiducial markers were designed first and foremost to be processed by computers, ours are visually attractive. By integrating positional information within the interior design, our system enables users to enjoy the benefits of position tracking without being constantly aware of the system's presence. We developed a method for making patterns in which positional information is encrypted and a method for calculating the 3D position of a user by decoding those patterns. We then constructed a system using three patterns that were made by the proposed method and evaluated that system},
	author = {Saito, Shigeru and Hiyama, Atsushi and Tanikawa, Tomohiro and Hirose, Michitaka},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352465},
	issn = {2375-5334},
	keywords = {Information science;Cameras;Image processing;Augmented reality;Cryptography;Floors;Navigation;Wearable computers;Costs;Global Positioning System;Indoor Position Tacking;Interior Decoration;Coded Pattern;Fiducial Marker;AR},
	month = {March},
	pages = {67-74},
	title = {Indoor Marker-based Localization Using Coded Seamless Pattern for Interior Decoration},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352465}}

@inproceedings{4161008,
	abstract = {Anywhere augmentation pursues the goal of lowering the initial investment of time and money necessary to participate in mixed reality work, bridging the gap between researchers in the field and regular computer users. Our paper contributes to this goal by introducing the GroundCam, a cheap tracking modality with no significant setup necessary. By itself, the GroundCam provides high frequency, high resolution relative position information similar to an inertial navigation system, but with significantly less drift. When coupled with a wide area tracking modality via a complementary Kalman filter, the hybrid tracker becomes a powerful base for indoor and outdoor mobile mixed reality work},
	author = {DiVerdi, Stephen and Hollerer, Tobias},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352466},
	issn = {2375-5334},
	keywords = {Virtual reality;Tracking;Calibration;Application software;Computer graphics;Layout;Image motion analysis;Instruments;Global Positioning System;Optical sensors;Anywhere augmentation;vision-based tracking;tracker fusion;mobile mixed reality;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Motion I.2.10 [Artificial Intelligence]: Vision and Scene Understanding-Motion},
	month = {March},
	pages = {75-82},
	title = {GroundCam: A Tracking Modality for Mobile Mixed Reality},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352466}}

@inproceedings{4161009,
	abstract = {The simulation of fracture leads to collision-intensive situations that call for efficient collision detection algorithms and data structures. Bounding volume hierarchies (BVHs) are a popular approach for accelerating collision detection, but they rarely see application in fracture simulations, due to the dynamic creation and deletion of geometric primitives. We propose the use of balanced trees for storing BVHs, as well as novel algorithms for dynamically restructuring them in the presence of progressive or instantaneous fracture. By paying a small loss of fitting quality compared with complete reconstruction, we achieve more than one order of magnitude speedup in the update of BVHs},
	author = {Otaduy, Miguel A. and Chassot, Olivier and Steinemann, Denis and Gross, Markus},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352467},
	issn = {2375-5334},
	keywords = {Object detection;Data structures;Costs;Computer graphics;Acceleration;Computational modeling;Solid modeling;Heuristic algorithms;Computational geometry;Surface cracks;Collision detection;AVL-trees;fracture;I.3.5 [Computer Graphics]: Computational Geometry and Object Modeling-Object Hierarchies},
	month = {March},
	pages = {83-90},
	title = {Balanced Hierarchies for Collision Detection between Fracturing Objects},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352467}}

@inproceedings{4161010,
	abstract = {We present a novel approach for real-time path planning of multiple virtual agents in complex dynamic scenes. We introduce a new data structure, Multi-agent Navigation Graph (MaNG), which is constructed from the first- and second-order Voronoi diagrams. The MaNG is used to perform route planning and proximity computations for each agent in real time. We compute the MaNG using graphics hardware and present culling techniques to accelerate the computation. We also address undersampling issues for accurate computation. Our algorithm is used for real-time multi-agent planning in pursuit-evasion and crowd simulation scenarios consisting of hundreds of moving agents, each with a distinct goal},
	author = {Sud, Avneesh and Andersen, Erik and Curtis, Sean and Lin, Ming and Manocha, Dinesh},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352468},
	issn = {2375-5334},
	keywords = {Path planning;Computational modeling;Data structures;Virtual environment;Layout;Navigation;Collision avoidance;Graphics;Hardware;Acceleration;crowd simulation;Voronoi diagram;motion planning;I.3.5 [Computing Methodologies]: Computational Geometry and Object Modeling-Geometric algorithms;I.3.7 [Computing Methodologies]: Three-Dimensional Graphics and Realism-nimation virtual reality},
	month = {March},
	pages = {91-98},
	title = {Real-time Path Planning for Virtual Agents in Dynamic Environments},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352468}}

@inproceedings{4161011,
	abstract = {This paper presents a method of pose synthesis based on a low-dimensional space and a set of characteristics of motion learned from examples. This method consists of two phases: learning and synthesis. In the learning phase, a low-dimensional and discrete representation of the space of natural poses is constructed by using a self organizing map (SOM). Meanwhile, a set of matrices is extracted from the motion data. These matrices describe how the poses change with the end-effectors' positions, and play a key role in synthesizing natural looking results. In the synthesis phase, a lightweight algorithm based on the learned parameters is used. The synthesis process is very efficient because there is no time-consuming calculation, like numeric optimization or matrix inverting. Compared with other methods, our method not only can produce natural looking poses in real-time, but also works well with constraints positioned in a larger range. We apply our method in applications of interactive pose editing, real-time motion modification, and pose reconstruction from image. The results have proven the robustness and effectiveness of our method},
	author = {Li, Chunpeng and Xia, Shihong and Wang, Zhaoqi},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352469},
	issn = {2375-5334},
	keywords = {Jacobian matrices;Kinematics;Humans;Computers;Virtual reality;Joints;Space technology;Organizing;Data mining;Animation;Inverse Kinematics;Self Organizing Map;Jacobian Matrix;Character Animation;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Animations;I.2.6 [Artificial Intelligence]: Learning-Connectionism and neural nets},
	month = {March},
	pages = {99-106},
	title = {Pose Synthesis Using the Inverse of Jacobian Matrix Learned from Examples},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352469}}

@inproceedings{4161012,
	abstract = {Large scale distributed virtual environments (DVEs) have become a major trend in distributed applications, mainly due to the enormous popularity of multi-player online games in the entertainment industry. Since architectures based on networked servers seem to be not scalable enough to support massively multi-player applications, peer-to-peer (P2P) architectures have been proposed as an efficient and truly scalable solution for this kind of systems. However, in order to design efficient DVEs based on peer-to-peer architectures these systems must be characterized, measuring the impact of different client behaviors on system performance. This paper presents the experimental characterization of peer-to-peer distributed virtual environments in regard to well-known performance metrics in distributed systems. Characterization results show that system saturation is inherently avoided due to the peer-to-peer scheme, as it could be expected. Also, these results show that the saturation of a given client exclusively has an effect on the surrounding clients in the virtual world, having no noticeable effect at all on the rest of avatars. Finally, the characterization results show that the response time offered to client computers greatly depends on the number of new connections that these clients have to make when new neighbors appear in the virtual world. These results can be used as the basis for an efficient design of peer-to-peer DVE systems.},
	author = {Rueda, S. and Morillo, P. and Orduna, J.M. and Duato, J.},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352470},
	issn = {2375-5334},
	keywords = {Peer to peer computing;Virtual environment;Avatars;Network servers;Computer architecture;Large-scale systems;Computer graphics;Computer networks;Application software;Toy industry;Distributed virtual environments;peer-to-peer;multi-player online games;I.3.2 [Computer graphics]: Graphics systems-Distributed/network graphics;C.2.4 [Computer-communication networks]: Distributed Systems-Distributed Applications},
	month = {March},
	pages = {107-114},
	title = {On the Characterization of Peer-To-Peer Distributed Virtual Environments},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352470}}

@inproceedings{4161013,
	abstract = {We present a system that enables, for the first time, effective transatlantic cooperative haptic manipulation of objects whose motion is computed using a physically-based model. We propose a technique for maintaining synchrony between simulations in a peer-to-peer system, while providing responsive direct manipulation for all users. The effectiveness of this approach is determined through extensive user trials involving concurrent haptic manipulation of a shared object. A CAD assembly task, using physically-based motion simulation and haptic feedback, was carried out between the USA and the UK with network latencies in the order of 120ms. We compare the effects of latency on synchrony between peers over the Internet with a low latency (0.5ms) local area network. Both quantitatively and qualitatively, when using our technique, the performance achieved over the Internet is comparable to that on a LAN. As such, this technique constitutes a significant step forward for distributed haptic collaboration},
	author = {Glencross, Mashhuda and Jay, Caroline and Feasel, Jeff and Kohli, Luv and Whitton, Mary and Hubbold, Roger},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352471},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Internet;Delay;Local area networks;Physics computing;Computational modeling;Peer to peer computing;Assembly;Feedback;IP networks;Human Factors;Haptic Cooperation;Virtual Environments;Multi-user;Networked Applications;I.3.2 [Computer Graphics]: Graphics Systems-Distributed/network graphics;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality;H.5.2 [Information Interfaces and Presentation]: User Interfaces-Haptic I/O},
	month = {March},
	pages = {115-122},
	title = {Effective Cooperative Haptic Interaction over the Internet},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352471}}

@inproceedings{4161014,
	abstract = {Application designers of collaborative distributed virtual environments must account for the influence of the network connection and its detrimental effects on user performance. Based upon analysis and classification of existing latency compensation techniques, this paper introduces a novel approach to latency amelioration in the form of a two-tier predictor-estimator framework. The technique is variability-aware due to its proactive sender-side prediction of a pose a variable time into the future. The prediction interval required is estimated based on current and past network delay characteristics. This latency estimate is subsequently used by a Kalman filter-based predictor to replace the measurement event with a predicted pose that matches the event's arrival time at the receiving workstation. The compensation technique was evaluated in a simulation through an offline playback of real head motion data and network delay traces collected under a variety of real network conditions. The experimental results indicate that the variability-aware approach significantly outperforms a state-of-the-art one, which assumes a constant system delay},
	author = {Tumanov, Alexey and Allison, Robert and Stuerzlinger, Wolfgang},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352472},
	issn = {2375-5334},
	keywords = {Virtual environment;Application software;Jitter;Humans;Collaboration;Delay estimation;Delay systems;Virtual reality;Computer science;Kalman filters;virtual environment;network delay;jitter},
	month = {March},
	pages = {123-130},
	title = {Variability-Aware Latency Amelioration in Distributed Environments},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352472}}

@inproceedings{4161015,
	abstract = {Autostereoscopic displays enable unencumbered immersive virtual reality, but at a significant computational expense. This expense impacts the feasibility of autostereo displays in high-performance real-time interactive applications. A new autostereo rendering algorithm named autostereo combiner addresses this problem using the programmable vertex and fragment pipelines of modern graphics processing units (GPUs). This algorithm is applied to the Varrier, a large-scale, head-tracked, parallax barrier autostereo virtual reality platform. In this capacity, the Combiner algorithm has shown performance gains of 4x over traditional parallax barrier rendering algorithms. It has enabled high-performance rendering at sub-pixel scales, affording a 2x increase in resolution and showing a 1.4x improvement in visual acuity},
	author = {Kooima, Robert L. and Peterka, Tom and Girado, Javier I. and Ge, Jinghua and Sandin, Daniel J. and DeFanti, Thomas A.},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352473},
	issn = {2375-5334},
	keywords = {Virtual reality;Computer displays;Rendering (computer graphics);Interleaved codes;Computer graphics;Three dimensional displays;Strips;Eyes;Visualization;Laboratories;autostereoscopic display;3D display;virtual reality;Varrier;parallax barrier},
	month = {March},
	pages = {131-137},
	title = {A GPU Sub-pixel Algorithm for Autostereoscopic Virtual Reality},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352473}}

@inproceedings{4161016,
	abstract = {Stereo displays suffer from crosstalk, an effect that reduces or even inhibits the viewer's ability to correctly perceive depth. Previous work on software crosstalk reduction focussed on the preprocessing of static scenes which are viewed from a fixed viewpoint. However, in virtual environments scenes are dynamic, and are viewed from various viewpoints in real-time on large display areas. In this paper, three methods are introduced for reducing crosstalk in virtual environments. A non-uniform crosstalk model is described, which can be used to accurately reduce crosstalk on large display areas. In addition, a novel temporal algorithm is used to address the problems that occur when reducing crosstalk in dynamic scenes. This way, high-frequency jitter caused by the erroneous assumption of static scenes can be eliminated. Finally, a perception based metric is developed that allows us to quantify crosstalk. We provide a detailed description of the methods, discuss their tradeoffs, and compare their performance with existing crosstalk reduction methods},
	author = {Smit, F.A. and van Liere, R. and Froehlich, B.},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352474},
	issn = {2375-5334},
	keywords = {Crosstalk;Layout;Computer displays;Virtual environment;Liquid crystal displays;Glass;Jitter;Computer graphics;Phosphors;Mathematics;Crosstalk;Ghosting;Stereoscopic display;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality;I.3.3 [Computer Graphics]: Picture/Image Generation-Display Algorithms},
	month = {March},
	pages = {139-146},
	title = {Non-Uniform Crosstalk Reduction for Dynamic Scenes},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352474}}

@inproceedings{4161017,
	abstract = {Projectors equipped with wide-angle lenses can have an advantage over traditional projectors in creating immersive display environments since they can be placed very close to the display surface to reduce user shadowing issues while still producing large images. However, wide-angle projectors exhibit severe image distortion requiring the image generator to correctively pre-distort the output image. In this paper, we describe a new technique based on Raskar's (1998) two-pass rendering algorithm that is able to correct for both arbitrary display surface geometry and the extreme lens distortion caused by fisheye lenses. We further detail how the distortion correction algorithm can be implemented in a real-time shader program running on a commodity GPU to create low-cost, personal surround environments},
	author = {Johnson, Tyler and Gyarfas, Florian and Skarbez, Rick and Towles, Herman and Fuchs, Henry},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352475},
	issn = {2375-5334},
	keywords = {Geometry;Lenses;Rendering (computer graphics);Computer displays;Computer graphics;Three dimensional displays;Shadow mapping;Image generation;Virtual reality;Shape;Projector displays;lens distortion correction;GPU programming;I.3.3 [Computer Graphics]: Picture/Image Generation-Display Algorithms;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {147-154},
	title = {A Personal Surround Environment: Projective Display with Correction for Display Surface Geometry and Extreme Lens Distortion},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352475}}

@inproceedings{4161018,
	abstract = {A novel barrier strip autostereoscopic (AS) display is demonstrated using a solid-state dynamic parallax barrier. A dynamic barrier mitigates restrictions inherent in static barrier systems such as fixed view distance range, slow response to head movements, and fixed stereo operating mode. By dynamically varying barrier parameters in real time, viewers may move closer to the display and move faster laterally than with a static barrier system. Furthermore, users can switch between 3D and 2D modes by disabling the barrier. Dynallax is head-tracked, directing view channels to positions in space reported by a tracking system in real time. Such head-tracked parallax barrier systems have traditionally supported only a single viewer, but by varying the barrier period to eliminate conflicts between viewers, Dynallax presents four independent eye channels when two viewers are present. Each viewer receives an independent pair of left and right eye perspective views based on their position in 3D space. The display device is constructed using a dual-stacked LCD monitor where a dynamic barrier is rendered on the front display and the rear display produces a modulated VR scene composed of two or four channels. A small-scale head-tracked prototype VR system is demonstrated.},
	author = {Peterka, Tom and Kooima, Robert L. and Girado, Javier I. and Ge, Jinghua and Sandin, Daniel J. and Johnson, Andrew and Leigh, Jason and Schulze, Jurgen and DeFanti, Thomas A.},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352476},
	issn = {2375-5334},
	keywords = {Solid state circuits;Virtual reality;Strips;Liquid crystal displays;Head;Real time systems;Spatial resolution;Delay;Eyes;Visualization;autostereoscopic display;3D display;virtual reality;Dynallax;Varrier;parallax barrier},
	month = {March},
	pages = {155-162},
	title = {Dynallax: Solid State Dynamic Parallax Barrier Autostereoscopic VR Display},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352476}}

@inproceedings{4161019,
	abstract = {What is often missing from many virtual worlds is a physical sense of the confinement and constraint of the virtual environment. To address this issue, we present a method for providing localized cutaneous vibratory feedback to the user's right arm. We created a sleeve of tactors linked to a real-time human model that activates when the corresponding body area collides with an object. The hypothesis is that vibrotactile feedback to body areas provides the wearer sufficient guidance to ascertain the existence and physical realism of access paths and body configurations. The results of human subject experiments clearly show that the use of full arm vibrotactile feedback improves performance over purely visual feedback in navigating the virtual environment. These results validate the empirical performance of this concept},
	author = {Bloomfield, Aaron and Badler, Norman I.},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352477},
	issn = {2375-5334},
	keywords = {Virtual environment;Force feedback;Haptic interfaces;Humans;Sensor arrays;Computer graphics;Layout;Costs;Imaging phantoms;Skin;haptics;sensory substitution;tactile array;tactor;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality H.5.2 [Information Interfaces and Presentation]: User Interfaces-Haptic I/O;Evaluation/Methodology H.1.2 [Models and Principles]: User/Machine Systems-Human Factors},
	month = {March},
	pages = {163-170},
	title = {Collision Awareness Using Vibrotactile Arrays},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352477}}

@inproceedings{4161020,
	abstract = {In this paper we present an in situ evaluation of a haptic system, with a representative test population, we aim to determine what, if any, benefit haptics can have in a biomolecular education context. We have developed a haptic application for conveying concepts of molecular interactions, specifically in protein-ligand docking. Utilizing a semi-immersive environment with stereo graphics, users are able to manipulate the ligand and feel its interactions in the docking process. The evaluation used cognitive knowledge tests and interviews focused on learning gains. Compared with using time efficiency as the single quality measure this gives a better indication of a system's applicability in an educational environment. Surveys were used to gather opinions and suggestions for improvements. Students do gain from using the application in the learning process but the learning appears to be independent of the addition of haptic feedback. However the addition of force feedback did decrease time requirements and improved the students understanding of the docking process in terms of the forces involved, as is apparent from the students' descriptions of the experience. The students also indicated a number of features which could be improved in future development},
	author = {Persson, Petter Bivall and Cooper, Matthew D. and Tibell, Lena A.E. and Ainsworth, Shaaron and Ynnerman, Anders and Jonsson, Bengt-Harald},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352478},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Educational technology;Force feedback;Computer science education;Visualization;Chemical technology;Physics education;Biology;Protein engineering;Testing;Haptics;Haptic Interaction;Life Science Education;Visualization;Protein Interactions;H.5.1 [Multimedia Information Systems]: Artificial augmented and virtual realities;[H5.2]: User Interfaces-Haptic I/O;K3.1 [Computer Uses in Education]: Computer-assisted instruction;[H.3.4]: Systems and SoftwarePerformance evaluation (efficiency and effectiveness);J.3 [LIFE AND MEDICAL SCIENCES]: Biology and genetics-[J.2]: PHYSICAL SCIENCES AND ENGINEERING-Chemistry;[J.2]: PHYSICAL SCIENCES AND ENGINEERINGChemistry},
	month = {March},
	pages = {171-178},
	title = {Designing and Evaluating a Haptic System for Biomolecular Education},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352478}}

@inproceedings{4161021,
	abstract = {The research on olfactory sense in virtual reality has gradually expanded even though the technology is still premature. We have developed an olfactory display composed of multiple solenoid valves. In the present study, an extended olfactory display, where 32 component odors can be blended in any recipe, is described; the previous version has only 8 odor components. The size was unchanged even though the number of odor components was four times larger than that in the previous display. The complexity of blending was greatly reduced because of algorithm improvement. The blending method and the fundamental experiment using a QCM (quartz crystal microbalance) sensor are described here},
	author = {Nakamoto, Takamichi and Minh, Hai Pham Dinh},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352479},
	issn = {2375-5334},
	keywords = {Olfactory;Displays;Solenoids;Valves;Motion pictures;Sensor phenomena and characterization;Virtual environment;Weight control;Space technology;Virtual reality;Olfactory display;solenoid valve;Pulse Width Modulation;QCM gas sensor;Movie with scents},
	month = {March},
	pages = {179-186},
	title = {Improvement of olfactory display using solenoid valves},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352479}}

@inproceedings{4161022,
	abstract = {An effective method of depth image based rendering is proposed by applying texture mapping onto virtual but pre-defined multiple planes in the scene, called virtual planes. The method allows the viewpoint either static or moving around, including crossing the plane of the source images. By this approach, the relief textures from depth images are mapped onto the virtual planes, and through the pre-warping process, the virtual planes are converted into standard polygonal textures. After the virtual plane mosaics, the resultant image that supports 3D objects and immersive scenes can be generated by polygonal texture mapping. In addition, both hardware and software implementation of the method can increase the power of conventional texture mapping in image based rendering. In particular, the scope of the viewpoint could be extended into the inner space of depth images, and as a result, a novel solution is provided for constructing real-time walkthrough systems as well as for panoramic modeling from an arbitrary viewpoint in the depth image space},
	author = {Sheng, Bin and Wu, Enhua},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352480},
	issn = {2375-5334},
	keywords = {Legged locomotion;Image-based modeling and rendering;relief texture mapping;depth Image;virtual plane;I.3.3 [Computer Graphics]: Picture/Image Generation-Display algorithms;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality},
	month = {March},
	pages = {187-194},
	title = {Walking into Images: Virtual Plane Mosaics for Plenoptic Modeling},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352480}}

@inproceedings{4161023,
	abstract = {We introduce a new concept for improved interaction with complex scenes: multi-frame rate rendering and display. Multi-frame rate rendering produces a multi-frame rate display by optically or digitally compositing the results of asynchronously running image generators. Interactive parts of a scene are rendered at the highest possible frame rates while the rest of the scene is rendered at regular frame rates. The composition of image components generated with different update rates may cause certain visual artifacts, which can be partially overcome with our rendering techniques. The results of a user study confirm that multi-frame rate rendering can significantly improve the interaction performance while slight visual artifacts are either not even recognized or gladly tolerated by users. Overall, digital composition shows the most promising results, since it introduces the least artifacts while requiring the transfer of frame buffer content between different image generators},
	author = {Springer, Jan P. and Beck, Stephan and Weiszig, Felix and Reiners, Dirk and Froehlich, Bernd},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352481},
	issn = {2375-5334},
	keywords = {Layout;Rendering (computer graphics);Image generation;Computer graphics;Optical buffering;Control systems;Head;Three dimensional displays;Computer displays;Visualization;Multi-Frame Rate Rendering;Multi-Frame Rate Display;3D Interaction;Projection-based Display Systems;I.3.3 [Computer Graphics]: Picture/Image Generation-Display Algorithms;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques;I.3.7 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {195-202},
	title = {Multi-Frame Rate Rendering and Display},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352481}}

@inproceedings{4161024,
	abstract = {A shell map (Porumbescu et al., 2005) is a bijective mapping between shell space (the space between a base surface and its offset) and texture space. It can be used to generate small-scale features on surfaces using a variety of modeling techniques. In this paper, we present an efficient algorithm, which reduces distortion by construction, for the offset surface generation of triangular meshes. The basic idea is to independently offset each triangle of the base mesh, and then stitch them up by solving a Poisson equation. We then introduce the details for computation of a stretch metric, which measures the distortion of shell maps. Our results show a substantial improvement compared to previous results},
	author = {Ye, Kai and Zhou, Kun and Pan, Zhigeng and Tong, Yiying and Guo, Baining},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352482},
	issn = {2375-5334},
	keywords = {Surface texture;Asia;Poisson equations;Distortion measurement;Mesh generation;Computer graphics;Computer errors;Error correction;Shape;Minimization methods;Shell Maps;Offset Surface;Volumetric Texture;Geometric Texture Mapping},
	month = {March},
	pages = {203-208},
	title = {Low Distortion Shell Map Generation},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352482}}

@inproceedings{4161025,
	abstract = {We create a framework for physical interaction with virtual human. The virtual human reacts to the input of the user in realtime. The virtual human has an ocular system model, which mimics human ocular system, to generate natural and human-like gaze motions and to create inputs for the cognitive model of the virtual human. The visual input for virtual human is generated from the dynamics simulator for the virtual world. The gaze target is chosen by vying of bottom-up and top-down attentions. The motion controller for the eyes and head is based on observations and models of human gaze motion. We create a virtual boxing game as an application of proposed framework. Proposed framework succeeds to create natural gaze reaction. In addition, by changing the balance of the bottom-up and top-down attention, looks of concentration of the virtual human can be tuned.},
	author = {Mitake, Hironori and Hasegawa, Shoichi and Koike, Yasuharu and Sato, Makoto},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352483},
	issn = {2375-5334},
	keywords = {Humans;Eyes;Computer graphics;Databases;Computational modeling;Virtual reality;Haptic interfaces;Motion control;Application software;Joining processes;Gaze;ocular system;attention;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques;I.2.0 [Artifical Intelligence]: Cognitive simulation;I.3.7 [Computer Graphics]: Three-dimensional Graphics and Realism-Virtual Reality},
	month = {March},
	pages = {211-214},
	title = {Reactive Virtual Human with Bottom-up and Top-down Visual Attention for Gaze Generation in Realtime Interactions},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352483}}

@inproceedings{4161026,
	abstract = {We investigated the effects of using immersive virtual humans to teach users social conversational verbal and non-verbal protocols in south Indian culture. The study was conducted using a between-subjects experimental design, and compared instruction and interactive feedback from immersive virtual humans against instruction based on a written study guide with illustrations of the social protocols. Participants were then tested on how well they learned the social conversational protocols by exercising the social conventions in front of videos of real people. The results of our study suggest that participants who trained with the virtual humans performed significantly better than the participants who studied from literature.},
	author = {Babu, Sabarish and Suma, Evan and Barnes, Tiffany and Hodges, Larry F.},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352484},
	issn = {2375-5334},
	keywords = {Humans;Protocols;Feedback;Videos;Computer science education;Virtual environment;Speech;Avatars;Computer science;Design for experiments;Virtual Characters;Embodied Agents;Multimodal Interaction;Human-Computer Interaction;Immersive Virtual Environments},
	month = {March},
	pages = {215-218},
	title = {Can Immersive Virtual Humans Teach Social Conversational Protocols?},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352484}}

@inproceedings{4161027,
	abstract = {This paper presents a first prototype of an interactive 3D reconstruction system for modeling urban scenes. An augmented reality scout is a person who is equipped with an ultra-mobile PC, an attached USB camera and a GPS receiver. The scout is exploring the urban environment and delivers a sequence of 2D images. These images are annotated with according GPS data and used iteratively as input for a 3D reconstruction engine which generates the 3D models on-the-fly. This turns modeling into an interactive and collaborative task},
	author = {Reitinger, Bernhard and Zach, Christopher and Schmalstieg, Dieter},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352485},
	issn = {2375-5334},
	keywords = {Augmented reality;Image reconstruction;Layout;Image databases;Engines;Visual databases;Cameras;Global Positioning System;Data visualization;Feedback;scouting;interactive 3d reconstruction;urban planning},
	month = {March},
	pages = {219-222},
	title = {Augmented Reality Scouting for Interactive 3D Reconstruction},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352485}}

@inproceedings{4161028,
	abstract = {We present a simple and efficient approach to turn laser-scanned human geometry into a realistically moving virtual avatar. Instead of relying on the classical skeleton-based animation pipeline, our method uses a mesh-based Laplacian editing scheme to drive the motion of the scanned model. Our framework elegantly solves the motion retargeting problem and produces realistic non-rigid surface deformation with minimal user interaction. Realistic animations can easily be generated from a variety of input motion descriptions, which we exemplify by applying our method to both marker-free and marker-based motion capture data},
	author = {de Aguiar, Edilson and Theobalt, Christian and Stoll, Carsten and Seidel, Hans-Peter},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352486},
	issn = {2375-5334},
	keywords = {Animation;Humans;Skeleton;Biological system modeling;Laplace equations;Shape;Geometrical optics;Pipelines;Virtual reality;Deformable models;Animation;virtual reality;motion capture;shape deformation;I.3.7 [Computer Graphics]: Graphics and Realism-Animation;Virtual Reality;I.4.8 [Image Processing and Computer Vision]: Scene Analysis-Motion;Tracking},
	month = {March},
	pages = {223-226},
	title = {Rapid Animation of Laser-scanned Humans},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352486}}

@inproceedings{4161029,
	abstract = {The urban environment is defined by the interaction between the city morphological characteristics and its physical behavior based on the human interpretation. Contemporary psychology emphasizes the relation between perception and action and shows that perception can be enhanced by the receptor's motion. Basing our work on these principles, we assume that virtual reality techniques, bringing both immersion and interaction to static computer generated images can be of use to study solar effects and therefore daylight ambience. In this paper, we show the methodology for solar effects analysis in both real and virtual world and build demonstrators to compare the emergence of solar effects},
	author = {Tahrani, Souha and Moreau, Guillaume},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352487},
	issn = {2375-5334},
	keywords = {Daylighting;Legged locomotion;Cities and towns;Visual perception;Humans;Psychology;Virtual reality;Virtual environment;Shape;Image generation;solar effects;urban planning;navigation in virtual environment;methodology;visual perception;J.5 [Arts and humanities]: Architecture H.5.1 [Multimedia Information Systems]: Artificial, augmented and virtual realities},
	month = {March},
	pages = {227-230},
	title = {Analyzing urban daylighting ambiences by walking in a virtual city},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352487}}

@inproceedings{4161030,
	abstract = {In this study, we discuss a display table suitable for collaborative work environments for medical use. Using an interactive stereoscopic display system allows simultaneous observation of accurate stereoscopic images generated from volume data. We further investigate all requirements for design guidelines of the display system, including hardware configuration, rendering software to generate the stereoscopic images, and the interface system to operate the displayed images},
	author = {Kitamura, Yoshifumi and Nakashima, Takashi and Tanaka, Keisuke and Johkoh, Takeshi},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352488},
	issn = {2375-5334},
	keywords = {Medical services;Biomedical equipment;Biomedical imaging;Surgery;Medical diagnostic imaging;Liquid crystal displays;Collaborative work;Rendering (computer graphics);Magnetic resonance imaging;Medical simulation;B.4.2 [Input/Output and Data Communications]: Input/Output Devices;I.3.7 [Computer Graphics]: Three-Dimensional Graphics;Realism;stereoscopic display;3D user interface;3D interaction;multiple users;collaborative work;volume visualization},
	month = {March},
	pages = {231-234},
	title = {The IllusionHole for Medical Applications},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352488}}

@inproceedings{4161031,
	abstract = {We present Muddleware, a communication platform designed for mixed reality multi-user games for mobile, lightweight clients. An approach inspired by Tuplespaces, which provides decoupling of sender and receiver is used to address the requirements of a potentially large number of mobile clients. A hierarchical database built on XML technology allows convenient prototyping and simple, yet powerful queries. Server side-extensions address persistence and autonomous behaviors through hierarchical state machines. The architecture has been tested with a number of multi-user games and is also used for non-entertainment applications},
	author = {Wagner, Daniel and Schmalstieg, Dieter},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352489},
	issn = {2375-5334},
	keywords = {Prototypes;Virtual reality;XML;Mobile communication;Computer architecture;Handheld computers;Databases;Network servers;Middleware;Computer interfaces;mixed reality games;client-server architectures;middleware;C.2.4 [Computer Communication Networks]: Distributed Systems - Client/server;H.5.1 [Information Interfaces and Presentation]: Multimedia Information Systems - Artificial, augmented, virtual realities},
	month = {March},
	pages = {235-238},
	title = {Muddleware for Prototyping Mixed Reality Multiuser Games},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352489}}

@inproceedings{4161032,
	abstract = {The development of domain-specific virtual reality applications is often a slow and laborious process. The integration of the domain-specific functionality in an interactive Virtual Environment requires close collaboration between domain expert and VR developer, as well as the integration of domain-specific data and software in a VR application. The software environment needs to support the entire development process and software life cycle, from the early stages of iterative, rapid prototyping to a final end-user application. In this paper, we propose the use of flexible abstraction layers in the form of a dynamic scripting language, which act as the glue between VR system components and external software libraries and applications. First, we discuss the motivation and potential of our approach, after which we overview related approaches. Then, we describe the integration of a Python interpreter in our VR toolkit. The potential of our integration approach is demonstrated by rapid prototyping features, the flexible extension of core functionality and the integration of an external toolkit. We conclude with an overview of implications our approach has for the future development of new framework features and application integration},
	author = {de Haan, Gerwin and Koutek, Michal and Post, Frits H.},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352490},
	issn = {2375-5334},
	keywords = {Virtual reality;Application software;Software prototyping;Software libraries;Virtual environment;Collaborative software;Prototypes;Software architecture;Debugging;Collaboration;Virtual Reality;Application Development;Scripting Languages;I.3.7 [Computing Methodologies]: Computer Graphics-Virtual Reality;D.2.6 [Software]: Software Engineering-Programming Environments},
	month = {March},
	pages = {239-242},
	title = {Flexible Abstraction Layers for VR Application Development},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352490}}

@inproceedings{4161033,
	abstract = {Wildfires are a frequent summer-time concern for land managers and communities neighboring wildlands throughout the world. Computational simulations have been developed to help analyze and predict wildfire behavior, but the primary visualization of these simulations has been limited to 2-dimensional graphics images. We are currently working with wildfire research groups and those responsible for managing the control of fire and mitigation of the wildfire hazard to develop an immersive visualization and simulation application. In our visualization application, the fire spread model will be graphically illustrated on a realistically rendered terrain created from actual DEM data and satellite photography. We are working to improve and benefit tactical and strategic planning, and provide training for firefighter and public safety with our application},
	author = {Sherman, William R. and Penick, Michael A. and Su, Simon and Brown, Timothy J. and Harris, Frederick C.},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352491},
	issn = {2375-5334},
	keywords = {Computational modeling;Analytical models;Predictive models;Fires;Data visualization;Image analysis;Graphics;Hazards;Rendering (computer graphics);Satellites;Applied Virtual Reality;Physically Based Simulations;Wild-fire Visualization;Scene-Graph},
	month = {March},
	pages = {243-246},
	title = {VRFire: an Immersive Visualization Experience for Wildfire Spread Analysis},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352491}}

@inproceedings{4161034,
	abstract = {This paper describes real-time volumetric haptic and visual algorithms developed to simulate burrhole creation for a virtual reality-based craniotomy surgical simulator. A modified Voxmap point-shell algorithm (McNeely et al., 1999), (Renz et al., 2001) is created to simulate haptic interactions between bone cutting tools and voxel-based bone. New surface boundary detection and force feedback calculation methods help reduce "force discontinuities" of the original Voxmap point-shell algorithm. To maintain stable haptic update rates, new forces are calculated outside the haptics rendering loop. A multi-rate haptic solution (Cavusoglu and Tendick, 2000) is used to introduce calculated forces into the haptics loop and to interpolate forces between updates. A bone erosion method is also created to simulate bone drilling capabilities of different tools. 3D texture-based volume rendering is used to display the bone and to visually remove bone material due to drilling in real-time. Volumetric shading is computed by the GPU of the video card. The algorithms described make it possible to simulate several tools typically used for a craniotomy. Realistic 3D models are also created from real surgical tools and controlled by the haptic device},
	author = {Acosta, Eric and Liu, Alan},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352492},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Bones;Drilling;Medical simulation;Surgery;Rendering (computer graphics);Force feedback;Computational modeling;Brain modeling;Computer graphics;Volumetric haptics;volume rendering;surgical simulation;bone drilling;I.3.6 [Computer Graphics]: Three-Dimensional Graphics and Realism-Virtual reality;I.3.6 [Computer Graphics]: Methodology and Techniques-Interaction Techniques},
	month = {March},
	pages = {247-250},
	title = {Real-time Volumetric Haptic and Visual Burrhole Simulation},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352492}}

@inproceedings{4161035,
	abstract = {A magic mirror paradigm is an augmented reality (AR) system where a camera and display device act as a mirror where one can see a reflection of oneself and virtual objects together. Fiducial markers mounted on a number of hand held and wearable objects allow them to be recognized by computer vision, different virtual objects can be rendered relative to the objects depending on the chosen theme. The experience can be enjoyed by many onlookers without special equipment, unlike other AR experiences such as with HMD's or tablet PC's. A series of theoretical and practical problems were overcome to produce a working system suitable for educational and entertainment for the public},
	author = {Fiala, Mark},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352493},
	issn = {2375-5334},
	keywords = {Mirrors;Cameras;Augmented reality;Reflection;Computer vision;Computer graphics;Real time systems;Magnetic sensors;Wearable sensors;Robustness;Magic Mirror;ARTag;Augmented Reality},
	month = {March},
	pages = {251-254},
	title = {Magic Mirror System with Hand-held and Wearable Augmentations},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352493}}

@inproceedings{4161036,
	abstract = {Augmented virtual environments (AVE) are very effective in the application of surveillance, in which multiple video streams are projected onto a 3D urban model for better visualization and comprehension of the dynamic scenes. One of the key issues in creating such systems is to estimate the parameters of each camera including the intrinsic parameters and its pose relative to the 3D model. Existing camera pose estimation approaches require known intrinsic parameters and at least three 2D to 3D feature (point or line) correspondences. This cannot always be satisfied in an AVE system. Moreover, due to noise, the estimated camera location may be far from the expectation of the users when the number of correspondences is small. Our approach combines the users' prior knowledge about the camera location and the constraints from the parallel relationship between lines with those from feature correspondences. With at least two feature correspondences, it can always output an estimation of the camera parameters that gives an accurate alignment between the projection of the image (or video) and the 3D model},
	author = {Lu Wang and You, Suya and Neumann, Ulrich},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352494},
	issn = {2375-5334},
	keywords = {Cameras;Calibration;Virtual environment;Layout;Surveillance;Urban planning;Visualization;Augmented reality;Matrix decomposition;Electronic mail;Camera calibration;pose estimation;augmented virtual environment},
	month = {March},
	pages = {255-258},
	title = {Single View Camera Calibration for Augmented Virtual Environments},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352494}}

@inproceedings{4161037,
	abstract = {A Nested Marker, a novel visual marker for camera calibration in augmented reality (AR), enables accurate calibration even when the observer is moving very close to or far away from the marker. Our proposed Nested Marker has a recursive layered structure. One marker at an upper layer contains four smaller markers at the lower layer. Smaller markers can also have lower-layer markers nesting inside them. Each marker can be identified by its inside pattern, so the system can select a proper calibration parameter set for the marker. When the observer views the marker close-up, the lowest layer marker will work. When the observer views the marker from a distance, the top-layer marker will work. It is also possible to simultaneously utilize all visible markers in different layers for more stable calibration. Note that Nested Marker can be used in a standard ARToolkit framework. We have also developed an AR system to demonstrate the ability of Nested Marker},
	author = {Tateno, Keisuke and Kitahara, Itaru and Ohta, Yuichi},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352495},
	issn = {2375-5334},
	keywords = {Augmented reality;Cameras;Calibration;Space technology;Watches;Systems engineering and theory;Intrusion detection;Robustness;Computer vision;Sensor systems},
	month = {March},
	pages = {259-262},
	title = {A Nested Marker for Augmented Reality},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352495}}

@inproceedings{4161038,
	abstract = {End-effector control of robots using just remote camera views is difficult due to lack of perceived correspondence between the joysticks and the end-effector coordinate frame. This paper reports the positive effects of augmented reality visual cues on operator performance during end-effector controlled tele-operation using only camera views. Our solution is to overlay a color-coded coordinate system on the end-effector of the robot using AR techniques. This mapped and color-coded coordinate system is then directly mapped to similarly color-coded joysticks used for control of both position and orientation. The AR view along with mapped markings on the joystick give the user a clear notion of the effect of their joystick movements on the end-effector of the robot. All camera views display this registered dynamic overlay information on-demand. An insertion task was used to compare performance with and without the coordinate mapping using fifteen subjects. Preliminary results indicate a significant reduction in distance and reversal errors},
	author = {Nawab, Aditya and Chintamani, Keshav and Ellis, Darin and Auner, Gregory and Pandya, Abhilash},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352496},
	issn = {2375-5334},
	keywords = {Augmented reality;Robot control;Cameras;Robot kinematics;Robot vision systems;Software testing;Intelligent robots;Displays;Human factors;Navigation;Robotics;Augmented Reality;Tele-operations;Kinematics;Performance Testing},
	month = {March},
	pages = {263-266},
	title = {Joystick mapped Augmented Reality Cues for End-Effector controlled Tele-operated Robots},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352496}}

@inproceedings{4161039,
	abstract = {High-end mobile devices are becoming increasing popular in every day life. Augmented reality (AR) builds on this trend by combining mobile computing with connectivity and location-awareness. In doing so, AR can provide a very rich user experience. In this paper we discuss the approach and development of an AR-based personal assistant, combining the familiar interface of a human person with the functionality of a location-aware digital information system. The paper discusses the main components of the system, including the anthropomorphic user interface as well as the results of an initial prototype evaluation.},
	author = {Schmeil, Andreas and Broll, Wolfgang},
	booktitle = {2007 IEEE Virtual Reality Conference},
	date-added = {2024-03-18 02:29:00 -0400},
	date-modified = {2024-03-18 02:29:00 -0400},
	doi = {10.1109/VR.2007.352497},
	issn = {2375-5334},
	keywords = {User interfaces;Anthropomorphism;Augmented reality;Multimedia databases;Mobile computing;Humans;Mobile handsets;Personal digital assistants;Computer interfaces;Pervasive computing;Augmented Reality;Mobile Computing;Virtual Humans;Anthropomorphic User Interfaces;Digital Assistants;Environment Model;Location Based Systems},
	month = {March},
	pages = {267-270},
	title = {MARA - A Mobile Augmented Reality-Based Virtual Assistant},
	year = {2007},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2007.352497}}

@inproceedings{1667620,
	abstract = {Numerous previous studies have suggested that distances appear to be compressed in immersive virtual environments presented via head mounted display systems, relative to in the real world. However, the principal factors that are responsible for this phenomenon have remained largely unidentified. In this paper we shed some new light on this intriguing problem by reporting the results of two recent experiments in which we assess egocentric distance perception in a high fidelity, low latency, immersive virtual environment that represents an exact virtual replica of the participant's concurrently occupied real environment. Under these novel conditions, we make the startling discovery that distance perception appears not to be significantly compressed in the immersive virtual environment, relative to in the real world.},
	author = {Interrante, V. and Ries, B. and Anderson, L.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.52},
	issn = {2375-5334},
	keywords = {Virtual environment;Computer graphics;Space technology;Process design;Computer science;Computer architecture;Computer displays;Delay;Chromium;Ergonomics;egocentric distance perception;immersive virtual environments},
	month = {March},
	pages = {3-10},
	title = {Distance Perception in Immersive Virtual Environments, Revisited},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.52}}

@inproceedings{1667621,
	abstract = {This paper investigates the use of camera motions, in order to improve the sensation of walking in a Virtual Environment. A simple model of camera motion is first proposed. This model uses: (1) oscillating motions for the position of the camera, and (2) a compensation motion which changes the orientation of the camera and simulate oculomotor compensation to keep a constant focal point when walking. Then we describe two experiments which were conducted to study the characteristics of our model and the preference of the users in terms of sensation of walking. The first experiment compared the use of oscillating camera motions along the three directions of space. The oscillating motions were all preferred to the control condition (i.e. a linear motion, as if the user was driving a car). Furthermore, the participants preferred oscillating motions along the vertical axis, compared with the two other directions of space. The second experiment was focused on the use of a compensation motion. It showed that on average participants preferred a compensated motion during the walk, as compared with a motion with a constant orientation of the camera. These results are consistent with the way our body and eyes move naturally when walking in real life. Taken together, our results suggest that camera motions can considerably improve the sensation of walking in virtual environments. Camera motions could be further introduced in numerous applications of virtual reality in which the simulation of walking is important, such as: architectural visits, training simulations, or videogames.},
	author = {Lecuyer, A. and Burkhardt, J.-M. and Henaff, J.-M. and Donikian, S.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.31},
	issn = {2375-5334},
	keywords = {Cameras;Legged locomotion;Virtual environment;Virtual reality;Information systems;Motion control;Eyes;Chromium;Multimedia systems;User interfaces;camera motion;walking;viewpoint;self-motion;sensation.},
	month = {March},
	pages = {11-18},
	title = {Camera Motions Improve the Sensation of Walking in Virtual Environments},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.31}}

@inproceedings{1667622,
	abstract = {A fundamental problem in optical, see-through augmented reality (AR) is characterizing how it affects the perception of spatial layout and depth. This problem is important because AR system developers need to both place graphics in arbitrary spatial relationships with real-world objects, and to know that users will perceive them in the same relationships. Furthermore, AR makes possible enhanced perceptual techniques that have no real-world equivalent, such as x-ray vision, where AR users are supposed to perceive graphics as being located behind opaque surfaces. This paper reviews and discusses techniques for measuring egocentric depth judgments in both virtual and augmented environments. It then describes a perceptual matching task and experimental design for measuring egocentric AR depth judgments at medium- and far-field distances of 5 to 45 meters. The experiment studied the effect of field of view, the x-ray vision condition, multiple distances, and practice on the task. The paper relates some of the findings to the well-known problem of depth underestimation in virtual environments, and further reports evidence for a switch in bias, from underestimating to overestimating the distance of AR-presented graphics, at 23 meters. It also gives a quantification of how much more difficult the x-ray vision condition makes the task, and then concludes with ideas for improving the experimental methodology.},
	author = {Swan, J.E. and Livingston, M.A. and Smallman, H.S. and Brown, D. and Baillot, Y. and Gabbard, J.L. and Hix, D.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.13},
	issn = {2375-5334},
	keywords = {Augmented reality;Displays;Layout;Virtual reality;Humans;Graphics;Visual system;Hardware;Eyes;Retina;Experimentation;Measurement;Performance;Depth Perception;Optical See-Through Augmented Reality},
	month = {March},
	pages = {19-26},
	title = {A Perceptual Matching Technique for Depth Judgments in Optical, See-Through Augmented Reality},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.13}}

@inproceedings{1667623,
	abstract = {We present a data-driven method for interactive simulation and visualization of fibrin fibers, a major component of blood clotting. A fibrin fiber is a complex system consisting of a hierarchy with at least three separate levels of detail. Using measurements acquired with an atomic force microscope (AFM) at the smallest scale in this hierarchy, a physically-based model for the larger scales can be constructed and then simulated. Unlike most traditional work dealing with Monte Carlo (MC) or Molecular Dynamics (MD) simulations, our method makes simplifying assumptions about the simulation and enables interactive visualization of simulated fibers in a virtual environment.},
	author = {Schoner, J.L. and Falvo, M.R. and Lord, S.T. and Taylor, R.M. and Lin, M.C.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.90},
	issn = {2375-5334},
	keywords = {Computational modeling;Coagulation;Mechanical factors;Medical simulation;Force measurement;Predictive models;Blood;Solid modeling;Virtual reality;Extraterrestrial measurements;physically-based simulation;medicine},
	month = {March},
	pages = {27-34},
	title = {Interactive Simulation of Fibrin Fibers in Virtual Environments},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.90}}

@inproceedings{1667624,
	abstract = {A central training objective of virtual reality based surgical simulation is the removal of pathologic tissue. This necessitates stable, real-time updates of the underlying mesh representation. Within the framework of a hysteroscopy simulator, we have developed a hybrid cutting approach for tetrahedral meshes. It combines the topological update by subdivision with adjustments of the existing topology. Moreover, the mechanical and the visual model are decoupled, thus allowing different resolutions for the underlying mesh representations. With our method, we can closely approximate an arbitrary, user-defined cut surface while avoiding the creation of small or badly shaped elements, thus strongly reducing stability problems in the subsequent deformation computation. The presented approach has been integrated into a virtual reality training system for hysteroscopic interventions. The performance of the algorithm is demonstrated by examples of intra-uterine tumor ablations.},
	author = {Steinemann, D. and Harders, M. and Gross, M. and Szekely, G.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.74},
	issn = {2375-5334},
	keywords = {Solids;Computational modeling;Virtual reality;Deformable models;Laboratories;Surgery;Electrodes;Cutting tools;Pathology;Computer graphics;tetrahedral meshes;cutting;surgical simulation;massspring models},
	month = {March},
	pages = {35-42},
	title = {Hybrid Cutting of Deformable Solids},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.74}}

@inproceedings{1667625,
	abstract = {A great challenge in information visualization today is to provide models and software that effectively integrate the graphics content of scenes with domain-specific knowledge so that the users can effectively query, interpret, personalize and manipulate the visualized information [1]. Moreover, it is important that the intelligent visualization applications are interoperable in the semantic web environment and thus, require that the models and software supporting them integrate state-of-the-art international standards for knowledge representation, graphics and multimedia. In this paper, we present a model, a methodology and a software framework for the semantic web (Intelligent 3D Visualization Platform - I3DVP) for the development of interoperable intelligent visualization applications that support the coupling of graphics and virtual reality scenes with domain knowledge of different domains. The graphics content and the semantics of the scenes are married into a consistent and cohesive ontological model while at the same time knowledge- based techniques for the querying, manipulation, and semantic personalization of the scenes are introduced. We also provide methods for knowledge driven information visualization and visualization- aided decision making based on inference by reasoning.},
	author = {Kalogerakis, E. and Christodoulakis, S. and Moumoutzis, N.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.41},
	issn = {2375-5334},
	keywords = {Ontologies;Graphics;Visualization;Layout;Application software;Semantic Web;Software standards;Knowledge representation;Virtual reality;Decision making;ontologies;web graphics;semantic driven visualization;intelligent virtual environments;domain knowledge},
	month = {March},
	pages = {43-50},
	title = {Coupling Ontologies with Graphics Content for Knowledge Driven Visualization},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.41}}

@inproceedings{1667626,
	abstract = {Our work introduces immersive collaborative learning to geometry education. More specifically, we present a system that uses collaborative augmented reality as a medium for teaching, and uses 3D dynamic geometry to facilitate mathematics and geometry education. Both these aspects are novel to geometry education. We describe improvements in the user interface and visual design of such an application. We also report on practical experiences with using our system for actual teaching of high school students, and present initial quantitative data on the educational value of such an approach.},
	author = {Kaufmann, H. and Schmalstieg, D.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.48},
	issn = {2375-5334},
	keywords = {Virtual reality;Computer science education;User interfaces;Computational geometry;Collaborative work;Augmented reality;Educational technology;Computer interfaces;Information geometry;Mathematics;Geometry education;computer supported collaborative work;augmented reality;user interface design},
	month = {March},
	pages = {51-58},
	title = {Designing Immersive Virtual Reality for Geometry Education},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.48}}

@inproceedings{1667627,
	abstract = {This paper reports on a study to examine the similarities and differences in experiencing an interpersonal scenario with real and virtual humans. A system that allows medical students to interview a life-size virtual patient using natural speech and gestures was used as a platform for this comparison. Study participants interviewed either a virtual patient or a standardized patient, an actor trained to represent a medical condition. Subtle yet substantial differences were found in the participants' rapport with the patient and the flow of the conversation. The virtual patient's limited expressiveness was a significant source of these differences. However, overall task performance was similar, as were perceptions of the educational value of the interaction.},
	author = {Raij, A. and Johnsen, K. and Dickerson, R. and Lok, B. and Cohen, M. and Bernard, T. and Oxendine, C. and Wagner, P. and Lind, D.S.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.91},
	issn = {2375-5334},
	keywords = {Humans;Medical diagnostic imaging;Natural languages;Computer science education;Speech recognition;Virtual reality;Information science;Biomedical engineering;Hospitals;Educational institutions;Virtual Characters;Multimodal Interaction;Human-Computer Interaction;Medical Education;Immersive Virtual Environments},
	month = {March},
	pages = {59-66},
	title = {Interpersonal Scenarios: Virtual \approx Real?},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.91}}

@inproceedings{1667628,
	abstract = {Post Traumatic Stress Disorder (PTSD) is reported to be caused by traumatic events that are outside the range of usual human experiences including (but not limited to) military combat, violent personal assault, being kidnapped or taken hostage and terrorist attacks. Initial data suggests that 1 out of 6 Iraq War veterans are exhibiting symptoms of depression, anxiety and PTSD. Virtual Reality (VR) exposure treatment has been used in previous treatments of PTSD patients with reports of positive outcomes. The aim of the current paper is to present the rationale, technical specifications, application features and user-centered design process for the development of a Virtual Iraq PTSD VR therapy application. The VR treatment environment is being created via the recycling of virtual graphic assets that were initially built for the U.S. Army-funded combat tactical simulation scenario and commercially successful X-Box game, Full Spectrum Warrior, in addition to other available and newly created assets. Thus far we have created a series of customizable virtual scenarios designed to represent relevant contexts for exposure therapy to be conducted in VR, including a city and desert road convoy environment. User-Centered tests with the application are currently underway at the Naval Medical Center-San Diego and within an Army Combat Stress Control Team in Iraq with clinical trials scheduled to commence in February 2006.},
	author = {Pair, J. and Allen, B. and Dautricourt, M. and Treskunov, A. and Liewer, M. and Graap, K. and Reger, G.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.23},
	issn = {2375-5334},
	keywords = {Virtual reality;Medical treatment;Stress;Humans;Terrorism;User centered design;Recycling;Graphics;Cities and towns;Roads;Virtual Reality;Post Traumatic Stress Disorder;Exposure Therapy;Full Spectrum Warrior;Clinical Interface},
	month = {March},
	pages = {67-72},
	title = {A Virtual Reality Exposure Therapy Application for Iraq War Post Traumatic Stress Disorder},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.23}}

@inproceedings{1667629,
	abstract = {A collaborative design environment (CDE) is a shared environment over some communication networks. It allows remote users to interact with each other to perform some design tasks. To support collaborative design, the human hand is a very natural and convenient tool as it is capable of accomplishing diverse tasks such as pointing, gripping, twisting and tearing. However, there is not much work that considers using the human hand as input in CDEs, in particular over the Internet. The main reasons for this are that the Internet suffers from high network latency, which affects the interaction, and the human hand has many degrees of freedom, which adds additional challenges to synchronizing the collaboration. In this paper, we propose a prediction method specifically designed for human hand motion to address the network latency. Through a thorough analysis of human finger motion, we have identified various finger motion constraints. By considering these motion constraints, we propose a constraint-based motion prediction method for hand motion. We present a number of experiments to demonstrate the performance of our prediction method and the dead reckoning algorithm based on the proposed predictor.},
	author = {Chan, A. and Lau, R.W.H. and Li, L.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.101},
	issn = {2375-5334},
	keywords = {Collaboration;Humans;Prediction methods;Collaborative work;Delay;Fingers;Motion analysis;Communication networks;Collaborative tools;Internet;motion prediction;gesture-based application;network latency},
	month = {March},
	pages = {73-80},
	title = {Motion Prediction in Gesture-based Collaborative Design Environments},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.101}}

@inproceedings{1667630,
	abstract = {Complex business processes and globally distributed teams require new means to support net-based teamwork. Here, virtual reality technologies offer new ways to a more intuitive collaboration. In this paper, a system setup dubbed HoloPort is presented, which enables intuitive point-to-point video conferencing between two small groups. The device features gaze awareness between local and remote conferees during video and data conferences, and pen-based on-screen interaction. After an introduction to tele-collaboration systems, the working principle of the proposed HoloPort is illustrated, and the device's individual components are described in detail. Then, different application scenarios using the HoloPort are discussed. Finally, the paper concludes with an outlook on future improvements.},
	author = {Kuechler, M. and Kunz, A.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.71},
	issn = {2375-5334},
	keywords = {Videoconference;Collaborative work;Cameras;Virtual reality;Computer interfaces;Computer networks;Remote monitoring;Displays;Technological innovation;Teamwork;Video and data conferencing;gaze awareness;computer supported cooperative work;networked collaboration;teleimmersion},
	month = {March},
	pages = {81-88},
	title = {HoloPort - A Device for Simultaneous Video and Data Conferencing Featuring Gaze Awareness},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.71}}

@inproceedings{1667631,
	abstract = {We present a novel software framework for developing highly animated virtual reality applications. Using a modular application design, our goal is to alleviate software engineering issues while yielding efficient execution on parallel machines. We target worlds involving numerous animated objects managed by physical based simulations. Mixing rigid objects, fluids, mass-spring or other deformable objects leads to complex interactions between them. Today no unified simulation algorithm with a reasonable complexity is available to manage all these types of objects. We propose a framework for coupling and distributing existing algorithms. We reuse and extend the data-flow model where an application is built from modules exchanging data through connections. The model relies on two main classes of modules, animators and interactors. Animators are responsible for updating objects' states from forces applied to them. These forces are computed in parallel by interactors using the objects' states they receive from animators. The network interconnecting modules can be progressively optimized. From a simple fully connected network enforcing a synchronous semantics, it can evolve towards an active network able to implement a bounding volume based dynamic routing or an asynchronous data re-sampling. As a result, we present an application managing interactions between rigid objects, mass-spring objects and a fluid. It is executed in real-time on a 54 processors cluster driving 5 cameras and 16 projectors for user interactions.},
	author = {Allard, J. and Raffin, B.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.53},
	issn = {2375-5334},
	keywords = {Virtual reality;Animation;Graphics;Application software;Software engineering;Cameras;Deformable models;Biological system modeling;Parallel machines;Fluid dynamics;Virtual Reality;Animations;Interactions;Physical based Simulation Coupling;Distributed and Parallel Graphics Applications;Modular Programming;Graphics Clusters},
	month = {March},
	pages = {89-96},
	title = {Distributed Physical Based Simulations for Large VR Applications},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.53}}

@inproceedings{1667632,
	abstract = {The experience of motion sickness in a virtual environment may be measured through pre- and post-experiment self-reported questionnaires such as the Simulator Sickness Questionnaire (SSQ). Although research provides converging evidence that users of virtual environments can experience motion sickness, there have been no controlled studies to determine to what extent the user's subjective response is a demand characteristic resulting from pre- and post-test measures. In this study, subjects were given either SSQ's both pre and post virtual environment immersion, or only post immersion. This technique was used to test for contrast effects due to demand characteristics in which administration of the questionnaire itself suggests to the participant that the virtual environment may produce motion sickness. Results indicate that reports of motion sickness after immersion in a virtual environment are much greater when both pre and post questionnaires are given than when only a post test questionnaire is used. The implications for assessments of motion sickness in virtual environments are discussed.},
	author = {Young, S.D. and Adelstein, B.D. and Ellis, S.R.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.44},
	issn = {2375-5334},
	keywords = {Virtual environment;Testing;Computer displays;Motion measurement;Horses;NASA;Motion control;Chromium;Human factors;Tracking;manual control;three dimensional tracking;placebo effect;virtual environment},
	month = {March},
	pages = {97-102},
	title = {Demand Characteristics of a Questionnaire Used to Assess Motion Sickness in a Virtual Environment},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.44}}

@inproceedings{1667633,
	abstract = {Most current VE applications make use of well-known 3D interaction techniques, such as ray-casting or the Go-Go technique. Such techniques, however, were designed generically, without considering the context of use. We argue that the design of 3D interaction techniques should take the application domain into account. In our work, we have developed interfaces for a VE application for structural engineering, including interfaces for cloning, a domain-specific task. We designed and compared six cloning interfaces. The techniques designed using domain characteristics resulted in significant performance gains and a better workflow. In addition, those techniques designed from scratch using domain characteristics were generally superior to generic techniques that were modified to consider domain characteristics. Finally, we found that the transition time between actions in these complex interfaces was a major bottleneck for task performance.},
	author = {Jian Chen and Bowman, D.A.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.57},
	issn = {2375-5334},
	keywords = {Cloning;Virtual environment;Application software;Computer graphics;Structural engineering;Usability;Computer science;Human computer interaction;Performance gain;Chromium;Domain-specific interaction;virtualv environments;structural engineering.},
	month = {March},
	pages = {103-110},
	title = {Effectiveness of Cloning Techniques for Architectural Virtual Environments},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.57}}

@inproceedings{1667634,
	abstract = {Virtual Reality (VR) is increasingly being used in industry, medicine, entertainment, education, and research. It is generally critical that the VR setups produce behavior that closely resembles real world behavior. One part of any task is the ability to control our posture. Since postural control is well studied in the real world and is known to be strongly influenced by visual information, it is an ideal metric for examining the behavioral fidelity of VR setups. Moreover, VR-based experiments on postural control can provide fundamental new insights into human perception and cognition. Here, we employ the "swinging room paradigm" to validate a specific VR setup. Furthermore, we systematically examined a larger range of room oscillations than previously studied in any single setup. We also introduce several new methods and analyses that were specifically designed to optimize the detection of synchronous swinging between the observer and the virtual room. The results show that the VR setup has a very high behavioral fidelity and that increases in swinging room amplitude continue to produce increases in body sway even at very large room displacements (+/- 80 cm). Finally, the combination of new methods proved to be a very robust, reliable, and sensitive way of measuring body sway.},
	author = {Cunningham, D.W. and Nusseck, H.-G. and Teufel, H. and Wallraven, C. and Bulthoff, H.H.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.14},
	issn = {2375-5334},
	keywords = {Psychology;Virtual reality;Computer graphics;Humans;Cognition;Robust stability;Application software;Cybernetics;Design optimization;Chromium},
	month = {March},
	pages = {111-118},
	title = {A Psychophysical Examination of Swinging Rooms, Cylindrical Virtual Reality Setups, and Characteristic Trajectories},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.14}}

@inproceedings{1667635,
	abstract = {Anyone who has gazed through the eyepiece of an astronomical telescope knows that, with the exception of the Moon and the planets, extra-solar astronomical objects are disappointing to observe visually. This is mainly due to their low surface brightness, but also depends on the visibility, sky brightness and telescope aperture. We propose a system which projects images of astronomical objects (with focus on nebulae and galaxies), animations and additional information directly into the eyepiece view of an astronomical telescope. As the telescope orientation is queried continuously, the projected image is adapted in real-time to the currently visible field of view. For projection, a custom-built video projection module with high contrast and low maximum luminance value was developed. With this technology visitors to public observatories have the option to experience the richness of faint astronomical objects while directly looking at them through a telescope.},
	author = {Lintu, A. and Magnor, M.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.24},
	issn = {2375-5334},
	keywords = {Augmented reality;Telescopes;Planets;Astronomy;Moon;Brightness;Apertures;Animation;Humans;Charge coupled devices;augmented reality;projection;tracking;astronomy},
	month = {March},
	pages = {119-126},
	title = {An Augmented Reality System for Astronomical Observations},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.24}}

@inproceedings{1667636,
	abstract = {Given two video sequences of different scenes acquired with moving cameras, it is interesting to seamlessly transfer a 3D object from one sequence to the other. In this paper, we present a video-based approach to extract the alpha mattes of rigid or approximately rigid 3D objects from one or more source videos, and then geometrycorrectly transfer them into another target video of a different scene. Our framework builds upon techniques in camera pose estimation, 3D spatiotemporal video alignment, depth recovery, key-frame editing, natural video matting, and image-based rendering. Based on the explicit camera pose estimation, the camera trajectories of the source and target videos are aligned in 3D space. Combinied with the estimated dense depth information, this allows us to significantly relieve the burdens of key-frame editing and efficiently improve the quality of video matting. During the transfer, our approach not only correctly restores the geometric deformation of the 3D object due to the different camera trajectories, but also effectively retains the soft shadow and environmental lighting properties of the object to ensure that the augmenting object is in harmony with the target scene.},
	author = {Jiangjian Xiao and Xiaochun Cao and Foroosh, H.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.3},
	issn = {2375-5334},
	keywords = {Cameras;Layout;Trajectory;Video sequences;Rendering (computer graphics);Computer graphics;Computer vision;Virtual reality;Spatiotemporal phenomena;Image restoration},
	month = {March},
	pages = {127-134},
	title = {3D Object Transfer Between Non-Overlapping Videos},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.3}}

@inproceedings{1667637,
	abstract = {In this paper, a novel model-based optical tracking and model estimation system for composite interaction devices is presented. Devices consist of a set of linked segments, where each segment can have combinations of translational and rotational degrees of freedom (DOFs) relative to a parent segment. The system automatically constructs the geometric skeleton structure, DOF relations, and DOF constraints between segments. Pre-defined models are not required. The system supports segments with only a single marker, so that interaction devices can be small with a low number of markers. The model is computed in an offline procedure. The tracking method uses the obtained device model to recognize the device and reconstruct all DOF parameters describing the pose of each segment. The tracking method can handle partial occlusion. Results show the proposed techniques are efficient and robust},
	author = {Van Rhijn, A. and Mulder, J.D.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.104},
	issn = {2375-5334},
	keywords = {Optical devices;Optical sensors;Computer graphics;Solid modeling;Cameras;Skeleton;Image processing;Virtual reality;Biological system modeling;Mathematical model;Optical Tracking;Automatic Model Estimation;Skeleton Reconstruction;Virtual Reality;Interaction Devices},
	month = {March},
	pages = {135-142},
	title = {Optical Tracking and Automatic Model Estimation of Composite Interaction Devices},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.104}}

@inproceedings{1667638,
	abstract = {to present a challenge in terms of performance, robustness and accuracy. In particular mobile augmented reality applications are in need of tracking systems which can be wearable and do not cause a high processing load. In this paper we introduce a novel iterative geometric method for pose estimation from four co-planar points and we present the current status of PTrack, a marker-based single camera tracking system benefiting from this approach. The system uses an IDS uEye [1] camera, equipped with infrared flash strobes and infrared pass filter, which acquires grayscale images and sends them to a computer where an image pre-processing algorithm identifies potential projections of retro-reflective markers. Our novel pose estimation algorithm identifies possible labels composed of markers in a 2D post-processing using a divide-and-conquer strategy to segment the camera's image space and attempts an iterative geometric 3D reconstruction of position and orientation in camera space. In the end successfully reconstructed labels are compared to a database for identification. Their 6 DoF data as well as their ID is made available to applications through OpenTracker [2] framework. To assess the performance of our approach we compared PTrack to ARToolKit [3] - which has been adapted to work with the same camera - and final results show that pose estimation is more accurate and precise, in both translation and rotation.},
	author = {Santos, P. and Stork, A. and Buaes, A. and Jorge, J.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.114},
	issn = {2375-5334},
	keywords = {Cameras;Infrared imaging;Iterative algorithms;Image reconstruction;Robustness;Augmented reality;Iterative methods;Intrusion detection;Filters;Gray-scale;Tracking System;Augmented Reality;Virtual Reality},
	month = {March},
	pages = {143-150},
	title = {PTrack: Introducing a Novel Iterative Geometric Pose Estimation for a Marker-based Single Camera Tracking System},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.114}}

@inproceedings{1667639,
	abstract = {We present a real-time reverse radiosity method for compensating indirect scattering effects that occur with immersive and semi-immersive projection displays. It computes a numerical solution directly on the GPU and is implemented with pixel shading and multi-pass rendering which together realizes a Jacobi solver for sparse matrix linear equation systems. Our method is validated and evaluated based on a stereoscopic two-sided wall display. The images appear more brilliant and uniform when compensating the scattering contribution.},
	author = {Bimber, O. and Grundhofer, A. and Zeidler, T. and Danch, D. and Kapakos, P.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.34},
	issn = {2375-5334},
	keywords = {Light scattering;Virtual reality;Computer displays;Rendering (computer graphics);Computer graphics;Three dimensional displays;Jacobian matrices;Sparse matrices;Equations;Visualization;Compensating Indirect Scattering;Interreflections;Projection Displays;Reverse Radiosity;Image;Correction;Real-Time Rendering;GPU Programming.},
	month = {March},
	pages = {151-158},
	title = {Compensating Indirect Scattering for Immersive and Semi-Immersive Projection Displays},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.34}}

@inproceedings{1667640,
	abstract = {Recently, the field of media art has become popular. However, most existing media art works are not suitable to exhibit in a public space. To develop technologies that enable media art works to be exhibited in a public space, we pay attention to the "spatial coexistence between the real and the virtual in public spaces"; the audience in a public space can feel as if the virtual space and real space exist together completely. In this paper, we propose a new concept, named the "controllable particle display", which is to display threedimensional objects by filling space with small particles. On the basis of this concept we developed a prototype system using water drops as particles. In this system, a set of water drops, falling from a tank, are designed to form a plane surface. Patterns of images are projected upward on the falling water drops by a projector under the water drops. Three-dimensional objects can be observed by projecting a set of tomographic images in accordance with the position of the water drops. We also demonstrated the effectiveness of our concept and system.},
	author = {Eitoku, S. and Tanikawa, T. and Suzuki, Y.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.51},
	issn = {2375-5334},
	keywords = {Three dimensional displays;Filling;Space technology;Art;Computer displays;Materials science and technology;Tomography;Prototypes;Virtual reality;Information science;Water Drop;Public Art;Volumetric Display;Threedimensional Scanning Display},
	month = {March},
	pages = {159-166},
	title = {Display Composed of Water Drops for Filling Space with Materialized Virtual Three-dimensional Objects},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.51}}

@inproceedings{1667641,
	abstract = {This paper describes a new interaction technique called haptic hybrid rotations to overcome the physical angular limitations of force-feedback devices when manipulating virtual objects. This technique is based on a hybrid control of the object manipulated with the device. When approaching the angular mechanical stops of the device, the control mode switches from angular position-control to rate-control. The forcefeedback of the device is used to simulate the use of an elastic device in the rate-control mode. An experiment was carried out to compare this technique with two other common alternatives that are used when manipulating virtual objects with force-feedback devices: rotations scaling, and the clutching technique. Our results showed that haptic hybrid rotations were both the fastest and the most appreciated technique for the proposed experiment.},
	author = {Dominjon, L. and Richir, S. and Lecuyer, A. and Burkhardt, J.-M.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.68},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Hardware;Virtual reality;Laboratories;Force control;Switches;Chromium;User interfaces;User centered design;Multimedia systems;interaction technique;hybrid control;position/rate control.},
	month = {March},
	pages = {167-174},
	title = {Haptic Hybrid Rotations: Overcoming Hardware Angular Limitations of Force-Feedback Devices},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.68}}

@inproceedings{1667642,
	abstract = {The design of virtual environments for applications that have several levels of scale has not been deeply addressed. In particular, navigation in such environments is a significant problem. This paper describes the design and evaluation of two navigation techniques for multiscale virtual environments (MSVEs). Issues such as spatial orientation and understanding were addressed in the design process of the navigation techniques. The evaluation of the techniques was done with two experimental and two control groups. The results show that the techniques we designed were significantly better than the control conditions with respect to the time for task completion and accuracy.},
	author = {Kopper, R. and Tao Ni and Bowman, D.A. and Pinho, M.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.47},
	issn = {2375-5334},
	keywords = {Navigation;Virtual environment;Computer graphics;Computer science;Application software;Planets;Lungs;Process design;Chromium;User interfaces;multiscale virtual environments;interaction techniques;levels of scale;navigation;wayfinding aids;usability evaluation.},
	month = {March},
	pages = {175-182},
	title = {Design and Evaluation of Navigation Techniques for Multiscale Virtual Environments},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.47}}

@inproceedings{1667643,
	abstract = {In this paper, we describe a novel voice menu presentation method, the vCocktail, designed for efficient human-computer interaction in wearable computing. This method is devised to reduce the length of the serial presentation of voice menus by introducing spatiotemporal multiplexed voices with enhanced separation cues. Perception error in judging voice direction was measured first to determine appropriate directions in which and interval angles at which menu items were placed allowing the user a clear distinction among the multiple items. Then voice menu items were presented under spatiotemporally multiplexed conditions with several different settings of spatial localization, number of words, and onset interval. The results of the experiments showed that the subjects could hear items very accurately with localization cues and appropriate onset intervals. In addition, the proposed attenuating menu voice and crosstype spatial sequence of presentation increased the correct answer ratio effectively improving distinction between menu items. A correct answer ratio of 99.7 % was achieved in the case of four-item multiplexing when an attenuating voice and a 0.2 sec onset interval were used with the cross-type spatial sequence.},
	author = {Ikei, Y. and Yamazaki, H. and Hirota, K. and Hirose, M.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.141},
	issn = {2375-5334},
	keywords = {Wearable computers;Spatiotemporal phenomena;Information processing;Computer interfaces;High performance computing;Speech synthesis;Speech recognition;Space technology;Design methodology;Chromium;Aural menu interface;Spatiotemporal multiplication;Sound localization;Onset interval;Wearable computing},
	month = {March},
	pages = {183-190},
	title = {vCocktail: Multiplexed-voice Menu Presentation Method for Wearable Computers},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.141}}

@inproceedings{1667644,
	abstract = {This paper describes a generalization of the god-object method for haptic interaction between rigid bodies. Our approach separates the computation of the motion of the six degree-of-freedom god-object from the computation of the force applied to the user. The motion of the god-object is computed using continuous collision detection and constraint-based quasi-statics, which enables high-quality haptic interaction between contacting rigid bodies. The force applied to the user is computed using a novel constraint-based quasi-static approach, which allows us to suppress force artifacts typically found in previous methods. Our approach has been implemented on a 3.2 GHz Xeon bi-processor and has been successfully tested on complex benchmarks. Our results show that the separation into asynchronous processes allows us to satisfy the different update rates required by the haptic and the visual displays. The constraint-based force applied to the user, which handles any number of simultaneous contact points, is typically computed within a few microseconds, while the update of the configuration of the rigid god-object is performed within a few milliseconds for rigid bodies containing up to tens of thousands of triangles.},
	author = {Ortega, M. and Redon, S. and Coquillart, S.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.18},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Displays;Computational modeling;Motion detection;Benchmark testing;Humans;Virtual environment;Design automation;Computer aided manufacturing;CADCAM;Haptics;God-Object;Six degrees of freedom;Rigid bodies;Constraint-based quasi-statics},
	month = {March},
	pages = {191-198},
	title = {A Six Degree-of-Freedom God-Object Method for Haptic Display of Rigid Bodies},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.18}}

@inproceedings{1667645,
	abstract = {Recently, there are various types of display systems that can present aural, visual and haptic information related to the user's position. It is also important to present olfactory information related to the user's position, and we focus on the spatiality of odor, which is one of its characteristics. In this research, we constructed and evaluated a wearable olfactory display to present the spatiality of odor in an outdoor environment. The prototype wearable olfactory display system treats odor in the gaseous state, and the odor air is conveyed to the user's nose through tubes. Using this system, we also present the spatiality of odor by controlling the odor strength according to the positions of the user and the odor source. With this prototype system, the user can specify the position of the odor source in an outdoor environment. To improve this prototype system, we constructed another wearable olfactory display. Because odor is treated in the gaseous state, the first prototype system has some problems such as the large size of the device and unintentional leakage of the odor into the environment. To solve these issues, we developed and evaluated an advanced wearable olfactory display that uses an inkjet head device to treat odor in the liquid state.},
	author = {Yamada, T. and Yokoyama, S. and Tanikawa, T. and Hirota, K. and Hirose, M.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.147},
	issn = {2375-5334},
	keywords = {Olfactory;Displays;Prototypes;Virtual reality;Sensor phenomena and characterization;Chemical sensors;Nose;Control systems;Wearable sensors;Haptic interfaces},
	month = {March},
	pages = {199-206},
	title = {Wearable Olfactory Display: Using Odor in Outdoor Environment},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.147}}

@inproceedings{1667646,
	abstract = {We propose a projection-based olfactory display method as an unencumbering way to deliver smells using a scent projector. This device allows us to deliver smells both spatially and temporally by carrying scented air within a vortex ring launched from an air cannon. However, our original scent projector had a problem: users often felt a significant, unnatural airflow when the vortex ring hit their faces. In this paper, we propose a novel configuration of a projection-based olfactory display that reduces this breeze effect. We use two air cannons that launch vortex rings that collide at a target point and break by themselves, distributing their smell at the point where they were broken to make a small "spot" of scent. With this configuration, users feel as if the smell came with a soft breeze, so that they can enjoy a more natural olfactory experience.},
	author = {Nakaizumi, F. and Noma, H. and Hosaka, K. and Yanagida, Y.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.122},
	issn = {2375-5334},
	keywords = {Olfactory;Virtual reality;Space technology;Nose;Auditory displays;Three dimensional displays;Face detection;Switches;Information science;Laboratories;olfactory display;air cannon;vortex ring;olfactory field},
	month = {March},
	pages = {207-214},
	title = {SpotScents: A Novel Method of Natural Scent Delivery Using Multiple Scent Projectors},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.122}}

@inproceedings{1667647,
	abstract = {In the late 90's the emergence of high performance 3D commodity graphics cards opened the way to use PC clusters for high performance Virtual Reality (VR) applications. Today PC clusters are broadly used to drive multi projector immersive environments. In this paper, we survey the different approaches that have been developed to use PC clusters for VR applications. We review the most common software tools that enable to take advantage of the power of clusters. We also discuss some new trends.},
	author = {Raffin, B. and Soares, L. and Tao Ni and Ball, R. and Schmidt, G.S. and Livingston, M.A. and Staadt, O.G. and May, R.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.107},
	issn = {2375-5334},
	keywords = {Virtual reality;Graphics;Application software;Rendering (computer graphics);Personal communication networks;Open source software;Software performance;Software tools;Animation;Distributed computing;Virtual Reality;PC Clusters},
	month = {March},
	pages = {215-222},
	title = {PC Clusters for Virtual Reality},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.107}}

@inproceedings{1667648,
	abstract = {Continued advances in display hardware, computing power, networking, and rendering algorithms have all converged to dramatically improve large high-resolution display capabilities. We present a survey on prior research with large high-resolution displays. In the hardware configurations section we examine systems including multi-monitor workstations, reconfigurable projector arrays, and others. Rendering and the data pipeline are addressed with an overview of current technologies. We discuss many applications for large high-resolution displays such as automotive design, scientific visualization, control centers, and others. Quantifying the effects of large high-resolution displays on human performance and other aspects is important as we look toward future advances in display technology and how it is applied in different situations. Interacting with these displays brings a different set of challenges for HCI professionals, so an overview of some of this work is provided. Finally, we present our view of the top ten greatest challenges in large highresolution displays.},
	author = {Tao Ni and Schmidt, G.S. and Staadt, O.G. and Livingston, M.A. and Ball, R. and May, R.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.20},
	issn = {2375-5334},
	keywords = {Computer displays;Liquid crystal displays;Hardware;Application software;User interfaces;Computer science;Laboratories;Computer networks;Workstations;Visual effects;Large high-resolution displays;visualization;virtualenvironments;multi-monitor;projector array;distributed rendering;collaboration;user interfaces;interaction techniques;evaluation},
	month = {March},
	pages = {223-236},
	title = {A Survey of Large High-Resolution Display Technologies, Techniques, and Applications},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.20}}

@inproceedings{1667649,
	abstract = {We present our experiences of combining wave field synthesis audio with a projection-based multi-viewer stereo display. Wave field synthesis is able to simulate spatial sound sources of various kinds without the need for headphones or user tracking. Multi-viewer systems support multiple tracked users with individual perspectively correct stereoscopic images. The combination of both approaches allows the consistent display of virtual objects and spatial audio sources for multiple participants. First impressions with two application scenarios confirm that sound sources can be quite well located in space by each user. The system allows the creation of sound sources attached to virtual objects, which can be moved around in real-time without perceivable latency. In addition users appreciated the possibility of natural communication while they were exploring the audio-visual scenarios.},
	author = {Springer, J.P. and Sladeczek, C. and Scheffler, M. and Hochstrate, J. and Melchior, F. and Frohlich, B.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.33},
	issn = {2375-5334},
	keywords = {Auditory displays;Headphones;Head;Loudspeakers;Virtual environment;Image reconstruction;Humans;Real time systems;Delay;Audio systems;Wave Field Synthesis;Multi-Viewer Stereo Displays;Immersive Virtual Environments},
	month = {March},
	pages = {237-240},
	title = {Combining Wave Field Synthesis and Multi-Viewer Stereo Displays},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.33}}

@inproceedings{1667650,
	abstract = {directly on top of physical objects in a video scene. Registration accuracy is a serious problem in these cases since any imprecisions are immediately apparent as virtual and physical edges and features coincide. We present a hardware-accelerated image-based post-processing technique that adjusts rendering of virtual geometry to better match edges present in images of a physical scene, reducing the visual effect of registration errors from both inaccurate tracking and oversimplified modeling. Our algorithm is easily integrable with existing AR applications, having no dependency on the underlying tracking technique. We use the advanced programmable capabilities of modern graphics hardware to achieve high performance without burdening the CPU.},
	author = {DiVerdi, S. and Hollerer, T.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.82},
	issn = {2375-5334},
	keywords = {Error correction;Hardware;Layout;Geometry;Computer errors;Computer graphics;Application software;Image edge detection;Solid modeling;Space technology;image-based techniques;hardware acceleration;registration},
	month = {March},
	pages = {241-244},
	title = {Image-space Correction of AR Registration Errors Using Graphics Hardware},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.82}}

@inproceedings{1667651,
	abstract = {Today's technology enables a rich set of virtual and mixedreality applications and provides a degree of connectivity and interactivity beyond that of traditional teleconferencing scenarios. In this paper, we present the Mixed-Reality Tabletop (MRT), an example of a teleconferencing framework for networked mixed reality in which real and virtual worlds coexist and users can focus on the task instead of computer interaction. For example, students could use real-world objects to participate in physical simulations such as orbital motion, collisions, and fluid flow in a common virtual environment. Our framework isolates the lowlevel system details from the developer and provides a simple programming interface for developing novel applications in as little as a few minutes, using low-cost hardware. We discuss our implementation complete with methods of hand and object tracking, user interface, and example applications focused on remote teaching and learning.},
	author = {Bekins, D. and Yost, S. and Garrett, M. and Deutsch, J. and Win Mar Htay and Dongyan Xu and Aliaga, D.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.100},
	issn = {2375-5334},
	keywords = {Virtual reality;Teleconferencing;Application software;Computer networks;Computational modeling;Fluid flow;Virtual environment;Hardware;User interfaces;Education;D.2.6.b Graphical environments;H.4.3.b Computer;conferencing;teleconferencing;and videoconferencing;H.5.1.b;Artificial;augmented;and virtual realities;I.3.2.a Distributed/ network graphics.},
	month = {March},
	pages = {245-248},
	title = {Mixed Reality Tabletop (MRT): A Low-Cost Teleconferencing Framework for Mixed-Reality Applications},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.100}}

@inproceedings{1667652,
	abstract = {Warfighters develop and maintain their skills through training. Since fully-manned live training in the real world is often too expensive (by many measures), scientists have developed many types of training systems ranging from classroom sessions to those using virtual reality. Recently, researchers have used augmented reality (AR) to insert virtual entities into the real world, attempting to create a low cost, repeatable, and effective substitute for fully-manned live training. However, very little evaluation of the effectiveness of AR for training has been performed. We performed a pilot study to evaluate the use of wearable AR in teaching urban skills, specifically, room clearing in teams. Eight teams of two were briefed on room clearing techniques, given hands-on instruction, and then allowed to practice those techniques with or without the AR system. After this instructional period, subjects performed several room clearing scenarios against real people using infrared-based practice weapons that logged the number of hits on the subjects and the enemy and neutral forces. During these trials, a subject matter expert evaluated how well the subjects applied the room-clearing techniques. In this paper, we describe the pilot study in more detail, including the hardware and software testbed, and then provide an analysis of the results of the pilot study.},
	author = {Brown, D.G. and Coyne, J.T. and Stripling, R.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.28},
	issn = {2375-5334},
	keywords = {Augmented reality;Virtual reality;Performance evaluation;Military computing;Education;Weapons;Computer applications;Testing;Control systems;Prototypes;augmented reality;training;evaluation},
	month = {March},
	pages = {249-252},
	title = {Augmented Reality for Urban Skills Training},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.28}}

@inproceedings{1667653,
	abstract = {We present InfrActables, a wireless technology for human computer interaction devices. It allows multiple users to interact simultaneously on back-projection displays. It recognizes each device's position, orientation, and identification, but also enables the tools to communicate their states to the application that the user interacts with. This makes it possible to build complex interaction devices for direct manipulation with buttons, sliders, and other input capabilities. The technology was developed for a computer-supported environment, allowing multiple users to interact simultaneously on a surface using multiple styli and other user input devices. The principle of operation is not limited to two-dimensional surfaces, three-dimensional user input devices can also profit from its advantages.},
	author = {Ganser, C. and Steinemann, A. and Kunz, A.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.86},
	issn = {2375-5334},
	keywords = {Computer displays;Collaborative work;Technological innovation;Virtual reality;Computer graphics;Collaborative software;Elbow;Delay;Human computer interaction;Application software;Tracking;User Input Devices;Single Display Groupware;Stylus-based User Input Device},
	month = {March},
	pages = {253-256},
	title = {InfrActables: Multi-User Tracking System for Interactive Surfaces},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.86}}

@inproceedings{1667654,
	abstract = {We have developed a physically-based VR system that enables users to interactively style dynamic virtual hair by using multiresolution simulation techniques and graphics hardware rendering acceleration for simulating and rendering hair in real time. With a 3D haptic interface, users can directly manipulate and position hair strands, as well as employ real-world styling applications (cutting, blow-drying, etc.) to create hairstyles more intuitively than previous techniques.},
	author = {Ward, K. and Galoppo, N. and Lin, M.C.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.17},
	issn = {2375-5334},
	keywords = {Virtual reality;Hair;Rendering (computer graphics);Haptic interfaces;Graphics;Hardware;Acceleration;Animation;Real time systems;Computational modeling},
	month = {March},
	pages = {257-260},
	title = {A Simulation-based VR System for Interactive Hairstyling},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.17}}

@inproceedings{1667655,
	abstract = {This paper describes a 3D, tangible user interface system and related interaction techniques for the CAVE environment. The developed system is based on off-the shelf software and hardware components to provide 3D input data for CAVE applications. A CAVE with four sides (three walls and a floor) is used as a display and interaction space. Interaction tasks (selection, manipulation, navigation) are performed using interaction techniques based on manipulation of physical objects (props). All virtual objects are directly manipulated using the corresponding props to which Augmented Reality physical markers are attached. Each physical marker corresponds to a specific virtual object. The floor projection (a white rectangle overlaid on top of the application generated video stream) or a directional light create illumination necessary for computer vision based marker detection using ARToolKit. Initial evaluation results are positive and provide directions for future research.},
	author = {Ji-Sun Kim and Gracanin, D. and Singh, H.L. and Matkovic, K. and Juric, J.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.21},
	issn = {2375-5334},
	keywords = {User interfaces;Application software;Augmented reality;Space technology;Displays;Streaming media;Virtual reality;Computer graphics;Real time systems;Military computing;3D interaction;mixed reality;augmented reality},
	month = {March},
	pages = {261-264},
	title = {A Tangible User Interface System for CAVE Applicat},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.21}}

@inproceedings{1667656,
	abstract = {We present a novel text input interface for immersive virtual environments called CTD, or "Connect the Dots". The CTD interface is a small virtual panel containing a grid of dots the user can connect to form alphanumeric characters using a hand held stylus. Coupled with a physical paddle or desk for force feedback, the CTD provides an intuitive text input interface similar to using a pen and paper.},
	author = {Frees, S. and Khouri, R. and Kessler, G.D.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.36},
	issn = {2375-5334},
	keywords = {Joining processes;US Department of Transportation;Character recognition;Keyboards;Virtual reality;Computer graphics;Virtual environment;Force feedback;Text recognition;Writing;text input},
	month = {March},
	pages = {265-268},
	title = {Connecting the Dots: Simple Text Input in Immersive Environments},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.36}}

@inproceedings{1667657,
	abstract = {The Proactive Desk is a force feedback system using a two-degree of freedom linear induction motor (LIM) designed for ordinary desktop operations. The system provides two-dimensional force feedback on a desktop surface, but only for a single object. This means that when several users operate this system together, only one user at a time can acquire force feedback from the system. In this paper, we propose a new-generation haptic display, Proactive Desk II, which uses an advanced LIM for multi-object driving, and develop a prototype system based on our approach. The system employs a cluster of coils for producing magnetic fluxes individually underneath the desktop, and those synthesize several traveling magnetic fields as a result. These magnetic fields simulate a local region of the field generated by our first-generation system, being able to drive multiple objects separately.},
	author = {Yoshida, S. and Noma, H. and Hosaka, K.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.110},
	issn = {2375-5334},
	keywords = {Haptic interfaces;Induction motors;Force feedback;Magnetic fields;Computer displays;Prototypes;Coils;Induction generators;Force control;Stators;haptic display;force feedback;linear induction motor;interaction;multi-user interface},
	month = {March},
	pages = {269-272},
	title = {Proactive Desk II: Development of a New Multi-object Haptic Display Using a Linear Induction Motor},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.110}}

@inproceedings{1667658,
	abstract = {The electromyogram (EMG) signal, which is a kind of biomedical information, might be promising as a computer-human interface. The purpose in this paper is to make a brief report on implementation of the EMG interface in CAVE-clone display and propose the advantages of it. In general, it is expected that the environmental electromagnetic noise would disturb the novel EMG signals. We measured the EMG signals and obtained high signal-to-noise ratio in CABIN immersive multiscreen display constructed at the University of Tokyo. The observed signal-tonoise ratio would make EMG signals a reliable input channel with simple signal processing, and real time control of virtual objects was successfully performed. The advantages of the EMG signals will be proposed as a promising human interface in immersive multiscreen environments. The user would control virtual objects reflecting the activations of the user's muscles to realize fine operations. Furthermore, by using this additional biosignal channel, the user might interact with virtual objects without definite motion.},
	author = {Touyama, H. and Hirota, K. and Hirose, M.},
	booktitle = {IEEE Virtual Reality Conference (VR 2006)},
	date-added = {2024-03-18 02:28:55 -0400},
	date-modified = {2024-03-18 02:28:55 -0400},
	doi = {10.1109/VR.2006.83},
	issn = {2375-5334},
	keywords = {Electromyography;Computer displays;Signal processing;Biomedical computing;Computer interfaces;Electromagnetic interference;Working environment noise;Biomedical measurements;Electromagnetic measurements;Signal to noise ratio;electromyogram EMG;computer human interface;CAVE;CABIN;biosignal;virtual reality VR},
	month = {March},
	pages = {273-278},
	title = {Implementation of Electromyogram Interface in CABIN Immersive Multiscreen Display},
	year = {2006},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2006.83}}

@inproceedings{1492747,
	abstract = {Without force feedback, a head-mounted display user's avatar may penetrate virtual objects. Some virtual environment designers prevent visual interpenetration, making the assumption that prevention improves user experience. However, preventing visual avatar interpenetration causes discrepancy between visual and proprioceptive cues. We investigated users' detection thresholds for visual interpenetration (the depth at which they see that two objects have interpenetrated) and sensory discrepancy (the displacement at which they notice mismatched visual and proprioceptive cues). We found that users are much less sensitive to visual-proprioceptive conflict than they are to visual interpenetration. We present our plan for using this result to create a better technique for dealing with virtual object penetration.},
	author = {Burns, E. and Razzaque, S. and Panter, A.T. and Whitton, M.C. and McCallus, M.R. and Brooks, F.P.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492747},
	issn = {2375-5334},
	keywords = {Avatars;Force feedback;Displays;Virtual environment;Computer graphics;Layout;Virtual reality;Object detection;Chromium;User interfaces},
	month = {March},
	pages = {3-10},
	title = {The hand is slower than the eye: a quantitative exploration of visual dominance over proprioception},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492747}}

@inproceedings{1492748,
	abstract = {A challenge in presenting augmenting information in outdoor augmented reality (AR) settings lies in the broad range of uncontrollable environmental conditions that may be present, specifically large-scale fluctuations in natural lighting and wide variations in likely backgrounds or objects in the scene. In this paper, we present a user-based study which examined the effects of outdoor background textures, changing outdoor illuminance values, and text drawing styles on user performance of a text identification task with an optical, see-through augmented reality system. We report significant effects for all of these variables, and discuss design guidelines and ideas for future work.},
	author = {Gabbard, J.L. and Swan, J.E. and Hix, D. and Schulman, R.S. and Lucas, J. and Gupta, D.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492748},
	issn = {2375-5334},
	keywords = {Augmented reality;Displays;Computer science;Virtual reality;Lighting;Optical devices;Position measurement;Engineering drawings;Statistics;Interference},
	month = {March},
	pages = {11-18},
	title = {An empirical user-based study of text drawing styles and outdoor background textures for augmented reality},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492748}}

@inproceedings{1492749,
	abstract = {This paper describes two psychophysical experiments which were conducted to evaluate the influence of the control/display (C/D) ratio on the perception of mass of manipulated objects in virtual environments (VE). In both experiments, a discrimination task was used in which participants were asked to identify the heavier object between two virtual balls. Participants could weigh each ball via a haptic interface and look at its synthetic display on a computer screen. Unknown to the participants, two parameters varied between each trial: the difference of mass between the balls and the C/D ratio used in the visual display when weighing the comparison ball. The data collected demonstrated that the C/D ratio significantly influenced the result of the mass discrimination task and sometimes even reversed it. The absence of gravity force largely increased this effect. These results suggest that if the visual motion of a manipulated virtual object is amplified when compared to the actual motion of the user's hand (i.e. if the C/D ratio used is smaller than 1), the user tends to feel that the mass of the object decreases. Thus, decreasing or amplifying the motions of the user in a VE can strongly modify the perception of haptic properties of objects that he/she manipulates. Designers of virtual environments could use these results for simplification considerations and also to avoid potential perceptual aberrations.},
	author = {Dominjon, L. and Lecuyer, A. and Burkhardt, J.-M. and Richard, P. and Richir, S.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492749},
	issn = {2375-5334},
	keywords = {Weight control;Virtual environment;Haptic interfaces;Computer displays;Gravity;Humans;Psychology;Computer interfaces;Large screen displays;Chromium},
	month = {March},
	pages = {19-25},
	title = {Influence of control/display ratio on the perception of mass of manipulated objects in virtual environments},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492749}}

@inproceedings{1492750,
	abstract = {We present a scalable implementation of a network partitioning scheme that we have called frontier sets. Frontier sets build on the notion of a potentially visible set (PVS). In a PVS, a world is sub-divided into cells and for each cell all the other cells that can be seen are computed. In contrast, a frontier set considers pairs of cells, A and B. For each pair, it lists two sets of cells, F/sub BA/ and F/sub BA/. By definition, from no cell in F/sub BA/ is any cell in F/sub BA/ visible and vice-versa. Our initial use of frontier sets has been to enable scalability in distributed networking. In this paper we build on previous work by showing how to avoid pre-computing frontier sets. Our previous algorithm, required O(N/sup 3/) space in the number of cells, to store pre-computed frontier sets. Our new algorithm pre-computes an enhanced potentially visible set that requires only O(N/sup 2/) space and then computes frontiers only as needed. Network simulations using code based on the Quake II engine show that frontiers have significant promise and may allow a new class of scalable peer-to-peer game infrastructures to emerge.},
	author = {Steed, A. and Angus, C.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492750},
	issn = {2375-5334},
	keywords = {Computational modeling;Computer graphics;Data structures;Scalability;Virtual environment;Computer science;Educational institutions;Engines;Peer to peer computing;Chromium},
	month = {March},
	pages = {27-34},
	title = {Supporting scalable peer to peer virtual environments using frontier sets},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492750}}

@inproceedings{1492751,
	abstract = {This paper develops a scalable system design for the creation, and delivery over the Internet, of a realistic voice communication service for crowded virtual spaces. Examples of crowded spaces include virtual market places or battlefields in online games. A realistic crowded audio scene including spatial rendering of the voices of surrounding avatars is impractical to deliver over the Internet in a peer-to-peer manner due to access bandwidth limitations and cost. A brute force server model, on the other hand, will face significant computational costs and scalability issues. This paper presents a novel server-based architecture for this service that performs simple operations in the servers (including weighted mixing of audio streams) to cope with access bandwidth restrictions of clients, and uses spatial audio rendering capabilities of the clients to reduce the computational load on the servers. This paper then examines the performance of two components of this architecture: angular clustering and grid summarization. The impact of two factors, namely a high density of avatars and realistic access bandwidth limitations, on the quality and accuracy of the audio scene is then evaluated using simulation results.},
	author = {Boustead, P. and Safaei, F. and Dowlatshahi, M.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492751},
	issn = {2375-5334},
	keywords = {Bandwidth;Layout;Avatars;Web server;Computer architecture;Web and internet services;Peer to peer computing;Costs;Computational efficiency;Scalability},
	month = {March},
	pages = {35-41},
	title = {DICE: Internet delivery of immersive voice communication for crowded virtual spaces},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492751}}

@inproceedings{1492752,
	abstract = {Advanced VR simulation systems are composed of several components with independent and heterogeneously structured databases. To guarantee a closed and consistent world simulation, flexible and robust data exchange between these components has to be realized. This multiple database problem is well known in many distributed application domains, but it is central for VR setups composed of diverse simulation components. Particularly complicated is the exchange between object-centered and graph-based representation formats, where entity attributes may be distributed over the graph structure. This article presents an abstract declarative attribute representation concept, which handles different representation formats uniformly and enables automatic data exchange and synchronization between them. This mechanism is tailored to support the integration of a central knowledge component, which provides a uniform representation of the accumulated knowledge of the several simulation components involved. This component handles the incoming-possibly conflicting-world changes propagated by the diverse components. It becomes the central instance for process flow synchronization of several autonomous evaluation loops.},
	author = {Heumer, G. and Schilling, M. and Latoschik, M.E.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492752},
	issn = {2375-5334},
	keywords = {Virtual reality;Artificial intelligence;Application software;Virtual environment;Knowledge representation;Software tools;Graphics;Knowledge based systems;Databases;Robustness},
	month = {March},
	pages = {43-50},
	title = {Automatic data exchange and synchronization for knowledge-based intelligent virtual environments},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492752}}

@inproceedings{1492753,
	abstract = {Scene graphs have become an established tool for developing interactive 3D applications, but with the focus lying on support for multi-processor and multi-pipeline systems, for distributed applications and for advanced rendering effects. Contrary to these developments, this work focusses on the expressiveness of the scene graph structure as a central tool for developing 3D user interfaces. We present the idea of a context for the traversal of a scene graph which allows to parameterize a scene graph and reuse it for different purposes. Such context sensitive scene graphs improve the inherent flexibility of a scene graph acting as a template with parameters bound during traversal. An implementation of this concept using an industry standard scene graph library is described and its use in a set of applications from the area of mobile augmented reality is demonstrated.},
	author = {Reitmayr, G. and Schmalstieg, D.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492753},
	issn = {2375-5334},
	keywords = {Layout;Application software;Rendering (computer graphics);User interfaces;Data structures;Software design;Navigation;Geometry;Software libraries;Augmented reality},
	month = {March},
	pages = {51-58},
	title = {Flexible parametrization of scene graphs},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492753}}

@inproceedings{1492754,
	abstract = {We present a fast collision culling algorithm for performing inter-and intra-object collision detection among complex models using graphics hardware. Our algorithm is based on CULLIDE (Govindaraju et al., 2003) and performs visibility queries on the GPUs to eliminate a sub-set of geometric primitives that are not in close proximity. We present an extension to CULLIDE to perform intra-object or self-collisions between complex models. Furthermore, we describe a novel visibility-based classification scheme to compute potentially-colliding and collision-free subsets of objects and primitives, which considerably improves the culling performance. We have implemented our algorithm on a PC with an NVIDIA GeForce FX 6800 Ultra graphics card and applied it to three complex simulations, each consisting of objects with tens of thousands of triangles. In practice, we are able to compute all the self-collisions for cloth simulation up to image-space precision at interactive rates.},
	author = {Govindaraju, N.K. and Lin, M.C. and Manocha, D.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492754},
	issn = {2375-5334},
	keywords = {Hardware;Computational modeling;Virtual reality;Computer graphics;Physics computing;Solid modeling;Object detection;Personal communication networks;Virtual environment;Avatars},
	month = {March},
	pages = {59-66},
	title = {Quick-CULLIDE: fast inter- and intra-object collision culling using graphics hardware},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492754}}

@inproceedings{1492755,
	abstract = {To enable a user to perform virtual reality tasks as efficiently as possible, reducing tracking inaccuracies from noise and latency is crucial. Much work has been done to improve tracking performance by using predictive filtering methods. However, it is unclear what the benefits of each of these methods are in practice, which parameters influence their performance, and what the extent of this influence is. We present an analysis of various orientation prediction and filtering methods using various hand tasks and synthetic signals, and evaluate their performance in relation to each other. We identify critical parameters and analyse their influence on accuracy. Our results show that for the tested datasets, the use of an EKF is sufficient for orientation prediction in VR/AR.},
	author = {van Rhijn, A. and van Liere, R. and Mulder, J.D.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492755},
	issn = {2375-5334},
	keywords = {Virtual reality;Performance analysis;Filtering algorithms;Delay;Signal analysis;Image processing;Computer vision;Electronic mail;Nonlinear filters;Noise reduction},
	month = {March},
	pages = {67-74},
	title = {An analysis of orientation prediction and filtering methods for VR/AR},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492755}}

@inproceedings{1492756,
	abstract = {Building intuitive user-interfaces for virtual reality applications is a difficult task, as one of the main purposes is to provide a "natural", yet efficient input device to interact with the virtual environment. One particularly interesting approach is to track and retarget the complete motion of a subject. Established techniques for full body motion capture like optical motion tracking exist. However, due to their computational complexity and their reliance on pre-specified models, they fail to meet the demanding requirements of virtual reality environments such as real-time response, immersion, and ad hoc configurability. Our goal is to support the use of motion capture as a general input device for virtual reality applications. In this paper we present a self-calibrating framework for optical motion capture, enabling the reconstruction and tracking of arbitrary articulated objects in real-time. Our method automatically estimates all relevant model parameters on-the-fly without any information on the initial tracking setup or the marker distribution, and computes the geometry and topology of multiple tracked skeletons. Moreover, we show how the model can make the motion capture phase robust against marker occlusions by exploiting the redundancy in the skeleton model and by reconstructing missing inner limbs and joints of the subject from partial information. Meeting the above requirements our system is well applicable to a wide range of virtual reality based applications, where unconstrained tracking and flexible retargeting of motion data is desirable.},
	author = {Hornung, A. and Sar-Dessai, S. and Kobbelt, L.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492756},
	issn = {2375-5334},
	keywords = {Tracking;Virtual reality;Skeleton;Virtual environment;Computational complexity;Solid modeling;Distributed computing;Geometrical optics;Computational geometry;Information geometry},
	month = {March},
	pages = {75-82},
	title = {Self-calibrating optical motion tracking for articulated bodies},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492756}}

@inproceedings{1492757,
	abstract = {Existing commercial technologies do not adequately meet the requirements for tracking in fully-enclosed VR displays. We present the Hedgehog, which overcomes several limitations imposed by existing sensors and tracking technology. The tracking system robustly and reliably estimates the 6DOF pose of the device with high accuracy and a reasonable update rate. The system is composed of several cameras viewing the display walls and an arrangement of laser diodes secured to the user. The light emitted from the lasers projects onto the display walls and the 2D centroids of the projections are tracked to estimate the 6DOF pose of the device. The system is able to handle ambiguous laser projection configurations, static and dynamic occlusions of the lasers, and incorporates an intelligent laser activation control mechanism that determines which lasers are most likely to improve the pose estimate. The Hedgehog is also capable of performing auto-calibration of the necessary camera parameters through the use of the SCAAT algorithm. A preliminary evaluation reveals that the system has an angular resolution of 0.01 degrees RMS and a position resolution of 0.2 mm RMS.},
	author = {Vorozcovs, A. and Hogue, A. and Stuerzlinger, W.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492757},
	issn = {2375-5334},
	keywords = {Computer displays;Virtual reality;Head;Tracking;Light emitting diodes;Robustness;Cameras;Spatial resolution;Layout;Photodiodes},
	month = {March},
	pages = {83-89},
	title = {The Hedgehog: a novel optical tracking method for spatially immersive displays},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492757}}

@inproceedings{1492758,
	abstract = {We present a physically-based approach to grasping and manipulation of virtual objects that produces visually realistic results, addresses the problem of visual interpenetration of hand and object models, and performs force rendering for force-feedback gloves in a single framework. Our approach couples tracked hand configuration to a simulation-controlled articulated hand model using a system of linear and torsional spring-dampers. We discuss an implementation of our approach that uses a widely-available simulation tool for collision detection and response. We illustrate the resulting behavior of the virtual hand model and of grasped objects, and we show that the simulation rate is sufficient for control of current force-feedback glove designs. We also present a prototype of a system we are developing to support natural whole-hand interactions in a desktop-sized workspace.},
	author = {Borst, C.W. and Indugula, A.P.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492758},
	issn = {2375-5334},
	keywords = {Grasping;Force control;Physics computing;Computer graphics;Virtual environment;Haptic interfaces;Force feedback;Computational modeling;Prototypes;Chromium},
	month = {March},
	pages = {91-98},
	title = {Realistic virtual grasping},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492758}}

@inproceedings{1492759,
	abstract = {A significant benefit of an immersive virtual environment is that it provides users the ability to interact with objects in a very natural, direct way; often realized by using a tracked, hand-held wand or stylus to "grab" and position objects. In the absence of force feedback or props, it is difficult and frustrating for users to move their arms, hands, or fingers to precise positions in 3D space, and more difficult to hold them at a constant position, or to move them in a uniform direction over time. The imprecision of user interaction in virtual environments is a fundamental problem that limits the complexity of the environment the user can interact with directly. We present PRISM (precise and rapid interaction through scaled manipulation), a novel interaction technique which acts on the user's behavior in the environment to determine whether they have precise or imprecise goals in mind. When precision is desired, PRISM dynamically adjusts the "control/ display" ratio which determines the relationship between physical hand movements and the motion of the controlled virtual object, making it less sensitive to the user's hand movement. In contrast to techniques like Go-Go, which scale up hand movement to allow "long distance" manipulation; PRISM scales the hand movement down to increase precision. We present the results of a user study which shows that PRISM significantly out-performs the more traditional direct manipulation approach.},
	author = {Frees, S. and Kessler, G.D.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492759},
	issn = {2375-5334},
	keywords = {Virtual environment;Arm;Computer graphics;Force feedback;Virtual reality;Haptic interfaces;Fingers;Motion control;Chromium;Computational modeling},
	month = {March},
	pages = {99-106},
	title = {Precise and rapid interaction through scaled manipulation in immersive virtual environments},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492759}}

@inproceedings{1492760,
	abstract = {In this paper, we introduce the idea of projecting different image elements from multiple viewpoints and combining them in a single stereoscopic image. We use this technique to enable multi-user interaction and co-located collaboration in large projection-based immersive display systems. Viewing stereoscopic images from a viewpoint outside of the projection viewpoint introduces parallax, a skew distortion of the spatial image, resulting in a misalignment between real and virtual object positions for multiple viewers sharing a single-view stereoscopic display system. With multi-viewpoint images, we can project different image elements from multiple viewpoints, corresponding to the viewing positions of multiple users, and combine them in a single image. We use this technique to project interaction elements for each user in the correct position and depth, matching, from the user's point of view, the tracked real positions of interaction devices with the virtual position of visual and functional interaction elements such as pointers, menus or picking rays. We introduce a rendering method to combine projections for different viewpoints in a single, consistent stereoscopic image. We have used multi-viewpoint images for interaction in applications, where a large audience is looking at a non-head-tracked immersive presentation and a guide user is controlling the application with direct interaction techniques. Furthermore we have developed complex interaction scenarios, where multiple users share a conventional single-view projection-based display environment for co-located collaboration.},
	author = {Simon, A. and Scholz, S.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492760},
	issn = {2375-5334},
	keywords = {Collaboration;Virtual environment;Rendering (computer graphics);Virtual reality;Layout;Chromium;Image generation;Graphics;Three dimensional displays;Collaborative work},
	month = {March},
	pages = {107-113},
	title = {Multi-viewpoint images for multi-user interaction},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492760}}

@inproceedings{1492761,
	abstract = {Trails are a little-researched type of aid that offers great potential benefits for navigation, especially in virtual environments (VEs). An experiment was performed in which participants repeatedly searched a virtual building for target objects assisted by: (1) a trail, (2) landmarks, (3) a trail and landmarks, or (4) neither. The trail was displayed as a white line that showed exactly where a participant had previously traveled. The trail halved the distance that participants traveled during first-time searches, indicating the immediate benefit to users if even a crude form of trail were implemented in a variety of VE applications. However, the general clutter or "pollution" produced by trails reduced the benefit during subsequent navigation and, in the later stages of these searches, caused participants to travel more than twice as far as they needed to, often accidentally bypassing targets even when a trail led directly to them. The proposed solution is to use gene alignment techniques to extract a participant's primary trail from the overall, polluted trail, and graphically emphasize the primary trail to aid navigation.},
	author = {Ruddle, R.A.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492761},
	issn = {2375-5334},
	keywords = {Navigation;Virtual environment;Virtual reality;Data mining;Pollution;Chromium;Graphics;Human computer interaction;Displays;Performance analysis},
	month = {March},
	pages = {115-122},
	title = {The effect of trails on first-time and subsequent navigation in a virtual environment},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492761}}

@inproceedings{1492762,
	abstract = {To compare and evaluate locomotion interfaces for users who are (virtually) moving on foot in VEs, we performed a study to characterize task behavior and task performance with different visual and locomotion interfaces. In both a computer-generated environment and a corresponding real environment, study participants walked to targets on walls and stopped as close to them as they could without making contact. In each of five experimental conditions participants used a combination of one of three locomotion interfaces (really walking, walking-in-place, and joystick flying), and one of three visual conditions (head-mounted display, unrestricted natural vision, or field-of-view-restricted natural vision). We identified metrics and collected data that captured task performance and the underlying kinematics of the task. Our results show: 1) Over 95% of the variance in simple motion paths is captured in three critical values: peak velocity; when, in the course of a motion, the peak velocity occurs; and peak deceleration. 2) Correlations of those critical value data for the conditions taken pairwise suggest a coarse ordering of locomotion interfaces by "naturalness." 3) Task performance varies with interface condition, but correlations of that value for conditions taken pairwise do not cluster by naturalness. 4) The perceptual variable, r (also known as the time-to-contact) calculated at the point of peak deceleration has higher correlation with task performance than r calculated at peak velocity.},
	author = {Whitton, M.C. and Cohn, J.V. and Feasel, J. and Zimmons, P. and Razzaque, S. and Poulton, S.J. and McLeod, B. and Brooks, F.P.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492762},
	issn = {2375-5334},
	keywords = {Foot;Legged locomotion;Virtual environment;Computer graphics;Motor drives;Virtual reality;Motion analysis;Laboratories;Educational institutions;Performance evaluation},
	month = {March},
	pages = {123-130},
	title = {Comparing VE locomotion interfaces},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492762}}

@inproceedings{1492765,
	abstract = {Despite recent technological advances, convincing self-motion simulation in virtual reality (VR) is difficult to achieve, and users often suffer from motion sickness and/or disorientation in the simulated world. Instead of trying to simulate self-motions with physical realism (as is often done for, e.g., driving or flight simulators), we propose in this paper a perceptually oriented approach towards self-motion simulation. Following this paradigm, we performed a series of psychophysical experiments to determine essential visual, auditory, and vestibular/tactile parameters for an effective and perceptually convincing self-motion simulation. These studies are a first step towards our overall goal of achieving lean and elegant self-motion simulation in virtual reality (VR) without physically moving the observer. In a series of psychophysical experiments about the self-motion illusion (circular vection), we found that (i) vection as well as presence in the simulated environment is increased by a consistent, naturalistic visual scene when compared to a sliced, inconsistent version of the identical scene, (ii) barely noticeable marks on the projection screen can increase vection as well as presence in an unobtrusive manner, (iii) physical vibrations of the observer's seat can enhance the vection illusion, and (iv) spatialized 3D audio cues embedded in the simulated environment increase the sensation of self-motion and presence. We conclude that providing consistent cues about self-motion to multiple sensory modalities can enhance vection, even if physical motion cues are absent. These results yield important implications for the design of lean and elegant self-motion simulators.},
	author = {Riecke, B.E. and Schulte-Pelkum, J. and Caniard, F. and Bulthoff, H.H.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492765},
	issn = {2375-5334},
	keywords = {Virtual reality;Aerospace simulation;Psychology;Biological system modeling;Layout;Human computer interaction;Cybernetics;Chromium;Information processing;Multimedia systems},
	month = {March},
	pages = {131-138},
	title = {Towards lean and elegant self-motion simulation in virtual reality},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492765}}

@inproceedings{1492766,
	abstract = {This paper describes the use and characteristics of a virtual reality system called the virtual technical trainer (VTT). This system is dedicated to vocational training courses on using and programming numerically-controlled milling machines. It aims at replacing the conventional mechanical milling machines which are currently used in such vocational training courses. VTT proposes an interactive manipulation of a cutter of a virtual milling machine, with visual, audio and haptic (force) feedback. VTT uses a haptic device which was specifically designed for the purpose of our pedagogical application. When the virtual cutter mills a piece of material, a plastic deformation algorithm is used and the material is progressively carved. Trainees can feel the cutting effort thanks to a force feedback which varies as a function of different simulation parameters (rotation speed of the tool, type of material to carve, etc). Realistic audio feedback (which was recorded in real situations) and additional visual assistance may be added, in order to increase the perception and understanding of the milling task. A preliminary evaluation of VTT showed that this simulator could be used by vocational trainers successfully. It could help them to teach the basic principles of machining at the first stages of vocational training courses on numerically-controlled milling machines.},
	author = {Crison, F. and Lecuyer, A. and d'Huart, D.M. and Burkhardt, J.-M. and Michel, G. and Dautin, J.-L.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492766},
	issn = {2375-5334},
	keywords = {Machine learning;Metalworking machines;Virtual reality;Vocational training;Haptic interfaces;Force feedback;Programming profession;Milling machines;Plastics;Machining},
	month = {March},
	pages = {139-145},
	title = {Virtual technical trainer: learning how to use milling machines with multi-sensory feedback in virtual reality},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492766}}

@inproceedings{1492768,
	abstract = {Surgical training in virtual environments, surgical simulation in other words, has previously had difficulties in simulating deformation of complex morphology in real-time. Even fast spring-mass based systems had slow convergence rates for large models. This paper presents two methods to accelerate a spring-mass system in order to simulate a complex organ such as the heart. Computations are accelerated by taking advantage of modern graphics processing units (GPUs). Two GPU implementations are presented. They vary in their generality of spring connections and in the speedup factor they achieve.},
	author = {Mosegaard, J. and Sorensen, T.S.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492768},
	issn = {2375-5334},
	keywords = {Acceleration;Surgery;Morphology;Computational modeling;Deformable models;Virtual environment;Convergence;Heart;Graphics;Springs},
	month = {March},
	pages = {147-153},
	title = {GPU accelerated surgical simulators for complex morphology},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492768}}

@inproceedings{1492769,
	abstract = {This paper presents steering behaviors that control autonomous vehicles populating roadways in virtual urban environments. Behavior programming is facilitated by a set of representations of the environment that use convenient frames of reference in natural coordinate systems. Roadway surfaces are modeled as three-dimensional ribbons that make the local orientation of the road explicit and allow relative distances on the road to be simply computed. Roads and intersections are connected to form a ribbon network. An egocentric representation called a path melds road and intersection segments into a single, continuous ribbon that captures the vehicle's short-term plan of navigation. A topological structure called a route supports wayfinding. We describe how the interrelated ribbon, path, and route representations are used to build multi-component behaviors that plan routes and safely navigate through traffic filled road networks - tracking lanes, shifting lanes to avoid congestion, anticipating lane changes needed to make turns dictated by the route, negotiating intersections, and respecting the rules of the road.},
	author = {Hongling Wang and Kearney, J.K. and Cremer, J. and Willemsen, P.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492769},
	issn = {2375-5334},
	keywords = {Remotely operated vehicles;Mobile robots;Traffic control;Road vehicles;Navigation;Computer science;Communication system traffic control;Virtual environment;Autonomous agents;Dynamic programming},
	month = {March},
	pages = {155-162},
	title = {Steering behaviors for autonomous vehicles in virtual environments},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492769}}

@inproceedings{1492770,
	abstract = {User-centered design is often performed without regard to individual user differences in aptitude and experience. The methodology of this study is an anthropological and observational approach observing users performing a selection task using common virtual environment raybased techniques and analyzes the interaction through psychology aptitude tests, questionnaires and observation. The results of this study show the approach yields useful information about users even in a simple task. The study indicates correlations between performance and aptitude test and user behavior performed to overcome difficulties in the task.},
	author = {Wingrave, C.A. and Tintner, R. and Walker, B.N. and Bowman, D.A. and Hodges, L.F.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492770},
	issn = {2375-5334},
	keywords = {Testing;Virtual environment;Psychology;Performance evaluation;Feedback;Measurement standards;Virtual reality;User centered design;Performance analysis;Chromium},
	month = {March},
	pages = {163-170},
	title = {Exploring individual differences in raybased selection: strategies and traits},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492770}}

@inproceedings{1492771,
	abstract = {Collaboration at a distance has long been a research goal of distributed virtual environments. A number of recent technologies, including immersive projection technology systems (IPTs) and head-mounted displays (HMDs), promise a new generation of technologies that are more intuitive to use than desktop-based systems. This paper presents an experiment that compares collaboration in five different settings. Pairs collaborated on the same puzzle-solving task using one of: an IPT connected to another IPT, an IPT connected to an HMD, an IPT connected to a desktop system, two connected desktop systems, or face-to-face collaboration with real objects. The findings demonstrate the benefits of using immersive technologies, and show the advantages of using symmetrical settings for better performance. Some usability problems of the different distributed settings are addressed, as well as factors such as "presence" and "copresence" and how these contribute to the participants' overall experiences.},
	author = {Heldal, I. and Schroeder, R. and Steed, A. and Axelsson, A.-S. and Spant, M. and Widestrom, J.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492771},
	issn = {2375-5334},
	keywords = {Collaboration;Collaborative work;Virtual environment;Virtual reality;Usability;Internet;Computer science;Educational institutions;Computer displays;Chromium},
	month = {March},
	pages = {171-178},
	title = {Immersiveness and symmetry in copresent scenarios},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492771}}

@inproceedings{1492772,
	abstract = {This paper presents a system which allows medical students to experience the interaction between a patient and a medical doctor using natural methods of interaction with a high level of immersion. We also present our experiences with a pilot group of medical and physician assistant students at various levels of training. They interacted with projector-based life-sized virtual characters using gestures and speech. We believe that natural interaction and a high level of immersion facilitates the education of communication skills. We present the system details as well as the participants' performance and opinions. The study confirmed that the level of immersion contributed significantly to the experience, and participants reported that the system is a powerful tool for teaching and training. Applying the system to formal communication skills evaluation and further scenario development will be the focus of future research and refinement.},
	author = {Johnsen, K. and Dickerson, R. and Raij, A. and Lok, B. and Jackson, J. and Min Shin and Hernandez, J. and Stevens, A. and Lind, D.S.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492772},
	issn = {2375-5334},
	keywords = {Medical diagnostic imaging;Pain;Computer science education;Abdomen;Feedback;Information science;Biomedical engineering;Computer science;Hospitals;Speech},
	month = {March},
	pages = {179-186},
	title = {Experiences in using immersive virtual characters to educate medical communication skills},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492772}}

@inproceedings{1492773,
	abstract = {While augmented reality (AR) technology is steadily maturing, application development is still lacking advanced authoring tools - even the simple presentation of information, which should not require any programming, is not systematically addressed by development tools. Moreover, there is also a severe lack of agreed techniques or best practices for the structuring of AR content. In this paper we present APRIL, the Augmented Presentation and Interaction Language, an authoring platform for AR presentations which provides concepts and techniques that are independent of specific applications or target hardware platforms, and should be suitable to raise the level of abstraction on which AR content creators can operate.},
	author = {Ledermann, F. and Schmalstieg, D.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492773},
	issn = {2375-5334},
	keywords = {Augmented reality;Software tools;Virtual reality;Application software;Hardware;Programming profession;Best practices;Computer displays;Prototypes;Chromium},
	month = {March},
	pages = {187-194},
	title = {APRIL: a high-level framework for creating augmented reality presentations},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492773}}

@inproceedings{1492774,
	abstract = {The ultimate goal of augmented reality is to provide the user with a view of the surroundings enriched by virtual objects. Practically all augmented reality systems rely on standard real-time rendering methods for generating the images of virtual scene elements. Although such conventional computer graphics algorithms are fast, they often fail to produce sufficiently realistic renderings. The use of simple lighting and shading methods, as well as the lack of knowledge about actual lighting conditions in the real surroundings, cause virtual objects to appear artificial. In this paper, we propose an entirely novel approach for generating augmented reality images in video see-through systems. Our method is based on the idea of applying stylization techniques for reducing the visual realism of both the camera image and the virtual graphical objects. A special painterly image filter is applied to the camera video stream. The virtual scene elements are generated using a non-photorealistic rendering method. Since both the camera image and the virtual objects are stylized in a corresponding "cartoon-like" or "sketch-like" way, they appear very similar. As a result, the graphical objects seem to be an actual part of the real surroundings. We describe both the new painterly filter for the camera image and the non-photorealistic rendering method for virtual scene elements, which has been adapted for this purpose. Both are fast enough for generating augmented reality images in real-time and are highly customizable. The results obtained using our method, are very promising and show that it improves immersion in augmented reality.},
	author = {Fischer, J. and Bartz, D. and Straber, W.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492774},
	issn = {2375-5334},
	keywords = {Augmented reality;Rendering (computer graphics);Cameras;Layout;Computer graphics;Filters;Image generation;Streaming media;Virtual reality;Biomedical imaging},
	month = {March},
	pages = {195-202},
	title = {Stylized augmented reality for improved immersion},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492774}}

@inproceedings{1492775,
	abstract = {This paper describes the incorporation of realistic occlusion effects into mirror-based, stereoscopic co-location augmented reality display systems. By adding a light-blocking device in the form of an LCD panel underneath the semi-transparent mirror, the view on the physical world can be selectively blocked out such that virtual objects can fully occlude physical objects. Furthermore, by removing areas of the virtual objects rendered on the reflected display, physical objects seen through the semi-transparent mirror and the transmissive LCD panel appear to occlude these virtual objects. We describe the governing principles of the approach, and present an efficient algorithm for the generation of the occlusion masks with the use of vision-based scene reconstruction. Finally, a first prototype implementation of the system is presented.},
	author = {Mulder, J.D.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492775},
	issn = {2375-5334},
	keywords = {Augmented reality;Virtual reality;Mirrors;Computer displays;Liquid crystal displays;Layout;Three dimensional displays;Prototypes;Computer graphics;Costs},
	month = {March},
	pages = {203-208},
	title = {Realistic occlusion effects in mirror-based co-located augmented reality systems},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492775}}

@inproceedings{1492776,
	abstract = {We present a set of cross-dimensional interaction techniques for a hybrid user interface that integrates existing 2D and 3D visualization and interaction devices. Our approach is built around one-and two-handed gestures that support the seamless transition of data between co-located 2D and 3D contexts. Our testbed environment combines a 2D multi-user, multi-touch, projection surface with 3D head-tracked, see-through, head-worn displays and 3D tracked gloves to form a multi-display augmented reality. We address some of the ways in which we can interact with private data in a collaborative, heterogeneous workspace. We also report on a pilot usability study to evaluate the effectiveness and ease of use of the cross-dimensional interactions.},
	author = {Benko, H. and Ishak, E.W. and Feiner, S.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492776},
	issn = {2375-5334},
	keywords = {User interfaces;Three dimensional displays;Two dimensional displays;Augmented reality;Virtual reality;Computer displays;Computer science;Data visualization;Testing;Collaborative work},
	month = {March},
	pages = {209-216},
	title = {Cross-dimensional gestural interaction techniques for hybrid immersive environments},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492776}}

@inproceedings{1492777,
	abstract = {By representing projection of a point at space on a spherical image, called spherical projection, the conventional stereo algorithm can be reformulated. This means that we can acquire the whole surrounding 3D structure of environments for the construction of immersive VR (virtual reality) environment by full view spherical images, called spherical stereo, instead of the conventional approach that uses a normal camera with a limited view to take multiple plane images, reconstructs parts of the environment structure from multiple image pairs, respectively, and finally fuses them as the whole surrounding 3D structure of environments. In this paper we give the concrete approach to realize this spherical stereo method and show the effectiveness of this method.},
	author = {Shigang Li and Fukumori, K.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492777},
	issn = {2375-5334},
	keywords = {Virtual reality;Cameras;Robot vision systems;Image converters;Lenses;Image reconstruction;Fuses;Stereo image processing;Concrete;Chromium},
	month = {March},
	pages = {217-222},
	title = {Spherical stereo for the construction of immersive VR environment},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492777}}

@inproceedings{1492778,
	abstract = {Nasal airway obstruction is a serious and common problem within the field of rhinology. This paper introduces VRhino II, a virtual-reality-based application for the analysis of the airflow through the human nasal cavity. This tool is currently developed within a research project striving to gain better insight into the flow conditions during respiration. The paper's focus lies on a general description of the application's design. Additionally, technical details about the main system components are given.},
	author = {Hentschel, B. and Kuhlen, T. and Bischof, C.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492778},
	issn = {2375-5334},
	keywords = {Visualization;Humans;Surgery;Nose;Geometry;Solid modeling;Aerodynamics;Virtual reality;Computer applications;Content addressable storage},
	month = {March},
	pages = {233-236},
	title = {VRhino II: flow field visualization inside the human nasal cavity},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492778}}

@inproceedings{1492779,
	abstract = {As of today one of the major application domains for augmented reality is the service and maintenance of manufactured products, e.g. cars, planes, large machinery. In this area, augmented reality is used to support service personnel in carrying out their repair tasks by displaying context-sensitive, additional, virtual information in their field of view. One major problem is the creation of such information. In this paper the authors describe a concept, which heavily simplifies the creation of AR based manuals and even allows people without special AR and IT skills to carry out this task.},
	author = {Knopfle, C. and Weidenhausen, J. and Chauvigne, L. and Stock, I.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492779},
	issn = {2375-5334},
	keywords = {Virtual reality;Augmented reality;Documentation;Personnel;Manufactured products;Machinery;Context-aware services;Chromium;Multimedia systems;Computer graphics},
	month = {March},
	pages = {237-240},
	title = {Template based authoring for AR based service scenarios},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492779}}

@inproceedings{1492780,
	abstract = {This application sketch describes a conceptual surgical laser system, designed for incorporation with a surgical robot, that provides haptic and visual feedback. Initial results from a prototype haptic laser system, built with a low-power (noncutting) laser, are also presented.},
	author = {Rizun, Peter R. and Sutherland, Garnette R.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492780},
	issn = {2375-5334},
	keywords = {Laser feedback;Laser surgery;Haptic interfaces;Laser theory;Prototypes;Surface emitting lasers;Force feedback;Robot sensing systems;Distance measurement;Laser modes},
	month = {March},
	pages = {241-244},
	title = {Surgical Laser Augmented with Haptic Feedback and Visible Trajectory},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492780}}

@inproceedings{1492781,
	abstract = {The ability to physically change properties of real objects used in augmented reality (AR) applications is limited. Geometrical properties (shape, size) and appearance (color, texture) of a real object remain unchanged during a single application run. However, an AR system can be used to provide a virtual texture for the real object. The texture can be changed dynamically based on user interactions. The developed AR system includes two components, the "3D Table" and the "Texture Painter". The 3D Table is a table where real objects are placed. The tabletop is used as a projection surface, making it possible to add a context to the real object. The Texture Painter makes it possible to paint on the real object, using a real brush and virtual ink (texture). ARToolkit markers are placed on the 3D Table tabletop to augment the environment with the virtual objects. Markers are either physical (printouts on the tabletop) or virtual (projections). The scene is recorded with a camera and the composed video is projected in real time. The projection shows a virtual environment, real objects painted with virtual ink, and virtual objects positioned where real or virtual ARToolkit markers are placed. The developed system is used in architectural design applications where, due to the different qualities of real architectural models and rendered architectural models, real models are still used. The system was tested at the Academy of Fine Arts in Vienna where it is used as a support tool for architecture students.},
	author = {Matkovic, K. and Psik, T. and Wagner, I. and Gracanin, D.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492781},
	issn = {2375-5334},
	keywords = {Augmented reality;Ink;Shape;Paints;Brushes;Layout;Cameras;Virtual environment;System testing;Art},
	month = {March},
	pages = {245-248},
	title = {Dynamic texturing of real objects in an augmented reality system},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492781}}

@inproceedings{1492782,
	abstract = {The present paper describes an enhanced haptic virtual reality application for geometry education. The proposed application allows the user to create and edit a scene that consists of three-dimensional geometrical objects in order to form and solve complex geometrical problems. The core of the proposed scheme is based on a novel interference detection algorithm, which utilizes implicit surfaces, such as superquadrics, and their analytical description to speed up collision detection between the virtual hand and the virtual environment. Intersection tests are executed utilizing the implicit analytical formulae of the superquadrics. Experimental results demonstrate the high applicability of the proposed application and the huge gain in speed of the proposed collision detection approach when compared to state of the art methods.},
	author = {Moustakas, K. and Nikolakis, G. and Tzovaras, D. and Strintzis, M.G.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492782},
	issn = {2375-5334},
	keywords = {Geometry;Haptic interfaces;Virtual reality;Augmented reality;Documentation;Personnel;Educational products;Manufactured products;Machinery;Context-aware services},
	month = {March},
	pages = {249-252},
	title = {A geometry education haptic VR application based on a new virtual hand representation},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492782}}

@inproceedings{1492783,
	abstract = {We present an application that allows users to interactively visualise data of medical studies. This application enables the users to get a "first view" on the data set, to interact with the data in an intuitive way and to analyse it collaboratively. The goal of these first studies of the data sets is to find relations between measured values.},
	author = {Neumaier, N. and Hinkenjann, A.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492783},
	issn = {2375-5334},
	keywords = {Virtual reality;Application software;Displays;Ergonomics;Product design;Prototypes;Design automation;Virtual environment;User centered design;Computer graphics},
	month = {March},
	pages = {253-255},
	title = {Explorative analysis of medical study data},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492783}}

@inproceedings{1492784,
	abstract = {The ability to physically change properties of real objects used in augmented reality (AR) applications is limited. Geometrical properties (shape, size) and appearance (color, texture) of a real object remain unchanged during a single application run. However, an AR system can be used to provide a virtual texture for the real object. The texture can be changed dynamically based on user interactions. The developed AR system includes two components, the "3D Table" and the "Texture Painter." The 3D Table is a table where real objects are placed. The tabletop is used as a projection surface, making it possible to add a context to the real object. The Texture Painter makes it possible to paint on the real object, using a real brush and virtual ink (texture). ARToolkit markers are placed on the 3D Table tabletop to augment the environment with the virtual objects. Markers are either physical (printouts on the tabletop) or virtual (projections). The scene is recorded with a camera and the composed video is projected in real time. The projection shows a virtual environment, real objects painted with virtual ink, and virtual objects positioned where real or virtual ARToolkit markers are placed. The developed system is used in architectural design applications where, due to the different qualities of real architectural models and rendered architectural models, real models are still used. The system was tested at the Academy of Fine Arts in Vienna where it is used as a support tool for architecture students.},
	author = {Matkovic, Kresimir and Psik, Thomas and Wagner, Ina and Gracanin, Denis},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492784},
	issn = {2375-5334},
	keywords = {Augmented reality;Ink;Shape;Paints;Brushes;Layout;Cameras;Virtual environment;System testing;Art},
	month = {March},
	pages = {257-260},
	title = {Dynamic Texturing of Real Objects in an Augmented Reality System},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492784}}

@inproceedings{1492785,
	abstract = {We introduce the early prototype of the Planar, a novel input/output device designed for application in task areas focusing on the generation and manipulation of 3D data, e.g. CAD or styling. Technically, the Planar offers a spatially aware pen-sensitive display, mounted on an adjustable, scooter-like autonomous platform. The movable screen, with 6 degrees of freedom, can act like a window into 3D virtual environments and allows for efficient 2D and 3D interaction at the same time. Through our research, we have developed an interdisciplinary approach that aims to overcome a variety of drawbacks within many VR installations while extending the concepts of existing conventional systems. Our primary areas of focus include: working environments, forms of interaction, ergonomics, and process integration. In order to demonstrate the potential of the Planar we show a small review and a sketching/annotation application. The overall goal of this work is to contribute to the development of real VR applications.},
	author = {Schoenfelder, R. and Spenling, F.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492785},
	issn = {2375-5334},
	keywords = {Virtual reality;Displays;Ergonomics;User interfaces;Prototypes;Design automation;Virtual environment;User centered design;Computer interfaces;Graphics},
	month = {March},
	pages = {261-264},
	title = {The Planar: an interdisciplinary approach to a VR enabled tool for generation and manipulation of 3D data in industrial environments},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492785}}

@inproceedings{1492789,
	abstract = {Local-lag mechanism can eliminate short-term inconsistencies for collaborative virtual environments (CVEs). However, local-lag mechanism increases the response time and deteriorates the task performance of the system when lag is large. This paper first analyzes the effects of local-lag mechanism on task performance in a pilot experiment. We find that the key reason for the task performance deterioration is that subjects do not know the current position of the virtual object controlled by him/her and how long the lag is. Based on this observation, this paper proposes a method named echo, which gives some cues to subjects and makes them perceive these information. Experiment results indicate that the method can improve the task performance significantly. Especially when lag exceeds 600 ms, almost half of subjects cannot finish the task without the help of echo method, but with the help of the echo method all subjects can finish the task fluently.},
	author = {Ling Chen and Hong Chen and Gencai Chen},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492789},
	issn = {2375-5334},
	keywords = {Delay;Virtual environment;Collaboration;Performance analysis;Virtual reality;Educational institutions;Computer science;Chromium;Multimedia systems;Information systems},
	month = {March},
	pages = {269-272},
	title = {Echo: a method to improve the task performance of CVEs},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492789}}

@inproceedings{1492791,
	abstract = {This paper describes a wearable electronic mnemonics, the iFlashBack system that promotes human memorization process for the action the user performed in the real-world. The goal of the system is to exploit unlimited capacity of the human brain for storing the variety of information, by assisting memorization using recorded video and related information captured during interaction process to real-world objects. Memorization reinforcement is achieved by drawing attention, promoting rehearsal by flashing a video of interaction, and organizing memory by providing relevant information. The action of the user is captured by a cap-mounted miniature camera, an RFID reader, and arm posture sensors worn by the user. For video-aided rehearsal, a small HMD presents the captured scenes immediately after the user's interaction behavior and/or at appropriate times in the user's activity. We conducted basic experiments to demonstrate our idea. Two memory tasks - to change cup positions within grids and to memorize random dot images - were performed by subjects. The results suggested that the iFlashBack system could reinforce memorization of the user's action by motion-controlled video aided rehearsal.},
	author = {Hirose, Y. and Ikei, Y. and Hirota, K. and Hirose, M.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492791},
	issn = {2375-5334},
	keywords = {Computer interfaces;Cameras;Radiofrequency identification;Mobile communication;Organizing;Wearable sensors;Layout;Chromium;Human computer interaction;Multimedia systems},
	month = {March},
	pages = {273-276},
	title = {iFlashBack: a wearable electronic mnemonics to retain episodic memory visually real by video aided rehearsal},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492791}}

@inproceedings{1492793,
	abstract = {Teleoperation is a very useful technique in the case of carrying out tasks in hazardous and/or remote sites. In order to carry out the tasks in high operational efficiency, a video system to display the workspace for an operator is important. In this study, we developed a spatial multi-resolution stereoscopic video system which has variable convergence point, and confirmed the effect of keeping an overlapped area large at the working point by varying the convergence point continuously with the spatial multi-resolution stereoscopic video system. As the results of this experiment, it was clarified that the operational efficiency with teleoperation through the spatial multi-resolution stereoscopic video system was improved by keeping an overlapped area large, which resulted from varying the convergence point. That is, in order to carry out tasks with teleoperation in high operational efficiency, the system is required to vary its convergence point to keep an overlapped area large.},
	author = {Ienaga, T. and Matsunaga, K. and Shidoji, K. and Otsuru, M. and Araki, S. and Matsuki, Y.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492793},
	issn = {2375-5334},
	keywords = {Convergence;Multiresolution analysis;Cameras;Information science;Displays;Virtual reality;Robot vision systems;Spatial resolution;Switches;Information technology},
	month = {March},
	pages = {277-280},
	title = {An effect of a large overlapped area of stereo pairs at the working point on a spatial multi-resolution stereoscopic video system},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492793}}

@inproceedings{1492796,
	abstract = {This article describes an integration of knowledge based techniques into simulative virtual reality (VR) applications motivated using a virtual construction task. An abstract knowledge representation layer (KRL) provides a base formalism for the integration of simulation semantics. The KRL approach is demonstrated using a generalized scene graph representation which introduces an abstract definition and implementation of geometric node interrelations.},
	author = {Latoschik, M.E. and Biermann, P. and Wachsmuth, I.},
	booktitle = {IEEE Proceedings. VR 2005. Virtual Reality, 2005.},
	date-added = {2024-03-18 02:28:49 -0400},
	date-modified = {2024-03-18 02:28:49 -0400},
	doi = {10.1109/VR.2005.1492796},
	issn = {2375-5334},
	keywords = {Virtual reality;Artificial intelligence;Knowledge representation;Computational modeling;Computer graphics;Data structures;Matrix decomposition;Layout;Chromium;Intelligent networks},
	month = {March},
	pages = {283-286},
	title = {High-level semantics representation for intelligent simulative environments},
	year = {2005},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2005.1492796}}

@inproceedings{1310049,
	abstract = {We present a real-time hybrid tracking system that integrates gyroscopes and line-based vision tracking technology. Gyroscope measurements are used to predict orientation and image line positions. Gyroscope drift is corrected by vision tracking. System robustness is achieved by using a heuristic control system to evaluate measurement quality and select measurements accordingly. Experiments show that the system achieves robust, accurate, and real-time performance for outdoor augmented reality.},
	author = {Jiang, B. and Neumann, U. and Suya You},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310049},
	issn = {1087-8270},
	keywords = {Robustness;Augmented reality;Machine vision;Computer vision;Gyroscopes;Cameras;Sensor systems;Real time systems;Position measurement;Tracking},
	month = {March},
	pages = {3-275},
	title = {A robust hybrid tracking system for outdoor augmented reality},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310049}}

@inproceedings{1310050,
	abstract = {This paper describes the implementation of hybrid tracking software that is capable of operating both indoors and outdoors. Commercially available outdoor trackers are combined with an indoor tracker based on fiducial markers and video cameras. The position and orientation of the user's body is measured in physical world coordinates at all times, and tracking of the hands is performed relative to the head. Each of the tracking components is designed to easily scale to large indoor and outdoor environments, supporting applications such as our existing Tinmith-Metro modelling system. This paper focuses on the integration of the indoor tracking subsystem. By using features such as multiple video cameras and combining various tracking data the system can produce results that meet the requirements for many mobile mixed reality applications.},
	author = {Piekarski, W. and Avery, B. and Thomas, B.H. and Malbezin, P.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310050},
	issn = {1087-8270},
	keywords = {Augmented reality;Cameras;Costs;Global Positioning System;Virtual reality;Application software;Magnetic heads;Wearable computers;Mobile computing;Firewire},
	month = {March},
	pages = {11-276},
	title = {Integrated head and hand tracking for indoor and outdoor augmented reality},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310050}}

@inproceedings{1310051,
	abstract = {This paper presents a three-level tracker calibration system that greatly reduces errors in tracked position and orientation. The first level computes an error-minimizing rigid body transform that eliminates the need for precise alignment of a tracker base frame. The second corrects for field warp by interpolating correction values stored with vertices in a tetrahedrization of warped space. The third performs an alternative field warp calibration by interpolating corrections in the parameter space of a tricubic spline model of field warp. The system is evaluated for field warp calibration near a passive-haptic panel in both low-warp and high-warp environments. The spline method produces the most accurate results, reducing median position error by over 90% and median orientation error by over 80% when compared to the use of only a rigid body transform.},
	author = {Borst, C.W.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310051},
	issn = {1087-8270},
	keywords = {Calibration;Spline;Computer vision;Polynomials;Computer errors;Tracking;Magnetic field measurement;Table lookup;Interpolation;Error correction},
	month = {March},
	pages = {19-26},
	title = {Tracker calibration using tetrahedral mesh and tricubic spline models of warp},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310051}}

@inproceedings{1310052,
	abstract = {Multimodality often exhibits synergistic effects: each modality compliments and compensates for other modalities in transferring coherent, unambiguous, and enriched information for higher interaction efficiency and improved sense of presence. In this paper, we explore one such phenomenon: a positive interaction among the geometric field of view, proprioceptive interaction, and tactile feedback. We hypothesize that, with proprioceptive interaction and tactile feedback, the geometric field of view and thus visibility can be increased such that it is larger than the physical field of view, without causing a significant distortion in the user's distance perception. This, in turn, would further help operation of the overall multimodal interaction scheme as the user is more likely to receive the multimodal feedback simultaneously. We tested our hypothesis with an experiment to measure the user's change in distance perception according to different values of egocentric geometric field of view and feedback conditions. Our experimental results have shown that, when coupled with physical interaction, the GFOV could be increased by up to 170 percent of the physical field of view without introducing significant distortion in distance perception. Second, when tactile feedback was introduced, in addition to visual and proprioceptive cues, the GFOV could be increased by up to 200 percent. The results offer a useful guideline for effectively utilizing of modality compensation and building multimodal interfaces for close range spatial tasks in virtual environments. In addition, it demonstrates one way to overcome the shortcomings of the narrow (physical) fields of views of most contemporary HMDs.},
	author = {Ungyeon Yang and Jounghyun Kim, G.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310052},
	issn = {1087-8270},
	keywords = {Feedback;Virtual reality;Space technology;Layout;Telecommunications;Computer science;Testing;Distortion measurement;Guidelines;Virtual environment},
	month = {March},
	pages = {27-34},
	title = {Increasing the effective egocentric field of view with proprioceptive and tactile feedback},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310052}}

@inproceedings{1310053,
	abstract = {This paper describes the underlying concepts and the technical implementation of a system for resolving multi-modal references in virtual reality (VR). In this system the temporal and semantic relations intrinsic to referential utterances are expressed as a constraint satisfaction problem, where the propositional value of each referential unit during a multimodal dialogue updates incrementally the active set of constraints. As the system is based on findings of human cognition research it also regards, e.g., constraints implicitly assumed by human communicators. The implementation takes VR related real-time and immersive conditions into account and adapts its architecture to well known scene-graph based design patterns by introducing a so-called reference resolution engine. Regarding the conceptual work as well as regarding the implementation, special care has been taken to allow further refinements and modifications to the underlying resolving processes on a high level basis.},
	author = {Pfeiffer, T. and Latoschik, M.E.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310053},
	issn = {1087-8270},
	keywords = {Virtual reality;Humans;Cognition;Engines;Shape;Computer graphics;Virtual environment;Artificial intelligence;Layout;Facial animation},
	month = {March},
	pages = {35-277},
	title = {Resolving object references in multimodal dialogues for immersive virtual environments},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310053}}

@inproceedings{1310054,
	abstract = {Most attempts to realize an olfactory display have involved capturing and synthesizing the odor, processes that still pose many challenging problems. These difficulties are mainly due to the mechanism of human olfaction, in which a set of so-called "primary odors" has not been found. Instead, we focus on spatio-temporal control of odor rather than synthesizing odor itself. Many existing interactive olfactory displays simply diffuse the scent into the air, which does not provide the ability of spatio-temporal control of olfaction. Recently, however, several researchers have developed olfactory displays that inject scented air under the nose through tubes. On the analogy of visual displays, these systems correspond to head-mounted displays (HMD). These yield a solid way to achieve spatio-temporal control of olfactory space, but they require the user to wear something on his or her face. Here, we propose an unencumbering olfactory display that does not require the user to attach anything on the face. It works by projecting a clump of scented air from a location near the user's nose through free space. We also aim to display a scent to the restricted space around a specific user's nose, rather than scattering scented air by simply diffusing it into the atmosphere. To implement this concept, we used an "air cannon" that generates toroidal vortices of the scented air. We conducted a preliminary experiment to examine this method's ability to display scent to a restricted space. The results show that we could successfully display incense to the target user. Next, we constructed prototype systems. We could successfully bring the scented air to a specific user by tracking the nose position of the user and controlling the orientation of the air cannon to the user's nose.},
	author = {Yanagida, Y. and Kawato, S. and Noma, H. and Tomono, A. and Tesutani, N.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310054},
	issn = {1087-8270},
	keywords = {Olfactory;Displays;Nose;Humans;Control system synthesis;Solids;Scattering;Atmosphere;Prototypes;Target tracking},
	month = {March},
	pages = {43-50},
	title = {Projection based olfactory display with nose tracking},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310054}}

@inproceedings{1310055,
	abstract = {The food simulator is a haptic interface that presents biting force. The taste of food arises from a combination of chemical, auditory, olfactory and haptic sensation. Haptic sensation while eating has been an ongoing problem in taste display. The food simulator generates a force on the user's teeth as an indication of food texture. The device is composed of four linkages. The mechanical configuration of the device is designed such that it will fit into the mouth, with a force sensor attached to the end effector. The food simulator generates a force representing the force profile captured from the mouth of a person biting real food. The device has been integrated with auditory and chemical display for multi-modal sensations in a taste the food simulator has been tested on a large number of participants. The results indicate that the device has succeeded in presenting food texture as well as chemical taste.},
	author = {Iwata, H. and Yano, H. and Uemura, T. and Moriya, T.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310055},
	issn = {1087-8270},
	keywords = {Haptic interfaces;Force sensors;Chemicals;Auditory displays;Mouth;Olfactory;Teeth;Couplings;End effectors;Testing},
	month = {March},
	pages = {51-57},
	title = {Food simulator: a haptic interface for biting},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310055}}

@inproceedings{1310056,
	abstract = {This paper describes in detail the design, development, and evaluation of the TWISTER III (Telexistence Wide-angle Immersive STEReoscope) and demonstrates how this system can display immersive three-dimensional full-color and live motion pictures without the need for special eye-wear. The device works as a cylindrical display by rotating 30 display units around an observer and presenting time-varying patterns, while immersive autostereoscopic vision is achieved by employing a "rotating parallax barrier" method. After explaining the principle, we discuss the designs and implementations for maximum performance in various aspects of the display. We also evaluate the display.},
	author = {Tanaka, K. and Hayashi, J. and Inami, M. and Tachi, S.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310056},
	issn = {1087-8270},
	keywords = {Virtual reality;Motion pictures;Holography;Optical arrays;Computer displays;Light emitting diodes;Information science;Three dimensional displays;Holographic optical components;Optical sensors},
	month = {March},
	pages = {59-278},
	title = {TWISTER: an immersive autostereoscopic display},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310056}}

@inproceedings{1310057,
	abstract = {This paper discusses the use of omnidirectional stereo for panoramic virtual environments. It presents two methods for real-time rendering of omnistereo images. Conventional perspective stereo is correct everywhere in the visual field, but only in one view direction. Omnistereo is correct in every view direction, but only in the center of the visual field, degrading in the periphery. Omnistereo images make it possible to use wide field of view virtual environment display systems-like the CAVE/spl trade/-without head tracking, and still show correct stereoscopic depth over the full 360/spl deg/ viewing circle. This allows the use of these systems as true multi-user displays, where viewers can look around and browse a panoramic scene independently. Because there is no need to rerender the image according to view direction, we can also use this technique to present static omnistereo images, generated by offline rendering or real image capture, in panoramic displays. We have implemented omnistereo in a four-sided CAVE/spl trade/ and in a 240/spl deg/ i-Con/spl trade/ curved screen projection system. Informal user evaluation confirms that omnistereo images present a seamless image with correct stereoscopic depth in every view direction without head tracking.},
	author = {Simon, A. and Smith, R.C. and Pawlicki, R.R.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310057},
	issn = {1087-8270},
	keywords = {Virtual environment;Displays;Layout;Head;Rendering (computer graphics);Graphics;Cameras;Virtual reality;Research and development;Degradation},
	month = {March},
	pages = {67-279},
	title = {Omnistereo for panoramic virtual environment display systems},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310057}}

@inproceedings{1310058,
	abstract = {We present a collaborative desktop virtual and augmented reality system with a shared physical workspace: the joint space station. It is constructed out of multiple, single-user near-field virtual or augmented reality stations using mirror-based displays. The joint space station provides high-resolution stereoscopic images and direct 3D interaction to all participants. The environment is modular, flexible, low-cost, and can be used under office working conditions. We describe the design, implementation, and application of a prototype joint space station, and discuss the merits and limitations of the concept.},
	author = {Mulder, J.D. and Boscker, B.R.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310058},
	issn = {1087-8270},
	keywords = {Collaborative work;Virtual reality;Displays;Space stations;Augmented reality;Visualization;Prototypes;Industrial training;Space technology;Costs},
	month = {March},
	pages = {75-280},
	title = {A modular system for collaborative desktop VR/AR with a shared workspace},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310058}}

@inproceedings{1310059,
	abstract = {Collision detection for dynamic objects in distributed virtual environments is still an open research topic. The problems of network latency and available network bandwidth prevent exact common solutions. The consistency-throughput tradeoff states that a distributed virtual environment cannot be consistent and highly dynamic at the same time. Remote object visualization is used to extrapolate and predict the movement of remote objects reducing the bandwidth required for good approximations of the remote objects. Few update messages aggravate the effect of network latency for collision detection. In this paper, new approach extending remote object visualization techniques is demonstrated to improve the results of collision detection in distributed virtual environments. We showed how this can significantly reduce the approximation errors caused by remote object visualization techniques. This is done by predicting collisions between remote objects and adaptively changing the parameters of these techniques.},
	author = {Ohlenburg, J.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310059},
	issn = {1087-8270},
	keywords = {Virtual environment;Delay;Object detection;Visualization;Avatars;Bandwidth;Virtual reality;Intelligent networks;Information technology;Throughput},
	month = {March},
	pages = {83-90},
	title = {Improving collision detection in distributed virtual environments by adaptive collision prediction tracking},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310059}}

@inproceedings{1310060,
	abstract = {Free-viewpoint video is a promising technology for next-generation virtual and augmented reality applications. Our goal is to enhance collaborative VR applications with 3D video-conferencing features. In this paper, we propose a 3D video streaming technique which can be deployed in telepresence environments. The streaming characteristics of real-time 3D video sequences are investigated under various system and networking conditions. We introduce several encoding techniques and analyze their behavior with respect to resolution, bandwidth and inter-frame jitter. Our 3D video pipeline uses point samples as basic primitives and is fully integrated with a communication framework handling acknowledgment information for reliable network transmissions and application control data. The 3D video reconstruction process dynamically adapts to processing and networking bottlenecks. Our results show that a reliable transmission of our pixel-based differential prediction encoding leads to the best performance in terms of bandwidth, but is also quite sensitive to packet losses. A redundantly encoded stream achieves better results in presence of burst losses and seamlessly adapts to varying network throughput.},
	author = {Lamboray, E. and Wurmlin, S. and Gross, M.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310060},
	issn = {1087-8270},
	keywords = {Streaming media;Encoding;Bandwidth;Telecommunication network reliability;Augmented reality;Collaboration;Virtual reality;Real time systems;Video sequences;Jitter},
	month = {March},
	pages = {91-281},
	title = {Real-time streaming of point-based 3D video},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310060}}

@inproceedings{1310061,
	abstract = {Technological advances in virtual environments facilitate the creation of distributed collaborative environments, in which the distribution of three-dimensional content at remote locations allows efficient and effective communication of ideas. One of the challenges in distributed shared environments is maintaining a consistent view of the shared information, in the presence of inevitable network delays and variable bandwidth. A consistent view in a shared 3D scene may significantly increase the sense of presence among participants and improve their interactivity. This paper introduces an adaptive scene synchronization algorithm and a framework for integration of the algorithm in a distributed real-time virtual environment. In spite of significant network delays, results show that objects can be synchronous in their viewpoint at multiple remotely located sites. Furthermore residual asynchronicity is quantified as a function of network delays and scalability.},
	author = {Hamza-Lup, F.G. and Rolland, J.P.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310061},
	issn = {1087-8270},
	keywords = {Layout;Virtual reality;Synchronization;Virtual environment;Optical computing;Optical sensors;Collaboration;Delay;Computer science;Communication effectiveness},
	month = {March},
	pages = {99-106},
	title = {Adaptive scene synchronization for virtual and mixed reality environments},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310061}}

@inproceedings{1310063,
	abstract = {With the recent advances in ray tracing technology, high-quality image generation at interactive rates has finally become a reality. As a consequence ray tracing likely plays a larger role in visualization systems, enabling the conception and creation of completely new interactive graphics applications. In this paper, we describe the design and development of a ray tracing-based VRML browser and editor as a case study of such an application. It exploits all advantages of ray tracing, including physically-correct plug and play shading and support for large models. In particular we demonstrate how to overcome the limitations that result from existing scene graph libraries and data exchange formats still being targeted towards rasterization technology. Also we show how to extend the VRML lighting model and to optimize scene graph handling for ray tracing.},
	author = {Dietrich, A. and Wald, I. and Wagner, M. and Slusallek, P.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310063},
	issn = {1087-8270},
	keywords = {Layout;Ray tracing;Engines;Virtual reality;Visualization;Libraries;Design optimization;Manufacturing industries;Process design;Hardware},
	month = {March},
	pages = {109-282},
	title = {VRML scene graphs on an interactive ray tracing engine},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310063}}

@inproceedings{1310064,
	abstract = {We present a fast algorithm for continuous collision detection between a moving avatar and its surrounding virtual environment. We model the avatar as an articulated body using line-skeletons with constant offsets and the virtual environment as a collection of polygonized objects. Given the position and orientation of the avatar at discrete time steps, we use an arbitrary in-between motion to interpolate the path for each link between discrete instances. We bound the swept-space of each link using a swept volume (SV) and compute a bounding volume hierarchy to cull away links that are not in close proximity to the objects in the virtual environment. We generate the SV's of the remaining links and use them to check for possible interferences and estimate the time of collision between the surface of the SV and the objects in the virtual environment. Furthermore, we use graphics hardware to perform collision queries on the dynamically generated swept surfaces. Our overall algorithm requires no precomputation and is applicable to general articulated bodies. We have implemented the algorithm on a 2.4 GHz Pentium IV PC with NVIDIA GeForce FX 5800 graphics card and applied it to an avatar with 16 links, moving in a virtual environment composed of hundreds of thousands of polygons. Our prototype system is able to detect all contacts between the moving avatar and the environment in 1.0 - 30 milliseconds.},
	author = {Redon, S. and Kim, Y.J. and Lin, M.C. and Manocha, D. and Templeman, J.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310064},
	issn = {1087-8270},
	keywords = {Avatars;Virtual environment;Interference;Virtual reality;Partitioning algorithms;Gamma ray detection;Gamma ray detectors;Graphics;Biological system modeling;Hardware},
	month = {March},
	pages = {117-283},
	title = {Interactive and continuous collision detection for avatars in virtual environments},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310064}}

@inproceedings{1310065,
	abstract = {We have designed a mobile -PDA-based- interface for real-time control of virtual characters in multiuser semi-immersive virtual environments - using a large rear-projection screen. The proof-of-concept implementation we present shows the potential of handheld devices as powerful interfaces to virtual reality applications. This technique eliminates the display of floating menus and other widgets over the simulation screen. A brief discussion on the advantages and disadvantages of using a handheld for 3D interaction is presented as well.},
	author = {Gutierrez, M. and Vexo, F. and Thalmann, D.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310065},
	issn = {1087-8270},
	keywords = {Animation;Collaboration;Virtual reality;Magnetic heads;Large screen displays;Cameras;Disaster management;Navigation;Magnetic sensors;Speech recognition},
	month = {March},
	pages = {125-284},
	title = {The mobile animator: interactive character animation in collaborative virtual environments},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310065}}

@inproceedings{1310066,
	abstract = {Interactive applications on mobile devices often reduce data fidelity to adapt to resource constraints and variable user preferences. In virtual reality applications, the problem of reducing scene graph fidelity can be stated as a combinatorial optimization problem, where a part of the scene graph with maximum fidelity is chosen such that the resources it requires are below a given threshold and the hierarchical relationships are maintained. The problem can be formulated as a variation of the tree knapsack problem, which is known to be NP-hard. For this reason, solutions to this problem result in a tradeoff that affects user navigation. On one hand, exact solutions provide the highest fidelity but may take long time to compute. On the other hand, greedy solutions are fast but lack high fidelity. We present a simplification architecture that allows the exploration of such navigation tradeoffs. This is achieved by a formulating the problem in a generic way and developing software components that allow the dynamic selection of algorithms and constraints. The experimental results show that the architecture is flexible and supports dynamic reconfiguration.},
	author = {Correa, C.D. and Marsic, I.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310066},
	issn = {1087-8270},
	keywords = {Navigation;Virtual reality;Layout;Mobile computing;Computer architecture;Application software;Tree graphs;Network servers;Bandwidth;Heuristic algorithms},
	month = {March},
	pages = {133-140},
	title = {A simplification architecture for exploring navigation tradeoffs in mobile VR},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310066}}

@inproceedings{1310067,
	abstract = {Data are reported for symptoms of virtual environment (VE) sickness that arose in 10 behavioral experiments. In total, 134 participants took part in the experiments and were immersed in VEs for approximately 150 hours. Nineteen of the participants reported major symptoms and two were physically sick. The tasks that participants ' performed ranged from manipulating virtual objects that they "held" in their hands, to traveling distances of 10 km or more while navigating virtual mazes. The data are interpreted within a framework provided by the virtual environment description and classification system. Environmental dimensions and visual complexity had little effect on the severity of participants ' symptoms. Long periods of immersion tended to produce major ocular-motor symptoms. Nausea was affected by the type of movement made to control participants ' view, and was particularly severe when participants had to spend substantial amounts of time (3%) looking steeply downwards at their virtual feet. Contrary to expectations, large rapid movements had little effect on most participants, and neither did movements that were not under participants ' direct control.},
	author = {Ruddle, R.A.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310067},
	issn = {1087-8270},
	keywords = {Virtual environment;Large screen displays;Virtual reality;Navigation;Biomedical monitoring;Capacitive sensors;Proposals;Performance evaluation;Motion measurement;Time measurement},
	month = {March},
	pages = {141-285},
	title = {The effect of environment characteristics and user interaction on levels of virtual environment sickness},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310067}}

@inproceedings{1310068,
	abstract = {We compared four different methods of travel in an immersive virtual environment and their effect on cognition using a between-subjects experimental design. The task was to answer a set of questions based on Crook's condensation of Bloom's taxonomy to assess the participants' cognition of a virtual room with respect to knowledge, understanding and application, and higher mental processes. Participants were also asked to draw a sketch map of the testing virtual environment and the objects within it. Users' sense of presence was measured using the Steed-Usoh-Slater presence questionnaire. Our results suggest that for applications where problem solving and interpretation of material is important, or where opportunity to train is minimal, then having a large tracked space so that the participant can physically walk around the virtual environment provides benefits over common virtual travel techniques.},
	author = {Zanbaka, C. and Babu, S. and Xiao, D. and Ulinski, A. and Hodges, L.F. and Lok, B.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310068},
	issn = {1087-8270},
	keywords = {Cognition;Legged locomotion;Space technology;Virtual environment;Space exploration;Tracking;Virtual reality;Design for experiments;Taxonomy;Testing},
	month = {March},
	pages = {149-286},
	title = {Effects of travel technique on cognition in virtual environments},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310068}}

@inproceedings{1310069,
	abstract = {The benefits of immersive visualization are primarily anecdotal; there have been few controlled user studies that have attempted to quantify the added value of immersion for problems requiring the manipulation of virtual objects. This research quantifies the added value of immersion for a real-world industrial problem: oil well-path planning. An experiment was designed to compare human performance between an immersive virtual environment (IVE) and a desktop workstation. This work presents the results of sixteen participants who planned the paths of four oil wells. Each participant planned two well-paths on a desktop workstation with a stereoscopic display and two well-paths in a CAVE/spl trade/-like IVE. Fifteen of the participants completed well-path editing tasks faster in the IVE than in the desktop environment. The increased speed was complimented by a statistically significant increase in correct solutions in the IVE. The results suggest that an IVE allows for faster and more accurate problem solving in a complex three-dimensional domain.},
	author = {Gruchalla, K.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310069},
	issn = {1087-8270},
	keywords = {Workstations;Displays;Navigation;Virtual environment;Buildings;Visualization;Petroleum;Graphics;Humans;Layout},
	month = {March},
	pages = {157-164},
	title = {Immersive well-path editing: investigating the added value of immersion},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310069}}

@inproceedings{1310070,
	abstract = {In this paper, we discuss application possibilities of augmented reality technologies in the field of mobility support for the deaf blind. We propose the navigation system called virtual leading blocks for the deaf-blind, which consists of a wearable interface for Finger-Braille, one of the commonly used communication methods among deaf-blind people in Japan, and a ubiquitous environment for barrier-free application, which consists of floor-embedded active radio-frequency identification (RFID) tags. The wearable Finger-Braille interface using two Linux-based wristwatch computers has been developed as a hybrid interface of verbal and nonverbal communication in order to inform users of their direction and position through the tactile sensation. We propose the metaphor of "watermelon splitting" for navigation by this system and verify the feasibility of the proposed system through experiments.},
	author = {Amemiya, T. and Yamashita, J. and Hirota, K. and Hirose, M.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310070},
	issn = {1087-8270},
	keywords = {Deafness;RFID tags;Space technology;Radio navigation;Augmented reality;Application software;Radio frequency;Radiofrequency identification;Active RFID tags;Computer interfaces},
	month = {March},
	pages = {165-287},
	title = {Virtual leading blocks for the deaf-blind: a real-time way-finder by verbal-nonverbal hybrid interface and high-density RFID tag space},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310070}}

@inproceedings{1310071,
	abstract = {Existing navigation techniques do not scale well to large virtual worlds. We present a new technique, navigation with place representations and visible landmarks that scales from town-sized to planet-sized worlds. Visible landmarks make distant landmarks visible and allow users to travel relative to those landmarks with a single gesture. Actual and symbolic place representations allow users to detect and travel to more distant locations with a small number of gestures. The world's semantic place hierarchy determines which visible landmarks and place representations users can see at any point in time. We present experimental results demonstrating that our technique allows users to navigate more efficiently than a modified panning and zooming W1M, completing within-place navigation tasks 22% faster and between-place tasks 38% faster on average.},
	author = {Pierce, J.S. and Pausch, R.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310071},
	issn = {1087-8270},
	keywords = {Navigation;Virtual reality;USA Councils},
	month = {March},
	pages = {173-288},
	title = {Navigation with place representations and visible landmarks},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310071}}

@inproceedings{1310072,
	abstract = {The fundamental question for an information-rich virtual environment is how to access and display abstract information. We investigated two existing navigation techniques: hand-centered object manipulation extending ray-casting (HOMER) and go-go navigation, and two text layout techniques: within-the-world display (WWD) and heads-up display (HUD). Four search tasks were performed to measure participants' performance in a densely packed environment. HUD enabled significantly better performance than WWD and the go-go technique enabled better performance than the HOMER technique for most of the tasks. We found that using HOMER navigation combined with the WWD technique was significantly worse than other combinations for difficult naive search tasks. Users also preferred the combination of go-go and HUD for all tasks.},
	author = {Jian Chen and Pyla, P.S. and Bowman, D.A.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310072},
	issn = {1087-8270},
	keywords = {Testing;Navigation;Virtual environment;Two dimensional displays;Computer displays;Animation;Virtual reality;Augmented reality;Computer science;Performance evaluation},
	month = {March},
	pages = {181-289},
	title = {Testbed evaluation of navigation and text display techniques in an information-rich virtual environment},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310072}}

@inproceedings{1310074,
	abstract = {Virtual environments and artificial worlds are becoming multi-user, complex, and long lasting. Someone who was away from the environment for a while may wish, to be informed of interesting events that happened during her absence without watching hours or even days of interaction. A movie is the natural medium for such a summary. A t the end of a long interaction, participants may wish for such a movie as a keepsake. However, creating a movie summary of a given set of events is delicate, complicated, and time-consuming. In this paper we discuss some of the issues involved in creating a tool that can create such movie summaries automatically. We describe the stages in transforming a chronicle of events into a movie and present an implementation of such, a system. We show results of transforming a log from a life-simulation game to a movie script.},
	author = {Friedman, D. and Shamir, A. and Feldman, Y.A. and Dagan, T.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310074},
	issn = {1087-8270},
	keywords = {Motion pictures;Virtual environment;Computer science;Educational institutions;Production;Virtual reality;Watches;Displays;Layout;USA Councils},
	month = {March},
	pages = {191-290},
	title = {Automated creation of movie summaries in interactive virtual environments},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310074}}

@inproceedings{1310075,
	abstract = {Repairing severe human skull injuries requires customized cranial implants, and current visualization research aims to develop a new approach to create these implants. Following pre-surgical design techniques pioneered at the University of Illinois at Chicago (VIC) in 1996, researchers have developed an immersive cranial implant application incorporating haptic force feedback and augmented reality. The application runs on the personal augmented reality immersive system (PARIS/spl trade/), allowing the modeler to see clearly both his hands and the virtual workspace. The strengths of multiple software libraries are maximized to simplify development. This research lays the foundation to eventually replace the traditional modeling and evaluation processes.},
	author = {Scharver, C. and Evenhouse, R. and Johnson, A. and Leigh, J.},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310075},
	issn = {1087-8270},
	keywords = {Cranial;Implants;Prototypes;Application software;Augmented reality;Humans;Skull;Injuries;Visualization;Haptic interfaces},
	month = {March},
	pages = {199-291},
	title = {Pre-surgical cranial implant design using the PARIS/spl trade/ prototype},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310075}}

@inproceedings{1310076,
	abstract = {In addition to the intended functionality of the product, its affective properties (or "Kansei") have emerged as important evaluation criteria for the successful marketing of the product. Recently, "immersive" virtual reality systems have been suggested as an ideal platform for affective analysis of an evolving design because of among other things, the natural style of interaction they offer when examining the product. In this paper, we compare the effectiveness of three types of virtual environments for evaluating the affective properties of mobile phones to that of the real. Each virtual environment offers different degrees of realism in terms of visual, aural, and tactile aspects. Our experiment has shown that the virtual affective evaluation results correlated very highly with that of the real, and but no statistically significant difference could be found between the three systems. This finding was contrary to our initial thought and the conventional notion that the characteristics of immersive virtual reality systems would contribute to making it a better platform for virtual evaluation of product designs. Thus, it goes to say that employing immersive systems is not necessarily cost effective solution for affective analysis of product designs (desktop VR system suffices).},
	author = {Sangyoon Lee and Tian Chen and Jongseo Kim and Kim, G.J. and Sungho Han and Zhi-geng Pan},
	booktitle = {IEEE Virtual Reality 2004},
	date-added = {2024-03-18 02:28:43 -0400},
	date-modified = {2024-03-18 02:28:43 -0400},
	doi = {10.1109/VR.2004.1310076},
	issn = {1087-8270},
	keywords = {Product design;Virtual reality;Virtual environment;Mobile handsets;Rendering (computer graphics);Design automation;Graphics;Laboratories;Character generation;Costs},
	month = {March},
	pages = {207-292},
	title = {Affective property evaluation of virtual product designs},
	year = {2004},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2004.1310076}}

@inproceedings{1191115,
	abstract = {We identified an important issue when supporting a large scale networked virtual environment (NVE) with a server cluster. This issue is similar to the process migration issue on the parallel computing study and we refer it as the avatar migration problem. That is, when an avatar of an NVE is moving from one region managed by a server to another region managed by a different server, the client site may perceive abrupt screen change due to the different contents managed by these two servers. This paper proposes equations to solve this problem and elaborates the proposed avatar migration mechanism with state diagrams. The implementing architecture is also given in this paper. Our experiments that successfully show the efficiency of the proposed mechanism are given at the last.},
	author = {Jiung-yao Huang and Yi-chang Du and Chien-Min Wang},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191115},
	issn = {1087-8270},
	keywords = {Avatars;Network servers;Content management;Bandwidth;Electronic mail;Scalability;Filters;Computer networks;Information science;Large-scale systems},
	month = {March},
	pages = {7-14},
	title = {Design of the server cluster to support avatar migration},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191115}}

@inproceedings{1191116,
	abstract = {The Syzygy software library consists of tools for programming VR applications on PC clusters. Since the PC cluster environment presents application development constraints, it is impossible to simultaneously optimize for efficiency, flexibility, and portability between the single-computer and cluster cases. Consequently Syzygy includes two application frameworks: a distributed scene graph framework for rendering a single application's graphics database on multiple rendering clients, and a master/slave framework for applications with multiple synchronized instances. Syzygy includes a simple distributed OS and supports networked input devices, sound renderers, and graphics renderers, all built on a robust networking layer.},
	author = {Schaeffer, B. and Goudeseune, C.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191116},
	issn = {1087-8270},
	keywords = {Virtual reality;Rendering (computer graphics);Application software;Graphics;Software libraries;Constraint optimization;Layout;Distributed databases;Master-slave;Robustness},
	month = {March},
	pages = {15-22},
	title = {Syzygy: native PC cluster VR},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191116}}

@inproceedings{1191117,
	abstract = {The full power of mobile augmented and virtual reality systems is realized when these systems are connected to one another to immersive virtual environments, and to remote information servers. Connections are usually made through wireless networks. However, wireless networks cannot guarantee connectivity and their bandwidth can be highly constrained. The authors present a robust event-based data distribution mechanism for mobile augmented reality and virtual environments. It is based on replicated databases, pluggable networking protocols, and communication channels. We demonstrate the mechanism in the Battlefield Augmented Reality System (BARS) situation awareness system, which is composed of several mobile augmented reality systems, immersive and desktop-based virtual reality systems, a 2D map-based multi-modal system, handheld PCs, and other sources of information.},
	author = {Brown, D. and Julier, S. and Baillot, Y. and Livingston, M.A.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191117},
	issn = {1087-8270},
	keywords = {Collaboration;Augmented reality;Virtual environment;Virtual reality;Wireless networks;Network servers;Bandwidth;Robustness;Databases;Protocols},
	month = {March},
	pages = {23-29},
	title = {An event-based data distribution mechanism for collaborative mobile augmented reality and virtual environments},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191117}}

@inproceedings{1191119,
	abstract = {The so-called "blue-c" is a novel three sided immersive projection system, which has been especially designed to support telecollaborative teamwork (M. Gross et al., 2000). Therefore, special projection screens are used, which can be switched to a transparent state electrically. The user is captured with sixteen cameras in total, standing outside of the projection room. These images are then processed to create a 3D representation of the user, which can be projected in a second installation. The whole installation has been engineered and implemented at the ETH Zurich (W. Elpass et al.). A special active stereo projection system, based on LCD projectors with additional shutters proved to work very well at a very competitive price. The paper describes the background, the decisions made and the problems that had to be overcome.},
	author = {Spagno, C.P. and Kunz, A.M.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191119},
	issn = {1087-8270},
	keywords = {Cameras;Virtual reality;Liquid crystal displays;Product development;Ice;Data visualization;Cables;Image generation;Electrical capacitance tomography;Inductors},
	month = {March},
	pages = {37-44},
	title = {Construction of a three-sided immersive telecollaboration system},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191119}}

@inproceedings{1191120,
	abstract = {The authors introduce their research for realizing a 3D video display system in a very large-scale space such as a soccer stadium, concert hall, etc. They propose a method for describing the shape of a 3D object with a set of planes in order to synthesize a novel view of the object effectively. The most effective layout of the planes can be determined based on the relative locations of an observer's viewing position, multiple cameras, and 3D objects. A method is described for controlling the LOD of the 3D representation by adjusting the orientation, interval, and resolution of planes. The data size of the 3D model and the processing time can be reduced drastically. The effectiveness of the proposed method is demonstrated by experimental results.},
	author = {Kitahara, I. and Ohta, Y.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191120},
	issn = {1087-8270},
	keywords = {Three dimensional displays;Large-scale systems;Space technology;Spatial resolution;Computer displays;Cameras;Image reconstruction;Power system modeling;Shape;TV},
	month = {March},
	pages = {45-52},
	title = {Scalable 3D representation for 3D video display in a large-scale space},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191120}}

@inproceedings{1191121,
	abstract = {Augmented reality (AR) superimposes computer-generated virtual images on the real world to allow users exploring both virtual and real worlds simultaneously. For a successful augmented reality application, an accurate registration of a virtual object with its physical counterpart has to be achieved, which requires precise knowledge of the projection information of the viewing device. The paper proposes a fast and easy off-line calibration strategy based on well-established camera calibration methods. Our method does not need exhausting effort on the collection of world-to-image correspondence data. All the correspondence data are sampled with an image based method and they are able to achieve sub-pixel accuracy. The method is applicable for all AR systems based on optical see-through head-mounted display (HMD), though we took a head-mounted projective display (HMPD) as the example. We first review the calibration requirements for an augmented reality system and the existing calibration methods. Then a new view projection model for optical see through HMD is addressed in detail, and proposed calibration method and experimental result are presented. Finally, the evaluation experiments and error analysis are also included. The evaluation results show that our calibration method is fairly accurate and consistent.},
	author = {Chunyu Gao and Hong Hua and Ahuja, N.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191121},
	issn = {1087-8270},
	keywords = {Calibration;Augmented reality;Virtual reality},
	month = {March},
	pages = {53-60},
	title = {Easy calibration of a head-mounted projective display for augmented reality systems},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191121}}

@inproceedings{1191122,
	abstract = {An augmented virtual environment (AVE) fuses dynamic imagery with 3D models. The AVE provides a unique approach to visualize and comprehend multiple streams of temporal data or images. Models are used as a 3D substrate for the visualization of temporal imagery, providing improved comprehension of scene activities. The core elements of AVE systems include model construction, sensor tracking, real-time video/image acquisition, and dynamic texture projection for 3D visualization. This paper focuses on the integration of these components and the results that illustrate the utility and benefits of the resulting augmented virtual environment.},
	author = {Neumann, U. and Suya You and Jinhui Hu and Bolan Jiang and JongWeon Lee},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191122},
	issn = {1087-8270},
	keywords = {Image sensors;Solid modeling;Vehicle dynamics;Virtual environment;Data visualization;Image databases;Image reconstruction;Layout;Sensor systems;Tracking},
	month = {March},
	pages = {61-67},
	title = {Augmented virtual environments (AVE): dynamic fusion of imagery and 3D models},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191122}}

@inproceedings{1191123,
	abstract = {Current cloth simulation systems have become powerful enough to be used interactively, and applications relevant to textile industries like virtually tailoring real garments seem to be possible in the near future. We show that interactive cloth simulation can be improved by virtual reality techniques. We present the Virtual Dressmaker, an application for interactive assembly and physically based interactive simulation of cloth. In particular, the system allows us to select and drag parts of the clothes during the simulation. The proposed application consists of a VR-interface client that provides nearly live-size stereo projection combined with 6DOF interaction and a server for physically based cloth simulation. In order to verify the usability of our approach, we elaborate on usability studies, which show that our application allows fast, easy, and precise interaction compared to interaction with a traditional 3D-desktop application based on 2D input devices.},
	author = {Keckeisen, M. and Stoev, S.L. and Feurer, M. and Strasser, W.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191123},
	issn = {1087-8270},
	keywords = {Computational modeling;Application software;Clothing;Assembly;Virtual reality;Computer simulation;Textile industry;Usability;Computer graphics;Manufacturing industries},
	month = {March},
	pages = {71-78},
	title = {Interactive cloth simulation in virtual environments},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191123}}

@inproceedings{1191124,
	abstract = {The paper presents a real-time database for modeling complex networks of intersecting roads and walkways in urban virtual environments. The database represents information about the layout of streets and sidewalks, the rules that govern behavior on roads and walkways, and the locations of agents with respect to road and sidewalk structures. This information is used by programs that control the behavior of autonomous vehicles and pedestrians populating the virtual urban environment. Roads and sidewalks are modeled as ribbons in space. The ribbon structure provides a natural coordinate frame for defining the local geometry of navigable surfaces. This geometry is important for way finding and also forms the geometric basis on which spatial relationships among agents are defined. The database includes a powerful run-time interface supported by robust and efficient code for locating objects on the ribbon network, for mapping between Cartesian and ribbon coordinates, and for determining behavioral constraints imposed by the environment.},
	author = {Willemsen, P. and Kearney, J.K. and Hongling Wang},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191124},
	issn = {1087-8270},
	keywords = {Autonomous agents;Roads;Geometry;Complex networks;Virtual environment;Remotely operated vehicles;Mobile robots;Spatial databases;Runtime environment;Robustness},
	month = {March},
	pages = {79-86},
	title = {Ribbon networks for modeling navigable paths of autonomous agents in virtual urban environments},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191124}}

@inproceedings{1191125,
	abstract = {We present a wireless virtual reality system and a prototype full body Tai Chi training application. Our primary contribution is the creation of a virtual reality system that tracks the full body in a working volume of 4 meters by 5 meters by 2.3 meters high to produce an animated representation of the user with 42 degrees of freedom. This - combined with a lightweight (<3 pounds) belt-worn video receiver and head-mounted display - provides a wide area, untethered virtual environment that allows exploration of new application areas. Our secondary contribution is our attempt to show that user interface techniques made possible by such a system can improve training for a full body motor task. We tested several immersive techniques, such as providing multiple copies of a teacher's body positioned around the student and allowing the student to superimpose his body directly over the virtual teacher None of these techniques proved significantly better than mimicking traditional Tai Chi instruction, where we provided one virtual teacher directly in front of the student. We consider the implications of these findings for future motion training tasks.},
	author = {Philo Tan Chua and Crivella, R. and Daly, B. and Ning Hu and Schaaf, R. and Ventura, D. and Camill, T. and Hodgins, J. and Pausch, R.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191125},
	issn = {1087-8270},
	keywords = {Virtual reality;Virtual environment;Displays;Animation;Optical feedback;Virtual prototyping;User interfaces;Testing;Tracking;Wires},
	month = {March},
	pages = {87-94},
	title = {Training for physical tasks in virtual environments: Tai Chi},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191125}}

@inproceedings{1191126,
	abstract = {Computer assisted operation planning systems win more and more recognition in the field of surgery. These systems offer new possibilities to prepare an intervention with the goal to shorten the expansive time in the operation room required for the intervention. The safest and most effective surgical approach should be selected. But often, it is difficult to transfer the output of the planning system to the intra-operative situation and so to consider the planning results in the real intervention. At the Fraunhofer Institute for Computer Graphics (IGD) in Darmstadt and the Centre for Advanced Media Technology (CAMTech) in Singapore, methods are developed to bridge the gap between the external planning session and the intra-operative case: augmented reality (AR) techniques are used to overlap preoperative scanned image data as well as results of the planning session to the operation field.},
	author = {Bockholt, U. and Bisler, A. and Becker, M. and Muller-Wittig, W. and Voss, G.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191126},
	issn = {1087-8270},
	keywords = {Augmented reality;Orthopedic surgery;Biomedical imaging;Neurosurgery;Displays;Technology planning;Shape;Biological tissues;Deformable models;Endoscopes},
	month = {March},
	pages = {97-101},
	title = {Augmented reality for enhancement of endoscopic interventions},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191126}}

@inproceedings{1191127,
	abstract = {Bone dissection is an important component of many surgical procedures. In this paper we discuss adaptive techniques for providing real-time haptic and visual feedback during a virtual bone dissection simulation. The simulator is being developed as a component of a training system for temporal bone surgery. We harness the difference in complexity and frequency requirements of the visual and haptic simulations by modeling the system as a collection of loosely coupled concurrent components. The haptic component exploits a multi-resolution representation of the first two moments of the bone characteristic function to rapidly compute contact forces and determine bone erosion. The visual component uses a time-critical particle system evolution method to simulate secondary visual effects, such as bone debris accumulation, blooding, irrigation, and suction.},
	author = {Agus, M. and Giachetti, A. and Gobbetti, E. and Zanetti, G. and Zorcolo, A.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191127},
	issn = {1087-8270},
	keywords = {Haptic interfaces;Bones;Surgery;Computational modeling;Visual effects;Irrigation;Humans;Frequency;Time factors;Cadaver},
	month = {March},
	pages = {102-109},
	title = {Adaptive techniques for real-time haptic and visual simulation of bone dissection},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191127}}

@inproceedings{1191128,
	abstract = {We introduce and present preliminary results for a hybrid display system combining head-mounted and projector-based displays. Our work is motivated by a surgical training application, where it is necessary to simultaneously provide both a high-fidelity view of a central close-up task (the surgery) and visual awareness of objects and events in the surrounding environment In particular, for trauma surgeons it would be valuable to learn to work in an environment that is realistically filled with both necessary and distracting objects and events. In this paper, we motivate the use of a hybrid display system, discuss previous work, describe a prototype along with methods for geometric calibration, and present results from a controlled human subject experiment.},
	author = {Kok-Lim Low and Ilie, A. and Welch, G. and Lastra, A.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191128},
	issn = {1087-8270},
	keywords = {Surgery;Computer displays;Collaboration;Prototypes;Virtual reality;Virtual environment;Computer science;Application software;Calibration;Control systems},
	month = {March},
	pages = {110-117},
	title = {Combining head-mounted and projector-based displays for surgical training},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191128}}

@inproceedings{1191130,
	abstract = {Immersive virtual environments (VEs) provide participants with computer-generated environments filled with virtual objects to assist in learning, training, and practicing dangerous and/or expensive tasks. But for certain tasks, does having every object being virtual inhibit the interactivity? Further, does the virtual object's visual fidelity affect performance? Overall VE effectiveness may be reduced if users spend most of their time and cognitive capacity learning how to interact and adapting to interacting with a purely virtual environment. We investigated how handling real objects and how self-avatar visual fidelity affects performance on a spatial cognitive task in an immersive VE. We compared participants' performance on a block arrangement task in both a real-space environment and several virtual and hybrid environments. The results showed that manipulating real objects in a VE brings task performance closer to that of real space, compared to manipulating virtual objects.},
	author = {Lok, B. and Naik, S. and Whitton, M. and Brooks, F.P.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191130},
	issn = {1087-8270},
	keywords = {Avatars;Virtual environment;Assembly;Lubricating oils;Engines;Feedback;Haptic interfaces;Problem-solving;Cognitive science;Libraries},
	month = {March},
	pages = {125-132},
	title = {Effects of handling real objects and avatar fidelity on cognitive task performance in virtual environments},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191130}}

@inproceedings{1191131,
	abstract = {This paper describes an experimental investigation into the relationship of delay to user performance in 3D object placement. In the first experiment, 10 participants performed a placement task as delay and placement difficulty varied. Results were analyzed with statistical analyses of variance. Placement errors and times rose as delay and difficulty increased. An interaction indicated that controlling delay is particularly important when placement is already difficult. In vehicle control tasks, previewing forecasts required input, and is a useful way of compensating for delay. The second experiment examined the relationship of previewing to delay and difficulty in placement. With regard to delay and difficulty, the 15 participants replicated the first experiment's results. Previewing reduced placement times directly, and mitigated the effects of delay and difficulty. A three-way interaction indicated that when previewing is used, more delay can be tolerated even in difficult placement tasks.},
	author = {Watson, B. and Walker, N. and Woytiuk, P. and Ribarsky, W.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191131},
	issn = {1087-8270},
	keywords = {Usability;Delay effects;Virtual environment;Vehicles;Human factors;Time measurement;Personal communication networks;Computer science;Analysis of variance;Statistical analysis},
	month = {March},
	pages = {133-140},
	title = {Maintaining usability during 3D placement despite delay},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191131}}

@inproceedings{1191132,
	abstract = {Previous research has shown that even low end-to-end latency can have adverse effects on performance in virtual environments (VE). This paper reports on an experiment investigating the effect of latency on other metrics of VE effectiveness: physiological response, simulator sickness, and self-reported sense of presence. The VE used in the study includes two rooms: the first is normal and non-threatening; the second is designed to evoke a fear/stress response. Participants were assigned to either a low latency (/spl sim/50 ms) or high latency (/spl sim/90 ms) group. Participants in the low latency condition had a higher self-reported sense of presence and a statistically higher change in heart rate between the two rooms than did those in the high latency condition. There were no significant relationships between latency and simulator sickness.},
	author = {Meehan, M. and Razzaque, S. and Whitton, M.C. and Brooks, F.P.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191132},
	issn = {1087-8270},
	keywords = {Delay;Virtual environment;Heart rate;Stress measurement;Displays;Position measurement;Signal generators;Skin;Demography;Data analysis},
	month = {March},
	pages = {141-148},
	title = {Effect of latency on presence in stressful virtual environments},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191132}}

@inproceedings{1191133,
	abstract = {We examine the law of steering - a quantitative model of human movement time in relation to path width and length previously established in hand drawing movement - in a VR locomotion paradigm. Participants drove a simulated vehicle in a virtual environment on paths whose shape and width were manipulated Results showed that the law of steering also applies to locomotion. Participants' mean trial completion times linearly correlated (r/sup 2/ between 0.985 and 0.999) with an index of difficulty quantified as path distance to width ratio for the straight and circular paths used in this experiment. Their average mean and maximum speed was linearly proportional to path width. Such human performance regularity provides a quantitative tool for 3D human machine interface design and evaluation.},
	author = {Shumin Zhai and Woltjer, R.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191133},
	issn = {1087-8270},
	keywords = {Virtual reality;User interfaces;Shape;Human computer interaction;Vehicles;Virtual environment;Electrical engineering;Keyboards;Navigation;Trajectory},
	month = {March},
	pages = {149-156},
	title = {Human movement performance in relation to path constraint - the law of steering in locomotion},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191133}}

@inproceedings{1191135,
	abstract = {This paper summarizes our NSF funded project, a PC-based multi-user learning environment: Multi-User Virtual Environment Experiential Simulator (MUVEES). The goal of this project is to create and evaluate graphical multi-user virtual environments that use digitized museum resources to enhance middle school students' motivation and learning about science. Here, we discuss the design, implementation, and applications of MUVEES. We present its structure, efficient approaches that achieve more realistic avatar behaviors, and pedagogical strategies that foster strong learning outcomes across a wide range of individual student characteristics. Our preliminary results indicate that MUVEES is a powerful vehicle for collaboration and learning. We believe that our system and implementation methods will help improve future multi-user virtual environments.},
	author = {Chen, J.X. and Yonggao Yang and Loffin, B.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191135},
	issn = {1087-8270},
	keywords = {Virtual environment;Educational institutions;Collaboration;Data analysis;Power generation;Avatars;Vehicles;Educational technology;Decision making;Laboratories},
	month = {March},
	pages = {163-170},
	title = {MUVEES: a PC-based multi-user virtual environment for learning},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191135}}

@inproceedings{1191136,
	abstract = {This paper presents a multi-user collaborative infrastructure, SCAPE (an acronym for Stereoscopic Collaboration in Augmented and Projective Environments), which is based on recent advancement in head-mounted projective display (HMPD) technology. The SCAPE mainly consists of a 3'/spl times/5' interactive workbench and a 12'/spl times/12'/spl times/9' room-sized walk-through display environment, multiple head-tracked HMPDs, multi-modality interface devices, and a generic application-programming interface (API) designed to coordinate the components. The infrastructure provides a shared space in which multiple users can simultaneously interact with a 3D synthetic environment from their individual viewpoints. We detail the SCAPE implementation and include an application example that demonstrates major interface and cooperation features.},
	author = {Hong Hua and Brown, L.D. and Chunyu Gao and Ahuja, N.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191136},
	issn = {1087-8270},
	keywords = {Collaboration;Virtual reality},
	month = {March},
	pages = {171-179},
	title = {A new collaborative infrastructure: SCAPE},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191136}}

@inproceedings{1191137,
	abstract = {We designed, developed, deployed, and evaluated the Collaborative nanoManipulator (CnM), a system supporting remote collaboration between users of the nanoManipulator interface to atomic force microscopes. To be accepted by users, the shared nanoManipulator application had to have the same high level of interactivity as the single user system and the application had to support a user's ability to interleave working privately and working collaboratively. The paper describes the entire collaboration system, but focuses on the shared nanoManipulator application. Based on our experience developing the CnM, we present: a method of analyzing applications to characterize the requirements for sharing data between collaborating sites, examples of data structures that support collaboration, and guidelines for selecting appropriate synchronization and concurrency control schemes.},
	author = {Hudson, T.C. and Helser, A.T. and Sonnenwald, D.H. and Whitton, M.C.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191137},
	issn = {1087-8270},
	keywords = {Collaboration;Collaborative work;Application software;Virtual environment;Concurrency control;Control systems;Collaborative software;Microscopy;Identity-based encryption;Guidelines},
	month = {March},
	pages = {180-187},
	title = {Managing collaboration in the nanoManipulator},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191137}}

@inproceedings{1191138,
	abstract = {We describe a novel optical tracker algorithm for the tracking of interaction devices in virtual and augmented reality. The tracker uses invariant properties of marker patterns to efficiently identify and reconstruct the pose of these interaction devices. Since invariant properties are sensitive to noise in the 2D marker positions, an offline training session is used to determine deviations in these properties. These deviations are taken into account when searching for the patterns once the tracker is used.},
	author = {van Liere, R. and Mulder, J.D.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191138},
	issn = {1087-8270},
	keywords = {Optical noise;Optical sensors;Optical devices;Image reconstruction;Image processing;Virtual reality;Computer vision;Information systems;Mathematics;Computer science},
	month = {March},
	pages = {191-198},
	title = {Optical tracking using projective invariant marker pattern properties},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191138}}

@inproceedings{1191139,
	abstract = {We present a demonstrated and commercially viable self-tracker, using robust software that fuses data from inertial and vision sensors. Compared to infrastructure-based trackers, self-trackers have the advantage that objects can be tracked over an extremely wide area, without the prohibitive cost of an extensive network of sensors or emitters to track them. So far, most AR research has focused on the long-term goal of a purely vision-based tracker that can operate in arbitrary unprepared environments, even outdoors. We instead chose to start with artificial fiducials, in order to quickly develop the first self-tracker which is small enough to wear on a belt, low cost, easy to install and self-calibrate, and low enough latency to achieve AR registration. We also present a roadmap for how we plan to migrate from artificial fiducials to natural ones. By designing to the requirements of AR, our system can easily handle the less challenging applications of wearable VR systems and robot navigation.},
	author = {Foxlin, E. and Naimark, L.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191139},
	issn = {1087-8270},
	keywords = {Wearable sensors;Costs;Robustness;Fuses;Sensor fusion;Belts;Delay;Virtual reality;Robots;Navigation},
	month = {March},
	pages = {199-206},
	title = {VIS-Tracker: a wearable vision-inertial self-tracker},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191139}}

@inproceedings{1191141,
	abstract = {Gesture recognition techniques often suffer from being highly device-dependent and hard to extend. If a system is trained using data from a specific glove input device, that system is typically unusable with any other input device. The set of gestures that a system is trained to recognize is typically not extensible, without retraining the entire system. We propose a novel gesture recognition framework to address these problems. This framework is based on a multi-layered view of gesture recognition. Only the lowest layer is device dependent, it converts raw sensor values produced by the glove to a glove-independent semantic description of the hand. The higher layers of our framework can be reused across gloves, and are easily extensible to include new gestures. We have experimentally evaluated our framework and found that it yields comparable performance to conventional techniques, while substantiating our claims of device independence and extensibility.},
	author = {Eisenstein, J. and Ghandeharizadeh, S. and Golubchik, L. and Shahabi, C. and Donghui Yan and Zimmermann, R.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191141},
	issn = {1087-8270},
	keywords = {Sensor systems;Data gloves;Sensor phenomena and characterization;Neural networks;Recurrent neural networks;Degradation;Virtual reality;Jacobian matrices;Computer science;Instruments},
	month = {March},
	pages = {207-214},
	title = {Device independence and extensibility in gesture recognition},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191141}}

@inproceedings{1191142,
	abstract = {The Proactive Desk is a new digital desk with haptic feedback. The Proactive Desk allows a user to handle both virtual and real objects on a digital desk with a realistic feeling. We proposed it for a co-experience web that would enable people to share the feelings and experiences of other users via the Internet. In the Proactive Desk, two linear induction motors are equipped to generate an omnidirectional translational force on a user's hand or a physical object on the desk without any mechanical link nor wire, thereby preserving the advantages of a digital desk. In this paper we report applications of the Proactive Desk and the performance of the first trial model.},
	author = {Noma, H. and Yanagida, Y. and Tetsutani, N.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191142},
	issn = {1087-8270},
	keywords = {Displays;Induction motors;Internet;Cultural differences;Graphical user interfaces;Information science;Laboratories;Haptic interfaces;Feedback;Induction generators},
	month = {March},
	pages = {217-224},
	title = {The Proactive Desk: a new force display system for a digital desk using a 2-DOF linear induction motor},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191142}}

@inproceedings{1191143,
	abstract = {The usefulness of modern day haptics equipment for virtual simulations of actual maintenance actions is examined. In an effort to categorize which areas haptic simulations may be useful, we have developed a taxonomy for haptic actions. This classification has two major dimensions: the general type of action performed and the type of force or torque required. Building upon this taxonomy, we selected three representative tasks from the taxonomy to evaluate in a virtual reality simulation. We conducted a series of human subject experiments to compare user performance and preference on a disassembly task with and without haptic feedback using CyberGlove, Phantom, and SpaceMouse interfaces. Analysis of the simulation runs shows Phantom users learned to accomplish the simulated actions significantly more quickly than did users of the CyberGlove or the SpaceMouse. Moreover, a lack of differences in the post-experiment questionnaire suggests that haptics research should include a measure of actual performance speed or accuracy rather than relying solely on subjective reports of a device's ease of use.},
	author = {Bloomfield, A. and Yu Deng and Wampler, J. and Rondot, P. and Harth, D. and McManus, M. and Badler, N.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191143},
	issn = {1087-8270},
	keywords = {Taxonomy;Haptic interfaces;Data gloves;Imaging phantoms;Analytical models;Torque;Virtual reality;Humans;Feedback;Velocity measurement},
	month = {March},
	pages = {225-231},
	title = {A taxonomy and comparison of haptic actions for disassembly tasks},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191143}}

@inproceedings{1191144,
	abstract = {We propose an approach to object manipulation. In the approach, the user's hand or fingers are represented by a set of interface points, the interaction force on each interface point is computed by the collision response computation algorithm, and the motion of the object is simulated based on the interaction force. We employed a fast collision response computation method to accommodate a large number of interaction points. Also, we devised a method for stable computation of object motion. We carried out experiments on interaction with models of varying complexity and demonstrated the feasibility of our approach for dexterous manipulations.},
	author = {Hirota, K. and Hirose, M.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191144},
	issn = {1087-8270},
	keywords = {Fingers;Computer interfaces;Computational modeling;Virtual environment;Object detection;Shape;Haptic interfaces;Rendering (computer graphics);Prototypes;Force feedback},
	month = {March},
	pages = {232-239},
	title = {Dexterous object manipulation based on collision response},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191144}}

@inproceedings{1191146,
	abstract = {Immersive virtual environments (VEs) offer interactive, real-time visualization capabilities for engineers, architects, and scientists. This paper presents Virtual-SAP, an immersive VE application allowing users to assess building structures and their response to various environmental conditions through an interactive design-build-test-redesign cycle. Virtual-SAP has three distinct user interfaces that support its use with a high-end, multi-tracker VE system, with a low-cost portable VE system, or on the desktop. Two of these interfaces have been proven highly usable in user testing, and the third will be tested soon. Virtual-SAP is being used for both research and education.},
	author = {Bowman, D.A. and Setareh, M. and Pinho, M.S. and Ali, N. and Kalita, A. and Yunha Lee and Lucas, J. and Gracey, M. and Kothapalli, M. and Qinwei Zhu and Datey, A. and Tumati, P.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191146},
	issn = {1087-8270},
	keywords = {Buildings;Data visualization;Virtual environment;User interfaces;Testing;Layout;Computer science;Computer architecture;Computer science education;Computer graphics},
	month = {March},
	pages = {243-250},
	title = {Virtual-SAP: an immersive tool for visualizing the response of building structures to environmental conditions},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191146}}

@inproceedings{1191147,
	abstract = {The paper describes the HOMERE system: a multimodal system dedicated to visually impaired people to explore and navigate inside virtual environments. The system addresses three main applications: preparation for the visit of an existing site, training for the use of a blind cane, and ludic exploration of virtual worlds. The HOMERE system provides the user with different sensations when navigating inside a virtual world: a force feedback corresponding to the manipulation of a virtual blind cane, a thermal feedback corresponding to the simulation of a virtual sun, and an auditory feedback in spatialized conditions corresponding to the ambient atmosphere and specific events in the simulation. A visual feedback of the scene is also provided to enable sighted people to follow the navigation of the main user. HOMERE has been tested by several visually impaired people who were all confident about the potential of this prototype.},
	author = {Lecuyer, A. and Mobuchon, P. and Megard, C. and Perret, J. and Andriot, C. and Colinot, J.-P.},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191147},
	issn = {1087-8270},
	keywords = {Virtual environment;Navigation;Force feedback;Atmospheric modeling;Discrete event simulation;Thermal force;Sun;Atmosphere;Layout;Testing},
	month = {March},
	pages = {251-258},
	title = {HOMERE: a multimodal system for visually impaired people to explore virtual environments},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191147}}

@inproceedings{1191149,
	abstract = {We present a comparative study of user performance with tasks involving navigation, visual search, and geometric manipulation, in a map-based battlefield visualization virtual environment (VE). Specifically, our experiment compared user performance of the same task across four different VE platforms: desktop, cave, workbench, and wall. Independent variables were platform type, stereopsis (stereo, mono), movement control mode (rate, position), and frame of reference (egocentric, exocentric). Overall results showed that users performed tasks fastest using the desktop and slowest using the workbench. Other results are detailed in the article. Notable is that we designed our task in an application context, with tasking much closer to how users would actually use a real-world battlefield visualization system. This is very uncommon for comparative studies, which are usually designed with abstract tasks to minimize variance. This is, we believe, one of the first and most complex studies to comparatively examine, in an application context, this many key variables affecting VE user interface design.},
	author = {Swan, J.E. and Gabbard, J.L. and Hix, D. and Schulman, R.S. and Keun Pyo Kim},
	booktitle = {IEEE Virtual Reality, 2003. Proceedings.},
	date-added = {2024-03-18 02:28:38 -0400},
	date-modified = {2024-03-18 02:28:38 -0400},
	doi = {10.1109/VR.2003.1191149},
	issn = {1087-8270},
	keywords = {Virtual environment;User interfaces;Navigation;Visualization;Usability;Performance evaluation;Design engineering;Statistics;MONOS devices;User centered design},
	month = {March},
	pages = {259-266},
	title = {A comparative study of user performance in a map-based virtual environment},
	year = {2003},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2003.1191149}}

@inproceedings{996499,
	abstract = {The availability of inexpensive and powerful graphics cards as well as fast Internet connections make Networked Virtual Environments viable for millions of users and many new applications. It is therefore necessary to cope with the growing heterogeneity that arises from differences in computing power, network speed and users' preferences. This paper describes an architecture that accommodates the heterogeneity mentioned above while allowing a manager to define system-wide policies. Policies and users' preferences can be expressed as simple linear equations forming a mathematical model that describes the system as a whole as well as its individual components. When solutions to this model are mapped back to the problem domain, viable solutions that accommodate heterogeneity and system policies are obtained. The results of our experiments with a proof-of-concept system are described.},
	author = {Trefftz, H. and Marsic, I. and Zyda, M.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996499},
	issn = {1087-8270},
	keywords = {Intelligent networks;Mathematical model;Virtual environment;Computer networks;Equations;Graphics;IP networks;Computer architecture;Power system management;Power system modeling},
	month = {March},
	pages = {7-14},
	title = {Handling heterogeneity in networked virtual environments},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996499}}

@inproceedings{996500,
	abstract = {Many VR platforms emphasise extensibility in order to support as wide a range of applications as possible. The current trend is to move this extensibility to lower levels of the system, in order to support extensibility of infrastructure mechanisms such as networking protocols. This kind of extensibility allows the run-time of the virtual environment system to evolve even while the system is running. This paper presents a new virtual environment platform that allows multiple infrastructure mechanisms to be added to and co-exist within the running system, with different elements of the virtual world using different mechanisms. This allows the virtual environment system to efficiently support a wider range of applications by, for example, having only certain virtual objects use conservative consistency and persistence. It can also optimise the performance of the CVE by tailoring the infrastructure mechanisms according to the different roles played by different objects in the virtual environment.},
	author = {Purbrick, J. and Greenhalgh, C.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996500},
	issn = {1087-8270},
	keywords = {Virtual environment;Protocols;Virtual reality;Runtime environment;Peer to peer computing;Java;Computer science;Application software;Hybrid power systems;Unicast},
	month = {March},
	pages = {15-21},
	title = {An extensible event-based infrastructure for networked virtual worlds},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996500}}

@inproceedings{996501,
	abstract = {This paper presents a 100% Java middleware that aims to offer an open and extensible foundation for building large-scale real-time networked virtual environment applications. This platform relies on a partial replication model in which the whole simulation space is spread on a federation of processes based on application-specific criteria. A configurable event communication framework allows arbitrary consistency and synchronization protocols to be implemented in close cooperation with the application semantics. This middleware has been experimented with success for multi-player game prototypes involving both human-driven and autonomous simulated entities.},
	author = {Tran, F.D. and Deslaugiers, M. and Gerodolle, A. and Hazard, L. and Rivierre, N.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996501},
	issn = {1087-8270},
	keywords = {Middleware;Large-scale systems;Object oriented modeling;Application software;Virtual environment;Java;Scalability;Collaborative software;Hazards;Telecommunications},
	month = {March},
	pages = {22-29},
	title = {An open middleware for large-scale networked virtual environments},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996501}}

@inproceedings{996502,
	abstract = {The aim of this study is to explore the feasibility of multiple display environment technology in the real world. In this paper, the wearable projector, with an infrared camera and light source, is used in combination with retro-reflective screens. The visible image and infrared light are projected to the screen and reflected back to the user's eye and the infrared camera. The screen location and the user's fingertip position are calculated using image processing to enable the interaction. Several examples or demonstrations have been constructed to show the usage of this configuration. The screen functions as both the visible screen and the high-gain infrared marker at the same time. Owing to high-reflection gain of retro-reflective screens, a small, lightweight projector can be used. Also, the high contrast between the screen and the other environmental object in the captured infrared image decreases the difficulty of image processing.},
	author = {Kijima, R. and Haza, K. and Tada, Y. and Ojika, T.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996502},
	issn = {1087-8270},
	keywords = {Cameras;Computer displays;Image processing;Head;Virtual reality;Mirrors;Infrared imaging;Personal digital assistants;Optical sensors;Information science},
	month = {March},
	pages = {33-40},
	title = {Distributed display approach using PHMD with infrared camera},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996502}}

@inproceedings{996503,
	abstract = {Predicted availability of broadband access will enable deployment of network virtual environments over the Internet. Online virtual worlds will require efficient runtime data replication solutions. This paper presents a quality of service (QoS) architecture for just-in-time data replication in network virtual environments. Quality of service is achieved by predicting the load and adapting to network traffic variations. Data is prefetched at the client based on network traffic estimates and viewpoint navigation prediction. QoS negotiation allows the server to control the network resources allocated per client. Experimental results show that QoS data replication can be implemented with reasonably small network and sever overload.},
	author = {Popescu, G.V. and Codella, C.F.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996503},
	issn = {1087-8270},
	keywords = {Intelligent networks;Quality of service;Virtual environment;Prefetching;Geometry;Resource management;Delay;IP networks;Runtime;Navigation},
	month = {March},
	pages = {41-48},
	title = {An architecture for QoS data replication in network virtual environments},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996503}}

@inproceedings{996504,
	abstract = {In this paper, we introduce DLoVe, a new paradigm for designing and implementing distributed and nondistributed virtual reality applications, using one-way constraints. DLoVe allows programs written in its framework to be executed on multiple computers for improved performance. It also allows easy specification and implementation of multi-user interfaces. DLoVe hides all the networking aspects of message passing among the machines in the distributed environment and performs the needed network optimizations. As a result, a user of DLoVe does not need to understand parallel and distributed programming to use the system; he or she needs only be able to use the serial version of the user interface description language. Parallelizing the computation is performed by DLoVe, without modifying the interface description.},
	author = {Deligiannidis, L. and Jacob, R.J.K.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996504},
	issn = {1087-8270},
	keywords = {Parallel processing;Virtual reality;User interfaces;Computer interfaces;Jacobian matrices;Message passing;Delay;Design optimization;Parallel programming;Writing},
	month = {March},
	pages = {49-56},
	title = {DLoVe: using constraints to allow parallel processing in multi-user virtual reality},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996504}}

@inproceedings{996505,
	abstract = {This paper focuses on the distributed architecture of the collaborative augmented reality system Studierstube. The system allows multiple users to experience a shared 3D workspace populated by multiple applications using see-through head mounted displays or other presentation media such as projection systems. The system design is based on a distributed shared scene graph that alleviates the application programmer from explicitly considering distribution, and avoids a separation of graphical and application data. The idea of unifying all system data in the scene graph is taken to its logical consequence by implementing application instances as nodes in the scene graph. Through the distributed shared scene graph mechanism, consistency of scene graph replicas and the contained application nodes is assured. Multi-user 3D widgets allow concurrent interaction with minimal coordination effort from the application. Special interest is paid to migration of application nodes from host to host allowing dynamic workgroup management, such as load balancing, late joining and early exit of hosts, and some firms of ubiquitous computing.},
	author = {Schmalstieg, D. and Hesina, G.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996505},
	issn = {1087-8270},
	keywords = {Collaboration;Augmented reality;Layout;Collaborative work;Load management;Virtual environment;Displays;Programming profession;Ubiquitous computing;Head},
	month = {March},
	pages = {59-66},
	title = {Distributed applications for collaborative augmented reality},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996505}}

@inproceedings{996506,
	abstract = {Previous research on augmented reality has been mainly focused on augmentation of visual or acoustic information. However, humans can receive information not only through vision and acoustics, but also through haptics. Haptic sensation is very intuitive, and some researchers are focusing on making use of haptics in augmented reality systems. While most previous research on haptics is based on static data, such as that generated from CAD, CT, and so on, these systems have difficulty responding to a changing real environment in real time. In this paper, we propose a new concept for the augmented reality of haptics, the SmartTool. The SmartTool responds to the real environment by using real time sensor(s) and a haptic display. The sensor(s) on the SmartTool measure the real environment then send us that information through haptic sensation. Furthermore, we will describe the prototype system we have developed.},
	author = {Nojima, T. and Sekiguchi, D. and Inami, M. and Tachi, S.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996506},
	issn = {1087-8270},
	keywords = {Augmented reality;Haptic interfaces;Humans;Displays;Surgery;Navigation;Cameras;Acoustics;Real time systems;Machine vision},
	month = {March},
	pages = {67-72},
	title = {The SmartTool: a system for augmented reality of haptics},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996506}}

@inproceedings{996507,
	abstract = {All augmented reality (AR) systems must deal with registration errors. While most AR systems attempt to minimize registration errors through careful calibration, registration errors can never be completely eliminated in any realistic system. In this paper, we describe a robust and efficient statistical method for estimating registration errors. Our method generates probabilistic error estimates for points in the world, in either 3D world coordinates or 2D screen coordinates. We present a number of examples illustrating how registration error estimates can be used in AR interfaces, and describe a method for estimating registration errors of objects based on the expansion and contraction of their 2D convex hulls.},
	author = {MacIntyre, B. and Coelho, E.M. and Julier, S.J.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996507},
	issn = {1087-8270},
	keywords = {Augmented reality;Computer errors;Windows;Virtual reality;Resistors;Laboratories;Computer displays;Eyes;Educational institutions;Information technology},
	month = {March},
	pages = {73-80},
	title = {Estimating and adapting to registration errors in augmented reality systems},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996507}}

@inproceedings{996508,
	abstract = {A head-mounted projective display (HMPD) consists of a pair of miniature projection lenses, beam splitters and displays mounted on the helmet and retro-reflective sheeting materials placed strategically in the environment. This has recently been proposed as an alternative to existing 3D visualization devices. In this paper, we first briefly review HMPD technology, including its featured capabilities and the recent development in both display implementations and applications. Then the implementation of a testbed for playing a "Go" game with a remote opponent in a 3D augmented environment is described. The testbed not only demonstrates the capabilities of virtual-real augmentation and registration, the natural occlusion of virtual objects by real ones, interaction with augmented environments and networking collaboration, but also embodies part of our long-term objective to develop a collaborative framework in 3D augmented environments. Through the testbed, major calibration issues, such as accommodation/convergence considerations and determination of viewing transformations, are studied and discussed in detail. Both calibration methods and results are included, which are applicable to other applications. Finally, experimental results of the testbed implementation are presented.},
	author = {Hong Hua and Chunyu Gao and Brown, L.D. and Ahuja, N. and Rolland, J.P.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996508},
	issn = {1087-8270},
	keywords = {Biomedical optical imaging;Optical attenuators;Displays;Layout;Visualization;Collaboration;Calibration;Materials testing;Lenses;Cameras},
	month = {March},
	pages = {81-89},
	title = {A testbed for precise registration, natural occlusion and interaction in an augmented environment using a head-mounted projective display (HMPD)},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996508}}

@inproceedings{996511,
	abstract = {While frameworks and application programming interfaces for virtual reality are commonplace today, designing scenarios for virtual environments still remains a tedious and time consuming task. We present a new authoring tool which combines scene assembly and visual programming in a desktop application with instant testing, tuning and planning in an immersive virtual environment. Two authors can work together, one with the desktop authoring application and the other in the immersive VR-simulation, to build a complete scenario.},
	author = {Holm, R. and Stauder, E. and Wagner, R. and Priglinger, M. and Volkert, J.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996511},
	issn = {1087-8270},
	keywords = {Virtual environment;Virtual reality;Railway safety;Computational modeling;Layout;Testing;Hazards;Assembly systems;Humans;Windows},
	month = {March},
	pages = {93-100},
	title = {A combined immersive and desktop authoring tool for virtual environments},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996511}}

@inproceedings{996512,
	abstract = {Temporal links allow recordings of multi-user sessions to be dynamically inserted into current virtual worlds in a flexible and principled way. We explore key applications of temporal links, showing how they can add new content to virtual worlds, support usability and system evaluation, and link VR to other media such as film and television. These applications illustrate just some of the possibilities of a ubiquitous and flexible record and replay facility such as temporal links. Building on our experience of implementing temporal links in the MASSIVE-3 system, we identify key requirements for system architectures that wish to support an equivalent mechanism.},
	author = {Greenhalgh, C. and Flintham, M. and Purbrick, J. and Benford, S.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996512},
	issn = {1087-8270},
	keywords = {Virtual reality;Virtual environment;Application software;Usability;TV;Buildings;Computer science;Geometry;Environmental management;Spatial resolution},
	month = {March},
	pages = {101-108},
	title = {Applications of temporal links: recording and replaying virtual environments},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996512}}

@inproceedings{996513,
	abstract = {This report covers two analyses that were conducted on commercial off-the-shelf (COTS) synthetic environments (SEs), in this case games. (1.) Analysis 1 used a factor analysis on a set of 19 self-reported cognitive skills exercised while respondents played various PC-based games. The results verified the hypothesis that a few general cognitive phases could be drawn from the 19 variables. The four meaningful and predictable factors were detection (D), understanding (U), decision-making (DM), and execution (E). These factors represent general cognitive stages found in human factors literature such as situational awareness theory. (2.) The second analysis correlated the four derived factors with the participants' self-report of specific game enjoyment features. The outcome of this project provided a set of blueprints or templates for proper selection of COTS SEs and their components when attempting to apply them for enhancing human performance or training utility.},
	author = {Morris, C.S. and Tarr, R.W.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996513},
	issn = {1087-8270},
	keywords = {Humans;Game theory;Artificial intelligence;Performance analysis;Computer simulation;Computational modeling;Aerospace simulation;Analytical models;Computer graphics;Computer interfaces},
	month = {March},
	pages = {109-115},
	title = {Templates for selecting PC-based synthetic environments for application to human performance enhancement and training},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996513}}

@inproceedings{996514,
	abstract = {We present a system, ArtNova, for 3D model design with a haptic interface. ArtNova offers the novel capability of interactively applying textures onto 3D surfaces directly by brush strokes, with the orientation of the texture determined the stroke. Building upon the framework of inTouch (Gregory et al., 2000), it further provides an intuitive physically-based force response when deforming a model. This system also uses a user-centric viewing technique that seamlessly integrates the haptic and visual presentation, by taking into account the user's haptic manipulation in dynamically determining the new viewpoint locations. Our algorithm permits automatic placement of the user viewpoint to navigate about the object. ArtNova has been tested by several users and they were able to start modeling and painting with just a few minutes of training. Preliminary user feedback indicates promising potential for 3D texture painting and modeling.},
	author = {Foskey, M. and Otaduy, M.A. and Lin, M.C.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996514},
	issn = {1087-8270},
	keywords = {Haptic interfaces;Painting;Solid modeling;Deformable models;Force feedback;Navigation;Virtual reality;Shape;Leg;Switches},
	month = {March},
	pages = {119-126},
	title = {ArtNova: touch-enabled 3D model design},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996514}}

@inproceedings{996515,
	abstract = {Presents a novel class of virtual reality input devices that combine pop-through buttons with 6-DOF trackers. Compared to similar devices that use conventional buttons, pop-through devices double the number of potential discrete interaction modes, since each button has two activation states corresponding to light and firm pressure. This additional state per button provides a foundation to address a range of shortcomings with conventional virtual environment (VE) input devices that includes reducing the physical dexterity required to perform interactions, reducing the cognitive complexity of some compound tasks and enabling the design of less obtrusive devices without sacrificing expressive power. Specifically, we present two novel input devices: the FingerSleeve was designed to be minimally obtrusive physically, whereas the TriggerGun was designed to be physically similar to, yet more functional than a conventional hand-held trigger device. Further, we present a set of novel navigation and interaction techniques that leverage the capabilities of our pop-through button devices to improve interaction quality and provide insight into harnessing the potential of pop-through buttons for other tasks. Finally, we discuss how we incorporated one of our devices into a real application.},
	author = {Zeleznik, R.C. and LaViola, J.J. and Acevedo Feliz, D. and Keefe, D.F.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996515},
	issn = {1087-8270},
	keywords = {Navigation;Fingers;Electronic switching systems;Scientific computing;Visualization;Virtual environment;Virtual reality},
	month = {March},
	pages = {127-134},
	title = {Pop through button devices for VE navigation and interaction},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996515}}

@inproceedings{996516,
	abstract = {When a 3D object is designed in a virtual world, it is essential that the designer can observe and manipulate the object directly in 3D space. The aim of our study is to develop a virtual workplace in which 3D virtual objects can be observed from various directions and can be touched directly by utilising force feedback. In this paper, we propose a 3D modelling system that consists of an inclined three-screen display and a force feedback device. Using this system, we propose a force feedback grid that extends the concept of 2D CAD manipulation to a grid that can be used in a 3D space, and which integrates with force feedback in order to support modelling in the 3D space. Finally, we report the results of an experiment that we conducted in order to verify the effectiveness of the force feedback grid.},
	author = {Yamada, T. and Tsubouchi, D. and Ogi, T. and Hirose, M.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996516},
	issn = {1087-8270},
	keywords = {Employment;Force feedback;Mirrors;Three dimensional displays;Space technology;Large screen displays;Computer displays;Head;Imaging phantoms;Computer graphics},
	month = {March},
	pages = {135-142},
	title = {Desk-sized immersive workplace using force feedback grid interface},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996516}}

@inproceedings{996517,
	abstract = {Virtual reality displays introduce spatial distortions that are very hard to correct because of the difficulty of precisely modelling the camera from the nodal point of each eye. How significant are these distortions for spatial perception in virtual reality? In this study, we used a helmet-mounted display and a mechanical head tracker to investigate the tolerance to errors between head motions and the resulting visual display. The relationship between the head movement and the associated updating of the visual display was adjusted by subjects until the image was judged as stable relative to the world. Both rotational and translational movements were tested, and the relationship between the movements and the direction of gravity was varied systematically. Typically, for the display to be judged as stable, subjects needed the visual world to be moved in the opposite direction to the head movement by an amount greater than the head movement itself, during both rotational and translational head movements, although a large range of movement was tolerated and judged as appearing stable. These results suggest that it not necessary to model the visual geometry accurately and suggest circumstances when tracker drift can be corrected by jumps in the display which will pass unnoticed by the user.},
	author = {Jaekl, P.M. and Allison, R.S. and Harris, L.R. and Jasiobedzka, U.T. and Jenkin, H.L. and Jenkin, M.R. and Zacher, J.E. and Zikovitz, D.C.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996517},
	issn = {1087-8270},
	keywords = {Stability;Virtual reality;Head;Displays;Cameras;Tracking;System testing;Gravity;Solid modeling;Geometry},
	month = {March},
	pages = {149-155},
	title = {Perceptual stability during head movement in virtual reality},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996517}}

@inproceedings{996518,
	abstract = {Attention deficit hyperactivity disorder (ADHD) is a childhood syndrome characterized by short attention span, impulsiveness and hyperactivity, which often leads to learning disabilities and various behavioral problems. For the treatment of ADHD, medication and cognitive-behavior therapy has been applied in recent years. Although psychostimulant medication has been widely used for many years, current findings suggest that, as the sole treatment for ADHD, it is an inadequate form of intervention, in that parents don't want their child to use drugs and the effects are limited to the period in which the drugs are physiologically active. On the other hand, EEG biofeedback treatment studies for ADHD have reported promising results not only in significant reductions in hyperactive, inattentive and disruptive behaviors, but also in improvements in academic performance and IQ scores. However, it is too boring for children to finish the whole treatment. The recent increase in computer usage in medicine and rehabilitation has changed the way health care is delivered. Virtual reality (VR) technology provides specific stimuli that can be used in removing distractions and in providing environments that get the subjects' attention and increase their ability to concentrate, and VR technology can hold a patient's attention for a longer period of time than other methods, because VR is immersive, interactive and imaginary. Based on these aspects, we developed the Attention Enhancement System (AES) using VR technology and EEG biofeedback for assessing and treating ADHD children as well as increasing the attention span of children who have attention difficulties.},
	author = {Cho, B.H. and Lee, J.M. and Ku, J.H. and Jang, D.P. and Kim, J.S. and Kim, I.Y. and Lee, J.H. and Kim, S.I.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996518},
	issn = {1087-8270},
	keywords = {Virtual reality;Electroencephalography;Biological control systems;Medical treatment;Biomedical imaging;Pediatrics;Psychology;Drugs;Medical services;Space technology},
	month = {March},
	pages = {156-163},
	title = {Attention Enhancement System using virtual reality and EEG biofeedback},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996518}}

@inproceedings{996519,
	abstract = {The effects of field-of-view (FOV) in a virtual environment (VE) on presence, enjoyment, memory and simulator sickness (SS) were studied. A refined scale, designed to assess subjects' engagement, enjoyment and immersion (E/sup 2/I), was developed. Items to examine subjects' memory of the VE were included. SS was examined using the Simulator Sickness Questionnaire (SSQ). Using a within-subjects design, data were collected from 10 subjects at four FOVs (60/spl deg/, 100/spl deg/, 140/spl deg/ and 180/spl deg/). The VE, called "Crayolaland", was presented in a driving simulator. The results indicated that presence, enjoyment and SS varied as a function of the display FOV. Subjects exhibited higher SSQ and presence subscale scores with increasing FOV. SSQ and presence values approached asymptotes for FOVs beyond 140/spl deg/. Presence and SS were positively correlated, while enjoyment and SS were negatively correlated.},
	author = {Lin, J.J.-W. and Duh, H.B.L. and Parker, D.E. and Abi-Rached, H. and Furness, T.A.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996519},
	issn = {1087-8270},
	keywords = {Displays;Virtual environment;Retina;Humans;Stability;Degradation;Navigation;Virtual reality;Testing},
	month = {March},
	pages = {164-171},
	title = {Effects of field of view on presence, enjoyment, memory, and simulator sickness in a virtual environment},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996519}}

@inproceedings{996520,
	abstract = {A head-mounted display (HMD) system suffers largely from the time lag between human motion and the display output. The concept of a reflex HMD to compensate for the time lag is proposed and discussed. Based on this notion, a prototype reflex HMD is constructed. The rotational movement of the user's head is measured by a gyroscope, modulating the driving signal for the LCD panel, and this shifts the viewport within the image supplied from the computer. The derivative distortion was investigated, and the dynamic deformation of the watched world was picked up as the essential demerit. Cylinderical rendering is introduced to solve this problem and is proved to cancel this dynamic deformation, and also to decrease the static distortion.},
	author = {Kijima, R. and Ojika, T.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996520},
	issn = {1087-8270},
	keywords = {Delay effects;Virtual reality;Hardware;Magnetic heads;Magnetic separation;Displays;Rendering (computer graphics);Adaptive filters;Data mining;Gyroscopes},
	month = {March},
	pages = {172-179},
	title = {Reflex HMD to compensate lag and correction of derivative deformation},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996520}}

@inproceedings{996521,
	abstract = {We present DIVERSE, a highly modular collection of complimentary software packages designed to facilitate the creation of device independent virtual environments. DIVERSE is free/open source software, containing both end-user programs and C++ APIs (Application Programming Interfaces). DgiPf is the DIVERSE graphics interface to OpenGL Performer/sup TM/. A program using DgiPf can run on platforms ranging from fully immersive systems such as CAVEs/sup TM/ to generic desktop workstations without modification. We describe DgiPf's design and present a specific example of how it is being used to aid researchers.},
	author = {Kelso, J. and Arsenault, L.E. and Satterfield, S.G. and Kriz, R.D.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996521},
	issn = {1087-8270},
	keywords = {Page description languages;Application software;Displays;Virtual environment;Software packages;Graphics;Visualization;Open source software;Navigation;Virtual reality},
	month = {March},
	pages = {183-190},
	title = {DIVERSE: a framework for building extensible and reconfigurable device independent virtual environments},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996521}}

@inproceedings{996522,
	abstract = {The specification, development and evaluation of a VE to allow people with Asperger's Syndrome to practise social interaction is described. A "front-ended" style has been adopted, where the emphasis on user consideration is placed during the early stages of the development life cycle, before any VE programming takes place. A model for user-centred design is presented that shows the involvement of different groups of users and professionals at various stages of design. Design guidelines have been derived from literature reviews and empirical examination of existing VEs. These have then been applied within a highly iterative cycle of low -tech VE specification and review. One VE module has been developed and evaluated with a focus on usability.},
	author = {Neale, H. and Cobb, S. and Wilson, J.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996522},
	issn = {1087-8270},
	keywords = {User centered design;Autism;Virtual reality;Variable speed drives;Usability;Virtual environment;Collaboration;Education;Programming profession;Guidelines},
	month = {March},
	pages = {191-198},
	title = {A front ended approach to the user-centred design of VEs},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996522}}

@inproceedings{996523,
	abstract = {VRML97 allows the description of dynamic worlds that can change with both the passage of time, and user interaction. Unfortunately, the current VRML usage model prevents its full potential from being realized. Initially, the whole world must be loaded into the user's desktop browser, and so large worlds can take a very long time to download and render, while a world cannot be shared among multiple users. This paper describes the design and implementation of a client-server architecture that was built to overcome these problems. The major novelty is the decoupling of VRML world execution from world rendering. Parallelism and information filtering are exploited to produce a highly scalable system that can support huge, highly active worlds, accessed simultaneously by large numbers of users. A cluster-based parallel server is responsible for maintaining the dynamic world state, and most of the world dynamics are evaluated on the server side. The server streams VRML to the client, using view frustum culling and dynamic LOD selection to reduce clients' network bandwidth, storage and rendering requirements. Clients with limited resources (e.g. wireless-connected PDAs) can therefore participate in highly complex virtual worlds. While the implementation of the design focuses on VRML worlds, the design ideas could be exploited in other types of VR system, e.g. X3D.},
	author = {Rischbeck, T. and Watson, P.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996523},
	issn = {1087-8270},
	keywords = {Read only memory;Vehicle dynamics;Virtual reality;Internet;Cities and towns;Traffic control;Filtering;Bandwidth;Personal digital assistants;Navigation},
	month = {March},
	pages = {199-206},
	title = {A scalable, multi-user VRML server},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996523}}

@inproceedings{996524,
	abstract = {Bone dissection is an important component of many surgical procedures. In this paper, we discuss a haptic and visual implementation of a bone-cutting burr that is being developed as a component of a training system for temporal bone surgery. We use a physically motivated model to describe the burr-bone interaction, which includes haptic force evaluation, the bone erosion process and the resulting debris. The current implementation, directly operating on a voxel discretization of patient-specific 3D CT and MRI data, is efficient enough to provide real-time feedback on a low-end multiprocessing PC platform.},
	author = {Agus, M. and Giachetti, A. and Gobbetti, E. and Zanetti, G. and Zorcolo, A.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996524},
	issn = {1087-8270},
	keywords = {Haptic interfaces;Bones;Surgery;Feedback;Computed tomography;Cost accounting;Magnetic resonance imaging;Layout;Friction;Sampling methods},
	month = {March},
	pages = {209-216},
	title = {Real-time haptic and visual simulation of bone dissection},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996524}}

@inproceedings{996525,
	abstract = {Recent advances in parallel computing have made it possible for scientists to perform atomistic simulations of materials involving billions of atoms. An immersive and interactive virtual environment such as ImmersaDesk is an ideal platform for exploring complex material processes in these simulations. However rendering such large datasets at an interactive speed is a major challenge. To solve this problem we have developed a visualization system by incorporating parallel and distributed computing paradigms. The system uses a parallelized fast visibility-culling algorithm based on the octree data structure to reduce the number of atoms sent to the graphics pipeline. An adaptive multiresolution algorithm based on atomic density is employed to further reduce the load on the graphics pipeline. The resulting system renders a billion-atom system at nearly interactive frame rates on a dual processor SGI Onyx2 with an InfiniteReality2 graphics pipeline connected to a 4-node PC cluster.},
	author = {Sharma, A. and Xinlian Liu and Miller, P. and Nakano, A. and Kalia, R.K. and Vashishta, P. and Wei Zhao and Campbell, T.J. and Haas, A.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996525},
	issn = {1087-8270},
	keywords = {Graphics;Pipelines;Computational modeling;Parallel processing;Virtual environment;Data visualization;Distributed computing;Data structures;Clustering algorithms;Rendering (computer graphics)},
	month = {March},
	pages = {217-223},
	title = {Immersive and interactive exploration of billion atom systems},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996525}}

@inproceedings{996526,
	abstract = {We discuss application possibilities of virtual reality technologies such as immersive projection technology, to the field of genome science, and the guiding principles for the visualization of genome information are proposed. In addition, we develop a visualization methodology by using immersive projection technology for pair-wise comparison between cluster sets generated from different gene expression datasets. Our methodology could display the distribution of overlaps between two hierarchical cluster sets based on hepatocellular carcinomas and hepatoblastomas.},
	author = {Kano, M. and Tsutsumi, S. and Nishimura, K.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996526},
	issn = {1087-8270},
	keywords = {Genomics;Bioinformatics;Data visualization;Displays;Data mining;Space technology;Virtual reality;Shape;Floods;Cathode ray tubes},
	month = {March},
	pages = {224-231},
	title = {Visualization for genome function analysis using immersive projection technology},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996526}}

@inproceedings{996527,
	abstract = {The extreme environment of a military cockpit requires a novel display technology, introduced as the virtual retinal display (VRD). A head-worn VRD generates an image by optical scanning light directly to the viewer's eye. This novel display allows the direct coupling of the display to an infrared optical head tracking system, resulting in an interactive VRD. With only minor adjustments, the interactive VRD can be used in many non-military AR and VR line-of-sight target tracking applications, such as image plane manipulations. Since the unique tracking technology shares the same aperture or scanned optical beam with the visual display, the tracking produces high accuracy and computational efficiency without the motion artifacts from video frame rate tracking. The apparatus and performance of static and dynamic 3D target tracking in 2D projection overlay are demonstrated.},
	author = {Chinthammit, W. and Seibel, E.J. and Furness, T.A.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996527},
	issn = {1087-8270},
	keywords = {Displays;Target tracking;Head;Ultraviolet sources;Retina;Image generation;Optical coupling;Virtual reality;Apertures;Optical beams},
	month = {March},
	pages = {235-242},
	title = {Unique shared-aperture display with head or target tracking},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996527}}

@inproceedings{996528,
	abstract = {We present a new virtual reality-based interaction metaphor for semi-automatic segmentation of medical 3D volume data. The mouse-based, manual initialization of deformable surfaces in 3D represents a major bottleneck in interactive segmentation. In our multi-modal system we enhance this process with additional sensory feedback. A 3D haptic device is used to extract the centerline of a tubular structure. Based on the obtained path a cylinder with varying diameter is generated which in turn is used as the initial guess for a deformable surface.},
	author = {Harders, M. and Szekely, G.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996528},
	issn = {1087-8270},
	keywords = {Haptic interfaces;Humans;Biomedical imaging;Data mining;Communications technology;Feedback;Laboratories;Computer vision;Computed tomography;Magnetic resonance imaging},
	month = {March},
	pages = {243-250},
	title = {Improving medical segmentation with haptic interaction},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996528}}

@inproceedings{996529,
	abstract = {We describe simple techniques for object group manipulation, an important operation in user interaction with a virtual environment. All presented manipulation techniques exploit constraints to simplify user interaction. The techniques are based on how humans perceive groups and afford direct manipulation of such groups. Furthermore, we introduce two new intuitive ways to create a whole group of objects: drag-add and random drag-add. Finally, we present an evaluation of the presented techniques.},
	author = {Stuerzlinger, W. and Smith, G.},
	booktitle = {Proceedings IEEE Virtual Reality 2002},
	date-added = {2024-03-18 02:28:33 -0400},
	date-modified = {2024-03-18 02:28:33 -0400},
	doi = {10.1109/VR.2002.996529},
	issn = {1087-8270},
	keywords = {Layout;Virtual reality;Packaging;User interfaces;Computer science;Virtual environment;Humans;Floors;Libraries},
	month = {March},
	pages = {251-258},
	title = {Efficient manipulation of object groups in virtual environments},
	year = {2002},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2002.996529}}

@inproceedings{913764,
	abstract = {Force information is often required for tele-operation systems and virtual reality. Conventional force displays are active systems with actuators. This, however, is inherently active, so that it may become a danger. Consequently, passive force display is an effective method for assuring safety. The authors developed a brake using ER (electrorheological) fluid and passive force display using ER brakes. They discuss two degree of freedom passive force display and basic control experiments.},
	author = {Sakaguchi, M. and Furusho, J. and Takesue, N.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913764},
	keywords = {Erbium;Force control;Electrodes;Stress;Computer displays;Virtual reality;Viscosity;Torque;Actuators;Rheology},
	month = {March},
	pages = {7-12},
	title = {Passive force display using ER brakes and its control experiments},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913764}}

@inproceedings{913765,
	abstract = {The authors first analyze dynamic properties of rapidly adapting mechanoreceptors in order to derive a principle of tactile displays. It is shown that Meissner corpuscles with coiled axons and Pacinian corpuscles with layered lamellae are suited to detect equivoluminal distortion of skin. Combining these analyses with a model of a contact and a relative motion between an object and the skin, it is shown that a prerequisite for tactile displays is to generate sources of shear stress that can be spatially dispersed at the skin surface and temporally modulated with a stick-slip frequency determined by parameters of the object to be displayed. As a device that satisfies this prerequisite, we propose a tactile display using Surface Acoustic Waves (SAW). The roughness of the surface can be changed continuously by controlling the burst frequency of the SAW.},
	author = {Nara, T. and Takasaki, M. and Maeda, T. and Higuchi, T. and Ando, S. and Tachi, S.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913765},
	keywords = {Surface acoustic waves;Acoustic waves;Displays;Mechanical factors;Skin;Surface acoustic wave devices;Rough surfaces;Surface roughness;Frequency;Nerve fibers},
	month = {March},
	pages = {13-20},
	title = {Surface Acoustic Wave (SAW) tactile display based on properties of mechanoreceptors},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913765}}

@inproceedings{913766,
	abstract = {The authors describe a new design of haptic texture display consisting of fifty vibratory pins that evoke a virtual touch sensation of textured surfaces contacted to the user's fingerpad. A pin drive mechanism was fabricated by adjusting a natural frequency to expand the displacement of a piezoelectric actuator efficiently. An improved control system enabled fine amplitude changes in 200 steps. Sensation intensity was scaled and indicated by a power function of pin amplitude. As a fundamental evaluation data, we measured the difference threshold of an object's spatial frequency presented on the display, regarding four intensity waveforms. Moreover, matching tests between real and virtual textures were performed to evaluate the quality of presentation.},
	author = {Ikei, Y. and Yamada, M. and Fukuda, S.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913766},
	keywords = {Haptic interfaces;Displays;Pins;Surface texture;Drives;Piezoelectric actuators;Control systems;Frequency measurement;Testing;Performance evaluation},
	month = {March},
	pages = {21-28},
	title = {A new design of haptic texture display - Texture Display2 - and its preliminary evaluation},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913766}}

@inproceedings{913767,
	abstract = {We present an autostereoscopic display named TWISTER I (Telexistence Wide-view-angle Immersive STEReoscope, Model I), which is designed for a face-to-face telecommunication system called "mutual telexistence". By rotating display units that consist of LED arrays and a barrier around a viewer, TWISTER I can display panoramic stereoscopic images that can be observed without the use of special eyewear. This "glassless" feature is essential for applying this apparatus to mutual telexistence because eye contact is important in non-verbal communication.},
	author = {Kunita, Y. and Ogawa, N. and Sakuma, A. and Inami, M. and Maeda, T. and Tachi, S.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913767},
	keywords = {Liquid crystal displays;Eyes;Lenses;Head;Tracking;Light emitting diodes;Virtual reality;Spatial resolution;Light sources;Apertures},
	month = {March},
	pages = {31-36},
	title = {Immersive autostereoscopic display for mutual telexistence: TWISTER I (Telexistence wide-angle immersive STEReoscope model I)},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913767}}

@inproceedings{913768,
	abstract = {We present our system for personalized face and speech communication over the Internet. The overall system consists of three parts: the cloning of real human faces to use as the representative avatars; the Networked Virtual Environment System performing the basic task of network and device management; and the speech system which includes a text-to-speech engine and a real time phoneme extraction engine from natural speech. The combination of these three elements provides a system to allow real humans, represented by their virtual counterparts, to communicate with each other even when they are geographically remote. In addition to this, all elements present use MPEG-4 as a common communication and animation standard and were designed and tested on the Windows operating system (OS). The paper presents the main aim of the work, the methodology and the resulting communication system.},
	author = {Kshirsagar, S. and Joslin, C. and Won-Sook Lee and Magnenat-Thalmann, N.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913768},
	keywords = {Oral communication;Internet;Humans;Search engines;Cloning;Face;Avatars;Virtual environment;Environmental management;Speech},
	month = {March},
	pages = {37-44},
	title = {Personalized face and speech communication over the Internet},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913768}}

@inproceedings{913769,
	abstract = {Immersive projection displays such as CABIN and COSMOS have been connected through a broadband network. This kind of network environment is expected to be used as a multimedia virtual laboratory. In particular, video avatar technology has been developed in order to realize high-presence communication in this multimedia virtual laboratory. A video avatar is a computer-synthesized 3D image created using live video. This method has the characteristics of being a natural, accurate and convenient communication tool. In this study, the communication capabilities of the video avatar were experimentally evaluated. In addition, the video avatar technology was applied to several communications applications, such as that of guiding colleagues and undertaking design work in networked immersive projection displays.},
	author = {Ogi, T. and Yamada, T. and Tamagawa, K. and Kano, M. and Hirose, M.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913769},
	keywords = {Avatars;Video sharing;Videoconference;Laboratories;Computer displays;Large screen displays;Application software;Virtual reality;Computer graphics;Collaboration},
	month = {March},
	pages = {45-51},
	title = {Immersive telecommunication using stereo video avatar},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913769}}

@inproceedings{913770,
	abstract = {With the widespread popularity of the Internet and advances in distributed computing and in virtual reality, more flexibility is needed in the development and use of collaborative virtual environments. In this paper, we present Octopus, a cross-platform, object-oriented API for constructing shared virtual worlds. The list of goals for Octopus, a description of its design and a detailed discussion of its implementation are provided. The design description gives explanations of the three components of Octopus: the core that handles networking and data sharing, the interface for implementing user representations in the virtual space (avatars), and the actual implementations of the avatars.},
	author = {Hartling, P. and Just, C. and Cruz-Neira, C.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913770},
	keywords = {Virtual reality;Chromium},
	month = {March},
	pages = {53-60},
	title = {Distributed virtual reality using Octopus},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913770}}

@inproceedings{913771,
	abstract = {Electromagnetic trackers have many favorable characteristics but are notorious for their sensitivity to magnetic field distortions resulting from metal and electronic equipment in the environment. We categorize existing tracker calibration methods and present an improved technique for reducing the static position and orientation errors that are inherent to these devices. A quaternion-based formulation provides a simple and fast computational framework for representing orientation errors. Our experimental apparatus consists of a 6-DOF mobile platform and an optical position measurement system, allowing the collection of full-pose data at nearly arbitrary orientations of the receiver. A polynomial correction technique is applied and evaluated using a Polhemus Fastrak resulting in a substantial improvement of tracking accuracy. Finally, we apply advanced visualization algorithms to give new insight into the nature of the magnetic distortion field.},
	author = {Ikits, M. and Brederson, J.D. and Hansen, C.D. and Hollerbach, J.M.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913771},
	keywords = {Calibration;Electromagnetic devices;Optical receivers;Electromagnetic fields;Magnetic fields;Electronic equipment;Optical distortion;Optical sensors;Position measurement;Polynomials},
	month = {March},
	pages = {63-70},
	title = {An improved calibration framework for electromagnetic tracking devices},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913771}}

@inproceedings{913772,
	abstract = {A novel framework enables accurate augmented reality (AR) registration with integrated inertial gyroscope and vision tracking technologies. The framework includes a two-channel complementary motion filter that combines the low-frequency stability of vision sensors with the high-frequency tracking of gyroscope sensors, hence achieving stable static and dynamic six-degree-of-freedom pose tracking. Our implementation uses an extended Kalman filter (EKF). Quantitative analysis and experimental results show that the fusion method achieves dramatic improvements in tracking stability and robustness over either sensor alone. We also demonstrate a new fiducial design and detection system in our example AR annotation systems that illustrate the behavior and benefits of the new tracking method.},
	author = {You, S. and Neumann, U.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913772},
	keywords = {Robustness;Augmented reality;Tracking;Cameras;Sensor phenomena and characterization;Sensor fusion;Low pass filters;Gyroscopes;Robust stability;Layout},
	month = {March},
	pages = {71-78},
	title = {Fusion of vision and gyro tracking for robust augmented reality registration},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913772}}

@inproceedings{913773,
	abstract = {Introduces a method for tracking a user's hand in 3D and recognizing the hand's gesture in real time without the use of any invasive devices attached to the hand. Our method uses multiple cameras for determining the position and orientation of a user's hand moving freely in a 3D space. In addition, the method identifies pre-determined gestures in a fast and robust manner by using a neural network which has been properly trained beforehand. This paper also describes results of user study of our proposed method and several types of applications, including 3D object handling for a desktop system and a 3D walkthrough for a large immersive display system.},
	author = {Sato, Y. and Saito, M. and Koike, H.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913773},
	keywords = {Human computer interaction;Graphical user interfaces;Application software;Mice;Real time systems;Cameras;Robustness;Neural networks;Three dimensional displays;Computer displays},
	month = {March},
	pages = {79-86},
	title = {Real-time input of 3D pose and gestures of a user's hand and its applications for HCI},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913773}}

@inproceedings{913774,
	abstract = {Today, scientists and engineers are exploring advanced applications and uses of immersive systems that can be cost-effectively applied in their fields. However, one of the impediments to the widespread use of these technologies is the extensive technical expertise required of application developers. A software environment that provides abstractions from specific details of hardware configurations and low-level software tools is needed to provide a common base for the prototyping, development, testing and debugging of applications. This paper describes VR Juggler, a virtual platform for the creation and execution of immersive applications, that provides a virtual reality system-independent operating environment. We focus on the approach taken to specify, design and implement VR Juggler and the benefits derived from our approach.},
	author = {Bierbaum, A. and Just, C. and Hartling, P. and Meinert, K. and Baker, A. and Cruz-Neira, C.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913774},
	keywords = {Virtual reality;Application software;Hardware;Software tools;Software prototyping;Debugging;Memory management;Packaging;Prototypes;Libraries},
	month = {March},
	pages = {89-96},
	title = {VR Juggler: a virtual platform for virtual reality application development},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913774}}

@inproceedings{913775,
	abstract = {Virtual reality (VR) installations are often unique; every one is a complex blend of hardware devices, displays and computing resources. The configuration of VR software is therefore a difficult and time-consuming process. VR Juggler, our toolkit for VR application development, addresses these problems with a number of innovations. VR Juggler provides a unique system for organizing the configuration information for a system and minimizing the proliferation of configuration files that many systems suffer. It provides a graphical tool, called VjControl, for editing configurations, which can protect users from many common mistakes. VR Juggler also has advanced capabilities for monitoring and altering the configuration of a running immersive application.},
	author = {Just, C. and Bierbaum, A. and Hartling, P. and Meinert, K. and Cruz-Neira, C. and Baker, A.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913775},
	keywords = {Virtual reality;Application software;Hardware;Computer displays;Open source software;Switches;Software libraries;Resource management;Technological innovation;Organizing},
	month = {March},
	pages = {97-104},
	title = {VjControl: an advanced configuration management tool for VR Juggler applications},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913775}}

@inproceedings{913776,
	abstract = {The client-server architecture is widely used in the construction of virtual environments because it allows specific services to be provided to the connected clients and management tasks to be performed, such as reducing the network load by performing message filtering. The majority of client-server systems are incompatible with each other, both in terms of hardware and software. In particular, the different communication protocols make the cooperation between the various systems difficult. This paper guides the reader through the construction and implementation of a server that consists of clearly distinct components, developed to reflect a reference model for a generic and open client-server system. This server, employed in the PAVR network, not only allows incompatible clients and applications to be linked into a common environment with little effort, but, in addition, the detailed description of its implementation can provide valuable help to whoever undertakes the task of coding such systems.},
	author = {Faisstnauer, C. and Purgathofer, W. and Gervautz, M. and Gascuel, J.-D.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913776},
	keywords = {Geometry;Environmental management;Animation;Network servers;Protocols;Peer to peer computing;Educational institutions;Virtual reality;Application software;Data structures},
	month = {March},
	pages = {105-112},
	title = {Construction of an open geometry server for client-server virtual environments},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913776}}

@inproceedings{913777,
	abstract = {Describes a psychophysical experiment designed to study the phenomenon of illusion which occurs with pseudo-haptic feedback, and to identify the moment when this illusion occurs: the "boundary of illusion". The subjects were given the task of deciding which of two virtual springs is the stiffer, these springs being simulated with a PHANToM/sup TM/ force feedback device and displayed on a monoscopic computer screen. The first spring had a realistic behavior, since its visual and haptic displacements were identical. The second spring-the pseudo-haptic one-was stiffer on a haptic basis, but sometimes less stiff on a visual basis. The data collected allowed us to calculate the visual point of subjective equality (PSE) between the two springs, which represents the boundary of the sensory illusion. On average, a high value of PSE turned out to be -24%. This value increased monotonically when the haptic difference between the springs increased. This implies that more visual deformation is necessary to compensate for large haptic differences and qualifies the notion of visual dominance. However, this boundary varies greatly depending on the subjects and their sensory integration strategy. The subjects were sensitive to this illusion to varying degrees. They were divided into different populations-from those who were "haptically oriented" to those who were "visually oriented".},
	author = {Lecuyer, A. and Burkhardt, J.-M. and Coquillart, S. and Coiffet, P.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913777},
	keywords = {Springs;Haptic interfaces;Psychology;Computational modeling;Computer simulation;Imaging phantoms;Force feedback;Computer displays;Virtual environment;Process design},
	month = {March},
	pages = {115-122},
	title = {"Boundary of illusion": an experiment of sensory integration with a pseudo-haptic system},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913777}}

@inproceedings{913778,
	abstract = {Describes the design and implementation of an artificial force display for immersive projection displays (IPD) such as CAVE or CABIN. In order to give the user maximum freedom of motion in the large virtual space generated by an IPD, it is necessary to use a portable force display that is self-contained on the user's body. Therefore, we developed a wearable force display called HapticGEAR. It is a backpack-type device that transmits applied forces to the wearer by using a wire-tension mechanism. The device is designed such that it minimizes fatigue for the wearer and does not interrupt the user's motion and view. Also, we integrated the device into CABIN and experimentally evaluated both the accuracy it achieved in tracing objects and the displayed haptic sensation.},
	author = {Hirose, M. and Hirota, K. and Ogi, T. and Yano, H. and Kakehi, N. and Saito, M. and Nakashige, M.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913778},
	keywords = {Displays;Haptic interfaces;Grasping;Fatigue;Force feedback;Virtual reality;Virtual environment;Data visualization;Virtual prototyping;Shape},
	month = {March},
	pages = {123-129},
	title = {HapticGEAR: the development of a wearable force display system for immersive projection displays},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913778}}

@inproceedings{913779,
	abstract = {The Gait Master is a locomotion interface that creates a sense of walking on an uneven surface. The device employs a turntable on which two motion platforms (one for each foot) are mounted. The motion platforms track the feet and carry them back to their neutral position. The user can physically walk in a virtual space while their position is maintained. The motion platforms move vertically, which simulates an uneven surface. The walker can climb up or go down a virtual staircase while their position is maintained. The turntable rotates the two motion platforms, so that the walker can walk in any direction. We have developed two prototypes and have evaluated them through user studies.},
	author = {Iwata, H. and Yano, H. and Nakaizumi, F.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913779},
	keywords = {Foot;Prototypes;Space technology;Belts;Manipulators;Legged locomotion;Virtual reality;Rough surfaces;Surface roughness;Space exploration},
	month = {March},
	pages = {131-137},
	title = {Gait Master: a versatile locomotion interface for uneven virtual terrain},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913779}}

@inproceedings{913780,
	abstract = {The paper reports empirical results from two studies of effective user interaction in immersive virtual environments. The use of 2D interaction techniques in 3D environments has received increased attention recently. We introduce two new concepts to the previous techniques: the use of 3D widget representations; and the imposition of simulated surface constraints. The studies were identical in terms of treatments, but differed in the tasks performed by subjects. In both studies, we compared the use of two-dimensional (2D) versus three-dimensional (3D) interface widget representations, as well as the effect of imposing simulated surface constraints on precise manipulation tasks. The first study entailed a drag-and-drop task, while the second study looked at a slider-bar task. We empirically show that using 3D widget representations can have mixed results on user performance. Furthermore, we show that simulated surface constraints can improve user performance on typical interaction tasks in the absence of a physical manipulation surface. Finally, based on these results, we make some recommendations to aid interface designers in constructing effective interfaces for virtual environments.},
	author = {Lindeman, R.W. and Sibert, J.L. and Templeman, J.N.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913780},
	keywords = {Mice;Virtual environment;Computational modeling;Computer simulation;Surface treatment;Fingers;Computer science;Feedback;Two dimensional displays;Virtual reality},
	month = {March},
	pages = {141-148},
	title = {The effect of 3D widget representation and simulated surface constraints on interaction in virtual environments},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913780}}

@inproceedings{913781,
	abstract = {Interfaces for system control tasks in virtual environments (VEs) have not been extensively studied. The paper focuses on various types of menu systems to be used in such environments. We describe the design of the TULIP menu, a menu system using Pinch Gloves/sup TM/, and compare it to two common alternatives: floating menus and pen and tablet menus. These three menus were compared in an empirical evaluation. The pen and tablet menu was found to be significantly faster, while users had a preference for TULIP. Subjective discomfort levels were also higher with the floating menus and pen and tablet.},
	author = {Bowman, D.A. and Wingrave, C.A.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913781},
	keywords = {Control systems;Virtual environment;Computer science;Navigation;Graphical user interfaces;Load modeling;Libraries;Visualization;Usability;System testing},
	month = {March},
	pages = {149-156},
	title = {Design and evaluation of menu systems for immersive virtual environments},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913781}}

@inproceedings{913782,
	abstract = {The Virtual Table presents stereoscopic graphics to usually a single user in a workbench like setting. The paper describes the pen and paper paradigm, which besides being an easy to use user interface for sketching or similar tasks, provides the possibility for several users to work on the Virtual Table at the same time. They are not only able to move objects or to sketch on them independently, but they even can view the objects they are working on in a stereoscopic mode with the correct perspective due to individual head tracking. This way the Virtual Table becomes usable for groups of users, too. At the same time the pen and paper paradigm in combination with two handed input creates a very intuitive user interface for drawing, sketching or painting tasks.},
	author = {Ehnes, J. and Knopfle, C. and Unbescheiden, M.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913782},
	keywords = {Virtual reality;User interfaces;Glass;Graphical user interfaces;Displays;Graphics;Painting;Costs;Product development;Hardware},
	month = {March},
	pages = {157-164},
	title = {The pen and paper paradigm - supporting multiple users on the Virtual Table},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913782}}

@inproceedings{913783,
	abstract = {For visualization in virtual reality, two topics are of major importance: real-time rendering and realism. To meet these requirements, modern graphics hardware has to be applied wherever possible. A commonly used method to improve realism without decreasing the rendering speed is texturing. Today, fast texture mapping algorithms are even integrated in low-cost hardware. However, high-resolving and non-distorting texturing is very difficult and sometimes even impossible for non-convex complex polyhedra. Nevertheless, the realism of many virtual reality applications could be improved by using non-distorted textures. Especially in surgical simulation, each anatomical detail has to be placed correctly on very complex models of human organs. For this, a new method has been developed, allowing the interactive placement of high-resolution bitmaps to any desired position on the model's surface. In addition, the visualization quality can be improved by using an antialiasing filter. This method, called arbitrary texture placement, utilizes polyhedron decomposition to split one object of complex shape into N triangles. Treating each triangle of the surface as independent object it is possible to assign them an unique part of the texture space where color information can be stored. If a bitmap is applied to the polyhedron's surface, the involved triangles are determined and the pieces of the bitmap inside the triangle are stored at the corresponding areas in the texture space. An example shows the appliance of the method to an anatomical model of the ventricular system in the human brain, used for simulating minimally invasive neurosurgery.},
	author = {Leeb, V. and Radetzky, A. and Auer, L.M.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913783},
	keywords = {Surface treatment;Visualization;Virtual reality;Rendering (computer graphics);Hardware;Humans;Surface texture;Brain modeling;Graphics;Surgery},
	month = {March},
	pages = {165-171},
	title = {Interactive texturing by polyhedron decomposition},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913783}}

@inproceedings{913784,
	abstract = {Head-mounted projective displays (HMPD) have been recently proposed as an alternative to conventional eyepiece-type head-mounted displays. HMPDs consist of a pair of miniature projection lenses and displays mounted on the helmet and retro-reflective sheeting materials placed strategically in the environment. Its novel concept and properties suggest solutions to part of the problems of state-of-the-art visualization devices and make it extremely suitable for multiple-user collaborative applications and wearable systems. A brief review of conventional visualization techniques is followed by an extensive discussion of HMPD technology, which includes a summary of its features and a comparison with conventional head-mounted displays (HMDs), projection-based displays, and HMPDs. An ultra-light and compact design (i.e. 8g) of a projection lens system using diffractive optical element (DOE) as well as plastic components for a HMPD is presented. Through the usage of fast prototyping technology, a compact stereoscopic head-mounted prototype with weight less than 700 grams was implemented, and optomechanical adjustments and ergonomic considerations are discussed. Finally, the motivated application in multiuser tele-collaboration is described.},
	author = {Hong Hua and Chunyu Gao and Biocca, F. and Rolland, J.P.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913784},
	keywords = {Displays;Lenses;Visualization;Prototypes;Optical materials;Sheet materials;Collaboration;Optical design;Optical diffraction;Optical devices},
	month = {March},
	pages = {175-182},
	title = {An ultra-light and compact design and implementation of head-mounted projective displays},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913784}}

@inproceedings{913785,
	abstract = {Immersive environments are successfully being used to support mission operations at the Jet Propulsion Laboratory. This technology contributed to the Mars Pathfinder Mission in planning sorties for the Sojourner rover. Results and operational experiences with these tools are being incorporated into the development of the second generation of mission planning tools. NASA's current plan includes two rovers being deployed to Mars in 2003 and early 2004. The next generation Rover Control Workstation utilizes existing technologies and more to provide a multimodal, collaborative, partially immersive environment. This system includes tools for planning long range sorties for highly autonomous rovers, tools for building the three-dimensional (3D) models of the terrain being explored, and advanced tools for visualizing telemetry from remote spacecraft and landers. These tools comprise a system for immersing the operator in the environment of another planet, body, or space to make the mission planning function more intuitive and effective.},
	author = {Wright, J. and Hartman, F. and Cooper, B.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913785},
	keywords = {Mars;Propulsion;Path planning;Technology planning;Workstations;Space technology;Collaborative work;Visualization;Telemetry;Space vehicles},
	month = {March},
	pages = {183-190},
	title = {Immersive environment technologies for planetary exploration},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913785}}

@inproceedings{913786,
	abstract = {We developed a stereoscopic video system, which has high-resolution images for central vision and it is called the Q system. The Q system uses a compound image that is a wide-angle image with an embedded high-resolution image. However, the Q system could not be used under situations where many robots work at the same time. This is because it needs four channels of video signals and the available channels could be limited under such situations. Thus, we have developed a digital Q system. The system can be used under such restricted situations, because the required data transfer rate is adjustable by changing the compression rates for a high-resolution image and a wide-angle image. In addition, an experiment confirmed that even though the systems used the same data transfer rate, digital Q system could make teleoperation more efficient and more precise than a conventional stereoscopic video system.},
	author = {Goshi, K. and Matsunaga, K. and Nagata, H. and Shidoji, K. and Matsugashita, H.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913786},
	keywords = {Image resolution;Humans;Orbital robotics;Cameras;Intelligent systems;Lenses;Robots;Video compression;Image reconstruction;Computer science},
	month = {March},
	pages = {191-197},
	title = {Digital stereoscopic video system with embedded high resolution images},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913786}}

@inproceedings{913787,
	abstract = {This paper presents our findings after the first year and a half of a multi-year deployment of an ImmersaDesk/sup (R)/ to a local elementary school, investigating its effectiveness in enhancing science education. These findings deal with how VR can aid in the coordination of multiple representations, and how to integrate the technology into the existing school culture.},
	author = {Johnson, A. and Moher, T. and Ohlsson, S. and Leigh, J.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913787},
	keywords = {Educational institutions;Virtual reality;Cognitive science;Visualization;Solids;History;Education;Laboratories;Displays;Acceleration},
	month = {March},
	pages = {201-208},
	title = {Exploring multiple representations in elementary school science education},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913787}}

@inproceedings{913788,
	abstract = {The use of virtual and augmented reality techniques in medicine is rapidly increasing particularly in the area of minimal access surgery. Such surgery is a form of teleoperation in which accurate perception of depth and orientation, navigation, and interaction with the operative space are vital. Virtual and augmented reality techniques will allow us to produce new views of the operative site and introduce extra information into the scene such as safe paths for instruments to follow etc. A path following task is developed and human factors issues are addressed by varying viewing conditions (standard mono, stereo, multiple views and tool-linked view), presence or absence of haptic feedback, and orientation of the task. The results show that performance is improved with haptic feedback, but not by the various viewing conditions and is significantly worse for side aligned orientations.},
	author = {Passmore, P.J. and Nielsen, C.F. and Cosh, W.J. and Darzi, A.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913788},
	keywords = {Augmented reality;Surgery;Haptic interfaces;Feedback;Navigation;Layout;Instruments;Human factors;Standards development;MONOS devices},
	month = {March},
	pages = {209-215},
	title = {Effects of viewing and orientation on path following in a medical teleoperation environment},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913788}}

@inproceedings{913789,
	abstract = {A lot of effort is now being put into developing collaborative distributed virtual environments. However very few projects address collaborative virtual sculpting in which the shapes of the target objects are likely changing continuously. Some major issues including user interaction, data transmission, concurrent object editing by multiple clients and rendering of deforming objects must be addressed in a real-time context. We propose a framework for collaborative virtual sculpting in a distributed virtual environment. The system is based on a hybrid model which merges the client-server and the peer-to-peer architectures to allow fast data replication. To support real-time deformation and rendering of deformable objects, we model each of these objects using NURBS surfaces and render them using the real-time deformable NURBS rendering method that we have developed. We present a data structure for the transmission of these deformable objects. We also introduce the idea of editing region and the corresponding locking mechanism for simultaneous editing of the same object by multiple clients. Toward the end of the paper we show some performance results of the prototype system.},
	author = {Li, F.W.B. and Lau, R.W.H. and Ng, F.F.C.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913789},
	keywords = {Collaboration;Virtual environment;Spline;Surface topography;Surface reconstruction;Shape;Data communication;Peer to peer computing;Deformable models;Data structures},
	month = {March},
	pages = {217-224},
	title = {Collaborative distributed virtual sculpting},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913789}}

@inproceedings{913790,
	abstract = {Many motion base simulators have been developed in the last thirty years for many different types of vehicles. In order to make a simulation more realistic, linear accelerations and angular rates are exerted on the pilot by moving the platform on which the mock-up vehicle is located. This has to be accomplished without driving the simulator out of its workspace. The software component that is in charge of this is commonly referred to as washout filter. Washout filters have been widely investigated in the past, mainly in the field of flight simulators. We present a washout filter designed for a motorcycle simulator. The solution is preliminary and follows, as a reference point, techniques previously adopted for large aircraft simulators. Differences between motorcycle and aircraft simulation are analyzed and a preliminary customized solution is proposed. The washout filter to be used to drive a motorcycle simulator currently being built at PERCRO, has been tested off-line showing good results and will soon be tested on real riders.},
	author = {Barbagli, F. and Ferrazzin, D. and Avizzano, C.A. and Bergamasco, M.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913790},
	keywords = {Filters;Motorcycles;Vehicles;Aerospace simulation;Aircraft;Computational modeling;Testing;Costs;Delta modulation;Actuators},
	month = {March},
	pages = {225-232},
	title = {Washout filter design for a motorcycle simulator},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913790}}

@inproceedings{913791,
	abstract = {With technological progress, wide field-of-view (FOV) displays will become increasingly common. Wide FOVs provide a more immersive environment and produce stronger self-motion perception. The objective of this study was to investigate the relationships between FOV and scene content on postural stability in an immersive environment. 10 subjects were tested using two different scenes (a simple radial pattern and a "meaningful" city scene) at six FOVs (30/spl deg/, 60/spl deg/, 90/spl deg/, 120/spl deg/, 150/spl deg/ 180/spl deg/) using a within-subjects design. Subjects exhibited more postural disturbance with increasing FOV. A surprisingly large increase in disturbance was found for the interval between 150/spl deg/ and 180/spl deg/ using the city scene. No statistically significant difference was found for effects of scene content. Two groups (postural stable group and postural unstable group) were identified during experiment. These groups performed differently in the two scene conditions. Future research plans are described.},
	author = {Duh, H.B.-L. and Lin, J.W. and Kenyon, R.V. and Parker, D.E. and Furness, T.A.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913791},
	keywords = {Displays;Layout;Retina;Virtual environment;Humans;Cities and towns;Stability;Testing;Degradation;Brain modeling},
	month = {March},
	pages = {235-240},
	title = {Effects of field of view on balance in an immersive environment},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913791}}

@inproceedings{913792,
	abstract = {We examine whether visual motion induces a sensation of motion for a static sound in order to investigate dynamic visual-auditory interactions in the virtual environment. The dynamic interaural time difference (ITD) was employed as the cue for sound motion, and the effect of the size of the visual stimuli on biasing the perceived direction of dynamic ITD was examined As a result, induced motion sensation for static sound was obtained, and its direction was dependent on the size and direction of the applied visual stimuli. For a small visual stimuli (an 1-deg disk), sound with a static ITD moved in the same direction as the moving visual stimuli. For a large visual stimuli (a 90-deg random-dot pattern), however, sound with a static ITD moved to the opposite direction to that of the moving visual stimuli.},
	author = {Kayahara, T. and Sato, T.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913792},
	keywords = {Rendering (computer graphics);Sociology;Virtual environment;Psychology;Humans;Art;Virtual reality;Navigation;Laboratories;Cathode ray tubes},
	month = {March},
	pages = {241-245},
	title = {Auditory motion induced by visual motion and its dependence on stimulus size},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913792}}

@inproceedings{913793,
	abstract = {To enhance presence, facilitate sensory motor performance, and avoid disorientation or nausea, virtual-reality applications require the perception of a stable environment. End-end tracking latency (display lag) degrades this illusion of stability and has been identified as a major fault of existing virtual-environment systems. Oscillopsia refers to the perception that the visual world appears to swim about or oscillate in space and is a manifestation of this loss of perceptual stability of the environment. The effects of end-end latency and head velocity on perceptual stability in a virtual environment were investigated psychophysically. Subjects became significantly more likely to report oscillopsia during head movements when end-end latency or head velocity were increased. It is concluded that perceptual instability of the world arises with increased head motion and increased display lag. Oscillopsia is expected to be more apparent in tasks requiring real locomotion or rapid head movement.},
	author = {Allison, R.S. and Harris, L.R. and Jenkin, M. and Jasiobedzka, U. and Zacher, J.E.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913793},
	keywords = {Head;Displays;Stability;Position measurement;Tracking;Delay effects;Psychology;Virtual environment;Virtual reality;Distortion measurement},
	month = {March},
	pages = {247-254},
	title = {Tolerance of temporal delay in virtual environments},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913793}}

@inproceedings{913794,
	abstract = {Applies the linear elastic finite element method to compute haptic force feedback and domain deformations of soft-tissue models for use in virtual reality simulators. Our results show that, for virtual object models of high-resolution 3D data (>10,000 nodes), haptic real-time computations (<500 Hz) are nor currently possible using traditional methods. Current research efforts are focused in the following areas: (1) efficient implementation of fully adaptive multi-resolution methods, and (2) multi-resolution methods with specialized basis functions to capture the singularity at the haptic interface (point loading). To achieve real-time computations, we propose parallel processing of a Jacobi pre-conditioned conjugate gradient method applied to a reduced system of equations resulting from surface domain decomposition. This can effectively be achieved using reconfigurable computing systems such as field programmable gate arrays (FPGAs), thereby providing a flexible solution that allows for new FPGA implementations as improved algorithms become available. The resulting soft-tissue simulation system would meet NASA Virtual Glovebox requirements and, at the same time, provide a generalized simulation engine for any immersive environment application, such as biomedical/surgical procedures or interactive scientific applications.},
	author = {Frank, A.O. and Twombly, I.A. and Barth, T.J. and Smith, J.D.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913794},
	keywords = {Finite element methods;Haptic interfaces;Virtual reality;Computational modeling;Field programmable gate arrays;Deformable models;Force feedback;Real time systems;Concurrent computing;Parallel processing},
	month = {March},
	pages = {257-263},
	title = {Finite element methods for real-time haptic feedback of soft-tissue models in virtual reality simulators},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913794}}

@inproceedings{913795,
	abstract = {Previous works have presented solutions for stability problems arising from the difference between the sampling rate requirements for haptic devices (about 1 kHz) and the update rates of the physical objects being simulated (about 10 Hz). These methods work well when the objects are convex and non-deformable but, when the object is deformable, these methods might fail in obtaining realistic force feedback and exact graphical rendering. The reason of this is due to the concavities and unknown shapes that may appear in the deformable objects. This paper proposes to perform haptic interaction with the local topology of the object, taking into account the unknown changes and the concavities in the object shape. This local model is updated at the simulation frequency rate.},
	author = {Mendoza, C.A. and Laugier, C.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913795},
	keywords = {Haptic interfaces;Virtual environment;Deformable models;Rendering (computer graphics);Frequency;Virtual reality;Force feedback;Shape;Computational modeling;Humans},
	month = {March},
	pages = {264-269},
	title = {Realistic haptic rendering for highly deformable virtual objects},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913795}}

@inproceedings{913796,
	abstract = {A path-planning algorithm for encountered-type haptic devices that render multiple virtual objects in 3D space is proposed. To render multiple virtual objects with a single haptic device, the following two behaviors must be realized: (i) the device should move to the target object location in advance of the user's hand so that the user can encounter it; (ii) otherwise, the device should keep away from the user's hand, avoiding an unexpected collision. To realize such a consistent encounter, the user's hand motion must be tracked. The proposed algorithm determines the device location based on the minimum distance problem between a point (the user's hand) and a convex polyhedron constructed from the reference points of multiple objects. An automobile virtual control panel was constructed using an encountered-type haptic device in which the proposed path-planning algorithm was implemented. The validity of the proposed algorithm was confirmed and the required performances, such as the device speed and the accuracy of hand tracking were evaluated.},
	author = {Yokokohji, Y. and Kinoshita, J. and Yoshikawa, T.},
	booktitle = {Proceedings IEEE Virtual Reality 2001},
	date-added = {2024-03-18 02:28:27 -0400},
	date-modified = {2024-03-18 02:28:27 -0400},
	doi = {10.1109/VR.2001.913796},
	keywords = {Path planning;Haptic interfaces;Switches;Target tracking;Automobiles;Performance evaluation;Robots;Mechanical engineering;Road accidents;Exoskeletons},
	month = {March},
	pages = {271-278},
	title = {Path planning for encountered-type haptic devices that render multiple objects in 3D space},
	year = {2001},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2001.913796}}

@inproceedings{840357,
	abstract = {Describes how a physical simulation can be integrated with our Responsive Workbench system to support complex assembly tasks involving multiple hands and users. Our system uses the CORIOLIS physical simulation package extended to meet the real-time requirements for our highly interactive virtual environment. We develop a new set of interface tools that exploit the natural properties of physical simulation (i.e. the superposition of forces). Our tools are based on sets of springs connecting the user's hand to a virtual object. Visualizing these springs provides "visual force feedback" of the applied forces and facilitates the prediction of the objects' behavior. Our force-based interaction concept allows multiple hands and users to manipulate a single object without the need for locking the object.},
	author = {Frohlich, B. and Tramberend, H. and Beers, A. and Agrawala, M. and Baraff, D.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840357},
	issn = {1087-8270},
	keywords = {Virtual environment;Object detection;Springs;Computational modeling;Displays;Information technology;Assembly systems;Packaging;Joining processes;Visualization},
	month = {March},
	pages = {5-11},
	title = {Physically-based manipulation on the Responsive Workbench},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840357}}

@inproceedings{840358,
	abstract = {The Perceptive Workbench enables a spontaneous, natural and unimpeded interface between the physical and virtual worlds. It uses vision-based methods for interaction that eliminate the need for wired input devices and wired tracking. Objects are recognized and tracked when placed on the display surface. Through the use of multiple light sources, the object's 3D shape can be captured and inserted into the virtual interface. This ability permits spontaneity since either preloaded objects or those objects selected on-the-spot by the user can become physical icons. Integrated into the same vision-based interface is the ability to identify 3D hand position, pointing direction and sweeping arm gestures. Such gestures can enhance selection, manipulation and navigation tasks. In this paper, the Perceptive Workbench is used for augmented reality gaming and terrain navigation applications, which demonstrate the utility and capability of the interface.},
	author = {Leibe, B. and Starner, T. and Ribarsky, W. and Wartell, Z. and Krum, D. and Singletary, B. and Hodges, L.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840358},
	issn = {1087-8270},
	keywords = {Wires;Shape;Navigation;Virtual environment;Augmented reality;Fingers;Surface reconstruction;Displays;Light sources;Application software},
	month = {March},
	pages = {13-20},
	title = {The Perceptive Workbench: toward spontaneous and natural interaction in semi-immersive virtual environments},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840358}}

@inproceedings{840359,
	abstract = {The paper introduces the idea of using real mirrors in combination with rear-projection systems for the purpose of interacting with and navigating through the displayed information. Subsequently, a derived application is described. For this, we use a hand-held planar mirror and address two fundamental problems of applying head tracking with rear-projection planes: the limited viewing volume of these environments and their incapability of simultaneously supporting multiple observers. Furthermore, we describe the possibility of combining a reflective pad with a transparent one, thus introducing a complementary tool for interaction and navigation.},
	author = {Bimber, O. and Encamacao, L.M. and Schmalstieg, D.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840359},
	issn = {1087-8270},
	keywords = {Mirrors;Layout;Navigation;Head;Reflection;Computer graphics;Electronic switching systems;Virtual environment;Research and development;Electrical capacitance tomography},
	month = {March},
	pages = {21-28},
	title = {Real mirrors reflecting virtual worlds},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840359}}

@inproceedings{840360,
	abstract = {Currently there are no deformable model implementations that model a wide range of geometric deformations while providing realistic force feedback for use in virtual environments with haptics. The few models that exist are computationally very expensive, are limited in terms of shape coverage and do not provide proper haptic feedback. We use dynamic deformable models with local and global deformations governed by physical principles in order to provide efficient and true force feedback. We extend the shape class of Deformable Superquadrics (DeSuq) to provide compact geometric representation using few parameters, while at the same time providing realistic haptic viscoelastic feedback. Dynamics associated with rigid and deformable bodies are modeled by the use of the Lagrange equations. Implementation of these is currently under progress using GHOST/sup TM/ libraries on a PHANToM/sup TM/ haptic device.},
	author = {Ramanathan, R. and Metaxas, D.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840360},
	issn = {1087-8270},
	keywords = {Deformable models;Haptic interfaces;Force feedback;Shape;Solid modeling;Virtual environment;Viscosity;Elasticity;Lagrangian functions;Equations},
	month = {March},
	pages = {31-35},
	title = {Dynamic deformable models for enhanced haptic rendering in virtual environments},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840360}}

@inproceedings{840361,
	abstract = {Previous interactive works have used springs, heuristics, and dynamics for surface placement applications. We present an analytical technique for kilohertz rate manipulation of CAD models with virtual surface and trimming constraints. The optimization approach allows best placement and sensitivity analysis for mechanical design objectives and parametric domain objectives. Such objectives are not readily incorporated into previous interactive methods. Force feedback is rendered to the user using previously developed haptics principles.},
	author = {Nelson, D.D. and Cohen, E.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840361},
	issn = {1087-8270},
	keywords = {Force control;Jacobian matrices;Force feedback;Design automation;Kinematics;Surface treatment;Prototypes;Constraint optimization;Animation;Optimization methods},
	month = {March},
	pages = {37-44},
	title = {Optimization-based virtual surface contact manipulation at force control rates},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840361}}

@inproceedings{840362,
	abstract = {We present an intuitive 3D interface for interactively editing and painting a polygonal mesh using a force feedback device. An artist or a designer can use the system to create and refine a three-dimensional multiresolution polygonal mesh. Its appearance can be further enhanced by directly painting onto its surface. The system allows users to naturally create complex forms and patterns not only aided by visual feedback, but also by their sense of touch.},
	author = {Gregory, A.D. and Ehmann, S.A. and Lin, M.C.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840362},
	issn = {1087-8270},
	keywords = {Painting;Haptic interfaces;Virtual reality;Power system modeling;Solid modeling;Feedback;Virtual environment;User interfaces;Animation;Software packages},
	month = {March},
	pages = {45-52},
	title = {inTouch: interactive multiresolution modeling and 3D painting with a haptic interface},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840362}}

@inproceedings{840363,
	abstract = {In recent years, virtual reality (VR) has had a growing impact on business in the oil and gas industry. In 1997, there were only two VR visualization centers in the oil and gas industry, but now there are more than 20 visualization centers using VR technology. However most VR software systems in use are customized to perform specific tasks, and though some positive results have been reported, no systematic evaluation has been performed to quantify the benefits of using VR to interact with geoscience data. Therefore, we designed and implemented a VR application for the geosciences and then invited geoscientists and software developers to evaluate this application. The results show that VR provides a significantly enhanced visualization environment. Our results also suggest that the major challenge to the successful implementation of VR involves the improvement of VR interaction techniques.},
	author = {Ching-Rong Lin and Loftin, R.B. and Nelson, H.R.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840363},
	issn = {1087-8270},
	keywords = {Geoscience;Virtual reality;Geology;Petroleum;Application software;Virtual environment;Visualization;Data analysis;High performance computing;Conferences},
	month = {March},
	pages = {55-62},
	title = {Interaction with geoscience data in an immersive environment},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840363}}

@inproceedings{840364,
	abstract = {Virtual environment (VE) technology is increasingly being recognized as a useful medium for the study, assessment, and rehabilitation of cognitive processes and functional abilities. The capacity of VE technology to create dynamic three-dimensional (3D) stimulus environments, within which all behavioral responding can be recorded, offers clinical assessment and rehabilitation options that are not available using traditional neuropsychological methods. This work has the potential to advance the scientific study of normal cognitive and behavioral processes and to improve our capacity to understand, measure, and treat the impairments typically found in clinical populations with central nervous system (CNS) dysfunction. The paper provides a rationale for the application of VE technology in the areas of neuropsychological assessment and cognitive rehabilitation, presents a tabled summary of the VE literature targeting cognitive/functional processes in clinical CNS populations and briefly describes two of our VE applications targeting attention and visuospatial processing.},
	author = {Rizzo, A. and Buckwalter, J.G. and van der Zaag, C. and Neumann, U. and Thiebaux, M. and Chua, C. and van Rooyen, A. and Humphrey, L. and Larson, P.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840364},
	issn = {1087-8270},
	keywords = {Virtual environment;Psychology;Rain;Chromium;Psychometric testing;Nervous system;Identity-based encryption;Instruments;Automobiles;Transportation},
	month = {March},
	pages = {63-70},
	title = {Virtual environment applications in clinical neuropsychology},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840364}}

@inproceedings{840366,
	abstract = {This paper describes "The Thing Growing", a work of interactive fiction implemented in virtual reality, in which the user is the main protagonist and interacts with computer controlled characters. This work of fiction depends on the user's emotional investment in the story and on her relationship to a central character, the Thing.},
	author = {Anstey, J. and Pape, D. and Sandin, D.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840366},
	issn = {1087-8270},
	keywords = {Virtual reality;Humans;Computer displays;Virtual environment;Animation;Automatic control;Visualization;Laboratories;Investments;Intelligent agent},
	month = {March},
	pages = {71-78},
	title = {The Thing Growing: autonomous characters in virtual reality interactive fiction},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840366}}

@inproceedings{840369,
	abstract = {This paper considers whether a passive isometric input device, such as a Spaceball/sup TM/, used together with visual feedback, could provide the operator with a pseudo-haptic feedback. For this aim, two psychophysical experiments have been conducted. The first experiment consisted of a compliance discrimination, between two virtual springs hand-operated by means of the Spaceball/sup TM/. In this experiment, the stiffness (or compliance) JND turned out to be 6%. The second experiment assessed stiffness discrimination between a virtual spring and the equivalent spring in reality. In this case, the stiffness (or compliance) JND was found to be 13.4%. These results are consistent with previous outcomes on manual discrimination of compliance. Consequently, this consistency reveals that the passive apparatus that was used can, to some extent, simulate haptic information. In addition, a final test indicated that the proprioceptive sense of the subjects was blurred by visual feedback. This gave them the illusion of using a nonisometric device.},
	author = {Lecuyer, A. and Coquillart, S. and Kheddar, A. and Richard, P. and Coiffet, P.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840369},
	issn = {1087-8270},
	keywords = {Force feedback;Electrical capacitance tomography;Springs;Psychology;Electronic switching systems;Haptic interfaces;Testing;Human computer interaction;Software prototyping;Prototypes},
	month = {March},
	pages = {83-90},
	title = {Pseudo-haptic feedback: can isometric input devices simulate force feedback?},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840369}}

@inproceedings{840486,
	abstract = {Fish tank VR systems provide head coupled perspective projected stereo images on a display device of limited dimensions that resides at a fixed location. Therefore, fish tank VR systems provide only a limited virtual workspace. As a result, such systems are less suited for displaying virtual worlds that extend beyond the available workspace and depth perception problems arise when displaying objects (virtually) located on the edge of the workspace in between the viewer and the display screen. In this paper we present two techniques to reduce this disadvantage: cadre viewing and amplified head rotations. The first aims to eliminate the problems in depth perception for objects with negative parallax touching the screen surround. Subjective observations from an informal user study indicate a reduction of confusion in depth perception. The second provides a transparent navigation technique to allow users to view larger portions of the virtual world without the need for an additional input device to navigate. A user study shows it performs equally well when compared to a technique based on the use of an additional spatial input device.},
	author = {Mulder, J.D. and van Liere, R.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840486},
	issn = {1087-8270},
	keywords = {Marine animals;Virtual reality;Head;Computer displays;Navigation;Monitoring;Electrical capacitance tomography;Costs;Mathematics;Computer science},
	month = {March},
	pages = {91-98},
	title = {Enhancing fish tank VR},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840486}}

@inproceedings{840487,
	abstract = {This paper describes a task-oriented approach to cope with the difference between the three dimensional (3D) space in a virtual reality (VR) system and that perceived by the user from its stereoscopic images with binocular disparity. This difference occurs for the reason that human visual perception is affected not only by binocular disparity but also by various kinds of visual cues. Although it is possible to set some of those cues consistent with each other when they are presented by a stereoscopic display, it is difficult to do the same for other cues especially vergence and focus. As the result, the users of VR systems often fail to manipulate virtual objects due to this difference. In this article, we propose to adjust the virtual space in a VR system through object manipulation by the user so that the user does not fail in each manipulation. We verified by experiments that this adjustment is effective to improve the rate of successful manipulation.},
	author = {Kakusho, K. and Kitawaki, J. and Hagihara, S. and Minoh, M.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840487},
	issn = {1087-8270},
	keywords = {Virtual reality;Humans;Visual perception;Focusing;Visualization;Multimedia systems;Three dimensional displays;Eyes;Layout;Retina},
	month = {March},
	pages = {99-106},
	title = {Adjusting the difference between 3D spaces in VR systems and human perception through object manipulation},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840487}}

@inproceedings{840488,
	abstract = {When a projector is oblique with respect to a planar display surface, it creates keystoning and the projected image is distorted. We present a rendering technique to display perspectively correct images for a moving user. This allows using roughly aligned projectors and eliminates the need for frequent electro-mechanical adjustments. The rendering process has no additional cost and can be implemented with traditional graphics hardware. We compute the collineation induced due to the display plane during preprocessing. The main idea of the paper is to use this collineation to render and warp the images of 3D scenes in a single pass via approximation of the depth buffer. We also describe how this method can be extended to display systems with multiple overlapping projectors. This technique can be easily used in CAVE, Immersive Workbenches and PowerWalls.},
	author = {Raskar, R.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840488},
	issn = {1087-8270},
	keywords = {Displays;Optical distortion;Rendering (computer graphics);Costs;Graphics;Pipelines;World Wide Web;Layout;Identity-based encryption;Lapping},
	month = {March},
	pages = {109-115},
	title = {Immersive planar display using roughly aligned projectors},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840488}}

@inproceedings{840489,
	abstract = {Projection-based visual display systems are expected to be effective platforms for virtual reality (VR) applications in which the displayed images are generated by computer graphics using 3D models of virtual worlds. However, these kinds of visual displays, as well as other kinds of fixed screen-based displays, such as various head-tracked displays (HTDs) and conventional CRT displays, have not been utilized to achieve precise telexistence in a real environment, which requires appropriate stereoscopic video images corresponding to the operator's head motion. We found that the time-varying off-axis projection required in these systems has prevented fixed screen-based displays from being used for telexistence, as ordinary cameras only have fixed and symmetric fields of view about the optical axis. After evaluating the problem, a method to realize a live video-based telexistence system with a fixed screen is proposed, aiming to provide the operator with a natural 3D sensation of presence. The key component of our method is a feature that keeps the orientation of the cameras fixed, regardless of the operator's head motion. Such a feature was implemented by designing a constant-orientation link mechanism.},
	author = {Yanagida, Y. and Maeda, T. and Tachi, S.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840489},
	issn = {1087-8270},
	keywords = {Visual system;Computer displays;Virtual reality;Head;Cameras;Three dimensional displays;Application software;Image generation;Computer graphics;Cathode ray tubes},
	month = {March},
	pages = {117-124},
	title = {A method of constructing a telexistence visual system using fixed screens},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840489}}

@inproceedings{840490,
	abstract = {We developed a pointer in 3D virtual space, using an eye-tracking system as a sensor. The eye mark pointer is installed to a virtual environment system which provides stereoscopic vision with an immersive projection display. The circular-polarization stereoscopic vision enables us to use the eye-tracking system in the immersive projection display. The eye-tracking system obtains relative gaze directions with respect to the head, so the absolute position requires compensation of the user's head motion with a head tracker. We then compare the eye mark pointer with a joystick in an experiment with the virtual environment system. The experimental result indicates the pointing of the eye mark pointer is 9.8 times quicker than that of the joystick, and suggests that the eye mark pointer is available for pointing at the target in the virtual environment.},
	author = {Asai, K. and Osawa, N. and Takahashi, H. and Sugimoto, Y.Y. and Yamazaki, S. and Samejima, A. and Tanimae, T.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840490},
	issn = {1087-8270},
	keywords = {Virtual environment;Polarization;Sensor systems;Mice;Liquid crystal displays;Humans;Target tracking;Grasping;Thermal force;Visualization},
	month = {March},
	pages = {125-132},
	title = {Eye mark pointer in immersive projection display},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840490}}

@inproceedings{840491,
	abstract = {In virtual environments containing a very large number of objects, the limited amount of available resources often proves to be a bottleneck, causing a competition for those resources, for example network bandwidth, processing power or the rendering pipeline. This leads to a degradation of the system's performance, as only a small number of elements can be granted the resource required. We present a generic scheduling algorithm that allows us to achieve a graceful degradation: it is output sensitive, minimizes the risk of starvation and enforces priorities based on a freely definable error metric. Hence it can be employed in virtual environments of almost any size, to schedule elements which are competing for a determined resource because of a bottleneck.},
	author = {Faisstnauer, C. and Schmalstieg, D. and Purgathofer, W.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840491},
	issn = {1087-8270},
	keywords = {Dead reckoning;Bandwidth;Operating systems;Virtual environment;Graphics;Pipelines;Rendering (computer graphics);Processor scheduling;Extrapolation;Acceleration},
	month = {March},
	pages = {135-142},
	title = {Priority round-robin scheduling for very large virtual environments},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840491}}

@inproceedings{840492,
	abstract = {In virtual environment systems, the ultimate goal is delivery of the highest-fidelity user experience possible. The paper describes a general form framework for optimizing fidelity on a per-task basis. The virtual world database is priority ordered; the ordering is dynamically recomputed based on display platform, virtual world state, and (possibly shifting) task objectives. Optimization is performed with the QUICK model, which integrates ratings of representational quality, scene node importance, and machine resource cost. The approach has been implemented in a prototype display and distributed cache management system, which can be incorporated into most existing networked virtual reality applications.},
	author = {Capps, M.V.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840492},
	issn = {1087-8270},
	keywords = {Virtual environment;Displays;Delay;Layout;Virtual prototyping;Capacitive sensors;Rendering (computer graphics);Collaboration;Application software;Databases},
	month = {March},
	pages = {143-150},
	title = {The QUICK framework for task-specific asset prioritization in distributed virtual environments},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840492}}

@inproceedings{840493,
	abstract = {Replication is often used to provide users of distributed virtual environments with high-performance interactions. Concurrency control is required to avoid inconsistent views among replicas due to multiple concurrent updates. G. Lann has developed a prediction-based concurrency control scheme to allow real-time interactions for users and to eliminate the need for repairs. The existing scheme does not scale in terms of delivering ownership on time as the number of users increases. In this paper, we propose a scalable prediction-based concurrency control scheme with entity-centric multicasting: only the users surrounding a target entity multicast the ownership requests, by using the multicast address assigned to the entity. The experimental results and analysis reported in this paper show that the proposed scheme achieves the benefits of prediction-based concurrency control with efficiency and scalability for large distributed virtual environments.},
	author = {Jeonghwa Yang and Dongman Lee},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840493},
	issn = {1087-8270},
	keywords = {Concurrency control;Virtual environment;Concurrent computing;Delay;Electrical capacitance tomography;Collaboration;Hip;Sociotechnical systems;Electronic switching systems;IP networks},
	month = {March},
	pages = {151-158},
	title = {Scalable prediction based concurrency control for distributed virtual environments},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840493}}

@inproceedings{840494,
	abstract = {Presents our work on a level-of-detail (LoD) technique for human-like face models in virtual environments. Conventional LoD techniques have been adapted to allow facial animation on simplified geometric models. This includes the optimization of both geometric and animation parameters. Simplified models are generated in a region-based manner, considering the mobility of each region. The animation process is decomposed into two sub-processes, and each step is optimized. In the MPA (minimum perceptible action) level optimization, a hierarchical structure is devised for the multi-level animation model. The deformation level is simplified by reducing the number of control points. At run-time, the animation level is selected in combination with viewpoint information at the geometric level.},
	author = {Hyewon Seo and Thalmann, N.M.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840494},
	issn = {1087-8270},
	keywords = {Facial animation;Humans;Virtual environment;Geometry;Layout;Runtime;Face;Graphics;Rendering (computer graphics);Biological system modeling},
	month = {March},
	pages = {161-168},
	title = {LoD management on animating face models},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840494}}

@inproceedings{840495,
	abstract = {Although technologies such as head-mounted displays and CAVEs can be used to provide large immersive visual displays within small physical spaces, it is difficult to provide virtual environments which are as large physically as they are visually. A fundamental problem is that tracking technologies which work well in a small enclosed environment do not function well over longer distances. In this paper, we describe Trike-a 'rideable' computer system which can be used to generate and explore large virtual spaces both visually and physically. This paper describes the hardware and software components of the system and a set of experiments which have been performed to investigate how the different perceptual cues that can be provided with Trike interact within an immersive environment.},
	author = {Allison, R.S. and Harris, L.R. and Jenkin, M. and Pintilie, G. and Redlick, F. and Zikovitz, D.C.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840495},
	issn = {1087-8270},
	keywords = {Computer displays;Space technology;Global Positioning System;Birds;Transmitters;Biology computing;Psychology;Computer science;Operating systems;Physics computing},
	month = {March},
	pages = {169-175},
	title = {First steps with a rideable computer},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840495}}

@inproceedings{840496,
	abstract = {Due to the rapid evolution of graphics hardware, interactive 3D graphics is becoming popular on desktop personal computers. However, it remains a challenging task for a novice user equipped with a 2D mouse to navigate in an architectural environment efficiently. We think the problem is partly due to the fact that precise navigation control is difficult to achieve with low frame rates. In this paper, we propose a novel approach to improve the effectiveness and efficiency of 3D navigation for architectural walkthrough applications. We adopt a path planner with a probabilistic roadmap to help users avoid unnecessary maneuvers due to collisions with the environment. We modify a Java3D implementation of a VRML browser to incorporate the path planner into the user interface. Experiments show that our implementation of the path planner is very efficient and can be seamlessly incorporated into the navigation control loop. The overall navigation time for traversing a sequence of checkpoints in a maze-like environment can be improved by about a factor of two if the intelligent user interface is used.},
	author = {Tsai-Yen Li and Hung-Kai Ting},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840496},
	issn = {1087-8270},
	keywords = {User interfaces;Navigation;Virtual reality;Graphics;Layout;Mice;Computer science;US Department of Transportation;Java;Acceleration},
	month = {March},
	pages = {177-184},
	title = {An intelligent user interface with motion planning for 3D navigation},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840496}}

@inproceedings{840498,
	abstract = {Virtual environments have shown great promise as a research tool in science and engineering. In this paper, we study a classical problem in mathematics: that of approximating globally optimal Fekete point configurations. We found that a highly interactive virtual environment, combined with a time-critical computation, can provide valuable insight into the symmetry and stability of Fekete point configurations. We believe that virtual environments provide more natural interfaces to complex systems, allowing users to perceive, interpret and interact with the problem more rapidly.},
	author = {van Liere, R. and Mulder, J. and Frank, J. and de Swart, J.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840498},
	issn = {1087-8270},
	keywords = {Computer aided software engineering;Mathematics;Virtual environment;Iron;Time factors;Computer science;Stability;Data mining;Testing;Data visualization},
	month = {March},
	pages = {189-195},
	title = {Virtual Fekete point configurations: a case study in perturbing complex systems},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840498}}

@inproceedings{840499,
	abstract = {Presents an interactive stereoscopic rendering algorithm of voxel-based terrain. It provides unambiguous depth information of a terrain scene by generating perspective images for a pair of eyes with a horizontal parallax. The left-eye image is generated using a fast ray-casting algorithm accelerated by exploiting a specific ray coherence method in the voxel-based terrain scene. The right-eye image is obtained by exploiting the frame coherence between the two views. Most of the pixel values are directly obtained from the left image by re-projection. The remaining pixels are computed by ray casting, which is further accelerated with ray coherence. An A-buffer is employed to reduce the image error caused by re-projection to non-integer pixel locations. Image-based task partitioning schemes are explored to effectively parallelize our algorithm on a multiprocessor.},
	author = {Ming Wan and Nan Zhang and Kaufman, A. and Huamin Qu},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840499},
	issn = {1087-8270},
	keywords = {Casting;Displays;Layout;Rendering (computer graphics);Image generation;Pixel;Visualization;Computer science;Eyes;Read only memory},
	month = {March},
	pages = {197-206},
	title = {Interactive stereoscopic rendering of voxel-based terrain},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840499}}

@inproceedings{840500,
	abstract = {A usability-centred design approach is critically important for content development of virtual environments in real-world applications. We provide a framework that allows one to review existing design techniques and tools against the special requirements of virtual environments, so that appropriate methods can be selected. Focusing on the specific usability requirement of interaction guidance in presentation and instructional environments and its implementation through interactive illustration techniques, we demonstrate the use of the framework. Based on the specific requirements, we derive a suitable design process for interactive illustration techniques and identify appropriate techniques from multimedia and GUI design. The use of the design process and techniques is then illustrated with an example.},
	author = {Paelke, V.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840500},
	issn = {1087-8270},
	keywords = {Electrical capacitance tomography;Virtual environment;Animation;Design methodology;Research and development;Interactive systems;Tail;Rendering (computer graphics);Layout;Cameras},
	month = {March},
	pages = {207-214},
	title = {Systematic design of interactive illustration techniques for user guidance in virtual environments},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840500}}

@inproceedings{840501,
	abstract = {Describes a series of stages in the development of a new virtual locomotion device that has been designed to enhance remote inter-personal communications. The latest system, called GSS (Ground Surface Simulator), inherits the features of two previous locomotion interfaces, viz. ATLAS (ATR Locomotion interface for Active Self-motion) and ALF (ALive Floor). GSS also incorporates two different features not found in ordinary treadmills: a movable belt and an active belt-speed controller. We built an initial prototype of GSS and developed a method that presents bumpy surfaces that are free from the mechanical limitations inherent in prior designs. Experimental results showed that a subject could distinguish a 1% difference in the virtual slope on the GSS.},
	author = {Noma, H. and Sugihara, T. and Miyasato, T.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840501},
	issn = {1087-8270},
	keywords = {Virtual reality;Legged locomotion;Belts;Teleconferencing;Space exploration;Spaceborne radar;Foot;Prototypes;Laboratories;Collaborative work},
	month = {March},
	pages = {217-224},
	title = {Development of Ground Surface Simulator for Tel-E-Merge system},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840501}}

@inproceedings{840502,
	abstract = {Roller coasters are an attractive, adventurous and exciting form of entertainment. In this paper, we present a virtual roller coaster system, which includes roller coaster simulation software, a motion platform and a motion control unit. We investigate the track structure of the roller coaster and propose a methodology to generate the track automatically. We also simulate the forces exerted on the running carriage to give the user the impression he/she is riding a real roller coaster. A motion platform with two degrees of freedom is constructed using hydraulic cylinders for its actuators. An electronic circuit is designed to allow the motion platform to communicate with the host computer. We also provide flexibility for changing the track type and the surrounding scenery, in order to give the rider a variety of exciting experiences. This system also considers safety concerns in the design of the motion platform.},
	author = {Zen-Chung Shih and Yuh-Sen Jaw and Mei-Ling Hsu},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840502},
	issn = {1087-8270},
	keywords = {Computational modeling;Virtual reality;Circuits;Motion control;Information science;Hydraulic actuators;Computer interfaces;Virtual environment;Testing;Process design},
	month = {March},
	pages = {225-232},
	title = {Virtual roller coaster},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840502}}

@inproceedings{840503,
	abstract = {Proposes a novel visuo-haptic display using a head-mounted projector (HMP) with X'tal Vision (Crystal Vision) optics. Our goal is to develop a device which enables an observer to touch a virtual object just as it is seen. We describe in detail the design of an HMP with X'tal Vision, which is very suitable for augmented reality. For instance, the HMP makes the occlusion relationship between the virtual and the real environments nearly correct. Accordingly, the user can observe his/her real hand with the virtual objects. Furthermore, the HMP reduces eye fatigue because of the low inconsistency of accommodation and convergence. Therefore, we applied HMP-model 2 to a visuo-haptic display using a camouflage technique. This technique, called optical camouflage, makes an obstacle object, such as a haptic display, become translucent. With this method, a user can observe a stereoscopic virtual object with a nearly correct occlusion relationship between the virtual and the real environments, and he can actually feel the object.},
	author = {Inami, M. and Kawakami, N. and Sekiguchi, D. and Yanagida, Y. and Maeda, T. and Tachi, S.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840503},
	issn = {1087-8270},
	keywords = {Displays;Haptic interfaces;Optical sensors;Holography;Augmented reality;Imaging phantoms;Virtual reality;Holographic optical components;Fatigue;Convergence},
	month = {March},
	pages = {233-240},
	title = {Visuo-haptic display using head-mounted projector},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840503}}

@inproceedings{840505,
	abstract = {Proposes a method for accurate image overlay on head-mounted displays (HMDs) using vision and accelerometers. The proposed method is suitable for video see-through HMDs in augmented reality applications but is not limited to them. Acceleration information is used for predicting the head motion to compensate the end-to-end system delay and to make the vision-based tracking robust. Experimental results showed that the proposed method can reduce the alignment errors within 6 pixels on average and 11 pixels at the maximum, even is the user moves his/her head quickly (with 10 m/s/sup 2/ and 49 rad/s/sup 2/ at the maximum). The viewing range was enlarged by placing additional landmarks in the environment.},
	author = {Yokokohji, Y. and Sugawara, Y. and Yoshikawa, T.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840505},
	issn = {1087-8270},
	keywords = {Accelerometers;Magnetic heads;Acceleration;Layout;Optical filters;Target tracking;Displays;Delay systems;Optical sensors;Virtual reality},
	month = {March},
	pages = {247-254},
	title = {Accurate image overlay on video see-through HMDs using vision and accelerometers},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840505}}

@inproceedings{840506,
	abstract = {In an augmented reality system, it is required to obtain the position and orientation of the user's viewpoint in order to display the composed image while maintaining a correct registration between the real and virtual worlds. All the procedures must be done in real time. This paper proposes a method for augmented reality with a stereo vision sensor and a video see-through head-mounted display (HMD). It can synchronize the display timing between the virtual and real worlds so that the alignment error is reduced. The method calculates camera parameters from three markers in image sequences captured by a pair of stereo cameras mounted on the HMD. In addition, it estimates the real-world depth from a pair of stereo images in order to generate a composed image maintaining consistent occlusions between real and virtual objects. The depth estimation region is efficiently limited by calculating the position of the virtual object by using the camera parameters. Finally, we have developed a video see-through augmented reality system which mainly consists of a pair of stereo cameras mounted on the HMD and a standard graphics workstation. The feasibility of the system has been successfully demonstrated with experiments.},
	author = {Kanbara, M. and Okuma, T. and Takemura, H. and Yokoya, N.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840506},
	issn = {1087-8270},
	keywords = {Augmented reality;Cameras;Displays;Stereo vision;Timing;Image sequences;Image generation;Standards development;Graphics;Workstations},
	month = {March},
	pages = {255-262},
	title = {A stereoscopic video see-through augmented reality system based on real-time vision-based registration},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840506}}

@inproceedings{840507,
	abstract = {We propose a linear algorithm that is useful for realizing geometric registration between the view of a real scene and that of a virtual object in an image-based rendering framework. In a unified framework, the novel view synthesis of a virtual object based on three views' matching constraints and the recovery of the camera pose that is necessary for the base image selection can be performed. The feasibility of the algorithm is demonstrated by using ground-truth synthesized data and real scene data.},
	author = {Kobayashi, T. and Inoue, G. and Ohta, Y. and Long Quan},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840507},
	issn = {1087-8270},
	keywords = {Cameras;Virtual reality;Rendering (computer graphics);Layout;Position measurement;Educational institutions;Systems engineering and theory;Augmented reality;Humans;Image sensors},
	month = {March},
	pages = {263-270},
	title = {A unified linear algorithm for a novel view synthesis and camera pose estimation in mixed reality},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840507}}

@inproceedings{840508,
	abstract = {CAVE/sup TM/ displays offer many advantages over other virtual reality (VR) displays, including a large, unencumbered viewing space. Unfortunately, the typical tracking subsystems used with CAVE/sup TM/ displays tether the user and lessen this advantage. We have designed a simple, low-cost foot tracker that is wireless, leaving the user free to move. The tracker can be assembled for less than $200 US, and achieves an accuracy of /spl plusmn/10 cm at a 20-Hz sampling rate. We have tested the prototype with two applications: a visualization supporting close visual inspection, and a walkthrough of the campus. Although the tracking was convincing, it was clear that the tracker's limitations make it less than ideal for applications requiring precise visual inspection. However the freedom of motion allowed by the tracker was a compelling supplement to our campus walkthrough, allowing users to stroll and look around corners.},
	author = {Sharlin, E. and Figueroa, P. and Green, M. and Watson, B.},
	booktitle = {Proceedings IEEE Virtual Reality 2000 (Cat. No.00CB37048)},
	date-added = {2024-03-18 02:28:18 -0400},
	date-modified = {2024-03-18 02:28:18 -0400},
	doi = {10.1109/VR.2000.840508},
	issn = {1087-8270},
	keywords = {Displays;Virtual reality;Inspection;Foot;Assembly;Sampling methods;Testing;Prototypes;Visualization;Tracking},
	month = {March},
	pages = {271-278},
	title = {A wireless, inexpensive optical tracker for the CAVE/sup TM/},
	year = {2000},
	bdsk-url-1 = {https://doi.org/10.1109/VR.2000.840508}}

@inproceedings{756917,
	abstract = {This paper focuses on the use of the CORBA (Common Object Request Broker Architecture) platform as a middleware layer to support distributed virtual environments, particularly those based on the WorldToolKit software. Some results of an application implemented by using the ILU (InterLanguage Unification) software that is compatible with the CORBA platform, are also discussed.},
	author = {Deriggi, F.V. and Kubo, M.M. and Sementille, A.C. and Brega, J.R.F. and dos Santos, S.G. and Kirner, C.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756917},
	issn = {1087-8270},
	keywords = {Virtual reality;Application software;Computer networks;Computer network management;Environmental management;Electrical capacitance tomography;Concurrent computing;Data processing;Context;Scattering},
	month = {March},
	pages = {8-13},
	title = {CORBA platform as support for distributed virtual environments},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756917}}

@inproceedings{756918,
	abstract = {We present Avocado, our object-oriented framework for the development of distributed, interactive virtual environment applications. Data distribution is achieved by transparent replication of a shared scene graph among the participating processes of a distributed application. A sophisticated group communication system is used to guarantee state consistency even in the presence of late joining and leaving processes. We also describe how the familiar data flow graph found in modern stand-alone 3D-application toolkits extends nicely to the distributed case.},
	author = {Tramberend, H.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756918},
	issn = {1087-8270},
	keywords = {Virtual reality;Layout;Programming profession;Electrical capacitance tomography;Geometry;Databases;Information technology;Identity-based encryption;Graphics;Voltage control},
	month = {March},
	pages = {14-21},
	title = {Avocado: a distributed virtual reality framework},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756918}}

@inproceedings{756919,
	abstract = {This paper is concerned with the concurrency control for collaborative virtual environments. In particular, we describe how concurrent actions are coordinated in a multi-user, large-scale 3-D layout system CIAO. In contrast to many existing systems that sacrifice responsiveness in order to maintain consistency, CIAO achieves optimal response and notification time without compromising consistency. The optimal responsiveness is achieved by a new multicast-based, optimistic concurrency control mechanism. Even operations on a group of related objects do not entail any latency for concurrency control. We also present the multi-user interfaces of CIAO that provide some sense of isolation as well as rich awareness.},
	author = {Un-Jae Sung and Jae-Heon Yang and Kwang-Yun Wohn},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756919},
	issn = {1087-8270},
	keywords = {Concurrency control;Delay;Virtual environment;Collaboration;Peer to peer computing;Large-scale systems;Computer science;Time factors;Urban planning;Electronic commerce},
	month = {March},
	pages = {22-28},
	title = {Concurrency control in CIAO},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756919}}

@inproceedings{756920,
	abstract = {This paper describes the software architecture of Dragon, a real-time situational awareness virtual environment for battlefield visualization. Dragon receives data from a number of different sources and creates a single, coherent, and consistent three-dimensional display. We describe the problem of Battlefield Visualization and the challenges it imposes. We discuss the Dragon architecture, the rational for its design, and its performance in an actual application. The battlefield VR system is also suitable for similar civilian domains such as large-scale disaster relief and hostage rescue.},
	author = {Julier, S. and King, R. and Colbert, B. and Durbin, J. and Rosenblum, L.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756920},
	issn = {1087-8270},
	keywords = {Software architecture;Visualization;Virtual environment;Virtual reality;Displays;Laboratories;Real time systems;Application software;Software prototyping;User interfaces},
	month = {March},
	pages = {29-36},
	title = {The software architecture of a real-time battlefield visualization virtual environment},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756920}}

@inproceedings{756921,
	abstract = {We present a framework for fast and accurate collision detection for haptic interaction with polygonal models. Given a model, we pre-compute a hybrid hierarchical representation, consisting of uniform grids and trees of tight-fitting oriented bounding box trees (OBB-Trees). At run time, we use hybrid hierarchical representations and exploit frame-to-frame coherence for fast proximity queries. We describe a new overlap test, which is specialized for intersection of a line segment with an oriented bounding box for haptic simulation and takes 6-36 operations excluding transformation costs. The algorithms have been implemented as part of H-COLLIDE and interfaced with a PHANToM arm and its haptic toolkit, GHOST, and applied to a number of models. As compared to the commercial implementation, we are able to achieve up to 20 times speedup in our experiments and sustain update rates over 1000 Hz on a 400 MHz Pentium II.},
	author = {Gregory, A. and Lin, M.C. and Gottschalk, S. and Taylor, R.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756921},
	issn = {1087-8270},
	keywords = {Haptic interfaces;Probes;Testing;Imaging phantoms;Virtual environment;Computer displays;Auditory displays;Frequency;Runtime;Computer science},
	month = {March},
	pages = {38-45},
	title = {A framework for fast and accurate collision detection for haptic interaction},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756921}}

@inproceedings{756922,
	abstract = {We present a new interaction metaphor for object-centric tasks in the form of a prototype VR system, ALCOVE. Through analytic calculations, we quantitatively demonstrate the benefits of restructuring the interaction volume offered by current systems. Our metrics show that many applications' interaction volume increases by 1.5 to 2.6 times when using the ALCOVE system. We also offer an informal user task analysis and evaluations of previous VR systems that qualitatively support this improved interaction volume as well as demonstrate the need for a shift from room and desk-sized systems to desktop units. We present some testbed applications which benefit from this object-centric design and discuss some of the advantages and shortcomings of our system.},
	author = {Meyer, M. and Barr, A.H.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756922},
	issn = {1087-8270},
	keywords = {Virtual reality;Visualization;Electrical capacitance tomography;Virtual environment;Displays;Virtual prototyping;Computer science;System testing;Visual system;Read only memory},
	month = {March},
	pages = {46-52},
	title = {ALCOVE: design and implementation of an object-centric virtual environment},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756922}}

@inproceedings{756923,
	abstract = {A voxel-based terrain visualization system is presented with real-time performance on general-purpose graphics multiprocessor workstations. Ray casting of antialiased 3D volume terrain and subvoxel sampling in true 3D space produce high quality images. Based on this rendering algorithm, an interactive flythrough system for mission planning and flight simulation has been developed on an SGI Power Challenge and a virtual reality environment using a Responsive Workbench. Arbitrary stereoscopic perspective views over the terrain and a 6D input device are supported.},
	author = {Ming Wan and Huamin Qu and Kaufman, A.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756923},
	issn = {1087-8270},
	keywords = {Visualization;Real time systems;Graphics;Workstations;Casting;Image sampling;Rendering (computer graphics);Power system planning;Aerospace simulation;Virtual reality},
	month = {March},
	pages = {53-60},
	title = {Virtual flythrough over a voxel-based terrain},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756923}}

@inproceedings{756924,
	abstract = {Face cloning and animation considering wrinkle formation and aging are an aspiring goal and a challenging task. This paper describes a cloning method and an aging simulation in a family. We reconstruct a father, mother, son and daughter of one family and mix their shapes and textures in 3D to get virtual persons with some variation. The idea of reconstruction of a head is to detect features from two orthogonal pictures, modify a generic model with an animation structure and use an automatic texture mapping method. It is followed by a simple method to do 3D-shape interpolation and 2D morphing based on triangulation for experiments of mixing 3D heads between family members. Finally, wrinkles within facial animation and aging are generated based on detected feature points. Experiments are made to generate aging wrinkles on the faces of the son and the daughter.},
	author = {Won-Sook Lee and Yin Wu and Magnenat-Thalmann, N.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756924},
	issn = {1087-8270},
	keywords = {Cloning;Aging;Virtual reality;Facial animation;Head;Computer vision;Face detection;Image reconstruction;Deformable models;Solid modeling},
	month = {March},
	pages = {61-68},
	title = {Cloning and aging in a VR family},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756924}}

@inproceedings{756928,
	abstract = {The real time headlight simulator is meant to provide VALEO, a worldwide car equipment manufacturer, with the means of carrying out validation tests on its new headlights from 1998 onwards. This simulator is physically exact, allowing the engineer to accurately assess and predict the performance of the headlights.},
	author = {Uson, C. and Eclairage, V. and Bouchon, P.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756928},
	issn = {1087-8270},
	keywords = {Circuit testing;System testing;Prototypes;Circuit simulation;Automotive engineering;Performance evaluation;Rendering (computer graphics);Image generation;Automobiles;Virtual reality},
	month = {March},
	pages = {76-},
	title = {A physically exact real time simulator for car headlight},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756928}}

@inproceedings{756929,
	abstract = {We propose a method of simulating and representing cutting force in real time. To reduce the computation time, we employed a simplified physical model for the simulation. We define the cutting edge as a finite set of discrete points (i.e., discrete edges). By calculating the force on each discrete edge, we obtain the approximate distribution of force on the edge.},
	author = {Hirota, K. and Tanaka, A. and Kaneko, T.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756929},
	issn = {1087-8270},
	keywords = {Computational modeling;Physics computing},
	month = {March},
	pages = {77-},
	title = {Representation of force in cutting operation},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756929}}

@inproceedings{756930,
	abstract = {The paper discusses a virtual mail system. V-mail is designed to be used in the CAVE virtual environment. The participants' mail messages and the ongoing modifications of the VE are maintained by a central server. V-mail's user interface is embodied in a virtual friend or pet that follows the participant as he/she interacts with the VE.},
	author = {Imai, T. and Johnson, A.E. and Leigh, J. and Pape, D.E. and DeFanti, T.A.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756930},
	issn = {1087-8270},
	keywords = {Postal services},
	month = {March},
	pages = {78-},
	title = {The virtual mail system},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756930}}

@inproceedings{756931,
	abstract = {We describe the design and the implementation of a force display for immersive projection displays. To give the user maximum freedom of motion in a large working space, it is necessary to use a portable force display that is grounded on the user's body. Therefore, we developed a portable (wearable) force display called HapticGEAR which makes use of the tension of wires grounded on the user's back. This device is designed to reduce user's fatigue and to have fewer influence on the user's motion and sight.},
	author = {Hirose, M. and Ogi, T. and Yano, H. and Kakehi, N.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756931},
	issn = {1087-8270},
	keywords = {Displays},
	month = {March},
	pages = {79-},
	title = {Development of wearable force display (HapticGEAR) for immersive projection displays},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756931}}

@inproceedings{756932,
	abstract = {An evacuation simulator utilizing virtual reality is proposed as a tool for the analysis of human decisions of evacuation from a ship under casualty. The configuration of the simulator was determined and the prototype of the simulator was built. Further test experiments were carried out for checking the prototype. From the experiments, it becomes clear that the analysis method of evacuees' decisions using the evacuation simulator would be appropriate for the purpose.},
	author = {Kaneko, F. and Ikemoto, Y. and Fukumoto, M.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756932},
	issn = {1087-8270},
	keywords = {Analytical models;Marine vehicles;Computational modeling;Virtual prototyping;Testing;Humans;Accidents;Virtual reality;Performance analysis;Fires},
	month = {March},
	pages = {80-},
	title = {Evacuation simulator for analysis of evacuees' decision in a ship under casualty},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756932}}

@inproceedings{756933,
	abstract = {We describe the efforts being carried out at the Naval Research Laboratory (NRL) towards VR scientific visualization. We are exploring scientific visualization in an immersive virtual environment: the NRL's CAVE/sup TM/-like device known as GROTTO (Graphical room for observation, Training and Tactical Orientation). We describe the AVS GROTTO viewer, a VR interface to the AVS visualization system. The AVS GROTTO viewer has been used by a number of scientists in current, ongoing research projects within NRL.},
	author = {Kuo, E. and Lanzagorta, M. and Rosenberg, R. and Julier, S. and Summers, J.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756933},
	issn = {1087-8270},
	keywords = {Virtual reality;Data visualization;Electrical capacitance tomography;Software libraries;Laboratories;Packaging;Investments;Information geometry;Displays;Rendering (computer graphics)},
	month = {March},
	pages = {81-},
	title = {VR scientific visualization in the GROTTO},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756933}}

@inproceedings{756934,
	abstract = {The paper discusses SiLVIA simulation library. The main purpose of SiLVIA is to supply procedures for simulating the dynamics of colliding rigid bodies and the interactive manipulation of these bodies in virtual environments. Detecting collisions between virtual objects and calculating their reaction to these collisions play an important role in VR applications such as ergonomy, studies or virtual assembly simulations.},
	author = {Hotz, G. and Kerzmann, A. and Lennerz, C. and Schmid, R. and Schomer, E. and Warken, T.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756934},
	issn = {1087-8270},
	keywords = {Libraries;Virtual reality;Electrical capacitance tomography;Computational modeling;Assembly;Data structures;Interference;Application software;Computer science;Manipulator dynamics},
	month = {March},
	pages = {82-},
	title = {SiLVIA-a simulation library for virtual reality applications},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756934}}

@inproceedings{756935,
	abstract = {We present a dynamic measure to capture temporal image distortions, such as popping artifacts, resulting from algorithms performing real-time rendering tasks. Experimental results show that it can form the basis for evaluating and comparing the algorithms' temporal image quality.},
	author = {Ping Yuan and Green, M. and Lau, R.W.H.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756935},
	issn = {1087-8270},
	keywords = {Image quality;Rendering (computer graphics);Distortion measurement;Electrical capacitance tomography;Image sequences;Testing;Layout;Switches;Area measurement;Position measurement},
	month = {March},
	pages = {83-},
	title = {Dynamic image quality measurements of real-time rendering algorithms},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756935}}

@inproceedings{756936,
	abstract = {This paper describes the design and development of a VR system that can assist orthopedic diagnosis and surgery. A realistic 3D knee surface model, integrated with motion analysis, is used to visualize the geometrical and biomechanical characteristics of human knee joint. The system can be used to perform pre-operative surgery simulation and evaluation in a VR environment.},
	author = {Ying Zhu and Chen, J.X. and Xiaodong Fu and Quammen, D.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756936},
	issn = {1087-8270},
	keywords = {Virtual reality;Knee;Orthopedic surgery;Virtual environment;Visualization;Animation;Legged locomotion;Image reconstruction;Surface reconstruction;Computational modeling},
	month = {March},
	pages = {84-},
	title = {A virtual reality system for knee diagnosis and surgery simulation},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756936}}

@inproceedings{756937,
	abstract = {A new approach is presented to automatically build a dynamic and multi-resolution 360/spl deg/ panorama (DMP) from image sequences taken by a hand-held camera. A multi-resolution representation is built for the more interesting areas by means of camera zooming. The dynamic objects in the scene can be detected and represented separately. The DMP construction method is fast, robust and automatic, achieving 1 Hz in a 266 MHz PC.},
	author = {Zhigang Zhu and Guangyou Xu and Heng Luo and Qiang Wang},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756937},
	issn = {1087-8270},
	keywords = {Layout;Cameras;Computer science;Image sequences;Virtual reality;Video sequences;Motion detection;Parameter estimation;Image analysis;Computer vision},
	month = {March},
	pages = {85-},
	title = {Automating the construction of dynamic and multi-resolution 360/spl deg/ panorama for natural scenes with moving objects},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756937}}

@inproceedings{756938,
	abstract = {We have created an immersive application for statistical graphics and have investigated what benefits it offers over more traditional data analysis tools. We present a description of both the traditional data analysis tools and our virtual environment, and results of an experiment designed to determine if an immersive environment based on the XGobi desktop system provides advantages over XGobi for analysis of high-dimensional statistical data. The experiment included two aspects of each environment: three structure detection (visualization) tasks and one ease of interaction task. The subjects were given these tasks in both the C2 virtual environment and a workstation running XGobi. The experiment results showed an improvement in participants' ability to perform structure detection tasks in the C2 to their performance in the desktop environment. However, participants were more comfortable with the interaction tools in the desktop system.},
	author = {Arms, L. and Cook, D. and Cruz-Neira, C.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756938},
	issn = {1087-8270},
	keywords = {Visualization;Computer displays;Virtual reality;Computer graphics;Manufacturing;Statistics;Statistical analysis;Testing;Data analysis;Electrical capacitance tomography},
	month = {March},
	pages = {88-95},
	title = {The benefits of statistical visualization in an immersive environment},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756938}}

@inproceedings{756939,
	abstract = {The ever-increasing power of computers and hardware rendering systems has, to date, primarily motivated the creation of visually rich and perceptually realistic virtual environment (VE) applications. Comparatively very little effort has been expended on the user interaction components of VEs. As a result, VE user interfaces are often poorly designed and are rarely evaluated with users. Although usability engineering is a newly emerging facet of VE development, user-centered design and usability evaluation in VEs as a practice still lags far behind what is needed. This paper presents a structured, iterative approach for the user-centered design and evaluation of VE user interaction. This approach consists of the iterative use of expert heuristic evaluation, followed by formative usability evaluation, followed by summative evaluation. We describe our application of this approach to a real-world VE for battlefield visualization, describe the resulting series of design iterations, and present evidence that this approach provides a cost-effective strategy for assessing and iteratively improving user interaction design in VEs. This paper is among the first to report applying an iterative, structured, user-centered design and evaluation approach to VE user interaction design.},
	author = {Hix, D. and Swan, J.E. and Gabbard, J.L. and McGee, M. and Durbin, J. and King, T.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756939},
	issn = {1087-8270},
	keywords = {User centered design;Usability;Iterative methods;Military computing;Hardware;Rendering (computer graphics);Virtual environment;Application software;User interfaces;Design engineering},
	month = {March},
	pages = {96-103},
	title = {User-centered design and evaluation of a real-time battlefield visualization virtual environment},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756939}}

@inproceedings{756940,
	abstract = {We assessed the effects of network latency and jitter on a cooperative teleoperation task in a collaborative virtual environment. Two remote partners worked together to manipulate shared virtual objects over a network. The task was to minimize the time to transfer a ring through one of four paths with the least number of collisions. The performance of human subjects was measured and analyzed quantitatively as a function of network latency: 10 and 200 msec delays with and without jitter. Jitter had the greatest impact on coordination performance when the latency was high and the task was difficult. These results are discussed in light of current and future CVE tasks.},
	author = {Kyoung Shin Park and Kenyon, R.V.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756940},
	issn = {1087-8270},
	keywords = {Humans;Intelligent networks;Collaboration;Virtual environment;Delay;Jitter;Quality of service;Collaborative work;Optical design;Wide area networks},
	month = {March},
	pages = {104-111},
	title = {Effects of network characteristics on human performance in a collaborative virtual environment},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756940}}

@inproceedings{756941,
	abstract = {We present an experiment that investigates the behaviour of small groups of participants in a wide-area distributed collaborative virtual environment (CVE). This is the third and largest study in a series of experiments that have examined trios of participants carrying out a highly collaborative puzzle-solving task. The results reproducing those of earlier studies suggest a positive relationship between place-presence and co-presence, between co-presence and group accord, with evidence supporting the notion that immersion confers leadership advantage.},
	author = {Steed, A. and Slater, M. and Sadagic, A. and Bullock, A. and Tromp, J.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756941},
	issn = {1087-8270},
	keywords = {Collaboration;Virtual environment;Computer science;Avatars;Educational institutions;Hip;Displays;Virtual groups;Design for experiments},
	month = {March},
	pages = {112-115},
	title = {Leadership and collaboration in shared virtual environments},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756941}}

@inproceedings{756942,
	abstract = {We present a distributed PC-based multimodal (haptic, visual and acoustic) telepresence and virtual presence system. Two desktop kinesthetic devices (DeKiFeD3 and DeKiTop3) with 3 degrees-of-freedom have been developed for multi-modal telepresence. Feedback to the human modalities of the visual, auditory, kinesthetic, tactile and temperature senses is generated using appropriate actuator hardware. We present several applications in virtual presence and teleoperation in physical remote environments.},
	author = {Baier, H. and Buss, M. and Freyberger, F. and Hoogen, J. and Kammermeier, P. and Schmidt, G.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756942},
	issn = {1087-8270},
	keywords = {Haptic interfaces;Temperature sensors;Virtual reality;Humans;Feedback;Force sensors;Acoustic devices;Actuators;Space technology;Communication networks},
	month = {March},
	pages = {118-125},
	title = {Distributed PC-based haptic, visual and acoustic telepresence system-experiments in virtual and remote environments},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756942}}

@inproceedings{756943,
	abstract = {The purpose of Project Pioneer is to develop an exploratory robot capable of creating a three-dimensional photo-realistic map of the inside of the damaged Chernobyl nuclear reactor, measure the environmental and radioactive conditions, and collect samples of concrete from the physical structure to determine its mechanical stability. This paper describes the virtual reality interface for Pioneer's three-dimensional mapping system. This interface addresses a wide variety of technical challenges, including several that are unique to the hostile Chernobyl environment.},
	author = {Steele, F. and Thomas, G. and Blackmon, T.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756943},
	issn = {1087-8270},
	keywords = {Robot vision systems;Cameras;Virtual reality;Concrete;Accidents;Robot sensing systems;Filters;Service robots;Cities and towns;Water pollution},
	month = {March},
	pages = {126-132},
	title = {An operator interface for a robot-mounted, 3D camera system: Project Pioneer},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756943}}

@inproceedings{756944,
	abstract = {Navigation tasks in large virtual environments often call for the use of a virtual map. However, all maps are not alike. Performance on navigation tasks in general has been shown to vary depending on the orientation of the map with respect to the user's frame of reference. This paper reports the results of an experiment investigating orientation issues of virtual maps for use during navigation tasks. Participants were given a virtual map in either a north-up or forward-up configuration. Performance on search tasks was measured in terms of search time and errors. Results indicate that targeted search tasks (tasks requiring only the egocentric reference frame) are best served by a forward-up alignment while primed and naive search tasks (tasks requiring information from the world reference frame) prefer a north-up alignment. Both types of maps are affected by the ability of user to perform mental rotations.},
	author = {Darken, R.P. and Cevik, H.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756944},
	issn = {1087-8270},
	keywords = {Navigation;Virtual environment;Time measurement;Large-scale systems;Psychology;Geography;Computer science;Turning;Legged locomotion},
	month = {March},
	pages = {133-140},
	title = {Map usage in virtual environments: orientation issues},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756944}}

@inproceedings{756945,
	abstract = {Navigation and interaction in virtual environments that use stereoscopic head-tracked displays and have very large data sets present several challenges beyond those encountered with smaller data sets and simpler displays. First, zooming by approaching or retreating from a target must be augmented by integrating scale as a seventh degree of freedom. Second, in order to maintain good stereoscopic imagery, the interface must: maintain stereo image pairs that the user perceives as a single 3D image, minimize loss of perceived depth since stereoscopic imagery cannot properly occlude the screen's frame, provide maximum depth information, and place objects at distances where they are best manipulated. Finally, the navigation interface must work when the environment is displayed at any scale. This paper addresses these problems for god's-eye-view or third person navigation of a specific large-scale virtual environment: a high-resolution terrain database covering an entire planet.},
	author = {Wartell, Z. and Ribarsky, W. and Hodges, L.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756945},
	issn = {1087-8270},
	keywords = {Navigation;Displays;Head;Virtual reality;Virtual environment;Read only memory;Electronic switching systems;Marine animals;Large-scale systems;Monitoring},
	month = {March},
	pages = {141-148},
	title = {Third-person navigation of whole-planet terrain in a head-tracked stereoscopic environment},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756945}}

@inproceedings{756946,
	abstract = {Creation of compelling 3-dimensional, multi-user virtual worlds for education and training applications requires a high degree of realism in the appearance, interaction, and behavior of avatars within the scene. Our goal is to develop and/or adapt existing 3-dimensional technologies to provide training scenarios across the Internet in a form as close as possible to the appearance and interaction expected of live situations with human participants. We have produced a prototype system, JackMOO, which combines Jack, a virtual human system, and LambdaMOO, a multiuser; network-accessible, programmable, interactive server: Jack provides the visual realization of avatars and other objects. LambdaMOO provides the Web-accessible communication, programmability, and persistent object database. The combined JackMOO allows us to store the richer semantic information necessitated by the scope and range of human actions that an avatar must portray, and to express those actions in the form of imperative sentences. We describe JackMOO, its components, and a prototype application with five virtual human agents.},
	author = {Jianping Shi and Smith, T.J. and Granieri, J.P. and Badler, N.I.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756946},
	issn = {1087-8270},
	keywords = {Avatars;Humans;Internet;Biological system modeling;Virtual prototyping;Lifting equipment;Communication system control;Facial animation;Electrical capacitance tomography;Layout},
	month = {March},
	pages = {156-163},
	title = {Smart avatars in JackMOO},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756946}}

@inproceedings{756947,
	abstract = {The Round Earth Project is investigating how virtual reality technology can be used to help teach concepts that are counter-intuitive to a learner's currently held mental model. Virtual reality can be used to provide an alternative cognitive starting point that does not carry the baggage of past experiences. In particular this paper describes our work in comparing two strategies for teaching young children that the Earth is spherical when their everyday experiences tell them it is flat.},
	author = {Johnson, A. and Moher, T. and Ohlsson, S. and Gillingham, M.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756947},
	issn = {1087-8270},
	keywords = {Earth;Collaboration;Virtual reality;Cognitive science;Geoscience;Computer science;Computer science education;Psychology;Electric breakdown;Space technology},
	month = {March},
	pages = {164-171},
	title = {The Round Earth Project: deep learning in a collaborative virtual world},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756947}}

@inproceedings{756948,
	abstract = {The Virtual Assembly Design Environment (VADE) is a virtual reality (VR) based engineering application which allows engineers to evaluate, analyze, and plan the assembly of mechanical systems. This system focuses on utilizing an immersive virtual environment tightly coupled with commercial computer aided design (CAD) systems. Salient features of VADE include: data integration (two-way) with a parametric CAD system; realistic interaction of the user with parts in the virtual environment; creation of valued design information in the virtual environment; reverse data transfer of design information back to the CAD system; significant interactivity in the virtual environment; collision detection; and physically-based modeling. This paper describes the functionality and applications of VADE. A discussion of the limitations of virtual assembly and a comparison with automated assembly planning systems are presented. Experiments conducted using real-world engineering models are also described.},
	author = {Jayaram, S. and Yong Wang and Jayaram, U. and Lyons, K. and Hart, P.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756948},
	issn = {1087-8270},
	keywords = {Virtual reality;Design automation;Design engineering;Virtual environment;Process planning;Integrated circuit modeling;Power engineering and energy;NIST;Assembly systems;Manufacturing systems},
	month = {March},
	pages = {172-179},
	title = {A Virtual Assembly Design Environment},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756948}}

@inproceedings{756949,
	abstract = {This paper presents an overview of the tele-immersion applications that have been built by collaborators around the world using the CAVERNsoft toolkit, and the lessons learned from building these applications. In particular the lessons learned are presented as a set of rules-of-thumb for developing tele-immersive applications in general.},
	author = {Leigh, J. and Johnson, A.E. and DeFanti, T.A. and Brown, M. and Ali, M.D. and Bailey, S. and Banerjee, A. and Benerjee, P. and Jim Chen and Curry, K. and Curtis, J. and Dech, F. and Dodds, B. and Foster, I. and Fraser, S. and Ganeshan, K. and Glen, D. and Grossman, R. and Heiland, R. and Hicks, J. and Hudson, A.D. and Imai, T. and Khan, M.A. and Kapoor, A. and Kenyon, R.V. and Kelso, J. and Kriz, R. and Lascara, C. and Liu, X. and Lin, Y. and Mason, T. and Millman, A. and Nobuyuki, K. and Park, K. and Parod, B. and Rajlich, P.J. and Rasmussen, M. and Rawlings, M. and Robertson, D.H. and Thongrong, S. and Stein, R.J. and Swartz, K. and Tuecke, S. and Wallach, H. and Hong Yee Wong and Wheless, G.H.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756949},
	issn = {1087-8270},
	keywords = {Intelligent networks;Laboratories;Visualization;Virtual reality;Collaborative work;Virtual environment;Image databases;Industrial training;Computer networks;Sea measurements},
	month = {March},
	pages = {180-187},
	title = {A review of tele-immersive applications in the CAVE research network},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756949}}

@inproceedings{756950,
	abstract = {The promise of immersive virtual environment (VE) applications has been that they can make interaction with computer information and processes easier by providing a medium that more closely matches the user's real environment. Meeting this promise, however; has proven to be difficult, as the interface between the user and the computer is not complete, the metaphors for interacting with information are not always obvious, and the tools for including current interaction techniques are not sufficient to the task. We present SVIFT, the Simple Virtual Interactor Framework and Toolkit, which supports efforts to meet the interaction needs of immersive VE applications. SVIFT allows for the design and implementation of various interaction techniques that can be easily incorporated into many VE applications and combined with other interaction techniques to produce more complex interactions. We also discuss the differences between desktop and immersive environment interaction, and the implications of those differences on the design of an interactor framework.},
	author = {Kessler, G.D.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756950},
	issn = {1087-8270},
	keywords = {Virtual environment;Application software;Computer interfaces;Animation;Keyboards;Programming;Software libraries;Runtime library;Graphical user interfaces;Java},
	month = {March},
	pages = {190-197},
	title = {A framework for interactors in immersive virtual environments},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756950}}

@inproceedings{756951,
	abstract = {A technique is proposed for object manipulation with a virtual tool using multiple exact interactions. Exact test is introduced that uses real-time collision detection for both hand-and-tool and tool-and-object interactions. Chopsticks are adopted for one of the trials of our technique; although they have a very simple shape, they do have multiple functions. Here, a virtual object is manipulated by the virtual chopsticks, which are used by the motion of a user's hand captured by a hand gesture input device. Exact interaction is applied between the hand and chopsticks based on correct table manners. Experimental results demonstrate the effectiveness of the proposed multiple exact interactions, especially for precise object alignment tasks.},
	author = {Kitamura, Y. and Higashi, T. and Masaki, T. and Kishino, F.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756951},
	issn = {1087-8270},
	keywords = {Shape;Virtual reality;Humans;Testing;User interfaces;Grasping;Fingers;Virtual environment;Cognitive science;Object detection},
	month = {March},
	pages = {198-204},
	title = {Virtual chopsticks: object manipulation using multiple exact interactions},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756951}}

@inproceedings{756952,
	abstract = {The study of human-computer interaction within immersive virtual environments requires us to balance what we have learned from the design and use of desktop interfaces with novel approaches to allow us to work effectively in three dimensions. While some researchers have called for revolutionary interfaces for these new environments, devoid of two-dimensional (2D) desktop widgets, others have taken a more evolutionary approach. Windowing within immersive virtual environments is an attempt to apply 2D interface techniques to three-dimensional (3D) worlds. 2D techniques are attractive because of their proven acceptance and widespread use on the desktop. With current methods environments, however, it is difficult for users of 3D worlds to perform precise manipulations, such as dragging sliders, or precisely positioning or orienting objects. We have developed a testbed designed to take advantage of bimanual interaction, proprioception, and passive-haptic feedback. We present preliminary results from an empirical study of 2D interaction in 3D environments using this system. We use a window registered with a tracked, physical surface, to provide support for precise manipulation of interface widgets displayed in the virtual environment.},
	author = {Lindeman, R.W. and Sibert, J.L. and Hahn, J.K.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756952},
	issn = {1087-8270},
	keywords = {Virtual environment;Read only memory;Feedback;User interfaces;Haptic interfaces;Mice;Computer graphics;Testing;Independent component analysis;Virtual reality},
	month = {March},
	pages = {205-212},
	title = {Hand-held windows: towards effective 2D interaction in immersive virtual environments},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756952}}

@inproceedings{756953,
	abstract = {Compared to most other virtual environments, the Responsive Workbench/sup TM/ offers a much better integration of the virtual and the real worlds. The Responsive Workbench/sup TM/ is a projection-based virtual environment with one horizontal table-sized projection plane. However interacting with these mixed 3D worlds remains a research challenge. This paper introduces a prop-like device, the virtual palette and describes the virtual remote control panel (VRCP), a two-handed interaction technique, based on the Virtual Palette, for controlling applications.},
	author = {Coquillart, S. and Wesche, G.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756953},
	issn = {1087-8270},
	keywords = {Electrical capacitance tomography;Communication system control;Identity-based encryption;Chromium;Graphics;Virtual reality;Head;Displays;Visualization;Virtual environment},
	month = {March},
	pages = {213-216},
	title = {The virtual palette and the virtual remote control panel: a device and an interaction paradigm for the Responsive Workbench/sup TM/},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756953}}

@inproceedings{756954,
	abstract = {We examined the effects of human 3D tracking performance of several common defects of immersing virtual environments: spatial sensor distortion, visual latency and low update rates. Results show: removal of relatively small static distortion had minor effects on tracking accuracy; an adapted Cooper-Harper controllability scale proved the most sensitive subjective indicator of simulation degradation; and RMS tracking error and subjective impressions were more influenced by changing visual latency than by update rate.},
	author = {Ellis, S.R. and Adelstein, B.D. and Baumeler, S. and Jense, G.J. and Jacoby, R.H.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756954},
	issn = {1087-8270},
	keywords = {Delay;Virtual environment;Distortion measurement;Error correction;Degradation;Computer displays;Position measurement;Volume measurement;Rotation measurement;Humans},
	month = {March},
	pages = {218-221},
	title = {Sensor spatial distortion, visual latency, and update rate effects on 3D tracking in virtual environments},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756954}}

@inproceedings{756955,
	abstract = {322 subjects participated in an experimental study to investigate the effects of tactile, olfactory, audio and visual sensory cues on a participant's sense of presence in a virtual environment and on their memory for the environment and the objects in that environment. Results strongly indicate that increasing the modalities of sensory input in a virtual environment can increase both the sense of presence and memory for objects in the environment. In particular, the addition of tactile, olfactory and auditory cues to a virtual environment increased the user's sense of presence and memory of the environment. Surprisingly, increasing the level of visual detail did not result in an increase in the user's sense of presence or memory of the environment.},
	author = {Dinh, H.Q. and Walker, N. and Hodges, L.F. and Chang Song and Kobayashi, A.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756955},
	issn = {1087-8270},
	keywords = {Virtual environment;Olfactory;Auditory displays;Graphics;Visualization;Usability;Motorcycles;Fans;Virtual prototyping;Haptic interfaces},
	month = {March},
	pages = {222-228},
	title = {Evaluating the importance of multi-sensory input on memory and the sense of presence in virtual environments},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756955}}

@inproceedings{756956,
	abstract = {The design of virtual environments usually concentrates on constructing a realistic visual simulation and ignores the non-visual cues normally associated with moving through an environment. The lack of the normal complement of cues may contribute to cybersickness and may affect operator performance. Previously (1998) we described the effect of adding vestibular cues during passive linear motion and showed an unexpected dominance of the vestibular cue in determining the magnitude of the perceived motion. Here we vary the relative magnitude of the visual and vestibular cues and describe a simple linear summation model that predicts the resulting perceived magnitude of motion. The model suggests that designers of virtual reality displays should add vestibular information in a ratio of one to four with the visual motion to obtain convincing and accurate performance.},
	author = {Harris, L. and Jenkin, M. and Zikovitz, D.C.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756956},
	issn = {1087-8270},
	keywords = {Acceleration;Virtual environment;Virtual reality;Optical sensors;Displays;Psychology;Computer science;Computational biology;Biological system modeling;Computational modeling},
	month = {March},
	pages = {229-236},
	title = {Vestibular cues and virtual environments: choosing the magnitude of the vestibular cue},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756956}}

@inproceedings{756957,
	abstract = {Head-coupled virtual reality systems can cause symptoms of sickness (cybersickness). A study has been conducted to investigate the effects of scene oscillations on the level and types of cybersickness. Sixteen male subjects participated in the experiments. They were exposed to four 20-minute virtual simulation sessions, in a balanced order with 10 days separation. The 4 simulation sessions exposed the subjects to similar visual scene oscillation in different axis: pitch axis, yaw axis, roll axis and no oscillation (speed: 30/spl deg//s, range: +/-60/spl deg/). Verbal ratings of nausea level were taken at 5-minute intervals and sickness symptoms were measured before and after the exposure using the Simulator Sickness Questionnaire (SSQ). Significant differences were found between the no oscillation condition and the oscillating conditions. With scene oscillation, nausea ratings increased significantly after 5-minute exposure for all the oscillation axes (pitch, yaw, and roll axes). Total sickness scores were obtained from the SSQ and their profiles with different scene oscillation axes were presented.},
	author = {So, R.H.Y. and Lo, W.T.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756957},
	issn = {1087-8270},
	keywords = {Layout;Virtual reality;Aerospace simulation;Electrical capacitance tomography;Virtual environment;Head;Marine vehicles;Industrial engineering;Research and development management;Isolation technology},
	month = {March},
	pages = {237-241},
	title = {Cybersickness: an experimental study to isolate the effects of rotational scene oscillations},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756957}}

@inproceedings{756958,
	abstract = {Registration for outdoor systems for Augmented Reality (AR) cannot rely on the methods developed for indoor use (e.g., magnetic tracking, fiducial markers). Although GPS and the earth's magnetic field can be used to obtain a rough estimate of position and orientation, the precision of this registration method is not high enough for satisfying AR overlay. Computer vision methods can help to improve the registration precision by tracking visual clues whose real world positions are known. We have developed a system that can exploit horizon silhouettes for improving the orientation precision of a camera which is aligned with the user's view. It has been shown that this approach is able to provide registration even as a stand-alone system, although the usual limitations of computer vision prohibit to use it under unfavorable conditions. This paper describes the approach of registration by using horizon silhouettes. Based on the known observer location (from GPS), the 360 degree silhouette is computed from a digital elevation map database. Registration is achieved, when the extracted visual horizon silhouette segment is matched onto this predicted silhouette. Significant features (mountain peaks) are cues which provide hypotheses for the match. Several criteria are tested to find the best matching hypothesis. The system is implemented on a PC under Windows NT. Results are shown in this paper.},
	author = {Behringer, R.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756958},
	issn = {1087-8270},
	keywords = {Augmented reality;Application software;Computer vision;Displays;Global Positioning System;Virtual reality;Virtual prototyping;Inspection;Data visualization;Magnetic sensors},
	month = {March},
	pages = {244-251},
	title = {Registration for outdoor augmented reality applications using computer vision techniques and hybrid sensors},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756958}}

@inproceedings{756959,
	abstract = {Almost all previous Augmented Reality (AR) systems work indoors. Outdoor AR systems offer the potential for new application areas. However, building an outdoor AR system is difficult due to portability constraints, the inability to modify the environment, and the greater range of operating conditions. We demonstrate a hybrid tracker that stabilizes an outdoor AR display with respect to user motion, achieving more accurate registration than previously shown in an outdoor AR system. The hybrid tracker combines rate gyros with a compass and tilt orientation sensor in a near real-time system. Sensor distortions and delays required compensation to achieve good results. The measurements from the two sensors are fused together to compensate for each other's limitations. From static locations with moderate head rotation rates, peak registration errors are /spl sim/2 degrees, with typical errors under 1 degree, although errors can become larger over long time periods due to compass drift. Without our stabilization, even small motions make the display nearly unreadable.},
	author = {Azuma, R. and Hoff, B. and Neely, H. and Sarfaty, R.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756959},
	issn = {1087-8270},
	keywords = {Augmented reality;Displays;Sensor systems;Global Positioning System;Appropriate technology;Laboratories;Tracking;Real time systems;Delay;Distortion measurement},
	month = {March},
	pages = {252-259},
	title = {A motion-stabilized outdoor augmented reality system},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756959}}

@inproceedings{756960,
	abstract = {The biggest single obstacle to building effective augmented reality (AR) systems is the lack of accurate wide-area sensors for trackers that report the locations and orientations of objects in an environment. Active (sensor-emitter) tracking technologies require powered-device installation. Limiting their use to prepared areas that are relatively free of natural or man-made interference sources. Vision-based systems can use passive landmarks, but they are more computationally demanding and often exhibit erroneous behavior due to occlusion or numerical instability. Inertial sensors are completely passive, requiring no external devices or targets, however, the drift rates in portable strapdown configurations are too great for practical use. In this paper, we present a hybrid approach to AR tracking that integrates inertial and vision-based technologies. We exploit the complementary nature of the two technologies to compensate for the weaknesses in each component. Analysis and experimental results demonstrate this system's effectiveness.},
	author = {Suya You and Neumann, U. and Azuma, R.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756960},
	issn = {1087-8270},
	keywords = {Augmented reality;Acceleration;Sensor systems;Interference;Computer vision;Cameras;Magnetic sensors;Machine vision;Position measurement;Robustness},
	month = {March},
	pages = {260-267},
	title = {Hybrid inertial and vision tracking for augmented reality registration},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756960}}

@inproceedings{756961,
	abstract = {This paper describes on-going research at Lancaster University to develop a brain-computer interface (BCI) with which to conduct neurofeedback training. We have built a system that translates EEG signals detected from the scalp of a subject into movement and interaction within a VRML world. The training protocol parameters can be set prior to a session commencing. These correspond to signal thresholds within which a subject will be rewarded for maintaining his or her EEG component signal amplitude for a predetermined period. The training environments are constructed from a set of VRML components. Interactivity parameters, in terms of VRML object appearance and behaviour corresponding to changes in the EEG signal, can be chosen to suit the requirements of the session.},
	author = {Allanson, J. and Mariani, J.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756961},
	issn = {1087-8270},
	keywords = {Neurofeedback;Electrical capacitance tomography;Electroencephalography;Scalp;Virtual reality;Noise measurement;Signal detection;Protocols;Read only memory;Signal processing},
	month = {March},
	pages = {270-273},
	title = {Mind over virtual matter: using virtual environments for neurofeedback training},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756961}}

@inproceedings{756962,
	abstract = {In this paper, we propose a device to stimulate only the superficial mechanoreceptors in the skin, and report the feeling caused by the stimulus. We describe the principle of the selective stimulation using air pressure, and we show the selectivity is more advanced than that of our previous system using magnet chips which was presented last year. We experimentally confirmed that a sparse array of the superficial stimulators could display realistic touch on objects including finer virtual textures than the stimulator spacing.},
	author = {Asamura, N. and Yokoyama, N. and Shinoda, H.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756962},
	issn = {1087-8270},
	keywords = {Epidermis;Skin;Feedback;Displays;Electrical capacitance tomography;Pins;Fingers;Humans;Agricultural engineering;Agriculture},
	month = {March},
	pages = {274-281},
	title = {A method of selective stimulation to epidermal skin receptors for realistic touch feedback},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756962}}

@inproceedings{756963,
	abstract = {We present a method for painting texture maps directly onto trimmed NURBS models using a haptic interface. The haptic interface enables an artist to use a natural painting style while creating a texture. It avoids the traditional difficulty of mapping between the 2D texture space and the 3D model space by using parametric information available from our haptic tracing algorithm. The system maps user movement in 3D to movement in the 2D texture space and adaptively resizes the paintbrush in texture space to create a uniform stroke on the model.},
	author = {Johnson, D. and Thompson, T.V. and Kaplan, M. and Nelson, D. and Cohen, E.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756963},
	issn = {1087-8270},
	keywords = {Painting;Haptic interfaces;Surface texture;Spline;Surface topography;Surface reconstruction;Integrated circuit modeling;Read only memory;Geometry;Paints},
	month = {March},
	pages = {282-285},
	title = {Painting textures with a haptic interface},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756963}}

@inproceedings{756964,
	abstract = {This paper presents a new configuration of locomotion interface for walking about virtual space. Traveling on foot is the most intuitive way for locomotion. Infinite surface driven by actuators is an ideal device for creation of sense of walking. We selected a torus-shaped surface to realize the locomotion interface. The locomotion interface employs twelve sets of treadmills. These trendmills are connected side by side and driven to perpendicular direction. Infinite surface is generated by the motion of the treadmills. The walker can go to any direction while his/her position is fixed in the real world. Effectiveness of the device is tested by motion analysis and study on sense of distance.},
	author = {Iwata, H.},
	booktitle = {Proceedings IEEE Virtual Reality (Cat. No. 99CB36316)},
	date-added = {2024-03-18 02:28:11 -0400},
	date-modified = {2024-03-18 02:28:11 -0400},
	doi = {10.1109/VR.1999.756964},
	issn = {1087-8270},
	keywords = {Legged locomotion;Milling machines;Testing;Electrical capacitance tomography;Motion measurement;Electronic switching systems;Navigation;Feedback;Virtual environment;Virtual reality},
	month = {March},
	pages = {286-293},
	title = {Walking about virtual environments on an infinite floor},
	year = {1999},
	bdsk-url-1 = {https://doi.org/10.1109/VR.1999.756964}}

@inproceedings{658416,
	abstract = {This paper presents cognitive studies and analyses relating to how augmented reality (AR) interacts with human abilities in order to benefit manufacturing and maintenance tasks. A specific set of applications is described in detail, as well as a prototype system and the software library that it is built upon. An integrated view of information flow to support AR is also presented, along with a proposal for an AR media language (ARML) that could provide interoperability between various AR implementations.},
	author = {Neumann, U. and Majoros, A.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658416},
	keywords = {Augmented reality;Application software;Humans;Computer aided manufacturing;Computer integrated manufacturing;Pulp manufacturing;Psychology;Aircraft;Performance analysis;Information retrieval},
	month = {March},
	pages = {4-11},
	title = {Cognitive, performance, and systems issues for augmented reality applications in manufacturing and maintenance},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658416}}

@inproceedings{658417,
	abstract = {The ability to use virtual environments as either an abstraction of a space, similar to a map, or as a simulation of the space itself has suggested to many that it would be a useful tool in terrain familiarization of unknown environments. Up to this point, all research in this area has focused on building interiors and urban environments which are significantly different from natural environments in terms of navigation cues and useful wayfinding techniques. The experiment we present uses a virtual environment, as compared to a map only or real-world conditions on navigation tasks in a natural environment. We show that navigation ability is more important to performance than the training method, with the virtual environment being most effective for intermediate orienteers as compared to advanced or beginner orienteers.},
	author = {Darken, R.P. and Banker, W.P.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658417},
	keywords = {Navigation;Virtual environment;Visualization;Costs;Testing;Computer science},
	month = {March},
	pages = {12-19},
	title = {Navigating in natural environments: a virtual environment training transfer study},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658417}}

@inproceedings{658418,
	abstract = {2D windows-based interfaces may not be appropriate for wearable computers. We draw on virtual reality techniques to design and evaluate alternative methods for information presentation in a wearable environment. We find that simple body-stabilised displays provide benefits over traditional head-stabilised displays. Users find body-stabilised displays easier to use, more enjoyable and more intuitive, and are able to perform better on a search task. Spatial audio and visual cues further enhance performance.},
	author = {Billinghurst, M. and Bowskill, J. and Dyer, N. and Morphett, J.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658418},
	keywords = {Wearable computers;Computer displays;Two dimensional displays;Auditory displays;Application software;Space technology;Laboratories;Virtual reality;Aircraft navigation;Humans},
	month = {March},
	pages = {20-27},
	title = {An evaluation of wearable information spaces},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658418}}

@inproceedings{658419,
	abstract = {In most applications of virtual environments (VEs), like training and design evaluation, a good sense of orientation is needed in the VE. Orientation performance when moving around in the real world relies on visual as well as proprioceptive feedback. However, the navigation metaphors which are used to move around the VE often lack proprioceptive feedback. Furthermore, the visual feedback in a VE is often relatively poor compared to the visual feedback available in the real world. Therefore, we have quantified the influence of visual and proprioceptive feedback on orientation performance in VEs. Subjects were immersed in a virtual forest and were asked to turn specific angles using three navigation metaphors, differing in the kind of proprioceptive feedback which is provided (no proprioceptive feedback, vestibular feedback, and vestibular and kinesthetic feedback). The results indicate that the most accurate turn performance is found when kinesthetic feedback is present, in a condition where subjects use their legs to turn around. This indicates that incorporating this kind of feedback in navigation metaphors is quite beneficial. Orientation on only the visual component is most inaccurate, leading to progressively larger undershoots for larger angles.},
	author = {Bakker, N.H. and Werkhoven, P.J. and Passenier, P.O.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658419},
	keywords = {Feedback},
	month = {March},
	pages = {28-33},
	title = {Aiding orientation performance in virtual environments with proprioceptive feedback},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658419}}

@inproceedings{658420,
	abstract = {People can feel various tactile feelings by touching and rubbing objects. In this paper, we propose a method to display such tactile feeling of fine texture with reality. We create the feeling by selective stimulation to each kind of mechanoreceptor using the elastic transfer property of the skin. Our system is composed of four small magnet tips attached on the hand in a line, which are controlled with precise force. The two driving modes, the common phase mode and the reversed phase mode, stimulate the deep receptors and shallow receptors in the skin, respectively. The system could give several types of tactile feeling with reality. The principle and experimental results are shown.},
	author = {Asamura, N. and Tomori, N. and Shinoda, H.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658420},
	keywords = {Displays;Skin;Stress;Frequency;Humans;Filtering;Rough surfaces;Surface roughness;Fingers;Surface waves},
	month = {March},
	pages = {36-42},
	title = {A tactile feeling display based on selective stimulation to skin receptors},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658420}}

@inproceedings{658421,
	abstract = {As a basis for a tactile display, it is important to understand the mechanism of tactile sensation, so we analyze the elastic waves inside a finger in tactile exploration with a two-layered, half-infinite elastic finger model, and derive a mapping from the shapes of objects to the displacements of the mechanoreceptors. It is shown that the mapping from displacements of the finger surface to displacements of the mechanoreceptors is a one-to-one mapping. Therefore, it is necessary to create the same displacements of the finger surface as in actual exploration for tactile virtual reality (VR). However, the mapping from shapes of objects to displacements of the finger surface is shown to be a many-to-one mapping because of the mechanical properties of skin. So, we use the envelope of the amplitude-modulated Lamb wave on an elastic plate as an alternative shape to the real object, because its shape can be easily controlled. The generation of the wave is confirmed with an experimental device of silicon rubber vibrated by voice coils.},
	author = {Nara, T. and Maeda, T. and Yanagida, Y. and Tachi, S.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658421},
	keywords = {Displays;Fingers;Surface waves},
	month = {March},
	pages = {43-50},
	title = {Tactile display using elastic waves},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658421}}

@inproceedings{658422,
	abstract = {The authors have investigated a tactile texture presentation method by which a tactile texture display is driven to convey the sensations of object's surface textures. The image data (a B/W photograph taken carefully) of surface textures proved to be compatible with the surface geometry data-a height map-by an experiment with two simple-shape real texture patches. As a raw geometry data-hence a raw image-was not suitable for the texture display, they proposed to apply a histogram transformation to make the sensation produced by the data close to the real one. They obtained two histogram transformations out of fifty, by which the geometry data were adjusted so that they could best produce texture sensations regarding six real patches. The data adjusted by the transformations proved to present appropriate sensations to novice users who compared the data and the real patches tactually.},
	author = {Ikei, Y. and Wakamatsu, K. and Fukuda, S.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658422},
	keywords = {Displays;Surface texture;Geometry;Histograms;Haptic interfaces;Skin;Pins;Fingers;Shape measurement;Optimized production technology},
	month = {March},
	pages = {51-58},
	title = {Image data transformation for tactile texture display},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658422}}

@inproceedings{658423,
	abstract = {The study explored the impact of physically touching a virtual object on how realistic the VE seems to the user. Subjects in a "no touch" group picked up a 3D virtual image of a kitchen plate in a VE, using a traditional 3D wand. "See and touch" subjects physically picked up a virtual plate possessing solidity and weight, using a mixed-reality force feedback technique. Afterwards, subjects made predictions about the properties of other virtual objects they saw but did not interact with in the VE. "See and touch" subjects predicted these objects would be more solid, heavier, and more likely to obey gravity than the "no touch" group. Results provide converging evidence for the value of adding physical qualities to virtual objects. The study first empirically demonstrates the effectiveness of mixed reality as a simple, safe, inexpensive technique for adding physical texture and force feedback cues to virtual objects with large freedom of motion. Examples of practical applications are discussed.},
	author = {Hoffman, H.G.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658423},
	keywords = {Virtual reality;Force feedback;Gravity;Solids;Calibration;Virtual environment;Humans;Laboratories;Psychology;Ceramics},
	month = {March},
	pages = {59-63},
	title = {Physically touching virtual objects using tactile augmentation enhances the realism of virtual environments},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658423}}

@inproceedings{658424,
	abstract = {ER fluids are functional fluids which have attracted attention in recent years. We have developed a new type of actuator using ER fluid. This ER actuator responds quickly, and has large torque/inertia ratio. ER actuators are thus suited to be the actuators used in force feedback systems. We have developed a force feedback system using ER actuators, and have enacted some basic experiments for force display. The ER actuators can be utilized in teleoperation systems and various other force feedback systems as well as in virtual reality systems.},
	author = {Sakaguchi, M. and Furusho, J.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658424},
	keywords = {Erbium;Actuators;Engine cylinders;Computer displays;Electrodes;Application software;Virtual reality;Humans;Shearing;Stress},
	month = {March},
	pages = {66-70},
	title = {Development of ER actuators and their applications to force display systems},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658424}}

@inproceedings{658425,
	abstract = {Cutting is an essential operation for forming and designing shapes. We have implemented a virtual cutting work space, in which we can create objects of various shapes through cutting operations. An object in the work space is defined as a closed surface that consists of triangle patches. The cutting operation is performed as boolean operations between the object and a cutting surface corresponding to the motion of the cutting blade. We have also employed a force feedback device and liquid crystal shutter glasses to represent the sensation of force while cutting and to provide stereoscopic views of the work space.},
	author = {Tanaka, A. and Hirota, K. and Kaneko, T.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658425},
	keywords = {Force feedback},
	month = {March},
	pages = {71-75},
	title = {Virtual cutting with force feedback},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658425}}

@inproceedings{658426,
	abstract = {A deformable non-uniform rational B-spline (NURBS) based volume is programmed for the Iowa State University (ISU) force-reflecting exoskeleton haptic device. A direct free-form deformation (DFFD) technique is applied for realistic manipulation. In order to implement real-time deformation, a nodal mapping technique is used to connect points on the virtual object with the NURBS volume. This geometric modeling technique is ideally incorporated with the force-reflecting haptic device as a virtual interface. The results presented in this paper introduce details for the complete set-up for a realistic virtual clay modeling task with force feedback. The ISU force-reflecting exoskeleton, coupled with a supporting PUMA 560 manipulator and the virtual clay model are integrated with the WorldToolKit (WTK) graphics display, and the results show that the force feedback from the realistic physically-based virtual environment can greatly enhance the sense of immersion.},
	author = {Young-Ho Chai and Luecke, G.R. and Edwards, J.C.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658426},
	keywords = {Exoskeletons;Spline;Surface topography;Surface reconstruction;Haptic interfaces;Force feedback;Solid modeling;Graphics;Displays;Virtual environment},
	month = {March},
	pages = {76-80},
	title = {Virtual clay modeling using the ISU exoskeleton},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658426}}

@inproceedings{658427,
	abstract = {We present the nanoWorkbench (nWB), which adds haptic feedback to a stereoscopic, head-tracked projection display to produce a system in which the user can both see and feel virtual objects. We describe the nWB's design parameters, size and layout, including the reasons for our choices. Solutions to the problems of tracker interference, calibration and occlusion are presented.},
	author = {Grant, B. and Helser, A. and Taylor, R.M.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658427},
	keywords = {Force feedback;Optical microscopy;Computer displays;Haptic interfaces;Force control;Shape;Computer science;Identity-based encryption;Three dimensional displays;Head},
	month = {March},
	pages = {81-88},
	title = {Adding force display to a stereoscopic head-tracked projection display},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658427}}

@inproceedings{658428,
	abstract = {Based on a general hierarchical data structure, we present a fast algorithm for exact collision detection of arbitrary polygonal rigid objects. Objects consisting of hundreds of thousands of polygons can be checked for collision at interactive rates. The pre-computed hierarchy is a tree of discrete oriented polytopes (DOPs). An efficient way of realigning DOPs during the traversal of such trees allows us to use simple interval tests for determining the overlap between DOPs. The data structure is very efficient in terms of memory and construction time. Extensive experiments with synthetic and real-world CAD data have been carried out to analyze the performance and memory usage of the data structure. A comparison with oriented bounding box (OBB) trees indicates that DOP-trees are as efficient in terms of collision query time and more efficient in memory usage and construction time.},
	author = {Zachmann, G.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658428},
	keywords = {Object detection;Data structures;Testing;Virtual reality;Motion detection;Virtual prototyping;Virtual environment;Computer graphics;Performance analysis;Interference},
	month = {March},
	pages = {90-97},
	title = {Rapid collision detection by dynamically aligned DOP-trees},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658428}}

@inproceedings{658451,
	abstract = {In this paper, we present a new method of simulating and visualizing smoke and steam clouds in virtual environments (VEs). We use a particle system approach, but reduce the particle numbers to decrease the computation and rendering times. Special care, however is taken for the presentation quality of the clouds. From every viewing point (even within the clouds), the VE user does not have the impression that the clouds are hollow polyhedrons, but rather fuzzy objects.},
	author = {Unbescheiden, M. and Trembilski, A.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658451},
	keywords = {Clouds;Computational modeling;Virtual environment;Animation;Data visualization;Urban planning;Fires;Layout;Virtual reality;Computer graphics},
	month = {March},
	pages = {98-104},
	title = {Cloud simulation in virtual environments},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658451}}

@inproceedings{658453,
	abstract = {This paper presents a systematic approach to automatically construct 3D natural scenes from video sequences. Dense layered depth maps are derived from image sequences captured by a vibrated camera with only approximately known motion. The approach consists of (1) image stabilization by motion filtering and (2) depth estimation by spatio-temporal texture analysis. The two stage method not only generalized the so called panoramic image method and epipolar plane image method to handle image sequence vibrations due to the uncontrollable camera fluctuations, but also bypasses the feature extraction and matching problems encountered in stereo or visual motion. Our approach allows automatic modeling of the real environment for inclusion in VR representations.},
	author = {Zhigang Zhu and Guangyou Xu and Xueyin Lin},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658453},
	keywords = {Layout;Video sequences;Image sequences;Cameras;Motion analysis;Filtering;Motion estimation;Image analysis;Image motion analysis;Image texture analysis},
	month = {March},
	pages = {105-112},
	title = {Constructing 3D natural scene from video sequences with vibrated motions},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658453}}

@inproceedings{658461,
	abstract = {This paper describes a new method for view synthesis from previously generated images. The method considers each image to be a slice of an overall light field which describes radiance as a function of position and direction. Each slice is then mapped onto the viewing parameters of a new slice as an orthogonally projected, texture-mapped, deformed surface whose z-values correspond to the disparity between rays passing through a given pixel. As a result, the slice can be sent to a traditional z-buffered texture-mapping graphics pipeline, wherein each pixel's depth corresponds to its error Therefore the z-buffer will implicitly select for each pixel the best match. We call this technique error-buffering (or /spl epsi/-buffering).},
	author = {Burton, L.C.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658461},
	keywords = {Layout;Rendering (computer graphics);Image generation;Computer graphics;Tellurium;Pipelines;Image reconstruction;Interpolation;Pixel;Computer errors},
	month = {March},
	pages = {113-120},
	title = {Viewing complex scenes with error-buffered light fields},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658461}}

@inproceedings{658465,
	abstract = {Iconic hand gestures are a natural and intuitive way to convey spatial information. Capturing and interpreting iconic hand gestures will augment users' abilities to convey spatial information with computers. This paper discusses the on-going research and the results of a study to employ iconic hand gestures as a human computer interaction (HCI) technique for the input and manipulation of objects and shapes within 3D computer generated graphical environments.},
	author = {Marsh, T. and Watt, A.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658465},
	keywords = {Shape;Handicapped aids;Human computer interaction;Electrical capacitance tomography;Computer science;Speech;Computer graphics;Virtual reality;Pressing;Counting circuits},
	month = {March},
	pages = {122-125},
	title = {Shape your imagination: iconic gestural-based interaction},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658465}}

@inproceedings{658467,
	abstract = {We present Virtual Notepad, a collection of interface tools that allows the user to take notes, annotate documents and input text using a pen, while still immersed in virtual environments (VEs). Using a spatially-tracked, pressure-sensitive graphics tablet, pen and handwriting recognition software, Virtual Notepad explores handwriting as a new modality for interaction in immersive VEs. This paper reports details of the Virtual Notepad interface and interaction techniques, discusses implementation and design issues, reports the results of initial evaluation and overviews possible applications of virtual handwriting.},
	author = {Poupyrev, I. and Tomokazu, N. and Weghorst, S.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658467},
	keywords = {Virtual environment;Graphics;Application software;Writing;Text recognition;Speech recognition;Ores;Books;Pervasive computing;Databases},
	month = {March},
	pages = {126-132},
	title = {Virtual Notepad: handwriting in immersive VR},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658467}}

@inproceedings{658469,
	abstract = {The vast majority of virtual environments concentrate on constructing a realistic visual simulation while ignoring non-visual environmental cues. Although these missing cues can to some extent be ignored by an operator, the lack of appropriate cues may contribute to cybersickness and may affect operator performance. We examine the role of vestibular cues to self-motion on an operator's sense of self-motion within a virtual environment. We show that the presence of vestibular cues has a very significant effect on an operator's estimate of self-motion. The addition of vestibular cues, however, is not always beneficial.},
	author = {Harris, L. and Jenkin, M. and Zikovitz, D.C.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658469},
	keywords = {Virtual environment;Optical sensors;Acceleration;Tracking;Optical devices;Retina;Visual perception;Psychology;Computer science;Computational biology},
	month = {March},
	pages = {133-138},
	title = {Vestibular cues and virtual environments},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658469}}

@inproceedings{658476,
	abstract = {The study investigated the ability to interactively perform a combined positioning and orientation task in a three-dimensional computer environment using a six degree of freedom input device. Two different visual feedback modes were tested: fixed viewpoint monoscopic perspective and fixed viewpoint stereoscopic perspective. Targets were located at one of six positions at plus and minus 10 cm along the X, Y or Z axes from a fixed starting location. Targets were oriented in one of seven orientations, a default orientation and at plus and minus 45 degrees from the default orientation about each axis. The results indicate that stereoscopic viewing improves trial completion time and positioning accuracy along the Z axis, but has no effect upon rotation accuracy. A significant variation in trial completion time across the target position and target orientation conditions was also found. Rotation error appears biased towards the X and Z axes.},
	author = {Boritz, J. and Booth, K.S.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658476},
	keywords = {Virtual environment;Computer graphics;Application software;Electrical capacitance tomography;Read only memory;Performance gain;Computer science;Testing;Reactive power;Computer errors},
	month = {March},
	pages = {139-146},
	title = {A study of interactive 6 DOF docking in a computerised virtual environment},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658476}}

@inproceedings{658482,
	abstract = {Presents a novel object-centric tracking architecture for presenting augmented reality media in spatial relationships to objects, regardless of the objects' positions or motions in the world. The advance this system provides over previous object-centric tracking approaches is the ability to sense and integrate new features into its tracking database, thereby extending the tracking region automatically. This lazy evaluation of the structure-from-motion problem uses images obtained from a single calibrated moving camera and applies recursive filtering to identify and estimate the 3D positions of new features. We evaluate the performance of two filters; a classic extended Kalman filter (EKF) and a filter based on a recursive average of covariances (RAC). Implementation issues and results are discussed in conclusion.},
	author = {Neumann, U. and Park, J.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658482},
	keywords = {Augmented reality;Cameras;Filters;Computer science;Tracking;Computer architecture;Image databases;Spatial databases;Filtering;Motion estimation},
	month = {March},
	pages = {148-155},
	title = {Extendible object-centric tracking for augmented reality},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658482}}

@inproceedings{658484,
	abstract = {In virtual reality applications, time delay is one of the essential factors which need compensation. We measured the end to end time delay of our system and developed a compensation technique using a head motion model and a predictive Kalman filter. Compared to a previously developed predictive filter with a model free approach, our filter reduces the error, by 20 to 30%. Due to recent advances in PC technology, it was possible to implement the entire system (including the compensation) on a single PC. Our system consists of a PC, head mounted display, and a 6 degrees of freedom magnetic tracking sensor.},
	author = {Akatsuka, Y. and Bekey, G.A.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658484},
	keywords = {Virtual reality;Delay effects;Magnetic heads;Predictive models;Filters;Magnetic separation;Motion measurement;Time measurement;Displays;Magnetic sensors},
	month = {March},
	pages = {156-159},
	title = {Compensation for end to end delays in a VR system},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658484}}

@inproceedings{658485,
	abstract = {The use of virtual prototypes generated from engineering simulations can be crucial to the efficient development of innovative products. Performance predictions and functional evaluations of a design are possible long before results of real prototype tests are available. With the rise in model complexity, data quantity, computing performance and accuracy, we increasingly fined ourselves lacking the tools, methods and metaphors to deal with the information that is being generated. We present new results of on-going research at the University of Erlangen and BMW in the development of a virtual environment for car-body engineering applications as illustrated by examples from acoustics, vibration and impact dynamics.},
	author = {Schulz, M. and Ertl, Th. and Reuding, Th.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658485},
	keywords = {Vehicle crash testing;Virtual environment;Design engineering;Computational modeling;Prototypes;Acoustical engineering;Computer simulation;Acoustic applications;Vehicle dynamics;Aerodynamics},
	month = {March},
	pages = {160-166},
	title = {Crashing in cyberspace-evaluating structural behaviour of car bodies in a virtual environment},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658485}}

@inproceedings{658486,
	abstract = {We present a virtual reality application to neurosurgical pre-operative planning which is undergoing clinical evaluation at the Singapore General Hospital. The application, based on the ISS Virtual Workbench, lets the neurosurgeon study the brain pathology, blood vessels, skull and the surrounding tissue using real-time volumetric rendering of the patient data. With this information, the surgeon can plan the best approach for surgery. At the moment, seven cases have been planned. The system features measuring markers, multi-modal data fusion of a patient's data, different visualization modes, tissue enhancement through manipulation of colour, look-up tables, cloning of region of interest, and interactive pathology outlining.},
	author = {Chua Gim Guan and Serra, L. and Kockro, R.A. and Hern, N. and Nowinski, W.L. and Chan, C.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658486},
	keywords = {Neoplasms;Neurosurgery;Pathology;Virtual reality;Hospitals;Blood vessels;Skull;Surges;Surgery;Data visualization},
	month = {March},
	pages = {167-173},
	title = {Volume-based tumor neurosurgery planning in the Virtual Workbench},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658486}}

@inproceedings{658487,
	abstract = {This paper describes the NICE project, an immersive learning environment for children implemented in the CAVE and related multi-user virtual reality (VR) technologies. The NICE project provides an engaging setting where children construct and cultivate simple virtual ecosystems, collaborate via networks with other remotely-located children, and create stories from their interactions in the real and virtual world.},
	author = {Johnson, A. and Roussos, M. and Leigh, J. and Vasilakis, C. and Barnes, C. and Moher, T.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658487},
	keywords = {Virtual reality;Virtual environment;Visualization;Collaboration;Hardware;Computational modeling;Computer simulation;Ecosystems;Collaborative work;Computer science education},
	month = {March},
	pages = {176-183},
	title = {The NICE project: learning together in a virtual world},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658487}}

@inproceedings{658488,
	abstract = {Skill decay after periods of skill disuse is well known and has substantial implications when relatively long periods of time separate training from the application of learned skills. We conducted a small study that examined the differential effects of virtual reality versus conventional computer-based media on skill retention. The results reported are preliminary, but were consistent with earlier research that reports that VR may not be superior to conventional electronic media for training certain intellectual skills. Little is known, however about the effects of VR in support of practice strategies far reducing skill decay. Implications for future research are discussed.},
	author = {Hall, C.R. and Stiles, R.J. and Horwitz, C.D.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658488},
	keywords = {Virtual reality;Laboratories;Costs;Virtual environment;Computer displays;Read only memory;Application software;Ear;Testing;Context modeling},
	month = {March},
	pages = {184-189},
	title = {Virtual reality for training: evaluating knowledge retention},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658488}}

@inproceedings{658489,
	abstract = {Prostate cancer is the second leading cause of cancer death among men (25% of men with prostate cancer will die of the disease). The most common method of detecting this malignancy is digital rectal examination (DRE). Current DRE training requires medical students to examine a large number of patients before attaining adequate experience. We propose to solve this problem using a virtual reality digital rectal examination simulation. The prototype system consists of a PHANToM haptic interface which provides feedback to the trainee's index finger, a motion restricting board and an SGI workstation, which renders the patient's anatomy in the region of interest. Four types of prostate were modeled using OpenGL and GHOST haptic library-normal, enlarged with no tumor, incipient malignancy (single tumor), and advanced malignancy (tumor cluster). Results of initial human factors studies are encouraging, while pointing out the need for more realistic physical modeling.},
	author = {Burdea, G. and Patounakis, G. and Popescu, V. and Weiss, R.E.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658489},
	keywords = {Virtual reality;Prostate cancer;Neoplasms;Haptic interfaces;Diseases;Medical diagnostic imaging;Medical simulation;Prototypes;Imaging phantoms;Feedback},
	month = {March},
	pages = {190-197},
	title = {Virtual reality training for the diagnosis of prostate cancer},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658489}}

@inproceedings{658490,
	abstract = {This paper presents a prototype virtual reality (VR) system for training medical first responders. The initial application is to battlefield medicine and focuses on the training of medical corpsmen and other front-line personnel who might be called upon to provide emergency triage on the battlefield. The system is built upon Sandia's multi-user, distributed VR platform and provides an interactive, immersive simulation capability. The user is represented by an Avatar and is able to manipulate his virtual instruments and carry out medical procedures. A dynamic casualty simulation provides realistic cues to the patient's condition (e.g. changing blood pressure and pulse) and responds to the actions of the trainee (e.g. a change in the color of a patient's skin may result from a check of the capillary refill rate). The current casualty simulation is of an injury resulting in a tension pneumothorax. This casualty model was developed by the University of Pennsylvania and integrated into the Sandia MediSim system.},
	author = {Stansfield, S. and Shawver, D. and Sobel, A.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658490},
	keywords = {Prototypes;Virtual reality;Medical simulation;Virtual prototyping;Personnel;Avatars;Instruments;Blood pressure;Skin;Injuries},
	month = {March},
	pages = {198-205},
	title = {MediSim: a prototype VR system for training medical first responders},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658490}}

@inproceedings{658492,
	abstract = {Because of its ability to furnish complex interactive visual and auditory stimuli, virtual reality has been lauded as a wonder tool for training. Surprisingly, very few articles have been published which critically examine these claims, or which even empirically examine them. The studies which have empirically looked at virtual reality training have produced conflicting results. Just as in any training situation, the tasks to be trained must be decomposed into their component cognitive, perceptual and motor demands, and these demands must be met in the training environment. In short, to create an effective training environment, one must match up the capabilities of the training environment with the demands of the actual task. Virtual environments (VEs) are currently very weak at haptic displays, kinesthetic feedback and vestibular feedback. VEs are strongest in visual information display and head motion feedback. The tasks which are most likely to benefit from training in a virtual environment are tasks which heavily depend on visual information for success. Attempting to train a manual placement task, a task whose performance relies as much on haptic and kinesthetic feedback as on fine visual feedback, will not work. On the other hand, training for a visuospatial perception task matches up well with the characteristics of current VE systems. Tasks such as inspection tasks and navigation tasks take advantage of VEs' strengths for training purposes. To test this logic, we trained a navigation task using a VE.},
	author = {Philbin, D.A. and Ribarsky, W. and Walker, N. and Ellis Hubbard, C.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658492},
	keywords = {Feedback;Navigation;Electrical capacitance tomography;Virtual environment;Haptic interfaces;Displays;Logic testing;Guidelines;Virtual reality;Inspection},
	month = {March},
	pages = {210-},
	title = {Training in virtual environments: analysis of task appropriateness},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658492}}

@inproceedings{658493,
	abstract = {We describe progress in developing real-time tools for animation choreography, placing an aerodynamic bird character under interactive gestural control. Since the bird model responds to commands through a multiple-level control system, the effect is that of a director working with an intelligent actor to choreograph motion sequences.},
	author = {Schmidt, G.S. and Ringham, M.L. and House, D.H.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658493},
	keywords = {Animation;Birds;Aerodynamics;Control system synthesis;Solid modeling;Kinematics;Polynomials;Computer graphics;Visualization;Laboratories},
	month = {March},
	pages = {211-},
	title = {Choreographing realistic animated birds using gesture recognition},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658493}}

@inproceedings{658494,
	abstract = {Registration is one of the major issues in augmented reality (AR). It requires high accuracy or an error-correction mechanism in tracking. Fiducial tracking has been gaining interest as a solution of the registration problem. A single-size fiducial might help fast fiducial detection, but the system will have a narrow tracking range, since all fiducials have the same detection range. Different size fiducials have different detection ranges, and by combining a series of different detection ranges from multi-size fiducials, the whole tracking range can be extended seamlessly. Multi-ring color fiducials have different number of rings at different fiducial levels. We extend the concentric circular fiducials to multi-ring, multi-size fiducial systems. These provide scalability to fiducial tracking AR. Because the fiducial systems are incremental, they allow the tracking range to be easily extended. The fiducial systems also introduce a large number of unique fiducials, and that makes fiducial identification easier. These fiducial systems help in building large-scale applications by providing a convenient way to unify multiple local coordinate systems. It makes it easy to determine fiducial positions in a large-scale application with a small-range digitizer. We analyze the optimality of ring widths and develop formulas to get an optimal set of fiducials easily for any size of working area by plugging in some system-specific parameters. We provide a simple and low-cost way to achieve wide-area tracking.},
	author = {Youngkwan Cho and Neumann, U.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658494},
	keywords = {Augmented reality;Cameras;Computer science;Computer errors;Virtual reality;Layout;Physics computing;Milling machines;Tracking;Scalability},
	month = {March},
	pages = {212-},
	title = {Multi-ring color fiducial systems for scalable fiducial tracking augmented reality},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658494}}

@inproceedings{658495,
	abstract = {To support larger numbers of simultaneous users, virtual environments (VEs) must be partitioned over multiple servers. This paper describes an architecture for interconnecting large-scale VEs and collaboration systems over the Internet. The user establishes and maintains a persistent network identity, transfers that identity between VEs and maintains that identity while moving between management domains of a single VE. Our implementation logically extends (and therefore easily integrates with) the existing World Wide Web.},
	author = {Singhal, S.K. and Nguyen, B.Q.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658495},
	keywords = {Portals;Large-scale systems;Virtual environment;Environmental management;Identity management systems;Collaboration;Network servers;Web server;Avatars},
	month = {March},
	pages = {213-},
	title = {Registration rooms, lobbies, and portals: interconnecting large-scale networked virtual environments and collaborations},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658495}}

@inproceedings{658496,
	abstract = {Summary form only given. Augmented reality (AR) provides an intuitive interface to enhance the user's understanding of a scene. We consider the problem of scene augmentation in the context of assembly of a mechanical object. Concepts from robot assembly planning are used to develop a systematic framework for presenting augmentation stimuli for this assembly domain. An interactive evaluation tool is developed, which uses augmentation schemes for visualizing and evaluating assembly sequences. This system also guides the user step-by-step through an assembly sequence. Computer vision provides the sensing mechanism necessary to interpret the assembly scene. The goal of this system is to help evaluate the feasibility and efficiency of a particular sequence to assemble a mechanical object from its components. This is done by guiding the operator through each step in the sequence. The augmentation is provided with the help of a see-through head-mounted display that superimposes 3D graphics over the assembly scene and on nearby computer monitors. We incorporate these ideas into the design of an integrated system that we call AREAS (Augmented Reality System for Evaluating Assembly Sequences) and explore its use for evaluating assembly sequences using the concept of mixed prototyping.},
	author = {Molineros, J. and Raghavan, V. and Sharma, R.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658496},
	keywords = {Computer vision;Augmented reality;Robotic assembly;Assembly systems;Layout;Computer displays;Robot sensing systems;Visualization;Three dimensional displays;Computer graphics},
	month = {March},
	pages = {214-},
	title = {Computer vision based augmented reality for guiding and evaluating assembly sequences},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658496}}

@inproceedings{658499,
	abstract = {Presents a new approach for using texture-mapped quadrilaterals as approximate representations for objects that are far away from the viewpoint. The method is suited for interactive visualization of complex indoor environments such as CAD models of large plants. In a pre-processing stage, the 3D model is partitioned by virtual walls. These virtual walls are simple quadrilaterals which divide a large room into a set of separated cells. During the walkthrough phase, the system only renders the geometry of cells surrounding the current viewpoint. All distant geometry is culled and replaced by "textured virtual walls" representing the same part of the model as the culled geometry. A description of techniques is given for minimizing visual artifacts and for controlling the transitions between textures and geometry if the viewpoint moves towards a virtual wall. The approach makes extensive use of texture-mapping hardware. It considerably reduces the number of polygons rendered by the 3D graphics pipeline and therefore contributes to achieving interactive frame rates.},
	author = {Ebbesmeyer, P.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658499},
	keywords = {Layout;Testing;Tellurium;Geometry;Runtime;Hard disks;Displays;Navigation;Virtual reality;Head},
	month = {March},
	pages = {220-227},
	title = {Textured virtual walls achieving interactive frame rates during walkthroughs of complex indoor environments},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658499}}

@inproceedings{658500,
	abstract = {Investigates methods to accelerate rendering of architectural walkthroughs. In this paper, we improve upon a cells-and-portals framework by using image-based rendering techniques. We first store a few reference images of the view through each portal. At run-time, we replace portals with these images warped to the current viewpoint. We begin with a well-known scheme for handling the complexity of a model, whereby the boundaries of enclosed spaces (cells) are used to divide the total space, and views of geometry beyond the currently occupied space are limited to the openings (portals) by walls. Our system improves upon the replacement of portals with conventional textures because the warping removes the popping effect when switching between image samples and significantly reduces the number of image samples needed.},
	author = {Rafferty, M.M. and Aliaga, D.G. and Lastra, A.A.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658500},
	keywords = {Portals;Geometry;Solid modeling;Rendering (computer graphics);Computer science;Acceleration;Layout;Interactive systems},
	month = {March},
	pages = {228-233},
	title = {3D image warping in architectural walkthroughs},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658500}}

@inproceedings{658501,
	abstract = {A new methodology to generate image-based wide-range virtual spaces is introduced. The basic principle is to display appropriate images accompanied by user's interaction, using position data. For this methodology, a huge image database indexed by position data must be sampled from the real world. A prototype of a vehicle-mounted image capturing system is developed for this purpose. The relationship between image distortion and the error of sensors when applying the morphing technique to this system is also discussed. At the end of this paper, an image-based walk-through system is actually demonstrated and the potential advantages of this methodology are discussed as an example.},
	author = {Hirose, M. and Watanabe, S. and Endo, T.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658501},
	keywords = {Image databases;Space technology;Image generation;Rendering (computer graphics);Displays;Sensor fusion;Image coding;Argon;Solid modeling;Cities and towns},
	month = {March},
	pages = {234-241},
	title = {Generation of wide-range virtual spaces using photographic images},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658501}}

@inproceedings{658502,
	abstract = {Dynamic multi-user interactions within a distributed collaborative virtual environment (CVE) suffer from abrupt state transitions due to communication delays-an action by one user only becoming apparent to another user after the delay. This results in a divergence of the environment for the duration of the delay, followed by an abrupt jump to resynchronise, so that the current state of the virtual world is displayed. Such discontinuities do not occur in the real world and thus appear unnatural and disconcerting to the users. This paper develops the concept of a 3 1/2 D perception model, as an alternative to prediction, which locally filters the underlying model, ensuring that each user views a continuous version of the environment, such that no jumps occur, despite delays arising from remote user interaction. Each user's 3 1/2 D filter is specific to their own circumstances, so that each user's perception of the environment is slightly different from that of other users.},
	author = {Sharkey, P.M. and Ryan, M.D. and Roberts, D.J.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658502},
	keywords = {Filters;Delay;Bandwidth;Predictive models;Interactive systems;Cybernetics;Collaboration;Virtual environment;Virtual reality;Databases},
	month = {March},
	pages = {242-249},
	title = {A local perception filter for distributed virtual environments},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658502}}

@inproceedings{658503,
	abstract = {Bamboo is a portable system supporting real-time, networked, virtual environments. Unlike previous efforts, this design focuses on the ability of the system to dynamically configure itself without explicit user interaction, allowing applications to take on new functionality after execution. In particular, this framework facilitates the discovery of virtual environments on the network at runtime. Fundamentally, Bamboo offers a compatible set of mechanisms needed for a wide variety of real-time, networked applications. Also included is a particular combination of these mechanisms supporting a dynamically extensible runtime environment. This paper serves as a general introduction to Bamboo. It describes the system's architecture, implementation and future directions. It also shows how the system can facilitate the rapid development of robust applications by promoting code reuse via community-wide exchange.},
	author = {Watsen, K. and Zyda, M.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658503},
	keywords = {Real time systems;Virtual environment;Graphics;Runtime;Virtual reality;Layout;Electrical capacitance tomography;Minimally invasive surgery},
	month = {March},
	pages = {252-259},
	title = {Bamboo-a portable system for dynamically extensible, real-time, networked, virtual environments},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658503}}

@inproceedings{658504,
	abstract = {Presents the Remote Attribute Virtual Environment Library (RAVEL), a support system for the development and implementation of distributed, multi-user virtual environment (VE) applications. The system extends earlier work in low-latency communication between sub-task processes of an application to provide communication of arbitrary environment attribute information between tasks, independent of the location of the tasks involved or which task produces or consumes attribute data. In addition, the RAVEL system provides for consistent updates to a particular attribute, and provides for causally consistent ordering of events that occur in the environment, while providing the level of service efficiency required for immersive and interactive VE applications.},
	author = {Kessler, G.D. and Hodges, L.F. and Ahamad, M.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658504},
	keywords = {Application software;Distributed computing;Voltage control;Information science;Educational institutions;Runtime;System testing;Communication system control;Tracking;Computer networks},
	month = {March},
	pages = {260-267},
	title = {RAVEL, a support system for the development of distributed, multi-user VE applications},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658504}}

@inproceedings{658505,
	abstract = {Introduces a collaborative augmented reality (AR) system for real-time interactive operations. AR enables us to enhance physical space with computer-generated virtual space. In addition, collaborative AR allows multiple participants to simultaneously share the physical space surrounding them and a virtual space that is visually registered with the physical one. They can also communicate with each other through the mixed space. This paper describes the AR/sup 2/Hockey (Augmented Reality AiR Hockey) system, where players can share a physical game field and mallets, and a virtual puck to play an air-hockey game, as a case study of a collaborative AR system. Since real-time accurate registration between both spaces and players is crucial for the collaboration, a video-rate registration algorithm is implemented with magnetic head-trackers and video cameras attached to optical see-through head-mounted displays (HMDs). The configuration of the system and the details of the registration are described. Our experimental collaborative AR system achieves higher interactivity than a totally immersive collaborative VR system.},
	author = {Ohshima, T. and Satoh, K. and Yamamoto, H. and Tamura, H.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658505},
	keywords = {Computer aided software engineering;Collaboration;Augmented reality;Virtual reality;Physics computing;Games;Space technology;Application software;Delay;Laboratories},
	month = {March},
	pages = {268-275},
	title = {AR/sup 2/Hockey: a case study of collaborative augmented reality},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658505}}

@inproceedings{658506,
	abstract = {Distributed virtual reality systems are facing limitations due to the vast amount of streaming communications and world state update data to handle at host and network level. In order to lessen these burden, good mastering of data flows is required. Host information needs mostly depend on how local observers perceive the virtual world surrounding them. The authors propose a structure storing a hierarchical description of the world as a basis for host information needs identification. The information rejection criterion is currently an approximation of the projected area along an axis toward the observer on a perpendicular plane. It can be applied to sources such as visual objects or sound sources. This structure, used to dynamically reference sources based on their position and scale, is called a space scale structure (SSS); an octree is the actual storing structure. The SSS is traversed in order to retrieve sources, with respect to the rejection function. Optimizations of SSS traversal and SSS update for moving sources, based on temporal coherency, are presented. A possible distributed architecture using the SSS is also proposed. This architecture makes use of multicast and client/server network topologies. The implementation of the SSS is evaluated locally in the context of large database culling.},
	author = {Farcet, N. and Torguet, P.},
	booktitle = {Proceedings. IEEE 1998 Virtual Reality Annual International Symposium (Cat. No.98CB36180)},
	date-added = {2024-03-18 02:28:04 -0400},
	date-modified = {2024-03-18 02:28:04 -0400},
	doi = {10.1109/VRAIS.1998.658506},
	keywords = {Large-scale systems;Virtual environment;Scalability;Virtual reality;Telecommunication traffic;Traffic control;Streaming media;Power system management;Environmental management;Collaboration},
	month = {March},
	pages = {276-283},
	title = {Space-scale structure for information rejection in large-scale distributed virtual environments},
	year = {1998},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1998.658506}}

@inproceedings{583038,
	abstract = {Time critical rendering (TCR) has recently attracted much attention as an important framework for creating immersive virtual environments. TCR trades time indulgent pursuit of high quality rendering for direct control over the timing of rendering according to the variable frame rates required for participants' interactions, so that more responsive interactivity can be achieved to keep him/her immersed in a virtual environment. The paper proposes a highly effective TCR approach to the level of detail control of textures used in image based virtual reality systems. Specifically, an adaptive texture mapping strategy based on a human behavior model is presented, where both the psychological and ergonomic aspects of interior space evaluation are taken into account to achieve more reasonable image qualities and frame rates than the conventional viewing distance based texture mapping. The feasibility of the new strategy is proven through preliminary space navigation experiments using a simple virtual showroom.},
	author = {Fujishiro, I. and Tanaka, R. and Maruyama, T.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583038},
	issn = {1087-8270},
	keywords = {Humans;Time factors;Virtual environment;Timing;Rendering (computer graphics);Control systems;Virtual reality;Adaptive systems;Psychology;Ergonomics},
	month = {March},
	pages = {4-11},
	title = {Human behavior-oriented adaptive texture mapping: a time-critical approach for image-based virtual showrooms},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583038}}

@inproceedings{583039,
	abstract = {Levels of detail (LODs) are used in interactive computer graphics to avoid overload of the rendering hardware with too many polygons. While conventional methods use a small set of discrete LODs, we introduce a new class of polygonal simplification: Smooth LODs. A very large number of small details encoded in a data stream allows a progressive refinement of the object from a very coarse approximation to the original high quality representation. Advantages of the new approach include progressive transmission and encoding suitable for networked applications, interactive selection of any desired quality, and compression of the data by incremental and redundancy free encoding.},
	author = {Schmalstieg, D. and Schaufler, G.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583039},
	issn = {1087-8270},
	keywords = {Data structures;Computer graphics;Rendering (computer graphics);Encoding;Computational modeling;Hardware;Layout;Image quality;Application software;Internet},
	month = {March},
	pages = {12-19},
	title = {Smooth levels of detail},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583039}}

@inproceedings{583040,
	abstract = {Because most existing multi resolution methods are slow, a common approach is to pregenerate a few key models of the object at different resolutions. During run time, the object's distance from the viewer determines which model to use for rendering. Although this approach is simple, it suffers from the sudden change in resolution as the object moves across the threshold distance. In addition, the model used to represent an object at a particular frame is not optimized for the given dynamic viewing and animation parameters. The quadtree type of methods for arranging the surface model may allow adaptive multi resolution modeling in a simple way and it reduces the sudden change of resolution from the object level to the node level. However, the square shape of the node, together with the four-time increment in size for representing surfaces, limits the types of surfaces that it can handle without creating excessive nodes. We present a real time adaptive multi resolution method for models of arbitrary topology.},
	author = {Lau, R.W.H. and To, D.S.P. and Green, M.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583040},
	issn = {1087-8270},
	keywords = {Animation;Page description languages;Runtime;Topology;Computer graphics;Laboratories;Computer science;Shape;Rendering (computer graphics);Optimization methods},
	month = {March},
	pages = {20-27},
	title = {An adaptive multi-resolution modeling technique based on viewing and animation parameters},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583040}}

@inproceedings{583041,
	abstract = {Alternative control technologies enable users to control human machine systems without using their hands For example, the Cyberlink/sup TM/ interface, a brain body actuated control technology, employs a combination of EEG and EMG signals produced at the user's forehead to generate computer inputs that can be used for a variety of tasks. An experiment was conducted in which participants used the CyberLink/sup TM/ interface to navigate or "fly" along a virtual flight course displayed on a wide field of view dome display. Tracking performance significantly increased across experimental sessions, while measures of perceived mental workload decreased across sessions. Ratings of cybersickness were relatively low and did not vary across experimental sessions. The results indicate that brain body actuated control, achieved using the CyberLink/sup TM/ interface, provides a viable means for performing simple, single axis, continuous control tasks without using one's hands.},
	author = {Nelson, W.T. and Hettinger, L.J. and Cunningham, J.A. and Roe, M.M. and Haas, M.W. and Dennis, L.B.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583041},
	issn = {1087-8270},
	keywords = {Navigation;Control systems;Humans;Electroencephalography;Electromyography;Forehead;Signal generators;Brain computer interfaces;Computer interfaces;Computer displays},
	month = {March},
	pages = {30-37},
	title = {Navigating through virtual flight environments using brain-body-actuated control},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583041}}

@inproceedings{583042,
	abstract = {We present a first study of the effects of frame time variations, in both deviation around mean frame times and period of fluctuation, on task performance in a virtual environment (VE). Chosen are open and closed loop tasks that are typical for current applications or likely to be prominent in future ones. The results show that at frame times in the range deemed acceptable for many applications, fairly large deviations in amplitude over a fairly wide range of periods do not significantly affect task performance. However, at a frame time often considered a minimum for immersive VR, frame time variations do produce significant effects on closed loop task performance. The results will be of use to designers of VEs and immersive applications, who often must control frame time variations due to large fluctuations of complexity (graphical and otherwise) in the VE.},
	author = {Watson, B. and Spaulding, V. and Walker, N. and Ribarsky, W.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583042},
	issn = {1087-8270},
	keywords = {Virtual reality;Layout;Virtual environment;Fluctuations;Feedback;Predictive models;Graphics;Visualization;Usability;Motion control},
	month = {March},
	pages = {38-44},
	title = {Evaluation of the effects of frame time variation on VR task performance},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583042}}

@inproceedings{583043,
	abstract = {Presents a categorization of techniques for first-person motion control, or travel, through immersive virtual environments, as well as a framework for evaluating the quality of different techniques for specific virtual environment tasks. We conduct three quantitative experiments within this framework: a comparison of different techniques for moving directly to a target object varying in size and distance, a comparison of different techniques for moving relative to a reference object, and a comparison of different motion techniques and their resulting sense of "disorientation" in the user. Results indicate that "pointing" techniques are advantageous relative to "gaze-directed" steering techniques for a relative motion task, and that motion techniques which instantly teleport users to new locations are correlated with increased user disorientation.},
	author = {Bowman, D.A. and Koller, D. and Hodges, L.F.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583043},
	issn = {1087-8270},
	keywords = {Motion control;Virtual environment;Navigation;User interfaces;Control systems;Three dimensional displays;Graphics;Visualization;Usability;Educational institutions},
	month = {March},
	pages = {45-52},
	title = {Travel in immersive virtual environments: an evaluation of viewpoint motion control techniques},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583043}}

@inproceedings{583044,
	abstract = {In the area of medical education, there is a strong need for palpation training to address the specific need of detecting subsurface tumors. A virtual reality training simulation was created to address this need. Utilizing the Rutgers Master II force feedback system, the simulation allows the user to perform a patient examination and palpate (touch) the patient's virtual liver to search for hard regions beneath the surface. When the user's fingertips pass over a "tumor", experimentally determined force/deflection curves are used to give the user the feeling of an object beneath the surface. A graphical user interface was developed to facilitate navigation as well as providing a training quiz. The trainee is asked to identify the location and relative hardness of tumors, and performance is evaluated in terms of positional and diagnostic errors.},
	author = {Dinsmore, M. and Langrana, N. and Burdea, G. and Ladeji, J.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583044},
	issn = {1087-8270},
	keywords = {Virtual reality;Neoplasms;Medical simulation;Surgery;Biomedical imaging;Medical diagnostic imaging;Force feedback;Biomedical engineering;Computational modeling;Aerospace engineering},
	month = {March},
	pages = {54-60},
	title = {Virtual reality training simulation for palpation of subsurface tumors},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583044}}

@inproceedings{583045,
	abstract = {A virtual environment (VE) of portions of the ex-USS Shadwell, the Navy's full-scale fire research and test ship, has been developed to study the feasibility of using immersive VE as a tool for shipboard firefighting training and mission rehearsal. The VE system uses a head-mounted display and 3D joystick to allow users to navigate through and interact with the environment. Fire and smoke effects are added to simulate actual firefighting conditions. This paper describes the feasibility tests that were performed aboard the Shadwell and presents promising results of the benefits of VE training over conventional training methods.},
	author = {Tate, D.L. and Sibert, L. and King, T.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583045},
	issn = {1087-8270},
	keywords = {Fires;Marine vehicles;Navigation;Virtual environment;Information technology;Tires;Space missions;Performance evaluation;System testing;Telecommunication computing},
	month = {March},
	pages = {61-68},
	title = {Virtual environments for shipboard firefighting training},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583045}}

@inproceedings{583046,
	abstract = {The Virtual Reality Gorilla Exhibit is a system for teaching users about gorilla behaviors and social interactions. The system includes an accurate model of the Zoo Atlanta gorilla habitats, anthropometrically correct gorilla models and true-to-life behaviors. In the virtual environment the user assumes the persona of an adolescent gorilla. By exploring the habitat and interacting with other gorillas, the user learns about issues in gorilla habitats and about gorilla social hierarchies. Results from preliminary user testing indicate that the system successfully accomplishes its goals.},
	author = {Allison, D. and Wills, B. and Hodges, L.F. and Wineman, J.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583046},
	issn = {1087-8270},
	keywords = {Virtual reality;Virtual environment;Computer displays;Graphics;Visualization;Usability;Education;System testing;Virtual prototyping;Physics},
	month = {March},
	pages = {69-76},
	title = {Gorillas in the bits},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583046}}

@inproceedings{583048,
	abstract = {The US Air Force Armstrong Synthesized Immersion Research Environment Facility is currently investigating the development and potential application of direct vestibular displays. The Electrical Vestibular Stimulus (EVS) technology described in the paper uses electrodes located behind the ears to deliver a low level electrical current in the vicinity of the eighth cranial nerve of the central nervous system to produce a compelling sensation of roll motion about the body's fore-aft axis. In the study described, subjects experienced the EVS display while simultaneously observing a large field of view visual display which depicted curvilinear motion through a tunnel. Both EVS and visual displays were driven in a sinusoidal fashion at various phase relationships relative to one another. After observing the two displays, subjects were asked to rate various aspects of quality and magnitude of self motion. Results revealed that the fidelity of the motion experience depended upon the phase relationship between the EVS and visual displays. Results also indicated that when an appropriate phase relationship was used, the vestibular display significantly improved the fidelity of the motion experience when compared to a visual only display.},
	author = {Cress, J.D. and Hettinger, L.J. and Cunningham, J.A. and Riccio, G.E. and McMillan, G.R. and Haas, M.W.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583048},
	issn = {1087-8270},
	keywords = {Displays;Virtual environment;Electrodes;Ear;Cranial;Humans;Control systems;Aircraft;Laboratories;Ergonomics},
	month = {March},
	pages = {80-86},
	title = {An introduction of a direct vestibular display into a virtual environment},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583048}}

@inproceedings{583049,
	abstract = {In order to assess physiological adaptation to virtual environment (VE) exposure, a measure of sensorimotor pointing errors was developed. This measure evaluated the kinesthetic position sense before and after exposure to a virtual environment. An empirical evaluation involving 34 participants revealed a statistically significant difference between the before and after pointing performance, thus implying that recalibrations had occurred. These results imply that users may have to undergo physiological adaptations in order to function appropriately in a VE, where altered perceptual information is displayed. These recalibrations can linger once interaction with the VE has concluded, rendering users physiologically maladaptive for the real world. Such aftereffects lead to safety concerns until pre exposure functioning has been regained. The results of this study have established the need for developing objective measures of post VE exposure aftereffects in order to objectively determine when these effects have dissipated.},
	author = {Stanney, K.M. and Kennedy, R.S.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583049},
	issn = {1087-8270},
	keywords = {Testing;Position measurement;Sea measurements;Virtual reality;Virtual environment;Distortion measurement;Velocity measurement;Industrial engineering;Environmental management;Engineering management},
	month = {March},
	pages = {87-94},
	title = {Development and testing of a measure of the kinesthetic position sense used to assess the aftereffects from virtual environment exposure},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583049}}

@inproceedings{583050,
	abstract = {The paper describes an approach to collision detection and response, and an experiment to examine the sensitivity of subjective presence to varying collision response parameters. In particular, a bowling game scenario was used with 18 subjects, and parameters representing elasticity, friction and accuracy of collision detection were varied. Presence was assessed through a questionnaire following the experiment. The results suggested that presence was sensitive to variation in these parameters, and in particular to the value of the parameter representing friction.},
	author = {Uno, S. and Slater, M.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583050},
	issn = {1087-8270},
	keywords = {Virtual reality;Friction;Virtual environment;Computer displays;Elasticity;Computer science;Educational institutions;Legged locomotion;Feedback;Matched filters},
	month = {March},
	pages = {95-103},
	title = {The sensitivity of presence to collision response},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583050}}

@inproceedings{583051,
	abstract = {The paper describes a software framework for the virtual windtunnel, a virtual reality based, near real time interactive system for scientific visualization. This framework meets the requirements of extensibility, interactive performance, and interface independence. Creating a framework which meets all of these requirements presented a major challenge. We describe this framework's object oriented structure and process architecture, including interprocess communications and control. Device independence of both the command and display structures are developed, providing the ability to use a wide variety of interface hardware options. The resulting framework supports a high performance visualization environment which can be easily extended to new capabilities as desired.},
	author = {Bryson, S. and Johan, S. and Schlecht, L.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583051},
	issn = {1087-8270},
	keywords = {Visualization;Hardware;Displays;NASA;Computational fluid dynamics;Object oriented modeling;Virtual reality;Prototypes;Computational modeling;Virtual prototyping},
	month = {March},
	pages = {106-113},
	title = {An extensible interactive visualization framework for the virtual windtunnel},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583051}}

@inproceedings{583052,
	abstract = {The study investigated the effects of stereopsis and head tracking on presence and performance in a desktop virtual environment. Twelve subjects viewed the virtual image of a bent wire and were required to select the correct representation of the virtual wire from one of three drawings presented on paper. After each trial, subjects completed a questionnaire designed to access their level of presence in the desktop virtual environment. The results indicated that neither stereopsis nor head tracking improved the accuracy of selecting the correct paper representation of the virtual wire. However, responses to the presence survey indicated that head tracking significantly improved the reported level of presence, whereas the addition of stereopsis did not. Implications of the results for the design of desktop virtual environments are discussed.},
	author = {Barfield, W. and Hendrix, C. and Bystrom, K.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583052},
	issn = {1087-8270},
	keywords = {Visualization;Head;Virtual environment;Wire;Computer displays;Industrial training;Wood industry;Systems engineering and theory;Haptic interfaces;Auditory displays},
	month = {March},
	pages = {114-120},
	title = {Visualizing the structure of virtual objects using head tracked stereoscopic displays},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583052}}

@inproceedings{583053,
	abstract = {The success of VRML-the Virtual Reality Modeling Language-which has established itself as the standard for 3D data on the Internet, shows that virtual reality is no longer limited to research labs but will become a part of everybody's life. Although VRML has just made its first steps from a static scene description language to an interactive VR specification, the realization of distributed virtual reality for everyone will only be the next step. We introduce a network architecture to support multiuser virtual environments on the Internet. The key issues of our approach as realized in our current prototype are scalability and interactivity. For that reason we consider a world wide distribution, large numbers of participants and the composition of very large virtual worlds.},
	author = {Broll, W.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583053},
	issn = {1087-8270},
	keywords = {Virtual reality;IP networks;Virtual environment;Internet;Protocols;Information technology;Layout;World Wide Web;Large-scale systems;Scalability},
	month = {March},
	pages = {121-128},
	title = {Distributed virtual reality for everyone-a framework for networked VR on the Internet},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583053}}

@inproceedings{583062,
	abstract = {The construction of virtual worlds often requires the user to use various tools in different environments to create several types of elements which have geometrical properties and behavioral characteristics. Due to the inconveniences associated with this task, a compound environment for the task of constructing virtual worlds was proposed. This environment contains both the popular workstation as well as a surrounding virtual world. To realize this compound environment, a Projective Head Mounted Display (PHMD) prototype was developed, which effectively minimized the difficulty of going and coming between workstation and virtual environments. The PHMD was also able to address the problem that is common to traditional HMDs which involve false images. The concept and development behind the PHMD and the compound environment are discussed, and the prototype PHMD and the prototype application examples are constructed.},
	author = {Kijima, R. and Ojika, T.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583062},
	issn = {1087-8270},
	keywords = {Workstations;Virtual environment;Head;Displays;Application software;Shape;Graphical user interfaces;Virtual reality;Cities and towns;Design automation},
	month = {March},
	pages = {130-137},
	title = {Transition between virtual environment and workstation environment with projective head mounted display},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583062}}

@inproceedings{583063,
	abstract = {A head mounted visual display was used in a see through format to present computer generated, space stabilized, nearby wire like virtual objects to 14 subjects. The visual requirements of their experimental tasks were similar to those needed for visually guided manual assembly of aircraft wire harnesses. In the first experiment subjects visually traced wire paths with a head referenced cursor, subjectively rated aspects of viewing, and had their vision tested before and after monocular, biocular, or stereo viewing. Only the viewing difficulty with the biocular display was adversely effected by the visual task. This viewing difficulty is likely due to conflict between looming and stereo disparity cues. A second experiment examined the precision with which operators could manually move ring shaped virtual objects over virtual paths without collision. Accuracy of performance was studied as a function of required precision, path complexity, and system response latency. Results show that high precision tracing is most sensitive to increasing latency. Ring placement with less than 1.8 cm precision will require system latency less than 50 msec before asymptotic performance is found.},
	author = {Ellis, S.R. and Breant, F. and Manges, B. and Jacoby, R. and Adelstein, B.D.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583063},
	issn = {1087-8270},
	keywords = {Computer displays;Rendering (computer graphics);Assembly;Delay;Wire;Computer graphics;Three dimensional displays;Laboratories;Aircraft;Hardware},
	month = {March},
	pages = {138-145},
	title = {Factors influencing operator interaction with virtual objects viewed via head-mounted see-through displays: viewing conditions and rendering latency},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583063}}

@inproceedings{583064,
	abstract = {Augmented reality provides factory workers and other touch laborers with visual information overlaid upon the workcell to aid in the performance of their tasks. This application of virtual reality technology requires high accuracy, wearable, tetherless, inexpensive, mechanically robust, and lightweight head tracking systems that operate in a highly noisy environment. The paper describes a prototype head tracking system, currently under development and testing, that is based on one small, lensless, quad-cell detector and a set of fixed location, active optical beacons, that can potentially meet these requirements.},
	author = {Kim, D. and Richards, S.W. and Caudell, T.P.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583064},
	issn = {1087-8270},
	keywords = {Augmented reality;Wearable computers;Production facilities;Application software;Virtual reality;Robustness;Working environment noise;Optical noise;Prototypes;System testing},
	month = {March},
	pages = {146-150},
	title = {An optical tracker for augmented reality and wearable computers},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583064}}

@inproceedings{583065,
	abstract = {Realistic real time articulated figure motion is achieved by reprocessing a stored database of motions. Motions are created to exact specification by interpolation from a set of example motions, effectively forming a parameterized motion model. A pre-processing step involving iterative calculations is used to allow efficient direct computations at run time. An inverse kinematics capability is shown that is based on interpolation. This method preserves the underlying qualities of the data, such as dynamical realism of motion capture, while generating a continuous range of required motions. Relevant applications include networked virtual reality and interactive entertainment.},
	author = {Wiley, D.J. and Hahn, J.K.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583065},
	issn = {1087-8270},
	keywords = {Interpolation;Databases;Virtual reality;Kinematics;Computational modeling;Computational efficiency;Libraries;Network synthesis;TV;Motion pictures},
	month = {March},
	pages = {156-160},
	title = {Interpolation synthesis for articulated figure motion},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583065}}

@inproceedings{583066,
	abstract = {In networked virtual environments, when the participants are represented by virtual human figures, the articulated structure of the human body introduces a new complexity in the usage of the network resources. This might create a significant overhead in communication, especially as the number of participants in the simulation increases. In addition, the animation should be realistic, as it is easy to recognize anomalies in the virtual human animation. This requires real-time algorithms to decrease the network overhead while considering characteristics of body motion. The dead-reckoning technique is a way to decrease the number of messages communicated among the participants, and has been used for simple non-articulated objects in popular systems. The authors introduce a dead-reckoning technique for articulated virtual human figures based on Kalman filtering, discuss main issues and present experimental results.},
	author = {Capin, T.K. and Pandzic, I.S.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583066},
	issn = {1087-8270},
	keywords = {Humans;Virtual environment;Biological system modeling;Animation;Visualization;Communication system control;Dead reckoning;Marine vehicles;Graphics;Kalman filters},
	month = {March},
	pages = {161-169},
	title = {A dead-reckoning algorithm for virtual human figures},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583066}}

@inproceedings{583067,
	abstract = {VRaptor, a VR system for situational training that uses trainer-defined scenarios is described. The trainee is represented by an avatar; the rest of the virtual world is populated by virtual actors, which are under the control of trainer-defined scripts. The scripts allow reactive behaviors, but the trainer can control the overall scenario. This type of training system may be very useful in supplementing physical training.},
	author = {Shawver, D.M.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583067},
	issn = {1087-8270},
	keywords = {Avatars;Virtual reality;Animation;Virtual environment;Security;Humans;Workstations;Layout;Buildings;Educational institutions},
	month = {March},
	pages = {170-177},
	title = {Virtual actors and avatars in a flexible user-determined-scenario environment},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583067}}

@inproceedings{583068,
	abstract = {Tactile display devices use an array of pins mounted in the form of a matrix to present three-dimensional shapes to the user by raising and lowering the pins. With a denser matrix of mounted pins, it can be expected that shape identification will become easier and the time required for identification will also become shorter, but that problems of difficulty in fabrication will arise. It is necessary to consider such trade-offs in the development of such devices. The authors conducted experiments to study the effect of pin pitch on shape identification as part of the fundamental investigation of this subject. The experiment used three tactile display devices with pin pitches of 2 mm, 3 mm and 5 mm for geometrical shape identification, with response time and rate of misidentification taken as the performance data. Surfaces, edges and vertices of three-dimensional shapes were used as the shape primitives for displayed shapes and several of each type were selected for presentation. The results obtained revealed that performance has different relationships to pin pitch with different shape primitives.},
	author = {Shimojo, M. and Shinohara, M. and Fukui, Y.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583068},
	issn = {1087-8270},
	keywords = {Three dimensional displays;Shape measurement;Pins;Computer displays;Virtual reality;Humans;Shape control;Fabrication;Research and development;Anthropometry},
	month = {March},
	pages = {180-187},
	title = {Shape identification performance and pin-matrix density in a 3 dimensional tactile display},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583068}}

@inproceedings{583069,
	abstract = {The authors describe an implemented system for touching 3D objects depicted in visual images using a two-dimensional, force-reflecting haptic interface. The system constructs 3D geometric models from real 2D stereo images. The force feedback at each position is computed to provide the sensation of moving a small ball over the surface of the 3D object.},
	author = {Shi, Y. and Pai, D.K.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583069},
	issn = {1087-8270},
	keywords = {Haptic interfaces;Force feedback;Solid modeling;Shape;Stereo image processing;Computer displays;Robot vision systems;Cameras;Virtual environment;Power system modeling},
	month = {March},
	pages = {188-191},
	title = {Haptic display of visual images},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583069}}

@inproceedings{583070,
	abstract = {Force feedback from the virtual world can greatly enhance the sense of immersion even for simple applications. The ISU force reflecting exoskeleton enables the user to interact dynamically with simulated environments by providing an electromagnetic haptic interface between the human and the environment. The paper describes the high bandwidth electromagnetic haptic interface and how it has been used to provide the sense of contact in the synthetic environment. The air gap between the magnetics, carried by the robot, and the coils attached to the human's digits, allows for small relative motion between the human and the robot without affecting the transmission of forces. This flexibility allows the robot to track the human as well as develop appropriate forces from the virtual world. Three different typical synthetic environments are programmed and tested using the ISU force reflecting exoskeleton haptic interface device. The experimental results shows that the magnetic interface gives adequate force levels for perception of virtual objects, enhancing the feeling of immersion in the virtual environment.},
	author = {Luecke, G.R. and Chai, Y.-H.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583070},
	issn = {1087-8270},
	keywords = {Humans;Haptic interfaces;Robot sensing systems;Exoskeletons;Electromagnetic forces;Magnetics;Force feedback;Bandwidth;Coils;Testing},
	month = {March},
	pages = {192-198},
	title = {Contact sensation in the synthetic environment using the ISU force reflecting exoskeleton},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583070}}

@inproceedings{583071,
	abstract = {The authors have developed a tactile display which has fifty vibrating pins to convey the surface texture sensation of object surfaces to the user's fingertip. The tactual sensation scaling was first performed to obtain a linear sensation scale of the display by means of the JND (just noticeable difference) method. One-dimensional curves on the scale were displayed to investigate human sensitivity to an intensity change rate. A tactile texture presentation method based on the image of an object surface is introduced, and two kinds of experiment were performed. Texture discrimination is the first, in which the effect of texture element size on the correct separation was discussed. Then the sensations produced by the display and that by a real object were compared using several samples that had vertical lines and did not contain low frequencies. The results are summarized and further research directions are discussed.},
	author = {Ikei, Y. and Wakamatsu, K. and Fukuda, S.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583071},
	issn = {1087-8270},
	keywords = {Displays;Fingers;Surface texture;Force sensors;Surface treatment;Vibrations;Humans;Frequency;Skin;Piezoelectric actuators},
	month = {March},
	pages = {199-205},
	title = {Texture presentation by vibratory tactile display-image based presentation of a tactile texture},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583071}}

@inproceedings{583072,
	abstract = {The authors' goal is to develop techniques for distributed driving simulation on low cost computers. Successful distributed environments have already been implemented for military and commercial applications (Macedonia et al., 1994, Stytz, 1996). These virtual environments are scalable and often use dead-reckoning algorithms to improve network performance. However, a driving simulator with multiple human controlled actors may require near or absolute synchronization. For example, when the lead driver in a car-following situation suddenly brakes, the following car driver needs to respond as quickly as possible to avoid a collision. Such driving paradigms suggest that broadcasting and dead-reckoning may be applicable only if the human controlled actors are further apart than some delta time value. Their multi-driver virtual driving simulator is an extension of the virtual environments driving simulator developed by Levine and Mourant (1995). The study compares two configurations. The first is a typical distributed virtual environment in that it will use standard networking. The second configuration utilizes cloned data acquisition. This is where the analog signals of each human controlled vehicle (gas pedal, brake pedal, and steering) are sent to every node. Since they currently have only two nodes that are located in close physical proximity, cloned data acquisition can be easily accomplished. Duplicate databases for the 3D environment and vehicles reside on each computer.},
	author = {Mourant, R.R. and Qiu, N. and Chiu, S.A.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583072},
	issn = {1087-8270},
	keywords = {Computational modeling;Virtual environment;Humans;Military computing;Data acquisition;Vehicles;Computer simulation;Costs;Distributed computing;Application software},
	month = {March},
	pages = {208-},
	title = {A distributed virtual driving simulator},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583072}}

@inproceedings{583073,
	abstract = {The advances in computer graphics technology plus the increased complexity of finite element (FE) simulations of the crash behavior of a car body have resulted in the need for new visualization techniques to facilitate the analysis of such engineering computations. The VR system VtCrash provides novel computer-human interface techniques for intuitive and interactive analysis of large amounts of crash simulation data. VtCrash takes geometry and physical properties data as input and enables the user to enter a virtual crash and to interact with any part of the vehicle.},
	author = {Kuschfeldt, S. and Schulz, M. and Ertl, T. and Reuding, T. and Holzner, M.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583073},
	issn = {1087-8270},
	keywords = {Virtual environment;Iron;Vehicle crash testing;Computational modeling;Analytical models;Computer simulation;Computer graphics;Finite element methods;Data visualization;Automotive engineering},
	month = {March},
	pages = {209-},
	title = {The use of a virtual environment for FE analysis of vehicle crash worthiness},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583073}}

@inproceedings{583074,
	abstract = {Transportation-related skills have been identified by parents as a critical area in which to teach children and youths to be more independent. Crossing Streets, the authors' initial effort to investigate skill acquisition and generalization in a virtual reality environment, attempts to teach children, including those with disabilities, a safe way to cross a street.},
	author = {Rusch, F.R. and Millar, D.S. and Cimera, R.E. and Shelden, D.L. and Thakkar, U. and Chapman, D.A. and Khan, Y.H. and Moore, D.D. and LeBoy, J.S.},
	booktitle = {Proceedings of IEEE 1997 Annual International Symposium on Virtual Reality},
	date-added = {2024-03-18 02:27:57 -0400},
	date-modified = {2024-03-18 02:27:57 -0400},
	doi = {10.1109/VRAIS.1997.583074},
	issn = {1087-8270},
	keywords = {Virtual reality;Knowledge acquisition;Educational institutions;Recruitment;Collaboration;Virtual environment;Computer applications;Navigation;Application software;Software performance},
	month = {March},
	pages = {211-},
	title = {Crossing Streets: a K-12 virtual reality application for understanding knowledge acquisition},
	year = {1997},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1997.583074}}

@inproceedings{490505,
	abstract = {The paper reviews a decade of work in applying virtual reality to medicine. Beginning with a brief history of simulations and surgery, we present a background to surgery simulators and then discuss the problem of limited human body models in past systems. We present our work at developing human body models beginning with the first virtual reality leg simulator in 1989. This model allowed simple tendon transfers and osteotomies with the computer able to predict the resulting mechanics and ability to walk. We also discuss the leg model's evolution into a performance machine which will allow a surgeon to predict position and subsequent function of an Anterior Cruciate Ligament (ACL) repair. This is paralleled by Department of Defense work on a leg model that can have a simulated wound and predicts blood loss and its ability to function. We review computer aided surgery and virtual reality technologies. We then present our work in plastic surgery computer aided planning and predict the importance of this work for surgical training.},
	author = {Rosen, J.M. and Laub, D.R. and Pieper, S.D. and Mecinski, A.M. and Soltanian, H. and McKenna, M.A. and Chen, D. and Delp, S.L. and Loan, J.P. and Basdogan, C.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490505},
	keywords = {Virtual reality;Biological system modeling;Surgery;Predictive models;Medical simulation;Computational modeling;Leg;Humans;History;Tendons},
	month = {March},
	pages = {5-13},
	title = {Virtual reality and medicine: from training systems to performance machines},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490505}}

@inproceedings{490506,
	abstract = {Interactive video and television viewers should have the power to control their viewing position. To realize this, we introduce the concept of immersive video, which employs computer vision and computer graphics technologies to provide viewers of live events a sense of total immersion by providing the viewer with a "virtual camera". Immersive video uses multiple videos of an event, captured from different perspectives, to generate a full 3D digital video of that event. While replaying this 3D digital movie, interactive viewers are able to explore the scene continuously from any perspective. This is accomplished by combining an a priori static model with a dynamic model that is created by assimilating dynamic information from each video stream into a comprehensive three dimensional environment model. We formalize the concept of immersive video, describe the architecture of our current implementation, and illustrate immersive video in staged karate demonstrations and basketball games. In its full realization, immersive video will be a paradigm shift in visual communication which will revolutionize television and video media, and will become an integral part of future telepresence and virtual reality systems.},
	author = {Moezzi, S. and Katkere, A. and Kuramura, D.Y. and Jain, R.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490506},
	keywords = {Layout;Computer vision;Cameras;TV;Motion pictures;Games;Computer architecture;Computer graphics;Streaming media;Three dimensional displays},
	month = {March},
	pages = {17-24},
	title = {Immersive video},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490506}}

@inproceedings{490507,
	abstract = {We describe the design and implementation of a video based augmented reality system capable of overlaying three dimensional graphical objects on live video of dynamic environments. The key feature of the system is that it is completely uncalibrated: it does not use any metric information about the calibration parameters of the camera or the 3D locations and dimensions of the environment's objects. The only requirement is the ability to track across frames at least four feature points that are specified by the user at system initialization time and whose world coordinates are unknown. Our approach is based on the following observation: given a set of four or more non coplanar 3D points, the projection of all points in the set can be computed as a linear combination of the projections of just four of the points. We exploit this observation by: tracking lines and fiducial points at frame rate; and representing virtual objects in a non Euclidean, affine frame of reference that allows their projection to be computed as a linear combination of the projection of the fiducial points.},
	author = {Kutulakos, K.N. and Vallino, J.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490507},
	keywords = {Augmented reality;Calibration;Cameras;Three dimensional displays;Computer displays;Animation;Computer science;Computer graphics;Application software;Data visualization},
	month = {March},
	pages = {25-36},
	title = {Affine object representations for calibration-free augmented reality},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490507}}

@inproceedings{490508,
	abstract = {The paper proposes aligning perspective contours with 3D objects without knowledge of the correspondence between 2D image points and a projected 3D model, by matching some global image characteristics from the images. It was found that simply using image moments was not sufficient for registration, but converting the image with orthogonal polynomials gave good results. The proposed approach only requires computational complexity of O(n), where n is the number of image contour points to be aligned, and provides the flexibility of not requiring the same number of 2D and 3D points. Furthermore, convergence regions for finding numerical solutions are enlarged significantly by using central moments. Global convergence is achieved using 64 different initial guesses. Experiments with Monte Carlo analysis of three different objects with different movements have been conducted to show the effectiveness of the proposed approach. The results of using three different orthogonal polynomials: Chebyshev, Gram and Legendre polynomials at three different noise levels are also compared.},
	author = {Siu-Leong Iu and Rogovin, K.W.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490508},
	keywords = {Polynomials;Virtual reality;Image converters;Application software;Laboratories;Computational complexity;Monte Carlo methods;Noise level;Augmented reality;Displays},
	month = {March},
	pages = {37-44},
	title = {Registering perspective contours with 3-D objects without correspondence, using orthogonal polynomials},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490508}}

@inproceedings{490509,
	abstract = {We propose a new concept of visual/haptic interfaces called WYSIWYF display. The proposed concept provides correct visual/haptic registration using a vision based object tracking technique and a video keying technique so that what the user can see via a visual interface is consistent with what he/she can feel through a haptic interface. Using Chroma Keying, a live video image of the user's hand is extracted and blended with the graphic scene of the virtual environment. The user's hand "encounters" the haptic device exactly when his/her hand touches a virtual object in the blended scene. The first prototype has been built and the proposed concept was demonstrated.},
	author = {Yokokohji, Y. and Hollis, R.L. and Kanade, T.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490509},
	keywords = {Haptic interfaces;Virtual environment;Head;Liquid crystal displays;Graphics;Layout;Prototypes;Robots;Virtual reality;Charge coupled devices},
	month = {March},
	pages = {46-53},
	title = {What you can see is what you can feel-development of a visual/haptic interface to virtual environment},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490509}}

@inproceedings{490510,
	abstract = {The research presented investigated the effects of haptic display modalities on human performance in virtual environments. Force feedback information was presented through visual, auditory and haptic feedback modalities. Results show that haptic feedback greatly increases performance and reduces error rates compared with the open loop case. Redundant haptic information (two modalities at the same time) does not improve performance significantly, but is very useful in order to increase sensitivity.},
	author = {Fabiani, L. and Burdea, G. and Langrana, N. and Gomez, D.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490510},
	keywords = {Humans;Force feedback;Haptic interfaces;Graphics;Liquid crystal displays;Virtual environment;Feedback loop;Virtual reality;System testing;Dynamic range},
	month = {March},
	pages = {54-59},
	title = {Human interface using the Rutgers Master II force feedback interface},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490510}}

@inproceedings{490511,
	abstract = {Walkthrough simulation is effective for design and presentation of buildings or urban space. The paper presents a method of implementing a new interface device for locomotion in virtual space. The walker wears omni directional sliding devices on the feet, which generate the feel of walking while the position of the walker is fixed in the physical world. A hoop is set around the walker's waist, by which the position of the walker is limited. The walker can freely change the direction of walking in the hoop. The scene of virtual space is displayed in a head mounted display, corresponding with the motion of the feet and head. Usability of the system is tested by 235 novice users at SIGGRAPH'95. 94% of the participants could walk freely around the virtual space.},
	author = {Iwata, H. and Fujii, T.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490511},
	keywords = {Virtual environment;Legged locomotion;Space technology;Virtual reality;Space exploration;Displays;Haptic interfaces;Feedback;Motion measurement;Virtual prototyping},
	month = {March},
	pages = {60-65},
	title = {Virtual perambulator: a novel interface device for locomotion in virtual environment},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490511}}

@inproceedings{490512,
	abstract = {A multisensory data sensualization environment was developed in which visual, acoustic, and touch sensation information could be used to display scientific data applying virtual reality technology. Specifically, a wind sensation display prototype using air flow pressure was developed to generate touch sensation. In scientific visualization, it is necessary to consider the characteristic of human perception in order to transmit data from the computer to the user accurately. The paper describes the scaling method of the sense displays based on psychological magnitude. Based on the experiment on sensory interference in perceiving data, the paper also proposes guidelines for the usage of multisensory information.},
	author = {Ogi, T. and Hirose, M.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490512},
	keywords = {Humans;Computer displays;Prototypes;Data visualization;Auditory displays;Virtual reality;Frequency;Guidelines;Three dimensional displays;Fans},
	month = {March},
	pages = {66-70},
	title = {Multisensory data sensualization based on human perception},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490512}}

@inproceedings{490513,
	abstract = {Advanced display technologies have made the virtual exploration of relatively complex models feasible in many applications. Unfortunately, only a few human interfaces allow natural interaction with the environment. Moreover in surgical applications, such realistic interaction requires real time rendering of volumetric data-placing an overwhelming performance burden on the system. We report on a collaboration of a unique interdisciplinary group developing a virtual reality system that provides intuitive interaction with complex volume data by employing real time realistic volume rendering and convincing force feedback (haptic) sensations. We describe our rendering methods and the haptic devices in detail and demonstrate the utilization of this system in the real world application of Endoscopic Sinus Surgery (ESS) simulation.},
	author = {Yagel, R. and Stredney, D. and Wiet, G.J. and Schmalbrock, P. and Rosenberg, L. and Sessanna, D.J. and Kurzion, Y. and King, S.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490513},
	keywords = {Surgery;Real time systems;Haptic interfaces;Displays;Humans;Rendering (computer graphics);Collaboration;Virtual reality;Force feedback;Electronic switching systems},
	month = {March},
	pages = {72-78},
	title = {Multisensory platform for surgical simulation},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490513}}

@inproceedings{490514,
	abstract = {The number of specialized medical doctors is decreasing. It is important to assist doctors in their operation of surgical tools. To solve this problem, we propose a distributed VR system using multimedia telecommunication for training, diagnosis, and assistance in surgery. To realize this system, it is important to exchange high quality moving pictures. We use high speed optical fiber network with ATM (Asynchronous Transfer Mode). ATM has excellent features such as bandwidth allocation which is suitable for multimedia communication on computer networks. Based on this new information infrastructure we built a prototype telesurgery system for intravascular neurosurgery. We made a virtual simulator for the operation of a catheter, that is designed for minimum invasive surgery inside complex and narrow brain blood vessels. A force display and visual assistance method are proposed to assist the doctor. We undertook teleoperation experiments between Nagoya and Tokyo, about 350 km away each other, using high speed optical fiber network, and evaluated the effectiveness of the proposed system.},
	author = {Arai, F. and Tanimoto, M. and Fukuda, T. and Shimojima, K. and Matsuura, H. and Negoro, M.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490514},
	keywords = {Virtual environment;Asynchronous transfer mode;Surgery;Optical fiber networks;Medical diagnostic imaging;Virtual reality;Multimedia systems;Channel allocation;Multimedia communication;Computer networks},
	month = {March},
	pages = {79-85},
	title = {Distributed virtual environment for intravascular tele-surgery using multimedia telecommunication},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490514}}

@inproceedings{490515,
	abstract = {Fear of flying is a serious problem that affects millions of individuals. Exposure therapy for fear of flying is an effective therapy technique. However, exposure therapy is also expensive, logistically difficult to arrange, and presents significant problems of patient confidentiality and potential embarrassment. We have developed a virtual airplane for use in fear of flying therapy. Using the virtual airplane for exposure therapy is a potential solution to many of the current problems of fear of flying exposure therapy. We describe the design of the virtual airplane and present a case report on its use for fear of flying exposure therapy.},
	author = {Hodges, L.F. and Rothbaum, B.O. and Watson, B. and Kessler, G.D. and Opdyke, D.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490515},
	keywords = {Airplanes;Medical treatment;Virtual reality;Airports;Management training;In vivo;Visualization;Usability;Psychiatry;Psychology},
	month = {March},
	pages = {86-93},
	title = {A virtual airplane for fear of flying therapy},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490515}}

@inproceedings{490516,
	abstract = {A uniform frame rate can be achieved by selecting a level of detail for each visible object such that all the objects can be rendered within the given frame time. With many visible objects a low rendering quality must be chosen if every frame is rendered from scratch. This paper presents a load-adaptive rendering algorithm which exploits frame to frame coherence and re-uses most of the image data generated during previous frames thus decreasing the number of polygons actually rendered by an order of magnitude. Complex distant objects are replaced by polygons with an image of the respective object mapped onto them. Dynamic updates of the images before frame rendering lets these "impostors" closely resemble the original objects. The approach makes efficient use of texture memory and can advantageously be incorporated into the uniform frame rate algorithm.},
	author = {Schaufler, G.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490516},
	keywords = {Virtual reality;Rendering (computer graphics);Image generation;Hardware;Computer graphics;Spatial databases;Europe;Chromium;Computational geometry;Solid modeling},
	month = {March},
	pages = {95-102},
	title = {Exploiting frame-to-frame coherence in a virtual reality system},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490516}}

@inproceedings{490517,
	abstract = {This paper presents a new method of rendering for interaction with 3D virtual space with the use of gaze detection devices. In this method, hierarchical geometric models of graphic objects are constructed prior to the rendering process. The rendering process first calculates the visual acuity, which represents the importance of a graphic object for a human operator, from the gaze position of the operator. Second, the process selects a level from the set of hierachical geometric models depending on the value of visual acuity. That is, a simpler level of detail is selected where the visual acuity is lower, and a more complicated level is used where it is higher. Then, the selected graphic models are rendered on the display. This paper examines three visual characteristics to calculate the visual acuity: the central/peripheral vision, the kinetic vision, and the fusional vision. The actual implementation and our testbed system are described, as well as the details of the visual acuity model.},
	author = {Ohshima, T. and Yamamoto, H. and Tamura, H.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490517},
	keywords = {Rendering (computer graphics);Graphics;Virtual reality;Solid modeling;Space technology;Humans;Image generation;Shape;Displays;Laboratories},
	month = {March},
	pages = {103-110},
	title = {Gaze-directed adaptive rendering for interacting with virtual space},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490517}}

@inproceedings{490518,
	abstract = {A general algorithm for decimating unstructured discretized data sets is presented. The discretized space may be a planar triangulation, a general 3D surface triangulation, or a 3D tetrahedrization. The decimation algorithm enforces Dirichlet boundary conditions, uses only existing vertices, and assumes manifold geometry. Local dynamic vertex removal is performed without history information, while preserving the initial topology and boundary geometry. The research focuses on how to remove a vertex from an existing unstructured n-dimensional tessellation, not on the formulation of decimation criteria. Criteria for removing a candidate vertex may be based on geometric properties or any scalar governing function specific to the application.},
	author = {Renze, K.J. and Oliver, J.H.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490518},
	keywords = {Topology;History;Information geometry;Biomedical imaging;Satellites;Mechanical engineering;Boundary conditions;Computer graphics;Application software;Visualization},
	month = {March},
	pages = {111-121},
	title = {Generalized surface and volume decimation for unstructured tessellated domains},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490518}}

@inproceedings{490519,
	abstract = {Sponsored by the U.S. Navy, a VE-based simulator for training submarine Officers of the Deck has been developed using a testbed of off-the-shelf hardware and software devices. The VE system uses a head-mounted display and voice recognition system to allow trainees to practice navigation of the surfaced submarine through a harbor channel marked by buoys, range markers, and other navigation aids. This paper describes the task analysis, derived requirements, and validation and verification (V&V) process used throughout development of the prototype simulation. Where possible, the general applicability of the V&V techniques to VE training is discussed.},
	author = {Zeltzer, D. and Pioch, N.J.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490519},
	keywords = {Virtual environment;Computational modeling;Underwater vehicles;Navigation;Virtual prototyping;Computer simulation;Military computing;Analytical models;Electronic equipment testing;Laboratories},
	month = {March},
	pages = {123-130},
	title = {Validation and verification of virtual environment training systems},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490519}}

@inproceedings{490520,
	abstract = {The United States Air Force Red Flag exercise is the premier combat flight training experience for fighter pilots. We created and evaluated a computer system for replay of Red Flag air-to-air combat training data with alternative display systems. Air combat data could either be displayed on a console display system (CDS) which mimicked existing replay displays or in a head-mounted display (HMD). The effectiveness of replaying air combat data using these two displays was compared in a human-performance experiment with USAF fighter pilots as the subjects. Quantitative and qualitative data about display performance and preference were collected from the pilots who used each display to review mission replays. Although there was no statistically significant difference between the subject performance when using familiar CDS or the new HMD, there was a trend favoring the HMD.},
	author = {Amburn, P. and Marshak, W.P.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490520},
	keywords = {Computer displays;Aerospace electronics;Large screen displays;Weapons;Aircraft manufacture;Application software;Military computing;Training data;Instruments;CD recording},
	month = {March},
	pages = {131-138},
	title = {Design and evaluation of an air-to-air combat debriefing system using a head-mounted display},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490520}}

@inproceedings{490521,
	abstract = {The paper provides an assessment of how selected properties of a virtual reality system impact size-distance judgments in a virtual environment. Manipulations are made in viewing conditions (biocular vs. stereoscopic), image resolution, field of view, scene contrast and target distance, while subjects attempt to match the attributes of a comparison object with a standard object. General findings suggest that under more natural viewing conditions, size-distance judgements in virtual environments differ from those found previously in physical environments whereas, under impoverished conditions, performance differences between the two environments are similar.},
	author = {Eggleston, R.G. and Janson, W.P. and Aldrich, K.A.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490521},
	keywords = {Virtual reality;Virtual environment;Humans;Psychology;Layout;Ergonomics;Laboratories;Image resolution;Costs;Hardware},
	month = {March},
	pages = {139-146},
	title = {Virtual reality system effects on size-distance judgements in a virtual environment},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490521}}

@inproceedings{490522,
	abstract = {Future airborne crewstations are currently being designed that will incorporate multisensory virtual displays to convey operationally relevant information to crew members. In addition, these displays and associated controls will be designed to adapt to the changing psychological and physiological state of the user, and the tactical/environmental state of the external world. In support of this design goal, research is being conducted to explore the information extraction capabilities of the sensory modalities. Toward this end, an experiment was conducted to assess the degree to which force-reflective haptic stimulation can be used to provide individuals with information about their location and movement through space. Specifically, a force-reflecting, haptically-augmented aircraft control stick was designed and utilized with the goal of providing pilots with real-time information concerning lateral deviation (or "line-up") with respect to the runway in a simulated instrument landing task. Pilots executed simulated landing approaches with either the force-reflecting stick or a standard aircraft displacement stick under either calm or turbulent conditions. The results indicated a consistent advantage in performance and perceived workload for the force-reflecting stick, particularly under conditions of simulated turbulence. The results are discussed in terms of their relevance for the design of advanced airborne crewstations that utilize multisensory, adaptive, virtual interfaces.},
	author = {Brickman, B.J. and Hettinger, L.J. and Roe, M.M. and Liem Lu and Repperger, D.W. and Haas, M.W.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490522},
	keywords = {Haptic interfaces;Virtual environment;Displays;Instruments;Data mining;Aerospace control;Aircraft;Information resources;Surgery;Vehicle dynamics},
	month = {March},
	pages = {147-153},
	title = {Haptic specification of environmental events: implications for the design of adaptive, virtual interfaces},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490522}}

@inproceedings{490523,
	abstract = {We examine the use of decision networks in animating virtual agents. We have developed a system that allows the realization of multiple, parallel behaviors for an agent. The networks we utilize, called PaT-Nets, are used both to represent individual behaviors and also to encode rules of engagement between agents. The multiple networks simultaneously attached to an individual agent are used to control locomotion, planning, visual attention and decision-making strategy. We discuss how human players may be substituted for autonomous players and still operate under the represented behaviors in the PaT-Nets.},
	author = {Trias, T.S. and Chopra, S. and Reich, B.D. and Moore, M.B. and Badler, N.I. and Webber, B.L. and Geib, C.W.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490523},
	keywords = {Avatars;Game theory;Humans;Animation;Decision making;Virtual reality;Strategic planning;Motion control;Virtual environment;Visualization},
	month = {March},
	pages = {156-162},
	title = {Decision networks for integrating the behaviors of virtual agents and avatars},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490523}}

@inproceedings{490524,
	abstract = {Construction of immersive virtual environments usually takes place outside the virtual environment in configuration files or application code. The system presented in this paper allows interaction with and behaviours of objects to be defined whilst immersed within the system by manipulating a dataflow representation of the dialogue occuring between the input devices and virtual objects. A concrete example is presented that illustrates the flexibility and customization opportunities that this approach provides.},
	author = {Steed, A. and Slater, M.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490524},
	keywords = {Virtual environment;Educational institutions;Computer science;Application software;Virtual reality;Computer displays;Pressing;Mice;Legged locomotion;Concrete},
	month = {March},
	pages = {163-167},
	title = {A dataflow representation for defining behaviours within virtual environments},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490524}}

@inproceedings{490525,
	abstract = {Virtual reality has made computer interfaces more intuitive but not more intelligent. This paper shows how a rule-based expert system can be combined with multimodal input and natural language techniques to add intelligence to the interface. In this way, virtual worlds can be built which recognize and respond intelligently to user actions. Coupling voice, gesture, body position and context together with an expert system creates an interface more powerful than any of these elements alone; one which establishes a true two-way dialog between human and machine. To illustrate these techniques, we present two prototype systems that allow intuitive multimodal communication within intelligent virtual environments.},
	author = {Billinghurst, M. and Savage, J.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490525},
	keywords = {Natural languages;Humans;Expert systems;Virtual environment;Virtual reality;Computer interfaces;Speech recognition;Machine intelligence;Virtual prototyping;Biological system modeling},
	month = {March},
	pages = {168-175},
	title = {Adding intelligence to the interface},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490525}}

@inproceedings{490526,
	abstract = {In a virtual environment with multiple participants, it is necessary that the user's actions be replicated by synthetic human forms. Whole-body digitizers would be the most realistic solution for capturing the individual participant's human form, however the best of the digitizers available are not interactive and are therefore not suitable for real-time interaction. Usually, a limited number of sensors are used as constraints on the synthetic human form. Inverse kinematics algorithms are applied to satisfy these sensor constraints. These algorithms result in slower interaction because of their iterative nature, especially when there are a large number of participants. To support real-time interaction in a virtual environment, there is a need to generate closed-form solutions and fast searching algorithms. In this paper, a new closed-form solution for the arms (and legs) is developed using two magnetic sensors. In developing this solution, we use the biomechanical relationship between the lower arm and the upper arm to provide an analytical, non-iterative solution. We have also outlined a solution for the whole human body by using up to ten magnetic sensors to break the human skeleton into smaller kinematic chains. In developing our algorithms, we use the knowledge of natural body postures to generate faster solutions for real-time interaction.},
	author = {Semwal, S.K. and Hightower, R. and Stansfield, S.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490526},
	keywords = {Humans;Iterative algorithms;Virtual environment;Kinematics;Closed-form solution;Magnetic sensors;Arm;Leg;Magnetic analysis;Skeleton},
	month = {March},
	pages = {177-184},
	title = {Closed form and geometric algorithms for real-time control of an avatar},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490526}}

@inproceedings{490527,
	abstract = {Current virtual environment and teleoperator applications are hampered by the need for an accurate, quick-responding head-tracking system with a large working volume. Gyroscopic orientation sensors can overcome problems with jitter, latency, interference, line-of-sight obscurations and limited range, but suffer from slow drift. Gravimetric inclinometers can detect attitude without drifting, but are slow and sensitive to transverse accelerations. This paper describes the design of a Kalman filter to integrate the data from these two types of sensors in order to achieve the excellent dynamic response of an inertial system without drift, and without the acceleration sensitivity of inclinometers.},
	author = {Foxlin, E.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490527},
	keywords = {Sensor fusion;Acceleration;Accelerometers;Prototypes;Delay;Interference;Magnetic field measurement;Position measurement;Stability;Virtual environment},
	month = {March},
	pages = {185-194},
	title = {Inertial head-tracker sensor fusion by a complementary separate-bias Kalman filter},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490527}}

@inproceedings{490528,
	abstract = {Natural and realistic object manipulation with the hand is a basic, yet vital issue for virtual reality technology to make the application a more useful one, and to display a higher degree of presence for the user via interaction. This paper aims to discuss the development of a globally-generic, realistic manipulation calculation model which is based on the integration of several subcomponent manipulation calculation models. First, the conceptual framework for composing manipulations is described. Manipulation is abstracted into three categories: the free, the restrictive and the boundary regions. The impetus method is introduced, which supports the boundary region and properly detects which region the current situation of an object belongs to. The manipulations are composed based on this type of capability. Next, the representative spherical plane method (RSPM) is proposed and realized for the restrictive region. This model can generate the behavior of an object manipulated with two or three fingertips. As a result of the successful integration of these models, a smooth sequence of different manipulations was realized. Experimental results support the advantage of this method.},
	author = {Kijima, R. and Hirose, M.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490528},
	keywords = {Virtual reality},
	month = {March},
	pages = {195-202},
	title = {Representative spherical plane method and composition of object manipulation methods},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490528}}

@inproceedings{490529,
	abstract = {There is a natural desire to make multi-user virtual environments large in spatial extent, in numbers of objects, and in numbers of users interacting with the environment. However, doing this brings up several problems: efficiently managing the flow of large amounts of data between large numbers of users, representing precise position and velocity information about objects that are arrayed across a large volume of space, and allowing designers to create parts of a virtual environment separately and combine them together later. Locales are an efficient method for solving these problems by breaking up a virtual world into compact chunks that can be described and communicated independently. In addition, locales can be used to support a number of special effects that allow virtual worlds to easily transcend reality. While having many benefits, locales introduce an additional problem: finding something when you do not know what locale it is in. This is solved by the companion concept of beacons, which makes it possible to find something no matter where it is.},
	author = {Barrus, J.W. and Waters, R.C. and Anderson, D.B.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490529},
	keywords = {Virtual environment;Cities and towns;Environmental management;Communication channels;Geometry;Shape;Spline},
	month = {March},
	pages = {204-213},
	title = {Locales and beacons: efficient and precise support for large multi-user virtual environments},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490529}}

@inproceedings{490530,
	abstract = {Virtual environment (VE) applications involve many different tasks, including interfacing with input and output devices, providing responsive user interaction, and simulating a dynamic environment. The variety and number of tasks lends the application to a distributed computing system, where different tasks are performed by different computing resources. A critical issue that arises from such a design is how information is communicated between tasks. In particular, for virtual environments, how information is communicated promptly is the critical issue. In this work, we describe a pattern of communication common between VE tasks which is not addressed by other communication protocols, namely the communication of state information that continuously changes. We describe a new protocol based on an updatable queue abstraction which allows obsolete state information to be discarded, and compare a prototype implementation of that abstraction with a standard communication protocol.},
	author = {Kessler, G.D. and Hodges, L.F.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490530},
	keywords = {Protocols;Virtual environment;Virtual reality},
	month = {March},
	pages = {214-221},
	title = {A network communication protocol for distributed virtual environment systems},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490530}}

@inproceedings{490531,
	abstract = {Investigates the trade-off of different network topologies and messaging protocols for multi-user virtual environment systems. We present message distribution techniques appropriate for constructing scalable multi-user systems for a variety of network characteristics. Hierarchical system designs utilizing servers that manage message distribution for entities in separate regions of a virtual environment are described that scale to arbitrary numbers of simultaneous users. Experimental results show that the rate of messages processed by server workstations in this system design are less than using previously described approaches.},
	author = {Funkhouser, T.A.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490531},
	keywords = {Network topology;Virtual reality},
	month = {March},
	pages = {222-228},
	title = {Network topologies for scalable multi-user virtual environments},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490531}}

@inproceedings{490532,
	abstract = {This paper describes an experimental exploratory information visualization system under development at PNNL for integrating multimedia data and visualizing the contents of large multimedia databases. Our system prototype, named Starlight, represents multimedia corpora graphically as collections of icons in 3-space. Each icon in the information display represents an individual element of a multimedia database. The proximity of any two icons in the display is an indication of their general similarity, providing fast access to thematically related information. We extend the effectiveness of this approach by coupling it with a linkage display system to enable the simultaneous visualization of database structure and contents at multiple levels of abstraction. The utility of the system is further extended by enabling a variety of graphical and text-based interactions with the information display. The system enables the interactive generation of multiple simultaneous "views" into multimedia databases, providing intelligence analysts with the potential to process large multimedia data volumes quickly and effectively.},
	author = {Risch, J. and May, R. and Thomas, J. and Dowson, S.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490532},
	keywords = {Data visualization;Data analysis;Information analysis;Multimedia databases;Displays;Deductive databases;Multimedia systems;Laboratories;Prototypes;Couplings},
	month = {March},
	pages = {230-238},
	title = {Interactive information visualization for exploratory intelligence data analysis},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490532}}

@inproceedings{490533,
	abstract = {Annotation is a key operation for developing understanding of complex data or extended spaces. We have developed a flexible set of annotation tools that can be placed in a variety of applications. These tools offer a full set of capabilities for inserting, iconizing, playing back and organizing annotations in a virtual space. They also have an intuitive and easy-to-use interface for employing these capabilities while immersed in the virtual environment. We illustrate the annotation system with two diverse examples: a general data visualization/analysis application and an architectural walkthrough.},
	author = {Harmon, R. and Patterson, W. and Ribarsky, W. and Bolter, J.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490533},
	keywords = {Data visualization;Organizing;Information analysis;Space technology;Virtual environment;Data analysis;Design engineering;Buildings;Process design;Navigation},
	month = {March},
	pages = {239-245},
	title = {The virtual annotation system},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490533}}

@inproceedings{490534,
	abstract = {Three virtual worlds have been built to investigate the effect of immersive, multisensory computer-generated experiences on learning topics in science. Currently targeted at high school and beginning college students, these worlds address Newtonian mechanics, electrostatics, and molecular structure and dynamics. Data has been collected on usability and learning through questionnaires, pre- and post-tests, in situ prediction and experiment and post-session interviews. The results are not uniformly conclusive but suggest that students can improve their mastery of abstract concepts through the use of virtual environments that have been designed for learning. Moreover, usability studies have identified many significant problems that have been addressed in successive refinements of these worlds. Future work will include collaborative learning studies (both local and distant), use of intelligent agents, and comparison with two-dimensional microworlds.},
	author = {Dede, C. and Salzman, M.C. and Bowen Loftin, R.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490534},
	keywords = {Educational institutions;Graphics;Virtual reality;Displays;Electrostatics;Usability;Collaborative work;Kinematics;Hardware;Workstations},
	month = {March},
	pages = {246-252},
	title = {ScienceSpace: virtual realities for learning complex and abstract scientific concepts},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490534}}

@inproceedings{490535,
	abstract = {We present an approach to applying virtual reality in architectural design and collaborative visualization which emphasizes the use of multiple perspectives. These perspectives, including multiple mental models as well as multiple visual viewpoints, allow virtual reality to be applied in the earlier, more creative, phases of the design process, rather than just as a walkthrough of the final design. CALVIN, a prototype interface which implements these ideas, has been created using the CAVE virtual reality theatre.},
	author = {Leigh, J. and Johnson, A.E. and Vasilakis, C.A. and DeFanti, T.A.},
	booktitle = {Proceedings of the IEEE 1996 Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:50 -0400},
	date-modified = {2024-03-18 02:27:50 -0400},
	doi = {10.1109/VRAIS.1996.490535},
	keywords = {Collaboration;Intelligent networks;Virtual reality;Visualization;Process design;Design automation;Buildings;Virtual environment;Displays;Design for experiments},
	month = {March},
	pages = {253-260},
	title = {Multi-perspective collaborative design in persistent networked virtual environments},
	year = {1996},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1996.490535}}

@inproceedings{380735,
	abstract = {A newly developed 3-D shape modeling system based on constructive solid geometry (CSG) is described, in which a direct and interactive shape modeling process is supported. This feature is realized by a voxel model for real-time set operations, and a volume scanning display for the direct visualization of the voxel model, besides the CSG model for data structure management. The set operations on the voxel model can be executed as simple bit calculations, and therefore, the voxel data for the set object can be created in real time. The volume scanning display is a suitable device for presenting the voxel data. There is not need for intensive calculations to display the set objects. The volume scanning display can provide exact 3-D images without goggles or glasses. The concept should be useful in an actual modeling environment.<>},
	author = {Kameyama, K.-I. and Ohtomi, K.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380735},
	keywords = {Power system modeling;Real time systems;Solid modeling;Data structures;Three dimensional displays;Shape control;Deformable models;Research and development;Geometry;Data visualization},
	month = {Sep.},
	pages = {519-524},
	title = {A direct 3-D shape modeling system},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380735}}

@inproceedings{380736,
	abstract = {Aristotle's physics provides a coherent set of physical laws which predict physical events less well than Newton's, but fit human expectations better. Embedded in a virtual reality, they thus provide a system which the user can more quickly learn to control, being more intuitive than Newton's laws and more consistent than ad hoc movement rules. Being simpler and cheaper to implement, they also give smoother behaviour (for given hardware investment) than the usual classical mechanics.<>},
	author = {Poston, T. and Fairchild, K.M.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380736},
	keywords = {Physics;Virtual reality;Friction;Hardware;Environmental economics;Aircraft navigation;Force;Humans;Control systems;Investments},
	month = {Sep.},
	pages = {512-518},
	title = {Virtual Aristotelean physics},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380736}}

@inproceedings{380737,
	abstract = {The algorithms which enable the dynamic deformation of a virtual object are investigated. To realize natural deformation observed in the real world, it is necessary to incorporate various physical constraints into the algorithms. But it is difficult to realize such deformation in real-time. Thus virtual objects are deformed by not moving points but generating and/or vanishing points in an object, by inspiring the task of carving an image in wood. By using this method, it is possible to generate reasonable interactive force sensation. As a rudimentary state of this research, the proposed method is carried out by simple experiments and simulations.<>},
	author = {Yamamoto, K. and Ishiguro, A. and Uchikawa, Y.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380737},
	keywords = {Heuristic algorithms;Deformable models;Virtual reality;Space technology;Product design;Product development;Force feedback;Shape control;Solids;Aerospace industry},
	month = {Sep.},
	pages = {505-511},
	title = {A development of dynamic deforming algorithms for 3D shape modeling with generation of interactive force sensation},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380737}}

@inproceedings{380738,
	abstract = {A new free form deforming interface named direct deformation method (DDM) is presented. DDM allows designers to touch and deform free formed surfaces directly, at any point of the surfaces, even if they are represented only by control points and knot vectors. Users do not need to know such parameters that represent surfaces. The deforming actions of a user are used to calculate new parameters that define the deformed shape of the object. The shape is then reconstructed using the new parameters, allowing the user to visualize his or her deformation. This technique is easy to apply and useful for various form representations.<>},
	author = {Yamashita, J. and Fukui, Y.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380738},
	keywords = {Virtual reality;Distributed decision making;Shape;Deformable models;Geometry;Surface reconstruction;Design automation;Two dimensional displays;Weight control;Elasticity},
	month = {Sep.},
	pages = {499-504},
	title = {A direct deformation method},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380738}}

@inproceedings{380739,
	abstract = {A method to assist an operator to place an object on a surface in a virtual environment without any force feed-back tool is described and evaluated. The method used is to apply an attractive power to two faces that are likely to be attached. The system checks the distance of these faces and assists the operator by attaching two faces whose distance is less than a certain threshold. The real time calculation is achieved by limiting the number of faces that can be attached. Using this interface, the operator can move an object in some predefined planes. An experiment shows that the method is effective when the operator is requested to precisely place a virtual object in a certain location.<>},
	author = {Chanezon, A. and Takemura, H. and Kitamura, Y. and Kishino, F.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380739},
	keywords = {Virtual environment;Computer graphics;Shape measurement;Face detection;Joining processes;Application software;Position measurement;Object detection;Computational modeling;Assembly},
	month = {Sep.},
	pages = {492-498},
	title = {A study of an operator assistant for virtual space},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380739}}

@inproceedings{380740,
	abstract = {Basic research to a virtual face-to-face communication environment between an operator and a machine is presented. In this system, a human natural face appears on the display of machine and can talk to a operator with natural voice and natural face expressions. A face expression synthesis scheme driven by natural voice is presented. Voice includes not only linguistic information but also emotional features. An expression control scheme driven by both features is proposed. A human head with a 3-D wire frame model is expressed. The surface model is generated by texture mapping with 2-D real image. All the motions and expressions are synthesized and controlled automatically by the movement of some feature points on the model.<>},
	author = {Morishima, S. and Harashima, H.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380740},
	keywords = {Facial animation;Face;Humans;Wire;Magnetic heads;Man machine systems;Multimedia communication;Network synthesis;Deformable models;Control system synthesis},
	month = {Sep.},
	pages = {486-491},
	title = {Facial expression synthesis based on natural voice for virtual face-to-face communication with machine},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380740}}

@inproceedings{380741,
	abstract = {Neural networks are used to generate facial animation models for use in virtual reality telecommuting systems. 2-D face silhouettes are used to train and test multilayer perceptrons with backpropagation learning. This approach overcomes the problems encountered with the integration of face sensing devices and visual displays.<>},
	author = {Caudell, T.P. and Janin, A.L. and Johnson, S.K.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380741},
	keywords = {Facial animation;Teleworking;Virtual reality;Telephony;Graphics;Face;Video compression;Displays;Telecommunication computing;Telecommunication control},
	month = {Sep.},
	pages = {478-485},
	title = {Neural modeling of face animation for telecommuting in virtual reality},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380741}}

@inproceedings{380742,
	abstract = {Fuzzy cognitive maps (FCMs) can structure virtual worlds. FCMs link causal events, values, goals, and trends in a fuzzy feedback dynamical system. They direct actors in virtual worlds as the actors react to events and to one another. In nested FCMs each causal concept can control its own FCM. This combines levels of fuzzy systems that can choose goals or move objects. Adaptive FCMs change as causal patterns change. They adapt with differential Hebbian learning. FCMs are applied to an undersea virtual world of dolphins.<>},
	author = {Dickerson, J.A. and Kosko, B.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380742},
	keywords = {Fuzzy cognitive maps;Dolphins;Nonlinear dynamical systems;Neurofeedback;Fuzzy sets;Signal processing;Image processing;Feedback;Mood;Visualization},
	month = {Sep.},
	pages = {471-477},
	title = {Virtual worlds as fuzzy cognitive maps},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380742}}

@inproceedings{380743,
	abstract = {The MR Toolkit Peer Package is an extension to the MR Toolkit that allows multiple independent MR Toolkit applications to communicate with one another across the Internet. The master process of an MR Toolkit application can transmit device data to other remote applications, and receive device data from remote applications. Application-specific data can also be shared between independent applications. Nominally, any number of peers may communicate together in order to run a multiprocessing application, and peers can join or leave the collaborative application at any time. The Peer Package is introduced and the theory of its operation is explained. The authors' experience with a demonstration program which they have written, called multi-player handball, that uses the Peer Package is discussed.<>},
	author = {Shaw, C. and Green, M.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380743},
	keywords = {Packaging;Virtual environment;Displays;Application software;Collaborative work;Real time systems;Military aircraft;Peer to peer computing;Collaboration;Virtual reality},
	month = {Sep.},
	pages = {463-469},
	title = {The MR Toolkit Peers Package and experiment},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380743}}

@inproceedings{380744,
	abstract = {A class of dynamic systems consisting of constrained planar rigid bodies is considered. This class of environments poses two significant challenges to the development of advanced kinesthetic interfaces. First, the environment structure changes dynamically due to collisions between bodies. Thus, the equations of motion must be formulated and solved in real-time. Second, the equations are necessarily stiff due to rigid constraints in the system. Thus, real-time simulation requires extremely high update rates to be stable and present the operator with a realistic "feel".<>},
	author = {Stanley, M.C. and Colgate, J.E.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380744},
	keywords = {Real time systems;Equations;Springs;Virtual environment;Computational modeling;Acceleration;Object detection;Libraries;Numerical stability;Mechanical engineering},
	month = {Sep.},
	pages = {456-462},
	title = {Real time simulation of stiff dynamic systems via distributed memory parallel processors},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380744}}

@inproceedings{380745,
	abstract = {Many tools and techniques have been developed to address specific aspects of interacting in a virtual world. Few have been designed with an architecture that allows large numbers of entities from disparate organizations to interact in such a world, in real time, and over large geographic distances. A system architecture that does this is described. The key technologies that have made these virtual worlds possible are discussed, and it is explained and how the technologies fit into the architecture. A sample implementation of this architecture, the SIMulation NETworking (SIMNET) system, is then presented, along with various design decisions and the reasoning behind them.<>},
	author = {Calvin, J. and Dickens, A. and Gaines, B. and Metzger, P. and Miller, D. and Owen, D.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380745},
	keywords = {Computer architecture;Paper technology;Humans;Computational modeling;Vehicle dynamics;Vehicles;Testing;Virtual reality;Government;Image generation},
	month = {Sep.},
	pages = {450-455},
	title = {The SIMNET virtual world architecture},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380745}}

@inproceedings{380746,
	abstract = {Different hardware platforms are best suited for different tasks in simulating a virtual world. Any distributed virtual world must be prepared to support communication among a large and heterogeneous set of software and hardware devices. By developing a scalable environment for virtual worlds based on heterogeneous platforms, researchers can utilize existing hardware, and so can begin to do research without a large capital outlay. For these reasons, it is imperative to explore the architectural constraints placed on a virtual world by distribution and parallelism. The author examines what it means to distribute functionality such as simulation, interaction detection and messaging in a virtual world, how to "scale up" such a world, and how to deal with communication delays. The WAVES (WAterloo Virtual Environment System) architecture attempts to address these concerns.<>},
	author = {Kazman, R.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380746},
	keywords = {Hardware;Aerospace simulation;Virtual environment;Computer networks;Computational modeling;Computer architecture;Computer science;Supercomputers;Buildings;Central Processing Unit},
	month = {Sep.},
	pages = {443-449},
	title = {Making WAVES: On the design of architectures for low-end distributed virtual environments},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380746}}

@inproceedings{380747,
	abstract = {A method for training a feedforward neural network to be fault tolerant against weight perturbations is described. The measure for fault tolerance is the deviation of the network's output after training, when each interconnection weight is perturbed, from that output without perturbation. In this method, an attempt is made to keep that deviation as low as possible. This measure is used because it can represent that kinds of error which arises when neural networks are implemented in hardware.<>},
	author = {Elsimary, H. and Mashali, S. and Shaheen, S.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380747},
	keywords = {Feeds;Neural networks;Feedforward neural networks;Fault tolerance;Artificial neural networks;Fault tolerant systems;Predictive models;Backpropagation algorithms;Neural network hardware;Computer networks},
	month = {Sep.},
	pages = {436-441},
	title = {A method for training feed forward neural network to be fault tolerant},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380747}}

@inproceedings{380748,
	abstract = {The primary focus of the Virtual Sailor project is the creation of virtual actors, for training and other applications where human figures must be coordinated. By the term virtual actor is meant computer animated figures that can mimic human beings in physical form, function and behavior. The underlying techniques and software systems for creating the skin, bone, and muscles of aspects of interactive human body models are discussed. The approach combines a kinematic description of the human skeletal system with a finite element model of the skin and soft tissue of the face. A set of numerical techniques is presented that allows implementation of these models efficiently so that they can be used in a real-time simulation environment. To animate these figures, a set of control systems to mimic basic human behaviors is being formulated.<>},
	author = {Chen, D.T. and Pieper, S.D. and Singh, S.K. and Rosen, J.M. and Zeltzer, D.L.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380748},
	keywords = {Humans;Biological system modeling;Animation;Skin;Application software;Physics computing;Software systems;Bones;Muscles;Kinematics},
	month = {Sep.},
	pages = {429-435},
	title = {The virtual sailor: An implementation of interactive human body modeling},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380748}}

@inproceedings{380749,
	abstract = {An extensive simulator has been developed to simulate the dynamics of an intelligent system. The simulation program is written in C and it runs on IBM RISC/6000 workstations in a UNIX environment. A graphical user interface using NGI (Northstar Graphics Interface) has been built and all the results are displayed graphically. The authors show the result obtained in trying to avoid several planes, the implementation result for pole balancing behavior, and the simulation result for hopping motion, when the maximum allowable acceleration during a hopping cycle is limited.<>},
	author = {Singh, S.K. and Kumar, A. and Shi, L.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380749},
	keywords = {Animation;Optimal control;Nonlinear dynamical systems;Differential equations;Robot kinematics;Educational institutions;Acceleration;Automatic generation control;Computational modeling;Oscillators},
	month = {Sep.},
	pages = {422-428},
	title = {Generating autonomous dynamic behavior for computer animation: A constrained optimal control approach},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380749}}

@inproceedings{380750,
	abstract = {Virtual reality technology aims at the expansion of the communication bandwidth by providing users with 3D immersive environments. For the true direct manipulation of the environments, fast collision detection must be provided to increase the sense of reality. A collision detection scheme for virtual reality applications is proposed. The method exploits a hierarchical object representation to facilitate the detection of colliding segments.<>},
	author = {Youn, J.-H. and Wohn, K.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380750},
	keywords = {Virtual reality;Object detection;Application software;Bandwidth;Humans;Artificial intelligence;Computer science;Computer graphics;Robot sensing systems;Data structures},
	month = {Sep.},
	pages = {415-421},
	title = {Realtime collision detection for virtual reality applications},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380750}}

@inproceedings{380751,
	abstract = {Real-time reproduction of a 3D human image is realized by an experimental system built for the realization of virtual space teleconferencing, in which participants at different sites can feel as if they are at one site and can work cooperatively. In the teleconferencing system, a 3D model of a participant is constructed by a wire-frame model mapped by color texture and is displayed on 3D screen at the receiving site. In the experimental system, to realize real-time detection of facial features at the sending site, tape marks are attached to facial muscles, and the marks are tracked visually. To detect movements of the head, body, hands and fingers in real-time, magnetic sensors and data glove are used. When the movements of the participant are reproduced at the receiving site, the detected results are used to drive the nodes in the wire frame model. Using the experimental system, the optimum number of nodes for real-time reproduction is obtained. Results for real-time cooperative work using the experimental system are demonstrated.<>},
	author = {Ohya, J. and Kitamura, Y. and Takemura, H. and Kishino, F. and Terashima, N.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380751},
	keywords = {Humans;Real time systems;Teleconferencing;Biological system modeling;Face detection;Facial features;Facial muscles;Magnetic heads;Fingers;Magnetic sensors},
	month = {Sep.},
	pages = {408-414},
	title = {Real-time reproduction of 3D human images in virtual space teleconferencing},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380751}}

@inproceedings{380752,
	abstract = {The design and operation of the Virtual Reality Distributed Environment and Construction Kit (VR-DECK) toolkit developed at IBM Research is reviewed. It provides a designer with a development environment while supporting distributed computing, multi-user capability, and a variety of I/O devices. Virtual worlds are built as collections of modules which communicate via events. Extensive run-time support in the form of extensive C++ class libraries insulates the application designer from the low-level system details such as networking, inter-module data transport, event queuing and matching, and I/O device communication. A library of pre-defined modules is provided for commonly used functions and devices. An X Window System graphical user interface is provided for aggregating modules into applications. The system enables a developer to focus on the design of the application rather than on systems and integration issues.<>},
	author = {Codella, C.F. and Jalili, R. and Koved, L. and Lewis, J.B.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380752},
	keywords = {Graphics;Distributed processing;Buildings;Speech recognition;Workstations;Libraries;Virtual reality;Sensor systems;Acoustic sensors;Displays},
	month = {Sep.},
	pages = {401-407},
	title = {A toolkit for developing multi-user, distributed virtual environments},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380752}}

@inproceedings{380753,
	abstract = {The Distributed Interactive Virtual Environment (DIVE) is a heterogeneous distributed virtual reality system based on UNIX and Internet networking protocols. Each participating process has a copy of a replicated database and changes are propagated to the other processes with reliable multicast protocols. DIVE provides a dynamic virtual environment where applications and users can enter and leave the environment on demand. Several user-related abstractions have been introduced to ease the task of application and user interface construction.<>},
	author = {Carlsson, C. and Hagsand, O.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380753},
	keywords = {Virtual reality;Virtual environment;Application software;Computer science;User interfaces;Collaboration;Navigation;IP networks;Databases;Multicast protocols},
	month = {Sep.},
	pages = {394-400},
	title = {DIVE A multi-user virtual reality system},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380753}}

@inproceedings{380754,
	abstract = {The Virtual Panel Architecture (VPA) has been designed to help implement an intermediate abstraction with elements of both a physical control panel and a computer-based control panel. This abstraction, handling 3D point-based gesticulative interaction with modeled object hierarchies, is suitable for both virtual reality environments and traditional environments. Based on the VPA, an environment of virtual panels is demonstrated which can be incorporated into keyboard-and-mouse-based computing. Users are able to turn virtual knobs, adjust virtual sliders, and point at virtual screens on these virtual panels in this environment.<>},
	author = {Su, S.A. and Furuta, R.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380754},
	keywords = {Physics computing;Computer architecture;Computer applications;Virtual reality;Computer science;Computer displays;Information filtering;Information filters;Educational institutions;Switches},
	month = {Sep.},
	pages = {387-393},
	title = {The virtual panel architecture: A 3D gesture framework},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380754}}

@inproceedings{380755,
	abstract = {Computing average rotations is a fundamental sensor fusion problem. In virtual reality, the problem naturally arises in connection with position sensing. Two methods are derived for additive averaging of rotations. One of these methods, based on quarternions, is limited to averages of three-dimensional rotations, but the other method, based on singular value decomposition, works in any Euclidean space.<>},
	author = {Curtis, W.D. and Janin, A.L. and Zikan, K.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380755},
	keywords = {Quaternions;Singular value decomposition;Matrix decomposition;Virtual reality;Space technology;Image sensors;Jitter},
	month = {Sep.},
	pages = {377-385},
	title = {A note on averaging rotations},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380755}}

@inproceedings{380756,
	abstract = {In time sequential stereoscopic displays a major contributor to ghost image intensity, as quantified by the system leakage ratio, is due to the persistence of the CRT's phosphor. Two approaches to solving this problem are presented. One is to use a system that has a sectioned electrooptical device mounted directly in front of the CRT. The second is to use phosphors that have less persistence.<>},
	author = {Bos, P.J.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380756},
	keywords = {Glass;Phosphors;Cathode ray tubes;Displays;Electrooptic devices;Switches;Liquid crystal devices;Surface treatment;Image segmentation;Earth},
	month = {Sep.},
	pages = {371-376},
	title = {Performance limits of stereoscopic viewing systems using active and passive glasses},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380756}}

@inproceedings{380757,
	abstract = {Color displays used in binocular virtual reality are generally LCD arrays, with the maximum resolution determined by how small the individual color pixel elements can be made and driven electronically. A high resolution color display using a monochrome CRT and a liquid crystal shutter operated in a field-sequential color mode is described. The Tektronix 1-in frame sequential color monitor system significantly improves the resolution of color displays for virtual reality head-mounted displays to at least 640/spl times/480. The requirements of a field sequential full color monitor are discussed, as is the video drive for this device.<>},
	author = {Allen, D.W.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380757},
	keywords = {Optical polarization;Filters;Liquid crystal displays;Computer displays;Wheels;Virtual reality;Cathode ray tubes;Color;Liquid crystal devices;Head},
	month = {Sep.},
	pages = {364-370},
	title = {A 1" high resolution field sequential display for head mounted applications},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380757}}

@inproceedings{380758,
	abstract = {Screen space rendering statistics are gathered from 150 3D objects, each modeled by between 2-K and 40-K triangles. While there is wide variance by individual object, the overall trend is that the distribution of triangles by screen size is roughly exponential in the direction of small triangles. From a subjective esthetics point of view, tessellations require 10-K visible triangles per quarter million pixels covered for acceptable results.<>},
	author = {Deering, M.F.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380758},
	keywords = {Virtual reality;Spline;Surface topography;Surface reconstruction;Space technology;Computer graphics;Rendering (computer graphics);Humans;Hardware;Shape},
	month = {Sep.},
	pages = {357-363},
	title = {Data complexity for virtual reality: where do all the triangles go?},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380758}}

@inproceedings{380759,
	abstract = {A reconfigurable heterogeneous multiprocessor architecture, called Shiva is introduced, and a configuration of it for generating stereoscopic images in parallel is described. The main feature of Shiva is its ability to incorporate different types of processors. This allows a computational task to be distributed over the various processors in a way that best users their combined capabilities. Some results of this system are presented.<>},
	author = {Nelson, M. and Cavaiuolo, M. and Yakovleff, A.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380759},
	keywords = {Visualization;Workstations;Computer architecture;Microprocessors;Image generation;Distributed computing;Graphics;Testing;Hardware;Pipelines},
	month = {Sep.},
	pages = {349-355},
	title = {A heterogeneous architecture for stereoscopic visualisation},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380759}}

@inproceedings{380760,
	abstract = {The virtual reality (VR) user interface style allows the user to manipulate virtual objects in a 3D environment. A software architecture which guides users in virtual worlds is introduced. The architecture is based on the concept of task-oriented agents which support users in acting within a virtual environment (VE). These agents use an application independent toolbox, the hyper-renderer, to retrieve information that is calculated during the rendering process. A virtual building is used to implement various prototypes in order to evaluate this approach.<>},
	author = {Emhardt, J. and Semmler, J. and Strothotte, T.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380760},
	keywords = {Virtual reality;Navigation;Buildings;Animation;User interfaces;Computer architecture;Virtual environment;Information retrieval;Legged locomotion;Computer science},
	month = {Sep.},
	pages = {342-348},
	title = {Hyper-navigation in virtual buildings},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380760}}

@inproceedings{380761,
	abstract = {A new conceptual solution is presented for the problem of providing force feedback for virtual reality, concentrating on potential CAD/CAM applications. The essential concept is that force feedback is provided by interactions between the human operator and specialized external (as opposed to worn) robots. This is called "robotic graphics" to express the analogy between robots simulating the feel of an object, and graphics displays simulating its appearance. This is illustrated by introducing the derivative concepts of "robotic shape displays" and "roboxels.".<>},
	author = {McNeely, W.A.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380761},
	keywords = {Robots;Graphics;Force feedback;Virtual reality;Computer aided manufacturing;CADCAM;Application software;Exoskeletons;Humans;Plugs},
	month = {Sep.},
	pages = {336-341},
	title = {Robotic graphics: a new approach to force feedback for virtual reality},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380761}}

@inproceedings{380762,
	abstract = {The construction of the virtual work space for pick-and-place tasks with a new 3D interface device named SPIDAR II is discussed. The device can measure the motions of the thumb and the forefinger, and can provide the force sensations to the thumb and the forefinger. The operator can manipulate the virtual objects directly in the virtual work space using the device. The pick-and-place tasks are performed in the virtual space. The effects of the force sensations which are provided by the device are estimated. The results indicate that the appropriate forces are important for the pick-and-place task. The virtual block gives the best performance of pick-and-place tasks in virtual work space.<>},
	author = {Ishii, M. and Sato, M.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380762},
	keywords = {Force feedback;Fingers;Thumb;Space technology;Pulleys;Length measurement;Motion measurement;Wheels;Wounds;Laboratories},
	month = {Sep.},
	pages = {331-335},
	title = {A 3D interface device with force feedback: a virtual work space for pick-and-place tasks},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380762}}

@inproceedings{380763,
	abstract = {The SIMNET program, the first large scale application of distributed simulation to tactical team training, is briefly described. Its success has fostered interest in distributed simulation technology and has provided a foundation for the requirements of the next generation of visual systems. The most challenging requirements for such systems specific to the needs of ground based tactical training environments are presented. An architectural approach to the design of an image generator (IG) satisfying these requirements is discussed. This includes a high level system diagram and the architectural concepts which provide solutions to high level occulting, localization of image complexity due to many moving models, dynamic allocation of processing resources, independent scalability of polygon and pixel processing, the need for standard interfaces, and simulation of special environmental effects such as tactical smoke. Architectural efficiencies and performance levels obtained with these methods are quantified.<>},
	author = {Soderberg, B. and Miller, D.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380763},
	keywords = {Image generation;Visual system;Costs;Large-scale systems;Military aircraft;Industrial training;Vehicle dynamics;Resource management;Scalability;Pixel},
	month = {Sep.},
	pages = {318-329},
	title = {Image generation design for ground-based network training environment},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380763}}

@inproceedings{380764,
	abstract = {Image generation (IG) computers for networked simulation and training systems require additional capabilities beyond those IG's employs in traditional forms of realtime simulation. The specific characteristics associated with the networking and tactical training nature of ground and near-ground vehicle training applications are reviewed. The IG implications discussed include computational loading, scene management, advanced graphics techniques, databases, interoperability, interfaces, and required support calculations. It is recommended that an industry standard benchmark to specify, evaluate, and verify the performance of this type of image generator is established.<>},
	author = {Bess, R.D.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380764},
	keywords = {Image generation;Management training;Computational modeling;Computer simulation;Industrial training;Computer networks;Land vehicles;Road vehicles;Application software;Computer interfaces},
	month = {Sep.},
	pages = {308-317},
	title = {Image generation implications for networked tactical training systems},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380764}}

@inproceedings{380765,
	abstract = {The authors investigate whether a high-resolution head-mounted display, roving around a much larger frame buffer image, can give a user the impression of viewing a single very large display screen. A prototype is constructed, consisting of an 1120 /spl times/ 900 pixel head-mounted display, an ultrasonic head-tracker, a 16,386 /spl times/ 6,144 pixel frame buffer, and suitable X-window control software, as a means of studying this question. Applications can write to the large frame buffer using the window system, and the view can navigate around the image rapidly using head rotations. The prototype system, although somewhat awkward to use due to a limited field of view in the head-mounted display, shows that head rotation is a fast, convenient way to switch display contexts.<>},
	author = {Reichlen, B.A.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380765},
	keywords = {Large screen displays;Computer displays;Navigation;Head;Software prototyping;Prototypes;Switches;Sun;Laboratories;Drives},
	month = {Sep.},
	pages = {300-307},
	title = {Sparcchair: A one hundred million pixel display},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380765}}

@inproceedings{380766,
	abstract = {A technique for redesigning a display controller within a traditional graphics display system is outlined. By performing complicated computations within the display controller, the system may be optimized for the needs of a virtual reality display system. Wide angle viewing lens effects and response to user head rotations may be compensated for within the display controller, avoiding the rendering bottleneck. The architecture allows image overlaying to provide more realistic scenes and a mechanism for focusing the time of the rendering engine on sections of the scene which change the most frequently.<>},
	author = {Regan, M. and Pose, R.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380766},
	keywords = {Graphics;Computer architecture;Control systems;Rendering (computer graphics);Layout;Computer displays;Virtual reality;Lenses;Head;Focusing},
	month = {Sep.},
	pages = {293-299},
	title = {An interactive graphics display architecture},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380766}}

@inproceedings{380767,
	abstract = {The development of a pen-based force display and its application to the direct manipulation of a free-form surface are described. A six degree-of-freedom force reflective master manipulator which has a pen-shaped grip has been developed. The system uses two three degree-of-freedom manipulators. Both ends of the pen are connected to these manipulators. The hardware of the force display is small and light-weight. The performance of the force display is exemplified in interactive deformation of a free-form surface.<>},
	author = {Iwata, H.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380767},
	keywords = {Haptic interfaces;Virtual environment;Displays;Manipulators;Solid modeling;Force feedback;Torque;Hardware;Shape;Humans},
	month = {Sep.},
	pages = {287-292},
	title = {Pen-based haptic virtual environment},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380767}}

@inproceedings{380768,
	abstract = {A computer-based simulation utilizing high-resolution 3D graphics and a force-feedback device to train students and/or residents in limiter puncture is discussed. The simulator consists of a combination of specialized hardware, custom software running on a high-resolution graphics workstation. The hardware under development for this project is described. The major piece of specialized hardware is the force-feedback needle simulator. Position sensors track the insertion angle or trajectory of the device, while a programmable motor or actuator provides variable resistance to insertion, depending upon which simulated anatomic structures the virtual needle is penetrating. Thus, the student may place the virtual needle at the desired angle and will feel realistic resistance while inserting. If an incorrect trajectory is used and bone is contacted, the resulting sudden sharp increase in resistance would be conveyed.<>},
	author = {Bostrom, M. and Singh, S.K. and Wiley, C.W.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380768},
	keywords = {Computational modeling;Hardware;Needles;Computer graphics;Computer simulation;Workstations;Trajectory;Actuators;Bones;Contact resistance},
	month = {Sep.},
	pages = {280-286},
	title = {Design of an interactive lumbar puncture simulator with tactile feedback},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380768}}

@inproceedings{380769,
	abstract = {For part I see ibid., p.263-70 (1994). The existing touch display technologies in the literature are surveyed. This survey indicates five main approaches to touch feedback, involving visual, pneumatic, vibro-tactile, electro-tactile and neuromuscular stimulations. A pneumatics approach could use air jets, air pockets or inflatable bladders to provide touch feedback cues to the operator. Similarly the vibro-tactile approach could use vibrating pins, voice coils, or piezoelectric crystals to provide tickling sensation to the human operator's skin to signal the touch. The electro-tactile stimulation method can provide electric pulses of appropriate width and frequency to the skin, while the neuromuscular stimulation approach provides the signals directly to the primary cortex of the operator's brain. With regard to this, 17 devices, most of which were built for sensory substitution purposes, are examined and compared for their suitability as touch feedback devices for dexterous telemanipulation.<>},
	author = {Shimoga, K.B.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380769},
	keywords = {Telerobotics;Tactile system;Output feedback;Computer input/output},
	month = {Sep.},
	pages = {271-279},
	title = {A survey of perceptual feedback issues in dexterous telemanipulation. II. Finger touch feedback},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380769}}

@inproceedings{380770,
	abstract = {Some specific requirements on the design of dexterous master devices meant for teleoperating multifingered robotic hands, within the context of finger force feedback, are identified and analyzed. The requirements are of two categories, i.e., constructional and functional. The constructional issues consist of the isomorphism, portability, motion range capability, and accommodation for human hand size variability. The functional issues consist of bandwidth compatibility with the human hand (which itself has asymmetric input/output characteristics), proprioceptive (force limit) compatibility, and consideration of the psychometric stability of the human hand in sensing force magnitudes and variations. Also of importance is the sensitivity of the master device that must be more than that of the human hand. In this regard, 14 existing designs of hand masters are evaluated to see how well they satisfy the stated constructional and functional requirements.<>},
	author = {Shimoga, K.B.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380770},
	keywords = {Manipulator kinematics;Telerobotics;Force control;Output feedback;Computer input/output},
	month = {Sep.},
	pages = {263-270},
	title = {A survey of perceptual feedback issues in dexterous telemanipulation. I. Finger force feedback},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380770}}

@inproceedings{380771,
	abstract = {As a methodology to develop a force feedback system, the concept of "surface displaying" is introduced. A prototype mechanism and its control system is then designed and developed. Methodology for model calculation and simulation is discussed. The idea of texture mapping in a tactile sense is suggested, and is proven to be effective.<>},
	author = {Hirota, K. and Hirose, M.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380771},
	keywords = {Displays;Force feedback;Fingers;Shape;Prototypes;Virtual environment;Magnetic field measurement;Magnetic sensors;Virtual prototyping;Control system synthesis},
	month = {Sep.},
	pages = {256-262},
	title = {Development of surface display},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380771}}

@inproceedings{380772,
	abstract = {The authors have developed "augmented reality" technology, consisting of a see-through head-mounted display, a robust, accurate position/orientation sensor, and their supporting electronics and software. Their primary goal is to apply this technology to touch labor manufacturing processes, enabling a factory worker to view index markings or instructions as if they were painted on the surface of a workpiece. In order to accurately project graphics onto specific points of a workpiece, it is necessary to have the coordinates of the workpiece, the display's virtual screen, the position sensor, and the user's eyes in the same coordinate system. The linear transformation and projection of each point to be displayed from world coordinates to virtual screen coordinates are described, and the experimental procedures for determining the correct values of the calibration parameters are characterized.<>},
	author = {Janin, A.L. and Mizell, D.W. and Caudell, T.P.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380772},
	keywords = {Calibration;Displays;Augmented reality;Robustness;Manufacturing processes;Production facilities;Graphics;Sensor phenomena and characterization;Sensor systems;Eyes},
	month = {Sep.},
	pages = {246-255},
	title = {Calibration of head-mounted displays for augmented reality applications},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380772}}

@inproceedings{380773,
	abstract = {The optical design, fabrication and performance of a solid Schmidt prism used for relaying an optical signal or image to the eye while simultaneously providing the viewer with the capability to see-through to the outside world is described. It can be either of solid glass or plastic construction like a prism, or made of separate components. The novel aspect of this approach over previous designs is folding the system between the image and the Schmidt mirror as opposed to folding it at the image itself. This allows for a more compact design. The solid glass approach extends the apparent field of view to the viewer through refraction at the glass to air interface. If the spherical reflector of the Schmidt is rendered opaque, the viewer will only see the information provided by the optical stimulus. If partially reflective, the viewer can augment real world scenes with computerized symbology superimposed in the visual field. The stimulus may be either a high resolution CRT or a flat panel liquid crystal display (LCD).<>},
	author = {Manhart, P.K. and Malcolm, R.J. and Frazee, J.G.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380773},
	keywords = {Solids;Relays;Optical refraction;Glass;Optical design;Optical device fabrication;Plastics;Mirrors;Rendering (computer graphics);Layout},
	month = {Sep.},
	pages = {234-245},
	title = {"Augeye" a compact, solid Schmidt optical relay for helmet mounted displays},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380773}}

@inproceedings{380774,
	abstract = {A head mounted display (HMD) that is optically opaque with respect to the outer world can be provided with see-through capability by mounting video cameras to the outside of the helmet. Stereoscopic views of the physical space surrounding the wearer are captured by the video cameras, and are projected to the display screens inside the HMD. The conditions necessary for mounting the cameras to a pre-existing, non-see-through HMD are described, and the advantages and disadvantages of different mounting schemes are discussed, together with the design of the video cameras lenses to be used with this specific application.<>},
	author = {Edwards, E.K. and Rolland, J.P. and Keller, K.P.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380774},
	keywords = {Merging;Cameras;Computer displays;Lenses;Eyes;Layout;Computer graphics;Head;Optical design;Optical imaging},
	month = {Sep.},
	pages = {223-233},
	title = {Video see-through design for merging of real and virtual environments},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380774}}

@inproceedings{380775,
	abstract = {The design of a four degree-of-freedom, force-reflecting manipulandum for manual interaction with virtual environments is presented. The device emulates a handtool which the operator can use to explore and manipulator virtual objects. The performance of the device (its including to generate a broad range of impedances) is determined by a variety of factors, including the inherent dynamics of the manipulator, the accuracy and resolution of sensors, and the speed of the digital controller.<>},
	author = {Millman, P.A. and Stanley, M. and Colgate, J.E.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380775},
	keywords = {Haptic interfaces;Computational modeling;Impedance;Virtual environment;Manipulator dynamics;Humans;Springs;Digital control;Computer simulation;Force measurement},
	month = {Sep.},
	pages = {216-222},
	title = {Design of a high performance haptic interface to virtual environments},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380775}}

@inproceedings{380776,
	abstract = {A dynamic force simulator (DFS) for force feedback in human-machine systems is proposed. The DFS simulates object dynamics contact model and friction characteristics of the human hand interacting with objects in a virtual reality environment. After derivation of kinematic and force relations between hand and object space, a method is proposed for calculation and feedback of appropriate forces to the force controlled actuators of the sensor glove which has been developed.<>},
	author = {Hashimoto, H. and Kunii, Y. and Buss, M. and Harashima, F.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380776},
	keywords = {Force feedback;Man machine systems;Force sensors;Force control;Friction;Humans;Virtual reality;Kinematics;Actuators;Sensor phenomena and characterization},
	month = {Sep.},
	pages = {209-215},
	title = {Dynamic force simulator for force feedback human-machine interaction},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380776}}

@inproceedings{380777,
	abstract = {The performance of a haptic interface is often reported in terms of the dynamic range of impedances it may represent. At the low end, the range is typically limited by inherent dynamics of the interface device, such as inertia and friction. At the high end, the range is typically limited by system stability. In a number of the applications, the principal limitation has proven to be the achievable upper limit on impedance. Therefore, a benchmark problem of considerable importance is the implementation of a stiff "wall". Contacting a wall may be described as the reversible transition from a region of very low impedance to one of very high impedance. A theoretical analysis (supplemented with discussion of experimental and simulation results) of stiff wall implementation is presented. The main result is a criterion for the passivity of a virtual wall in terms of two nondimensional parameters.<>},
	author = {Colgate, J.E. and Grafing, P.E. and Stanley, M.C. and Schenkel, G.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380777},
	keywords = {Impedance;Haptic interfaces;Actuators;Telerobotics;Sensor systems;Hardware;Force sensors;Mechanical engineering;Humans;Jacobian matrices},
	month = {Sep.},
	pages = {202-208},
	title = {Implementation of stiff virtual walls in force-reflecting interfaces},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380777}}

@inproceedings{380778,
	abstract = {A system for haptic (i.e. kinesthetic and cutaneous) stimulation of the hand is described. While the immediate application involves display of speech information, a number of other man-machine interface applications may be feasible, including force-feedback devices for computer interaction and human pattern extraction from multiple datastreams. In an attempt to model more closely the information streams available via the Tadoma method, OMAR was developed to stimulate kinesthetic as well as tactile receptors, by moving and vibrating fingers in one or two dimensions using hard-disk head-positioning actuators. OMAR is being used in experiments involving basic haptic perception and supplementation of speechreading with haptic codings of speech correlates obtained via X-ray microbeam measurements.<>},
	author = {Eberhardt, S.P. and Bernstein, L.E. and Coulter, D.C. and Hunckler, L.A.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380778},
	keywords = {Deafness;Haptic interfaces;Speech;Computer displays;Application software;User interfaces;Computer interfaces;Humans;Data mining;Fingers},
	month = {Sep.},
	pages = {195-201},
	title = {OMAR a haptic display for speech perception by deaf and deaf-blind individuals},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380778}}

@inproceedings{380779,
	abstract = {A functional representation for head-related transfer functions (HRTFs) is desirable as it overcomes many of the limitations associated with use of measured HRTFs. It provides a continuous representation of auditory space, such that the synthesis of HRTF at any given spatial location can be performed by functional evaluation of the model. Such a model that establishes a mathematical representation of the external ear transformation characteristics based on spatial feature extraction and regularization is proposed.<>},
	author = {Chen, J. and Van Veen, B.D. and Hecox, K.E.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380779},
	keywords = {Feature extraction;Ear;Irrigation;Azimuth;Loudspeakers;Noise measurement;Transfer functions;Auditory displays;Headphones;Virtual environment},
	month = {Sep.},
	pages = {188-193},
	title = {Synthesis of 3D virtual auditory space via a spatial feature extraction and regularization model},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380779}}

@inproceedings{380780,
	abstract = {A new methodology of human interface through three-dimensional virtual space created in the computer by using the audio signal is introduced. The user can access three-dimensional virtual space by her/his vocal and auditory ability. As an input function, the system contains a multiple 3-D locator which locates several 3-D pointers in the virtual space, and, as an output function, a certain type of 3-D sound generator. By showing some of the interface system, the effectiveness in the sense of direct manipulation as a spatial interaction tool can be confirmed.<>},
	author = {Yonekura, T. and Ariyoshi, N. and Watanabe, Y.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380780},
	keywords = {Computer interfaces;Graphics;Three dimensional displays;Costs;Microphones;Signal generators;Information science;Humans;Multimedia systems;Data gloves},
	month = {Sep.},
	pages = {183-187},
	title = {ASPECT: audio spatial environment for communication As a three dimensional auditory interaction tool},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380780}}

@inproceedings{380781,
	abstract = {The addition of spatially localized sound to an existing graphics oriented synthetic environment (virtual reality system) is investigated. To build 3-D audio systems that are robust, listener-independent, real-time, multi-source, and able to give stable sound localization is beyond the current state-of-the-art. The "auralizer" or "aural renderer" described is built as a test-bed for experimenting with the known techniques for generating sound localization cues based on the geometrical models available in a synthetic 3-D world. The psychoacoustical background of sound localization is introduced, and the design and usage of the Distributed Interactive Virtual Environment (DIVE) auralizer is described. The system's implementation and performance are evaluated.<>},
	author = {Pope, S.T. and Fahlen, L.E.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380781},
	keywords = {Rendering (computer graphics);Graphics;Virtual reality;Audio systems;Robustness;Real time systems;Acoustic testing;Solid modeling;Psychoacoustic models;Psychology},
	month = {Sep.},
	pages = {176-182},
	title = {The use of 3-D audio in a synthetic environment: an aural renderer for a distributed virtual reality system},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380781}}

@inproceedings{380782,
	abstract = {The Student Chapter of the Association for Computing Machinery at the University of Illinois at Urbana-Champaign was developed the PowerGlove Serial Interface (PGSI) as the next step in low-cost virtual reality interface technology. The authors discuss initial design problems, provide a short history of the project, and give a basic description of the device. Applications and software are discussed.<>},
	author = {Gross, B. and Brain, J.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380782},
	keywords = {Costs;Virtual reality;Protocols;Connectors;Computer interfaces;Manufacturing;Central Processing Unit;Brain computer interfaces;Power engineering computing;Software performance},
	month = {Sep.},
	pages = {169-175},
	title = {Limbo champion in the low cost VR competition: the Power Glove Serial Interface},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380782}}

@inproceedings{380783,
	abstract = {The Space Perception System with a 3D man-computer interface device, has been built for human factors experiments. An algorithm using a distribution function is utilized to define model shapes in a virtual environment. In this algorithm, the system is able to recognize the interference between the 3D cursor following an operator's hand and virtual objects in real time. Impedance control of robotics is utilized to calculate the reaction force which is transmitted in real time from the virtual object to the operator's hand through the 3D man-computer interface device. With the combination of the algorithm using the distribution function and the impedance control, the operator can touch and trace the surface of the virtual model which is composed of free-form surfaces in a virtual environment, and can recognize its hardness.<>},
	author = {Adachi, Y.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380783},
	keywords = {Distribution functions;Force feedback;Shape;Virtual environment;Virtual reality;Impedance;Force control;Robot control;Interference;Density functional theory},
	month = {Sep.},
	pages = {162-168},
	title = {Touch and trace on the free-form surface of virtual object},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380783}}

@inproceedings{380784,
	abstract = {Presents the results of one of the first experiments in a research program aimed at systematically investigating manipulation schemes for spatial input, from a human factors point of view. A 3D design space model is proposed as a framework for such investigations, and four options within this model are tested in a six degree-of-freedom target acquisition task within a virtual environment. Experimental results indicate strong performance advantages for isometric sensing combined with rate control and for isotonic sensing combined with position control. A strong interaction between sensing mode and mapping function is found. The findings are discussed in relation to the literature on spatial manipulation.<>},
	author = {Zhai, S. and Milgram, P.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380784},
	keywords = {Taxonomy;Virtual environment;Telerobotics;Human factors;Position control;Virtual reality;Systematics;Transfer functions;Ergonomics;Control systems},
	month = {Sep.},
	pages = {155-161},
	title = {Human performance evaluation of manipulation schemes in virtual environments},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380784}}

@inproceedings{380785,
	abstract = {A phenomenon observed in speech and cursive handwriting is discussed, namely, the visual detection of a coarticulatory anticipation. The contribution of visible motor information in linguistic message decoding is examined. Visual dynamic and static digram and word stimuli prepared by reproducing identical spatial templates, and on the basis of the presence of coarticulatory anticipation in the handwriting movements are administered without providing spatial cues on the identity of the subsequent letter. Results indicate that when kinematic information is provided in the dynamic visual stimulus, subjects are able to predict with fair accuracy the identity of the following letter. When the only information supplied by the stimulus is the static trace of the production, subjects cannot predict the identity of the following one. Therefore, the visual system detects temporal differences that translate anticipatory gestures in the handwriting movements.<>},
	author = {Kandel, S. and Boe, L.-J. and Orliaguet, J.-P.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380785},
	keywords = {Psychology;Visual system;Humans;Decoding;Kinematics;Audio-visual systems;Facial animation;Speech synthesis;Speech enhancement;Acoustic signal detection},
	month = {Sep.},
	pages = {148-154},
	title = {Visual detection of coarticulatory anticipation or...guessing what has not yet been written},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380785}}

@inproceedings{380786,
	abstract = {Three alternatives to the traditional head mounted virtual reality display are described. One configuration is the Virtual Holographic Workstation, an external stereo CRT viewed by a user wearing head tracking stereo shutter glasses. Another is the Computer Augmented Reality Camcorder, where virtual objects are composited onto live video using six axis tracking information about the location of the video camera. The third system is the Virtual Portal, where an entire room is turned into a high-resolution inclusive display by replacing three walls with floor-to-ceiling rear projection stereo displays. Details of the systems and experiences and limitations in their use are discussed.<>},
	author = {Deering, M.F.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380786},
	keywords = {Virtual reality;Computer displays;Holography;Workstations;Cathode ray tubes;Glass;Augmented reality;Video equipment;Cameras;Portals},
	month = {Sep.},
	pages = {141-147},
	title = {Explorations of display interfaces for virtual reality},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380786}}

@inproceedings{380787,
	abstract = {Tracking position and orientation accurately and precisely is important in effective virtual reality systems. Large volume trackers are necessary to build effective augmented reality systems. A new approach to large-volume six-degree-of-freedom tracking is presented. The approach uses ultrasonic measurement technology and cellular deployment to achieve a relatively low-cost large-volume tracker.<>},
	author = {Sowizral, H.A. and Barnes, J.C.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380787},
	keywords = {Virtual reality;Wire;Tiles;Light emitting diodes;Assembly systems;Optical feedback;Augmented reality;Displays;Space technology;Ultrasonic variables measurement},
	month = {Sep.},
	pages = {132-139},
	title = {Tracking position and orientation in a large volume},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380787}}

@inproceedings{380788,
	abstract = {By using virtual reality technology, a communication system with realistic sensation can be implemented. This enables the user to see live images of a remote place without losing his/her sense of orientation. The most important element of this system is a buffering mechanism to overcome the time-delay problem which is harmful when trying to generate the realistic sensations of a remote place. Also, by adding 3D cues to the virtual dome (although not so accurate) 3-D sensation can be sufficiently synthesized.<>},
	author = {Hirose, M. and Yokoyama, K. and Sato, S.-I.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380788},
	keywords = {Head;Cameras;Graphics;Displays;Delay effects;Workstations;Virtual reality;Sensor systems;Silicon;Telerobotics},
	month = {Sep.},
	pages = {125-131},
	title = {Transmission of realistic sensation: Development of a virtual dome},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380788}}

@inproceedings{380789,
	abstract = {The ImageGlove is 3D input device with six degrees-of-freedom that can be used to interface with non-immersive virtual environments. It is possible to detect the position and orientation of the human hand in real time. The author's device can be used as a wireless replacement for the Polhemus position sensor. The system allows one to use the human hand as an input device to interact with 3D virtual-reality applications in a very natural way.<>},
	author = {Maggioni, C.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380789},
	keywords = {Virtual reality;Humans;Computer applications;Data gloves;Virtual environment;Image processing;Head;Computer interfaces;Application software;Optical fiber cables},
	month = {Sep.},
	pages = {118-124},
	title = {A novel gestural input device for virtual reality},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380789}}

@inproceedings{380790,
	abstract = {Nonlinear motion control techniques that allow fast and intuitive control of the viewpoint as well as the hand position within a virtual workspace are presented. They allow users to quickly move in and out of the simulation box, and, at the same time, interact with the simulation using the hand device, no matter where the viewpoint is located. The principle is to divide the working range of a physical input device into several parts and use different mapping functions to map the parameters of the device into virtual space. Providing a consistent cognitive model and maintaining smooth transitions between subspaces are the basic requirements for this type of technique. The authors' proposed techniques are natural to users working with 3D input and output devices. They do not add additional time lags to many virtual reality applications that are already tied by system response time. The application of these nonlinear motion control techniques in the Cosmic Explorer, a virtual reality visualization system in cosmology, shows that users adapt to these techniques instantly and their responses are rather positive.<>},
	author = {Song, D. and Norman, M.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380790},
	keywords = {Motion control;Navigation;Virtual reality;Layout;Data visualization;Delay;Three dimensional displays;Two dimensional displays;Computer displays;Mice},
	month = {Sep.},
	pages = {111-117},
	title = {Nonlinear interactive motion control techniques for virtual space navigation},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380790}}

@inproceedings{380791,
	abstract = {In the design of a machine which adapts to the user by considering personal differences, this paper considers the interactive relationship that both user and machine employ to try to adapt to each other. This relationship is called the 'interactive adaptation'. The interactive adaptive interface (IAI/F) is an intelligent interface which is designed under consideration of the interactive adaptation. This interface changes the characteristics of the system according to the given task, considering the states of the user, such as skill level, technique, characteristics, physical condition, etc. A design and realization method for the IAI/F which is based on recursive fuzzy reasoning is proposed. As an application example, a virtual reality simulation game is presented and the IAI/F based on the user's performance and the skin potential reflex (G.S.R.) is applied. The authors present the system, considering the interactive adaptation and show some experimental results and statistical evaluation results to discuss its effectiveness.<>},
	author = {Arai, F. and Fukuda, T. and Yamamoto, Y. and Naito, T. and Matsui, T.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380791},
	keywords = {Fuzzy reasoning;Humans;Design methodology;Virtual reality;Skin;Process design;Machine intelligence;Adaptive systems;Collaborative work;Concrete},
	month = {Sep.},
	pages = {104-110},
	title = {Interactive adaptive interface using recursive fuzzy reasoning},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380791}}

@inproceedings{380792,
	abstract = {A design methodology for task-oriented virtual world applications is proposed. An object-oriented approach is used for developing an agent (task) model, a problem domain data model, and an interaction domain model. These models are under the control of a particular component, namely the virtual world control mechanism. The introduced object hierarchy supports the construction of a design base with reusable objects, as well as the integration of further design knowledge, such as help concepts.<>},
	author = {Stary, C.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380792},
	keywords = {Object oriented modeling;Humans;Power system modeling;Virtual reality;Expert systems;Design methodology;Data models;Man machine systems;Animation;Machine intelligence},
	month = {Sep.},
	pages = {97-103},
	title = {Task-oriented design of virtual worlds},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380792}}

@inproceedings{380793,
	abstract = {Immersive virtual environments (IVEs) provide a tightly coupled human-computer interface; input to the sensory organs of the human participant are directly generated through computer displays, in the visual, auditory, tactile and haptic modalities. Some of the results of a pilot experimental study of presence in IVEs are outlined. This is a contribution to a project involved in constructing a system for architectural walkthrough, where architects and their clients are able to navigate through and effect changes to a virtual building interior. Emphasis is placed on the interface provided by the virtual environment generator (VEG) to the human user, and initially on the problem of the establishment of the presence of the human inside the virtual environment (VE).<>},
	author = {Slater, M. and Usoh, M.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380793},
	keywords = {Humans;Motion pictures;Sense organs;Computer displays;Virtual environment;Computer science;Application software;Haptic interfaces;Auditory displays;Navigation},
	month = {Sep.},
	pages = {90-96},
	title = {Presence in immersive virtual environments},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380793}}

@inproceedings{380794,
	abstract = {A research testbed developed to investigate the use of virtual environment (VE) technology for army training is described. The objectives of the testbed and the first experiments conducted using the testbed are described, in which performance data is collected as participants complete a variety of basic tasks: vision (acuity, color vision, distance estimation, and search); location (walking and flying through structures); tracking and object manipulation (placing and keeping a cursor on an object, and using it to move objects); and reaction time.<>},
	author = {Moshell, J.M. and Blau, B.S. and Knerr, B. and Lampton, D.R. and Bliss, J.P.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380794},
	keywords = {Testing;Virtual environment;Military computing;Computational modeling;Displays;Motion control;Head;Legged locomotion;Application software;Computer applications},
	month = {Sep.},
	pages = {83-89},
	title = {A research testbed for virtual environment training applications},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380794}}

@inproceedings{380795,
	abstract = {In order to enhance operator performance and understanding within remote environments, most research and development of telepresence systems has been directed towards improving the fidelity of the link between operator and environment. Although higher fidelity interfaces are important to the advancement of a telepresence system, the beneficial effects of corrupting the link between operator and remote environment by introducing abstract perceptual information into the interface called virtual fixtures are described.<>},
	author = {Rosenberg, L.B.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380795},
	keywords = {Fixtures;Telerobotics;Feedback;Protection;Humans;Problem-solving;Research and development;Virtual environment;Physics computing;Concrete},
	month = {Sep.},
	pages = {76-82},
	title = {Virtual fixtures: Perceptual tools for telerobotic manipulation},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380795}}

@inproceedings{380796,
	abstract = {The Robotic Systems Technology Branch at the NASA/Johnson Space Center has completed a baseline ground control testbed for use in developing and evaluating technology for Space Station Freedom (SSF) Robotic Tasks. The focus of the first phase of this work has been addressing the effects of significant ground-to-orbit time delays on operations. This testbed uses a predictive display to enable virtual realtime control of a remote robot. The operator commands a graphical kinematic manipulator through hand controllers or automated sequences which, in turn, drive the actual manipulator after a user-defined delay. The predictive display provides artificial camera views that enable the operator to measure clearances not available in actual camera views. A delayed camera control interface, and a robot verification display are also available. All testbed components are connected in a distributed processing environment. The ground control testbed architecture and technology utilized to address the time delays are described.<>},
	author = {Askew, R.S. and Diftler, M.A.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380796},
	keywords = {Testing;Space stations;Orbital robotics;Manipulators;Space technology;Robotics and automation;Displays;Automatic control;Robot vision systems;Cameras},
	month = {Sep.},
	pages = {69-75},
	title = {Ground control testbed for Space Station Freedom robot manipulators},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380796}}

@inproceedings{380797,
	abstract = {Virtual environments will be naturally integrated with high performance computing (HPC) since: (a) advanced grand challenge type simulations will benefit from dynamic virtual environment front-ends, and (b) advanced virtual environments will require simulation engines based on high performance computing resources. Such an integration minimally requires interoperability and standardized interfaces between both components. The discussion of these requirements is extended and it is shown how relevant design issues can be elegantly addressed by the multilashing object-oriented visual interactive environment (MOVIE) system, currently under development at North Parllel Architectures Center as an infrastructure layer for dynamic interactive HPC.<>},
	author = {Faigle, C. and Fox, G.C. and Furmanski, W. and Niemiec, J. and Simoni, D.A.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380797},
	keywords = {High performance computing;Virtual environment;Virtual reality;Computational modeling;Object oriented modeling;Motion pictures;Engines;Parallel architectures;Visualization;Educational institutions},
	month = {Sep.},
	pages = {62-68},
	title = {Integrating virtual environments with high performance computing},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380797}}

@inproceedings{380798,
	abstract = {Existing network management systems typically use a combination of textual displays and 2-D directed graph representations of network topology. A network management system is being designed that uses a virtual world presented through a 3-D stereo display and manipulated with a 3-D mouse. The goal is to allow the user to better understand and control the structure and behavior of a large, complex network. In the current prototype, the user interacts with a 3-D representation of a network whose topology and behavior are specified by a separate network emulator. The user can choose from among a set of different views of the network. For example, one view shows a selected virtual path as a series of logical links contained within a physical path. The system will serve as a testbed for the knowledge-based design of network visualizations.<>},
	author = {Feiner, S. and Zhou, M. and Crutcher, L. and Lazar, A.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380798},
	keywords = {Three dimensional displays;Mice;Cathode ray tubes;Two dimensional displays;Network topology;Visualization;User interfaces;Telecommunication network management;Complex networks;Prototypes},
	month = {Sep.},
	pages = {55-61},
	title = {A virtual world for network management},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380798}}

@inproceedings{380799,
	abstract = {When people first experience virtual reality they are bound to come away with some misconceptions as to the potential and usefulness of the technology. There is a need for a novice environment in which first-time users can become acquainted with the basic technology and its potential by means of a demonstrative application. A novice environment called the Novice Design Environment is described, in which users can perform basic interior decoration design. A design metaphor called the Heaven and Earth is used in order to customize the design. This environment has been tested with a large of novice users. An evaluation of tools for supporting novice interaction and rules of thumb for building virtual realities for novices are presented.<>},
	author = {Fairchild, K.M. and Lee, B.H. and Loo, J. and Ng, H. and Serra, L.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380799},
	keywords = {Earth;Virtual reality;Geoscience;Layout;Testing;Buildings;Space stations;Technology management;Process design;Job design},
	month = {Sep.},
	pages = {47-53},
	title = {The heaven and earth virtual reality: Designing applications for novice users},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380799}}

@inproceedings{380800,
	abstract = {The author argues an integrated family of standards must be developed, for the sake of the virtual reality (VR) industry and its customers. He suggests that this will bring stability to the marketplace, thus increasing customer acceptance and encouraging developers to provide additional applications. Techniques to produce quality standards for computer graphics are being used to improve the standards currently under development. There are problems to be overcome, for example in getting adequate staffing to produce standards in a timely manner.<>},
	author = {Shepherd, B.J.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380800},
	keywords = {Virtual reality;Standards development;ISO standards;Computer graphics;Navigation;IEC standards;Workstations;Application software;Software standards;Hardware},
	month = {Sep.},
	pages = {41-46},
	title = {Rationale and strategy for VR standards},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380800}}

@inproceedings{380801,
	abstract = {An experiment was conducted to compare and explore the relationship between the way people perceive real and virtual spaces. Twenty-four architects toured either a real museum gallery or a realtime computer generated model of the same gallery under one of three increasingly inclusive viewing conditions, i.e., looking at a monitor, viewing through stereoscopic head-mounted displays without and with head-position tracking. Subjects were asked to perform spatial dimension, orientation and evaluation tasks. The most significant results indicate that subjects consistently underestimate the dimensions of the gallery in all three computer simulation conditions when compared to touring the real gallery. The most inclusive viewing condition yields underestimates for spatial dimensions which are significantly greater than the other two simulation conditions.<>},
	author = {Henry, D. and Furness, T.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380801},
	keywords = {Space technology;Virtual environment;Computer displays;Computerized monitoring;Cameras;Computational modeling;Humans;Laboratories;Condition monitoring;Performance evaluation},
	month = {Sep.},
	pages = {33-40},
	title = {Spatial perception in virtual environments: Evaluating an architectural application},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380801}}

@inproceedings{380802,
	abstract = {When presenting virtual images to a user performing a simple task requiring depth perception, the use of stereoscopic projections results in a ten-fold reduction in mean alignment error as compared to the use of monocular projections. Although average physiological interocular distance is 6.3 cm, it is found that any interocular distance of greater than 3 cm used in the stereo projection model is adequate to provide a user with maximal performance in the depth perception task. No statistically significant increase in performance can be correlated to increasing interocular distances greater than 3 cm. Since it is often beneficial to reduce retinal disparity between the left and right images to increase the presentable depth range, reduce image fusion problems, and reduce operator fatigue, these results suggests that smaller than physiological interocular distances should be considered when implementing a stereoscopic vision system for virtual environments and telepresence systems.<>},
	author = {Rosenberg, L.B.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380802},
	keywords = {Liquid crystal displays;Virtual environment;Humans;Design optimization;Rendering (computer graphics);Machine vision;Physiology;Image fusion;Fatigue;Degradation},
	month = {Sep.},
	pages = {27-32},
	title = {The effect of interocular distance upon operator performance using stereoscopic displays to perform virtual depth tasks},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380802}}

@inproceedings{380803,
	abstract = {Work in progress on a virtual environment designed for the visualization of pre-computed fluid flows is described. The overall problems involved in the visualization of fluid flow are summarized, including computational, data management, and interface issues. Requirements for a flow visualization are summarized. Many aspects of the implementation of the virtual windtunnel have been uniquely determined by these requirements. The user interface is described in detail.<>},
	author = {Bryson, S.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380803},
	keywords = {Virtual reality;Data visualization;Fluid flow;Virtual environment;Computational fluid dynamics;Application software;NASA;Computer interfaces;Computer graphics;Aircraft},
	month = {Sep.},
	pages = {20-26},
	title = {The virtual windtunnel: A high-performance virtual reality application},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380803}}

@inproceedings{380804,
	abstract = {Some of the virtual reality (VR) and visualization efforts at Boeing's Advanced Computing Group in Huntsville Alabama are described. These labs are primarily focused on supporting aerospace and defense efforts including several NASA and DoD projects. Computer aided design (CAD) systems are playing a major role in these efforts. In order to better support and augment the current CAD design process and analysis tasks, the use of VR and visualization are being tested and used at Boeing's Huntsville facilities.<>},
	author = {Tanner, S.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380804},
	keywords = {Virtual reality;Laboratories;Moon;Space stations;Design automation;Process design;Mars;Data visualization;Radiation effects;Aerospace engineering},
	month = {Sep.},
	pages = {14-19},
	title = {The use of virtual reality at Boeing's Huntsville laboratories},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380804}}

@inproceedings{380805,
	abstract = {Distributed interactive simulation provides an environment for realistic participation in virtual worlds. Humans interact with the virtual world through interface devices such as switches and knobs, keyboards and mice, touch screens and data gloves. The time has come for the seamless integration of these physical, real-world human interface devices with the systems that generate and display the virtual environments. The merging of these two areas will result in virtual world experiences more realistic than any available today.<>},
	author = {Metzger, P.J.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380805},
	keywords = {Computational modeling;Vehicles;Virtual environment;Humans;Image generation;Layout;Space technology;Computer simulation;Computer displays;Switches},
	month = {Sep.},
	pages = {7-13},
	title = {Adding reality to the virtual},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380805}}

@inproceedings{380806,
	abstract = {Since a power network control system constitutes a large-scale computer network, intricate functions of software such as processing synchronization and data transmission among the constituent computers are required for improvement of system performance and efficiency. Methods for the support of overall large-scale system design are still scarce. Conventional text-oriented tools are insufficient to design, construct, and maintain large and complicated systems. As a means for solving this problem, an analytical method has been designed based on computer graphics visualization of static structural data and execution trace data of a software application's functional units. Prototyping tools for the development of software, enabling the direct manipulation of graphical elements through user-machine interfaces based on virtual reality technology, have been developed.<>},
	author = {Amari, H. and Nagumo, T. and Okada, M. and Hirose, M. and Ishii, T.},
	booktitle = {Proceedings of IEEE Virtual Reality Annual International Symposium},
	date-added = {2024-03-18 02:27:42 -0400},
	date-modified = {2024-03-18 02:27:42 -0400},
	doi = {10.1109/VRAIS.1993.380806},
	keywords = {Virtual reality;Application software;Data visualization;Large-scale systems;Computer networks;Control systems;Software performance;Data communication;System performance;Design methodology},
	month = {Sep.},
	pages = {1-6},
	title = {A virtual reality application for software visualization},
	year = {1993},
	bdsk-url-1 = {https://doi.org/10.1109/VRAIS.1993.380806}}
